{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Dict, List, Optional, Union, cast\n",
    "\n",
    "# ------------- #\n",
    "# Local Imports #\n",
    "# ------------- #\n",
    "\n",
    "## importing sys\n",
    "import sys\n",
    "\n",
    "## adding 00_helper_files to the system path as First Location to look\n",
    "sys.path.insert(0, '/Users/qmcbt/codeup-data-science/00_helper_files')\n",
    "## adding 03_projects Personal Work folder for current project to the system path as Second Location to look\n",
    "sys.path.insert(1, '/Users/qmcbt/codeup-data-science/03_projects/Codeup-Mirzakhani-GitHub-Scrape-NLP-Project/justin_docs')\n",
    "## adding 03_projects Root folder for current project to the system path as Third Location to look\n",
    "sys.path.insert(2, '/Users/qmcbt/codeup-data-science/03_projects/Codeup-Mirzakhani-GitHub-Scrape-NLP-Project')\n",
    "\n",
    "## env containing sensitive access credentials\n",
    "import env\n",
    "from env import github_token, github_username\n",
    "from env import user, password, host\n",
    "from env import get_db_url\n",
    "\n",
    "## Import Helper Modules\n",
    "import QMCBT_00_quicktips as qt\n",
    "import QMCBT_01_acquire as acq\n",
    "import QMCBT_02_prepare as prep\n",
    "import QMCBT_03_explore as exp\n",
    "import QMCBT_04_visualize as viz\n",
    "import QMCBT_05_model as mod\n",
    "import acquire as a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPOS = a.get_repo_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'repo': 'r-spacex/SpaceX-API',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '<p align=\"center\"><img src=\"https://live.staticflickr.com/65535/49185149122_37f5c52e43_k.jpg\"></p>\\n\\n<h1 align=\"center\">SpaceX REST API</h1>\\n\\n<h3 align=\"center\">\\nOpen Source REST API for launch, rocket, core, capsule, starlink, launchpad, and landing pad data.\\n</h3>\\n\\n<p align=\"center\">\\n<a href=\"https://github.com/r-spacex/SpaceX-API/actions?query=workflow%3ATest\"><img src=\"https://img.shields.io/github/workflow/status/r-spacex/SpaceX-API/Test?style=flat-square\"></a>\\n<a href=\"https://hub.docker.com/r/jakewmeyer/spacex-api/\"><img src=\"https://img.shields.io/docker/pulls/jakewmeyer/spacex-api?style=flat-square\"></a>\\n<a href=\"https://github.com/r-spacex/SpaceX-API/releases\"><img src=\"https://img.shields.io/github/release/r-spacex/SpaceX-API.svg?longCache=true&style=flat-square\"></a>\\n<a href=\"https://en.wikipedia.org/wiki/Representational_state_transfer\"><img src=\"https://img.shields.io/badge/interface-REST-brightgreen.svg?longCache=true&style=flat-square\"></a>\\n</p>\\n\\n<h4 align=\"center\">\\n  <i>\\n    We are not affiliated, associated, authorized, endorsed by, or in any way officially connected with Space Exploration Technologies Corp (SpaceX), or any of its subsidiaries or its affiliates. The names SpaceX as well as related names, marks, emblems and images are registered trademarks of their respective owners.\\n  </i>\\n</h4>\\n\\n<h3 align=\"center\">\\n<a href=\"docs/README.md\">Docs</a> - <a href=\"docs/clients.md\">API Clients</a> - <a href=\"docs/apps.md\">Apps</a> - <a href=\"https://status.spacexdata.com\">Status</a> - <a href=\"https://backups.spacexdata.com\">Database Exports</a>\\n<br/>\\n</h3>\\n\\n## Usage\\n\\n```\\nGET https://api.spacexdata.com/v5/launches/latest\\n```\\n\\n```json\\n{\\n  \"fairings\": null,\\n  \"links\": {\\n    \"patch\": {\\n      \"small\": \"https://images2.imgbox.com/eb/0f/Vev7xkUX_o.png\",\\n      \"large\": \"https://images2.imgbox.com/ab/79/Wyc9K7fv_o.png\"\\n    },\\n    \"reddit\": {\\n      \"campaign\": \"https://www.reddit.com/r/spacex/comments/fjf6rr/dm2_launch_campaign_thread/\",\\n      \"launch\": \"https://www.reddit.com/r/spacex/comments/glwz6n/rspacex_cctcap_demonstration_mission_2_general\",\\n      \"media\": \"https://www.reddit.com/r/spacex/comments/gp1gf5/rspacex_dm2_media_thread_photographer_contest/\",\\n      \"recovery\": \"https://www.reddit.com/r/spacex/comments/gu5gkd/cctcap_demonstration_mission_2_stage_1_recovery/\"\\n    },\\n    \"flickr\": {\\n      \"small\": [],\\n      \"original\": [\\n        \"https://live.staticflickr.com/65535/49927519643_b43c6d4c44_o.jpg\",\\n        \"https://live.staticflickr.com/65535/49927519588_8a39a3994f_o.jpg\",\\n        \"https://live.staticflickr.com/65535/49928343022_6fb33cbd9c_o.jpg\",\\n        \"https://live.staticflickr.com/65535/49934168858_cacb00d790_o.jpg\",\\n        \"https://live.staticflickr.com/65535/49934682271_fd6a31becc_o.jpg\",\\n        \"https://live.staticflickr.com/65535/49956109906_f88d815772_o.jpg\",\\n        \"https://live.staticflickr.com/65535/49956109706_cffa847208_o.jpg\",\\n        \"https://live.staticflickr.com/65535/49956109671_859b323ede_o.jpg\",\\n        \"https://live.staticflickr.com/65535/49955609618_4cca01d581_o.jpg\",\\n        \"https://live.staticflickr.com/65535/49956396622_975c116b71_o.jpg\",\\n        \"https://live.staticflickr.com/65535/49955609378_9b77e5c771_o.jpg\",\\n        \"https://live.staticflickr.com/65535/49956396262_ef41c1d9b0_o.jpg\"\\n      ]\\n    },\\n    \"presskit\": \"https://www.nasa.gov/sites/default/files/atoms/files/commercialcrew_press_kit.pdf\",\\n    \"webcast\": \"https://youtu.be/xY96v0OIcK4\",\\n    \"youtube_id\": \"xY96v0OIcK4\",\\n    \"article\": \"https://spaceflightnow.com/2020/05/30/nasa-astronauts-launch-from-us-soil-for-first-time-in-nine-years/\",\\n    \"wikipedia\": \"https://en.wikipedia.org/wiki/Crew_Dragon_Demo-2\"\\n  },\\n  \"static_fire_date_utc\": \"2020-05-22T17:39:00.000Z\",\\n  \"static_fire_date_unix\": 1590169140,\\n  \"tdb\": false,\\n  \"net\": false,\\n  \"window\": 0,\\n  \"rocket\": \"5e9d0d95eda69973a809d1ec\",\\n  \"success\": true,\\n  \"failures\": [],\\n  \"details\": \"SpaceX will launch the second demonstration mission of its Crew Dragon vehicle as part of NASA\\'s Commercial Crew Transportation Capability Program (CCtCap), carrying two NASA astronauts to the International Space Station. Barring unexpected developments, this mission will be the first crewed flight to launch from the United States since the end of the Space Shuttle program in 2011. DM-2 demonstrates the Falcon 9 and Crew Dragon\\'s ability to safely transport crew to the space station and back to Earth and it is the last major milestone for certification of Crew Dragon. Initially the mission duration was planned to be no longer than two weeks, however NASA has been considering an extension to as much as six weeks or three months. The astronauts have been undergoing additional training for the possible longer mission.\",\\n  \"crew\": [\\n    \"5ebf1b7323a9a60006e03a7b\",\\n    \"5ebf1a6e23a9a60006e03a7a\"\\n  ],\\n  \"ships\": [\\n    \"5ea6ed30080df4000697c913\",\\n    \"5ea6ed2f080df4000697c90b\",\\n    \"5ea6ed2f080df4000697c90c\",\\n    \"5ea6ed2e080df4000697c909\",\\n    \"5ea6ed2f080df4000697c90d\"\\n  ],\\n  \"capsules\": [\\n    \"5e9e2c5df359188aba3b2676\"\\n  ],\\n  \"payloads\": [\\n    \"5eb0e4d1b6c3bb0006eeb257\"\\n  ],\\n  \"launchpad\": \"5e9e4502f509094188566f88\",\\n  \"auto_update\": true,\\n  \"flight_number\": 94,\\n  \"name\": \"CCtCap Demo Mission 2\",\\n  \"date_utc\": \"2020-05-30T19:22:00.000Z\",\\n  \"date_unix\": 1590866520,\\n  \"date_local\": \"2020-05-30T15:22:00-04:00\",\\n  \"date_precision\": \"hour\",\\n  \"upcoming\": false,\\n  \"cores\": [\\n    {\\n      \"core\": \"5e9e28a7f3591817f23b2663\",\\n      \"flight\": 1,\\n      \"gridfins\": true,\\n      \"legs\": true,\\n      \"reused\": false,\\n      \"landing_attempt\": true,\\n      \"landing_success\": true,\\n      \"landing_type\": \"ASDS\",\\n      \"landpad\": \"5e9e3032383ecb6bb234e7ca\"\\n    }\\n  ],\\n  \"id\": \"5eb87d46ffd86e000604b388\"\\n}\\n```\\n\\n## Cron Job Status\\n\\n<p align=\"left\">\\n<img src=\"https://healthchecks.io/badge/a99e6369-9795-417e-9a1c-31ea91/zDDqPvw1-2/Capsules.svg\">\\n<br/>\\n<img src=\"https://healthchecks.io/badge/a99e6369-9795-417e-9a1c-31ea91/iJIWcg9u-2/Cores.svg\">\\n<br/>\\n<img src=\"https://healthchecks.io/badge/a99e6369-9795-417e-9a1c-31ea91/soA1Z2t1-2/Landpads.svg\">\\n<br/>\\n<img src=\"https://healthchecks.io/badge/a99e6369-9795-417e-9a1c-31ea91/tc7aK4Iw-2/Launchpads.svg\">\\n<br/>\\n<img src=\"https://healthchecks.io/badge/a99e6369-9795-417e-9a1c-31ea91/uB7PIyUo-2/Past-Launches.svg\">\\n<br/>\\n<img src=\"https://healthchecks.io/badge/a99e6369-9795-417e-9a1c-31ea91/bQw1ZrmZ-2/Payloads.svg\">\\n<br/>\\n<img src=\"https://healthchecks.io/badge/a99e6369-9795-417e-9a1c-31ea91/HhIoHcF6-2/Roadster.svg\">\\n<br/>\\n<img src=\"https://healthchecks.io/badge/a99e6369-9795-417e-9a1c-31ea91/wPz7gFQJ-2/Starlink.svg\">\\n<br/>\\n<img src=\"https://healthchecks.io/badge/a99e6369-9795-417e-9a1c-31ea91/3L5HxZKX-2/Upcoming-Launches.svg\">\\n<br/>\\n<img src=\"https://healthchecks.io/badge/a99e6369-9795-417e-9a1c-31ea91/YvnZYkED-2/Webcast.svg\">\\n<br/>\\n<img src=\"https://healthchecks.io/badge/a99e6369-9795-417e-9a1c-31ea91/_Z-lNpev-2/launch-library-v2.svg\">\\n</p>\\n\\n## Sponsors\\n\\n### [Studio 3T](https://studio3t.com/)\\n\\n[![Studio 3T](https://imgur.com/DbJSfAo.png)](https://studio3t.com/)\\n\\n## FAQ\\'s\\n\\n* If you have any questions or corrections, please open an issue and we\\'ll get it merged ASAP\\n* For any other questions or concerns, just shoot me an email.\\n'},\n",
       " {'repo': 'KeenSoftwareHouse/SpaceEngineers',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '\\nARCHIVED VERSION OF Space Engineers. This version is not up-to-date version of the game.\\n\\nIf you find some issue with the game please report on https://support.keenswh.com/.\\n\\nSpace Engineers\\n===============\\n\\nWelcome to the Space Engineers source code! \\n\\nFrom this repository you can build Space Engineers. To play Space Engineers you need to own the game on Steam. Assets (audio, models, textures) are not included in this repository.\\n\\nBefore downloading the source code please read the EULA carefully - [End User License Agreement](https://github.com/KeenSoftwareHouse/SpaceEngineers/blob/master/EULA.txt).\\n\\nSee the [Change log](https://github.com/KeenSoftwareHouse/SpaceEngineers/wiki/Change-log) for latest changes.   \\nHave you found a problem related to the source code? Report an [Issue](https://github.com/KeenSoftwareHouse/SpaceEngineers/issues).   \\nDiscuss source code on our [source code sub-forum](http://forum.keenswh.com/forums/source-code.423135/).\\n\\nPrerequisities\\n--------------\\n- [Visual Studio 2013 Community Edition with Update 4] (https://www.visualstudio.com/en-us/downloads/download-visual-studio-vs#d-community) or different version of VS2013 with Update 4\\n- Steam Client + Space Engineers game (to run and test the game)\\n\\nQuickstart\\n----------\\nSpace Engineers must be installed on your computer, Steam must be running.\\n\\n- [Clone](github-windows://openRepo/https://github.com/KeenSoftwareHouse/SpaceEngineers) or [download](https://github.com/KeenSoftwareHouse/SpaceEngineers/archive/master.zip) and unpack the repository.\\n- Open **SpaceEngineers.sln** in Visual Studio.\\n- Open file **global.props** (it\\'s in configuration folder).\\n- Make sure **ContentPath** tag contains path to SpaceEngineers **Content** directory in Steam folder.\\n- Start debugging by pressing **F5** or select **Debug** - **Start Debugging** in main menu\\n\\nInstead of modifying **global.props**, you can create **user.props**, more information [here](https://github.com/KeenSoftwareHouse/SpaceEngineers/wiki/Initial-setup).\\n\\nHow to contribute\\n-----------------\\n\\nOne way to contribute changes is to send a GitHub [Pull Request](https://help.github.com/articles/using-pull-requests).\\n\\n**To get started using GitHub:**\\n\\n- Create your own Space Engineers **fork** by clicking the __Fork button__ in the top right of this page.\\n- [Install a Git client](http://help.github.com/articles/set-up-git) on your computer.\\n- Use the GitHub program to **Sync** the project\\'s files to a folder on your computer.\\n- Open up **SpaceEngineers.sln** in Visual Studio.\\n- Modify the source codes and test your changes.\\n- Using the GitHub program, you can easily **submit contributions** back up to your **fork**.\\n- Do not **commit to master**, for each feature **create new branch**.\\n- When you\\'re ready to send the changes to the Keen Software House for review, simply create a [Pull Request](https://help.github.com/articles/using-pull-requests).\\n- Following [Coding rules](https://github.com/KeenSoftwareHouse/SpaceEngineers/wiki/Coding-rules) will help us lower the time needed to process the PR and merge it thus increase the amount of PRs that can be merged\\n\\n**Advanced topics:**\\n- You can update your master branch by executing:\\n  - git pull https://github.com/KeenSoftwareHouse/SpaceEngineers.git master\\n- If your master is tainted and any branch you make contains junk, you can do **hard reset**. All unmerged commits on master branch will be lost.\\n  - git checkout master\\n  - git fetch https://github.com/KeenSoftwareHouse/SpaceEngineers.git master\\n  - git reset --hard FETCH_HEAD\\n  - git push --force origin master\\n\\nCommon issues\\n-------------\\n**Build error: The command \"..\\\\3rd\\\\Utils\\\\RunTemplate.bat \"....\\\\MyEnumToStringsGenerated\"\" exited with code 1.**\\nThis is common when using old versions of Visual Studio, see [Visual Studio support](https://github.com/KeenSoftwareHouse/SpaceEngineers/wiki/Visual-Studio-support). It can also happen when  TextTemplating.exe was not found for some reason (it should be installed with Visual Studio).\\n\\n**Assert: unable to find audio/model/texture file: \\'xxxxxx\\'**.\\nThis happens because repository is slightly ahead of content in Steam folder. Definitions (Content/Data) are taken from repository and may contain new definitions referencing assets which are not yet in Steam content folder. We decided to use definitions from repository by default, so you can easily modify it. You can edit **global.props** to use definitions from Steam (that should fix the issue). When running on **Release** asserts won\\'t be shown; missing assets won\\'t crash the game.  More info [here](https://github.com/KeenSoftwareHouse/SpaceEngineers/wiki/Initial-setup#setting-path-to-the-games-content).\\n\\nWhere is the 64-bit version?\\n------------------------\\n\\nWe\\'re unable to provide a 64-bit version of all 3rd party libraries because of licensing. We\\'re working on this and trying to negotiate a better license which will allow us to do that.\\n'},\n",
       " {'repo': 'a1studmuffin/SpaceshipGenerator',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Spaceship Generator\\n\\nA Blender script to procedurally generate 3D spaceships from a random seed.\\n\\n![Spaceship screenshots](https://raw.githubusercontent.com/a1studmuffin/SpaceshipGenerator/master/screenshots/spaceships_grid.jpg)\\n\\nUsage\\n-----\\n* Install Blender 2.80 or greater: http://blender.org/download/\\n* Download newest `add_mesh_SpaceshipGenerator.zip` from the [Releases](https://github.com/a1studmuffin/SpaceshipGenerator/releases) section\\n* Under Edit > Preferences... > Add-ons > Install... open the downloaded ZIP file\\n* Under Edit > Preferences... > Add-ons enable the \"Add Mesh: Spaceship Generator\" script (search for \"spaceship\")\\n* Add a spaceship in the 3D View under Add > Mesh > Spaceship\\n* Expand the Spaceship tab that appears in the bottom left of the viewport to adjust procedural generation settings\\n\\nHow it works\\n------------\\n\\n![Step-by-step animation](https://raw.githubusercontent.com/a1studmuffin/SpaceshipGenerator/master/screenshots/step-by-step-animation.gif)\\n\\nWatch on YouTube: https://www.youtube.com/watch?v=xJZyXqJ6nog\\n\\n* Start with a box.\\n* Build the hull: Extrude the front/rear faces several times, adding random translation/scaling/rotation along the way.\\n* Add asymmetry to the hull: Pick random faces and extrude them out in a similar manner, reducing in scale each time.\\n* Add detail to the hull: Categorize each face by its orientation and generate details on it such as engines, antenna, weapon turrets, lights etc.\\n* Sometimes apply horizontal symmetry.\\n* Add a Bevel modifier to angularize the shape a bit.\\n* Apply materials to the final result.\\n* Take over the universe with your new infinite fleet of spaceships.\\n\\nExtreme examples\\n----------------\\nThe following screenshots were created using extreme values for the number of hull segments and asymmetry segments to show how the algorithm works.\\n\\n![Extreme spaceship screenshots](https://raw.githubusercontent.com/a1studmuffin/SpaceshipGenerator/master/screenshots/extreme_examples.jpg)\\n\\nTips and Tricks\\n---------------\\n* By default the script will delete all objects starting with `Spaceship` before generating a new spaceship. To disable this feature, remove or comment out the call to `reset_scene()` around line 735 in the main function.\\n* You can provide a seed to the `generate_spaceship()` function to always generate the same spaceship. For example, `generate_spaceship(\\'michael\\')`.\\n* The `generate_spaceship()` function takes many more parameters that affect the generation process. Try playing with them!\\n* You can replace the textures with your own ones. All textures are applied using global-space cube UVs. `hull_normal.png` is a normal map that adds extra surface \"greebles\". `hull_lights_diffuse.png` is an additive diffuse texture to set the color of the window lights. `hull_lights_emit.png` is an emissive texture to make the windows glow in darkness.\\n\\nCredits\\n-------\\nWritten for fun as part of the [/r/proceduralgeneration](https://www.reddit.com/r/proceduralgeneration/) June 2016 [monthly challenge](https://www.reddit.com/r/proceduralgeneration/comments/4mn9gj/monthly_challenge_7_june_2016_procedural/).\\n\\nReleased under the [MIT License].\\n\\nAuthored and maintained by Michael Davies.\\n\\n> GitHub [@a1studmuffin](https://github.com/a1studmuffin)\\n> Twitter [@butterparty](https://twitter.com/butterparty)\\n\\nSpecial thanks to [@panzi](https://github.com/panzi) for bugfixes, a proper GUI and build script. Also to [@mjrthemes](https://github.com/mjrthemes) for bugfixing, and [@LendoK](https://github.com/LendoK) for the 2.80 port.\\n\\n[MIT License]: http://mit-license.org/\\n'},\n",
       " {'repo': 'SublimeText/Spacegray',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Spacegray\\n\\nA set of custom UI themes for Sublime Text 2/3. It\\'s all about hype and minimal. Comes in different flavors with accompanying [Base16](https://github.com/chriskempson/base16) color schemes.\\n\\n***\\n\\n### Themes\\n\\n#### Spacegray\\n\\nDefault flavor based on Base16 Ocean Dark color scheme.\\n\\n![image](screenshots/spacegray.png)\\n\\n#### Spacegray Light\\n\\nLight variation based on Base16 Ocean Light color scheme.\\n\\n![image](screenshots/spacegray-light.png)\\n\\n#### Spacegray Eighties\\n\\nA variation based on Base16 Eighties Dark color scheme.\\n\\n![image](screenshots/spacegray-eighties.png)\\n\\n*The font used in the screenshots is [__Source Code Pro__](https://github.com/adobe-fonts/source-code-pro).*\\n\\n***\\n\\n### How to Install\\n\\n#### Via Package Control\\n\\nThe easiest way to install is using [Sublime Package Control](https://sublime.wbond.net), where Spacegray is listed as `Theme - Spacegray`.\\n\\n1. Open Command Palette using menu item `Tools -> Command Palette...` (<kbd>⇧</kbd><kbd>⌘</kbd><kbd>P</kbd> on Mac)\\n2. Choose `Package Control: Install Package`\\n3. Find `Theme - Spacegray` and hit <kbd>Enter</kbd>\\n\\n#### Manual\\n\\nYou can also install the theme manually:\\n\\n1. [Download the .zip](https://github.com/kkga/spacegray/archive/master.zip)\\n2. Unzip and rename the folder to `Theme - Spacegray`\\n3. Copy the folder into `Packages` directory, which you can find using the menu item `Sublime Text -> Preferences -> Browse Packages...`\\n\\n***\\n\\n### How to Activate\\n\\nActivate the UI theme and color scheme by modifying your user preferences file, which you can find using the menu item `Sublime Text -> Preferences -> Settings - User` (<kbd>⌘</kbd><kbd>,</kbd> on Mac).\\n\\nYou can choose whichever flavor you like, but don\\'t forget to change *both* color scheme and UI theme so they match.\\n\\n***Note: Don\\'t forget to restart Sublime Text after activating the theme.***\\n\\n#### Settings for Spacegray\\n\\n```json\\n{\\n  \"theme\": \"Spacegray.sublime-theme\",\\n  \"color_scheme\": \"Packages/Theme - Spacegray/base16-ocean.dark.tmTheme\"\\n}\\n```\\n\\n#### Settings for Spacegray Light\\n\\n```json\\n{\\n  \"theme\": \"Spacegray Light.sublime-theme\",\\n  \"color_scheme\": \"Packages/Theme - Spacegray/base16-ocean.light.tmTheme\"\\n}\\n```\\n\\n#### Settings for Spacegray Eighties\\n\\n```json\\n{\\n  \"theme\": \"Spacegray Eighties.sublime-theme\",\\n  \"color_scheme\": \"Packages/Theme - Spacegray/base16-eighties.dark.tmTheme\"\\n}\\n```\\n\\n***\\n\\n### Settings\\n\\n#### Tab labels font size\\n\\nCopy and paste one of four options in your user preferences file:\\n\\n```json\\n  \"spacegray_tabs_font_small\": true\\n```\\n```json\\n  \"spacegray_tabs_font_normal\": true\\n```\\n```json\\n  \"spacegray_tabs_font_large\": true\\n```\\n```json\\n  \"spacegray_tabs_font_xlarge\": true\\n```\\n\\n#### Tabs size\\n\\nTabs height:\\n\\n```json\\n  \"spacegray_tabs_small\": true\\n```\\n```json\\n  \"spacegray_tabs_normal\": true\\n```\\n```json\\n  \"spacegray_tabs_large\": true\\n```\\n```json\\n  \"spacegray_tabs_xlarge\": true\\n```\\n\\nTabs width:\\n\\n```json\\n  \"spacegray_tabs_auto_width\": true\\n```\\n\\n#### Sidebar labels font size\\n\\n```json\\n  \"spacegray_sidebar_font_small\": true\\n```\\n```json\\n  \"spacegray_sidebar_font_normal\": true\\n```\\n```json\\n  \"spacegray_sidebar_font_large\": true\\n```\\n```json\\n  \"spacegray_sidebar_font_xlarge\": true\\n```\\n\\n#### Sidebar tree rows height\\n\\n```json\\n  \"spacegray_sidebar_tree_xsmall\": true\\n```\\n```json\\n  \"spacegray_sidebar_tree_small\": true\\n```\\n```json\\n  \"spacegray_sidebar_tree_normal\": true\\n```\\n```json\\n  \"spacegray_sidebar_tree_large\": true\\n```\\n```json\\n  \"spacegray_sidebar_tree_xlarge\": true\\n```\\n\\n#### Hide navigation icons in Sublime Text 3\\n\\n```json\\n  \"enable_tab_scrolling\": false,\\n```\\n\\n#### Enable sidebar fileicons (only works in Sublime Text 3)\\n\\n```json\\n  \"spacegray_fileicons\": true,\\n```\\n\\n#### Disable custom OSX Title Bar in Sublime Text 3\\n\\nUseful if you\\'re using native OSX tabs since the custom Title Bar breaks the native tabs\\n\\n```json\\n  \"disable_custom_title_bar\": true,\\n```\\n\\n***\\n\\n### Thanks\\n\\nThe Spacegray Eighties variation is contributed by [Yoshua Wuyts](https://github.com/yoshuawuyts).\\n'},\n",
       " {'repo': 'sar-gupta/space',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# S P A C E\\n\\n![cover](resources/space-readme.jpg)\\n\\nChat application for developers\\n---\\n\\nIf you want to chat up, join the room `test` on the app.\\n\\nThere is a known bug in firebase; it doesn\\'t have permission to read certain users\\' display name. I\\'ve stored their email ID instead of the name.\\n\\n\\n## Website\\nspacedev.herokuapp.com\\n---\\nAlthough this is a desktop application made with electron, it runs on the web as well.\\n\\n\\n## About\\nA real-time chat application made using `React`, `Redux`, `Electron` and `Firebase`.\\n---\\nRight now, it\\'s just a barebones chat app, but developer-friendly features will be arriving soon.\\n\\n\\n## Running locally for direct usage\\nYou can download the executables from the following links:\\n\\n[Linux](https://drive.google.com/open?id=18_2hv8h_9CdxnqeN-TTjSUTct_3QQCmy)\\n\\n[Windows](https://drive.google.com/open?id=1B_2jTv1Ih6HKrBmihSNORK_JqrVKyhyH)\\n\\n[Mac](https://drive.google.com/open?id=18_2hv8h_9CdxnqeN-TTjSUTct_3QQCmy)\\n\\n\\n(I haven\\'t been able to test the app on mac yet, so if there\\'s any problem,feel free to open an issue).\\n\\n\\n## Contributing\\n**This is a good first project to contribute to if you\\'ve recently learned react and redux, or want to learn the same.**\\n---\\nIf you\\'re well-versed with these frameworks, you can help by improving the current app, or adding new features!\\n---\\n\\nIf you want to get into the code and start contributing, you need to do a little bit of setup for your machine:\\n\\n### Setting up firebase\\n\\nFirst, download/clone this repository and `cd` into it:\\n```\\ngit clone git@github.com:sar-gupta/space.git\\ncd space\\n```\\n\\nCreate a new project in firebase by heading over to console.firebase.google.com \\n\\nHead over to the authentication tab, click on `Sign in method` and select Github. Click on `enable`. Copy the authorization calback URL that\\'s provided to you right there.\\n\\nAfter this, you need to register the app with github. Head over to https://github.com/settings/applications/new and fill in details. App homepage can be literally any valid URL, it doesn\\'t matter as long as the URL is valid. Here, in the authorization callback URL, paste the URL that you copied from firebase to your clipboard. Click on `Register Application`.\\n\\nYou\\'ll be redirected to a page that has a client ID and a client secret. Copy those and paste them where they are required in firebase, and click on `Save`. Github authentication should now be enabled.\\n\\n\\nNow, localhost should be an authorized domain by default, but if it isn\\'t there, just click on `Add domain`and enter `localhost`.\\n\\nThen, go to the `database` tab, click on `real-time database` and select `Start in test mode`. Go to `Rules`, they should look like this: \\n```\\n{\\n  \"rules\": {\\n    \".read\": \"auth!=null\",\\n    \".write\": \"auth!=null\"\\n  }\\n}\\n```\\nNow go the `Project Overview` and click on `Add firebase to web app`.\\nYou\\'ll be promted with a screen that has a `config` object like this:\\n\\n![config oject](/resources/config-object.jpg)\\n\\nNow, back in your code editor, create a new file `.env.development` in the root of the project, and enter the following contents in this file: \\n```\\nFIREBASE_API_KEY=\\nFIREBASE_AUTH_DOMAIN=\\nFIREBASE_DATABASE_URL=\\nFIREBASE_PROJECT_ID=\\nFIREBASE_STORAGE_BUCKET=\\nFIREBASE_MESSAGING_SENDER_ID=\\n```\\nEnter the values from the firebase config object here (**without the double quotes**), and save the file.\\n\\n### Running locally for development\\nTo run the electron app, first build locally to create the `public/dist/` folder. If you just want to run it on the web, then this isn\\'t required since webpack dev server can serve from memory without physically creating the `public/dist/` folder.\\n```\\nyarn run build:dev\\n```\\nor\\n```\\nyarn run build:prod\\n```\\n\\nTo start the desktop application, run the following commands from the root directory of the project:\\n```\\nyarn\\nyarn run electron\\n```\\nTo create various executables or packaged files for the desktop application, run any of the following depending on your target platform:\\n```\\nyarn run package-linux\\nyarn run package-win\\nyarn run package-mac\\n```\\nIt will put the output in the `release-builds/` directory inside the root of the project.\\n\\nNOTE: If you\\'re building for windows in a non-windows environment, you need to have `wine` installed.\\n\\n\\nIf you want to run just the web version, run the following commands: \\n```\\nyarn\\nyarn run dev-server\\n```\\nThe app will be live on localhost:4172\\n\\nAnd you\\'re good to go!\\n---\\n\\nYou can view the database contents in your firebase project\\'s database tab.\\n\\n\\nIf you want to contribute to this project (solve a bug or implement a new feature), feel free to create an issue and/or submit a pull request.\\n---\\n\\n### NOTE\\nI recently learned React, Redux, Electron and Firebase, and this is my first project using these technologies/frameworks. So, you\\'ll find some instances where performance of the application can be improved. Feel free to open an issue/submit pull requests if you find any such instance :)\\n\\n## Usage\\nAfter logging in with github, you can create new rooms or join existing ones. I\\'ve only allowed unique room names for a better user experience.\\n\\nNOTE: Number of unread messages don\\'t go to 0 when you click on the chat name (that\\'s intentional). It goes to 0 if you send a message to that room, **or** if you click on the **number** of unread messages.\\n\\n## Author\\n[Sarthak Gupta](https://www.github.com/sar-gupta)\\n\\n## Article\\nI also wrote an article about this project. Feel free to read it to get a general idea. It\\'ll also be helpful if you want to start contributing to this app!\\n\\nIt has been published on [Hackernoon](https://hackernoon.com/).\\n\\nYou can read it here:\\n\\nhttps://hackernoon.com/https-medium-com-sargupta-how-i-built-a-real-time-chat-app-using-react-and-firebase-dc8690bf41f7\\n\\n'},\n",
       " {'repo': 'long1eu/SpaceTabLayout',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '# Not actively maintained, but PRs are welcomed\\n\\n# Synopsis\\nThis is a custom implementation of a RelativeLayout that you can use along with a ViewPager to navigate between fragments.\\n\\n![alt text](/readmeSource/3.gif \"Logo Title Text 1\")  ![alt text](/readmeSource/4.gif \"Logo Title Text 1\")  ![alt text](/readmeSource/5.gif \"Logo Title Text 1\")\\n\\n\\n# Code usage\\nYou can use this properties in xml for convenience. You can set the number of tabs you want to use and the starting tab position.\\n```xml\\n        app:number_of_tabs=\"five\"\\n        app:starting_position=\"three\"\\n        \\n        app:tab_color=\"@color/colorPrimary\"\\n        app:button_color=\"@color/colorAccent\"\\n        app:text_color=\"#61FFFFFF\"\\n        \\n        app:icon_one=\"@drawable/ic_hotel_black_24dp\"\\n        app:icon_two=\"@drawable/design_ic_visibility\"\\n        app:icon_three=\"@drawable/ic_content_cut_black_24dp\"\\n        app:icon_four=\"@drawable/ic_check_black_24dp\"\\n        app:icon_five=\"@drawable/ic_fingerprint_black_24dp\"\\n         \\n        app:text_one=\"Action\"\\n        app:text_two=\"Action\"\\n        app:text_three=\"Action\"\\n```\\n\\n\\nYou can also use the fallowing methods to customize its appearance and behavior:\\n\\n| Method                                                    | Description                                                                                                 |\\n| ----------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|\\n| getTabLayout();                                           | Gives you the base RelativeLayout.                                                                          |\\n| setTabColor(@ColorInt int backgroundColor);               | Change the color of the Tab.                                                                                |\\n| getCurrentPosition();                                     | Gives you the current tab position.                                                                         |\\n| getButton();                                              | Give you the Floating Action Button View.                                                                   |\\n| setButtonColor(@ColorInt int backgroundColor);            | Change the color of the Floating Button.                                                                    |\\n| setOnClickListener(View.OnClickListener l);               | Set the same OnClickListener to all positions.                                                              |\\n| setTabOneOnClickListener(View.OnClickListener ll);        | Set the OnClickListener for the first position.                                                             |\\n| setTabTwoOnClickListener(View.OnClickListener cl);        | Set the OnClickListener for the second position.                                                            |\\n| getTabOneView();                                          | Get you the first base LinearLayout that contains an ImageView or an ImageView and a TextView.              |\\n| setTabOneView(View tabOneView);                           | You can set the base layout for the first position.                                                         |\\n| getTabTwoView();                                          | Get you the second base LinearLayout that contains an ImageView or an ImageView and a TextView.             |\\n| setTabTwoView(View centerView);                           | You can set the base layout for the second position.                                                        |\\n| setTabOneIcon(@DrawableRes int tabOneIcon);               | Set the icon for both the first tab and the Action Button when in the first position.                       |\\n| setTabTwoIcon(@DrawableRes int tabTwoIcon);               | Set the icon for both the second tab and the Action Button when in the second position.                     |\\n\\n# Motivation\\nI get the inspiration from the Google Space app that uses a center Floating Action Button. I liked the idea so much that I though that it would be so nice if we could have this like a TabLayout.\\n\\nYou can see in the image:\\n\\n![alt text](/readmeSource/motivation.png \"Logo Title Text 1\")\\n\\n# Installation\\n### Declaring dependencies\\nAdd this to dependencies brackets in the gradle.build file:\\n```\\ncompile \\'eu.long1:spacetablayout:1.0.4\\'\\n```\\n\\n### XML implementation\\nIn your layout just include this. You can customize the Tab here with text, icons, colors...\\n```xml\\n <eu.long1.spacetablayout.SpaceTabLayout\\n        android:id=\"@+id/spaceTabLayout\"\\n        android:layout_width=\"wrap_content\"\\n        android:layout_height=\"wrap_content\" \\n        app:number_of_tabs=\"five\"\\n        app:starting_position=\"three\"\\n        app:tab_color=\"@color/colorPrimary\"\\n        app:button_color=\"@color/colorAccent\"\\n        app:text_color=\"#61FFFFFF\"\\n        app:icon_one=\"@drawable/ic_hotel_black_24dp\"\\n        app:icon_two=\"@drawable/design_ic_visibility\"\\n        app:icon_three=\"@drawable/ic_content_cut_black_24dp\"\\n        app:icon_four=\"@drawable/ic_check_black_24dp\"\\n        app:icon_five=\"@drawable/ic_fingerprint_black_24dp\"/>\\n```\\n#### SnackBar behavior\\nIf you are using a CoordinatorLayout as the root of your layout you can add this line to proper handle the behavior when a SnackBar is visible. \\n```xml\\n app:layout_behavior=\"eu.long1.spacetablayout.SpaceTabLayoutBehavior\" \\n```\\n#### XML Example\\n```xml\\n<android.support.design.widget.CoordinatorLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\\n    xmlns:app=\"http://schemas.android.com/apk/res-auto\"\\n    android:id=\"@+id/activity_main\"\\n    android:layout_width=\"match_parent\"\\n    android:layout_height=\"match_parent\">\\n\\n    <android.support.v4.view.ViewPager\\n        android:id=\"@+id/viewPager\"\\n        android:layout_width=\"match_parent\"\\n        android:layout_height=\"wrap_content\"\\n        android:layout_marginBottom=\"56dp\" />\\n\\n    <eu.long1.spacetablayout.SpaceTabLayout\\n        android:id=\"@+id/spaceTabLayout\"\\n        android:layout_width=\"wrap_content\"\\n        android:layout_height=\"wrap_content\"\\n        app:layout_behavior=\"eu.long1.spacetablayout.SpaceTabLayoutBehavior\"\\n        app:number_of_tabs=\"five\"\\n        app:starting_position=\"three\" />\\n\\n</android.support.design.widget.CoordinatorLayout>\\n```\\n### Code implementation\\nIn your MainActivity.java you need to initialize the SpaceTabLayout like this:\\n\\n```java\\n    SpaceTabLayout tabLayout;\\n    \\n        @Override\\n        protected void onCreate(Bundle savedInstanceState) {\\n            super.onCreate(savedInstanceState);\\n            setContentView(R.layout.activity_main);\\n    \\n            //add the fragments you want to display in a List\\n            List<Fragment> fragmentList = new ArrayList<>();\\n            fragmentList.add(new FragmentA());\\n            fragmentList.add(new FragmentB());\\n            fragmentList.add(new FragmentC());\\n            \\n            ViewPager viewPager = (ViewPager) findViewById(R.id.viewPager);\\n            tabLayout = (SpaceTabLayout) findViewById(R.id.spaceTabLayout);\\n            \\n            //we need the savedInstanceState to get the position\\n            tabLayout.initialize(viewPager, getSupportFragmentManager(), \\n                            fragmentList, savedInstanceState);\\n        }\\n    \\n    \\n        //we need the outState to save the position\\n        @Override\\n        protected void onSaveInstanceState(Bundle outState) {\\n            tabLayout.saveState(outState);\\n            super.onSaveInstanceState(outState);\\n        }\\n```\\n# Issues\\n\\nIf you have an issues with this library, please open a issue here: https://github.com/thelong1EU/SpaceTabLayout/issues and provide enough information to reproduce it. The following information needs to be provided:\\n\\n1. Which version of the SDK are you using?\\n* What device are you using?\\n* What steps will reproduce the problem?\\n* Relevant logcat output.\\n* Optional: Any screenshot(s) that demonstrate the issue.\\n\\n\\n# License\\n    Copyright (c) 2016 Lung Răzvan\\n\\n    Licensed under the Apache License, Version 2.0 (the \"License\");\\n    you may not use this file except in compliance with the License.\\n    You may obtain a copy of the License at\\n\\n       http://www.apache.org/licenses/LICENSE-2.0\\n\\n    Unless required by applicable law or agreed to in writing, software\\n    distributed under the License is distributed on an \"AS IS\" BASIS,\\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n    See the License for the specific language governing permissions and\\n    limitations under the License.\\n'},\n",
       " {'repo': 'Unity-Technologies/SpaceshipDemo',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# Spaceship Demo\\n\\n![](https://blogs.unity3d.com/wp-content/uploads/2019/08/image10.png)\\n\\nSpaceship Demo is a AAA Playable First person demo showcasing effects made with Visual Effect Graph and rendered with High Definition Render Pipeline.\\n\\n**[DOWNLOAD LATEST RELEASE HERE](https://github.com/Unity-Technologies/SpaceshipDemo/releases/latest)**\\n\\nFor update information, see the [Changelog](https://github.com/Unity-Technologies/SpaceshipDemo/blob/master/CHANGELOG.md).\\n\\n## Requirements\\n\\nIn order to download and run the latest Spaceship demo project, make sure you have the following\\n* [Github Desktop](https://desktop.github.com/) or [Git For Windows](https://git-scm.com/download/win) + [Git LFS](https://git-lfs.github.com/) (Required for Cloning the Repository) or any other git client.\\n* Unity 2021.2.1f1 or newer (See each release notes in [changelog](https://github.com/Unity-Technologies/SpaceshipDemo/blob/master/CHANGELOG.md) for version requirements)\\n\\n## How to Download/Install\\n\\n### Method 1 : Clone the repository\\n\\n**Important Note**: This repository uses **Git-LFS** to store large files. In order to get the data correctly you need to [install Git-LFS](https://git-lfs.github.com/) before starting to clone the repository. **Do NOT use the Download ZIP button as it will not get the LFS files correctly**\\n\\n#### Using Github Desktop\\n\\nIf you have [Github Desktop](https://desktop.github.com/) installed :  use the **Clone or Download** green button, then select **Open in Desktop**.\\n\\n#### Using Git Command Line (or another Git Client)\\n\\nYou can clone this repository and start opening directly the project using the following command : `git clone https://github.com/Unity-Technologies/SpaceshipDemo.git`\\n\\n### Method 2 : Download in Releases page\\n\\nYou can also download project archives in the [Releases](https://github.com/Unity-Technologies/SpaceshipDemo/releases) tab. These zip files contains the full project for a one-time download without Git. \\n'},\n",
       " {'repo': 'StrykerKKD/SpaceInvaders',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': 'SpaceInvaders\\n=============\\nThis is a remake of the space invader phaser example, which you can find here:\\nhttp://examples.phaser.io/\\n\\nThis remake is made with require.js, which breaks up the code into modules.\\nModules are in assets/javascript/module\\n\\nCode is more organized thanks to State and Statemanager class from Phaser.\\nYou can find the states in assets/javascript/state\\n\\nI used Phaser 2.0.1(no Physics) from the Dev branch.\\nThe dev branch has a lot of bug fixes so it\\'s recommended to use it.\\n\\nThanks to require.js, i made an optimized version of my game,\\nwhich can be viewed with indexOpt.html. This use the \"compiled\" code, which can be found in assets/javascript/built\\n\\nKnown issue: In every new play state(after the end state) the game makes new DOM nodes.\\nThe cause: in every cycle i make a new text to show the score.\\nI tried to destroy the texts but i never succeed.\\n\\nYou can play with it here: http://strykerkkd.github.io/SpaceInvaders/\\n'},\n",
       " {'repo': 'tasdikrahman/spaceShooter',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '## Space Shooter\\n\\nThe classic retro game recreated using `Pygame` and `python`.\\n\\n<a href=\"https://news.ycombinator.com/item?id=10925168\"><img src=\"https://raw.githubusercontent.com/wingify/across-tabs/master/images/hn.png\" width=\"150\" height=\"20\"/></a>\\n<a href=\"https://www.producthunt.com/posts/space-shooter\"><img src=\"https://raw.githubusercontent.com/wingify/across-tabs/master/images/product_hunt.png\" width=\"100\" height=\"20\"/></a>\\n\\n\\n## Index\\n\\n- [Demo](https://github.com/tasdikrahman/spaceShooter#demo)\\n  - [Screenshots](https://github.com/tasdikrahman/spaceShooter#screenshots)\\n- [Game Features](https://github.com/tasdikrahman/spaceShooter#game-features)\\n  - [Controls](https://github.com/tasdikrahman/spaceShooter#controls)\\n- [Installation](https://github.com/tasdikrahman/spaceShooter#installation)\\n  - [For Windows](https://github.com/tasdikrahman/spaceShooter#for-windows)\\n  - [Linux/Debian based systems](https://github.com/tasdikrahman/spaceShooter#linuxdebian-based-systems)\\n    - [Option 1: Download the zipped executable file](https://github.com/tasdikrahman/spaceShooter#option-1-download-the-zipped-executable-file)\\n    - [Option 2: Build from source](https://github.com/tasdikrahman/spaceShooter#option-2-build-from-source)\\n  - [For MAC OS X](https://github.com/tasdikrahman/spaceShooter#for-mac-os-x)\\n- [Contributing](https://github.com/tasdikrahman/spaceShooter#contributing)\\n  - [Contributers](https://github.com/tasdikrahman/spaceShooter#contributers)\\n  - [To-do](https://github.com/tasdikrahman/spaceShooter#to-do)\\n- [Issues](https://github.com/tasdikrahman/spaceShooter#issues)\\n- [Credits](https://github.com/tasdikrahman/spaceShooter#credits)\\n- [Similar](https://github.com/tasdikrahman/spaceShooter#similar)\\n- [License](https://github.com/tasdikrahman/spaceShooter#license)\\n- [Donation](https://github.com/tasdikrahman/spaceShooter#donation)\\n\\n## Demo\\n\\n[[Back to top]](https://github.com/tasdikrahman/spaceShooter#index)\\n\\nFollow the youtube video to see how I fared on ``spaceShooter``\\n\\n[![Space Shooter Demo - Youtube](http://i.imgur.com/bHjlJfG.jpg)](https://www.youtube.com/watch?v=o99zpLsM-ZI)\\n\\n## Screenshots\\n\\n[[Back to top]](https://github.com/tasdikrahman/spaceShooter#index)\\n\\n| ![Screen 1](http://i.imgur.com/3MzfmbT.jpg) | ![Screen 2](http://i.imgur.com/4OgIByR.png) |\\n|---------------------------------------------|---------------------------------------------|\\n| ![Screen 3](http://i.imgur.com/PFQJjE8.png) | ![Screen 4](http://i.imgur.com/lV4aIur.png) |\\n\\n## Game Features\\n\\n[[Back to top]](https://github.com/tasdikrahman/spaceShooter#index)\\n\\n- Health bar for the space ship\\n- Score board to show how you are faring so far\\n- Power ups like\\n  - shield: increases the space ships life\\n  - bolt: increases the shooting capability of the ship by firing 2 to 3 bullets instead of one at time.\\n- Custom sounds and sprite animation for things like\\n  - meteorite explosion\\n  - bullet shoots\\n  - player explosion\\n- 3 lives per game\\n- Fun to play :)\\n\\n## Controls\\n\\n[[Back to top]](https://github.com/tasdikrahman/spaceShooter#index)\\n\\n|              | Button              |\\n|--------------|---------------------|\\n| Move Left    | <kbd>left</kbd>     |\\n| Move right   | <kbd>right</kbd>    |\\n| Fire bullets | <kbd>spacebar</kbd> |\\n| Quit game    | <kbd>Esc</kbd>      |\\n\\n## Installation\\n\\n[[Back to top]](https://github.com/tasdikrahman/spaceShooter#index)\\n\\n### For `Windows`\\n\\n- :arrow_down: [Download the prebuilt zip file and unzip it.](https://github.com/tasdikrahman/spaceShooter/releases/latest)\\n- Run the executable named `spaceShooter` inside the extracted file.\\n\\n### `Linux/Debian` based systems\\n\\n#### Option 1: Download the zipped executable file\\n\\n- :arrow_down: [Download the latest zip file for linux](https://github.com/tasdikrahman/spaceShooter/releases/latest)\\n- Unzip the file\\n\\nIf your download was saved on the `~/Downloads` folder\\n\\nPress <kbd>Ctrl</kbd> + <kbd>Alt</kbd> + <kbd>T</kbd> to open the shell if you are on `GNU/Linux` based systems and type\\n\\n```bash\\n$ unzip ~/Downloads/SpaceShooter-0.0.3.Linux.zip -d ~/Desktop\\n$ cd ~/Desktop\\n$ ## navigate to the unzipped file and change the file permissions for the executable\\n~/Desktop $ chmod +x spaceShooter\\n~/Desktop $ ./spaceShooter\\n```\\n\\nThis will unzip the file on your `Desktop`, you can replace it with the directory of your choice\\n\\n**NOTE** : If it gives you an error, you probably don\\'t have `unzip` installed in your system.\\n\\n```bash\\n$ sudo apt-get install unzip\\n```\\nThat should fix the error.\\n\\n- Run the executable named `spaceShooter`\\n\\nA Similar process would be followed for `OS X`\\n\\n#### Option 2: Build from source\\n\\nYou need to have `pygame` installed for this option. \\n\\n### For `FreeBSD`\\n\\n```sh\\n$ sudo pkg install devel/py-game\\n```\\n\\n##### Clone the repo\\n```sh\\n$ git clone https://github.com/tasdikrahman/spaceShooter.git\\n$ cd spaceShooter/\\n$ chmod +x spaceShooter.py\\n$ python spaceShooter.py\\n```\\n\\n\\n### For `Ubuntu/Debian`\\n\\n```bash\\n$ sudo apt-get install python-pygame\\n```\\n\\n##### Clone the repo\\n\\n```bash\\n$ git clone https://github.com/tasdikrahman/spaceShooter.git\\n$ cd spaceShooter/ \\n$ python spaceShooter.py\\n```\\n\\n### For `MAC OS X` \\n\\nYou have to build from source to get it up and running on `OS X`. Reason?\\nI don\\'t have an `OS X` system to build the executable! So I would love for a Pull request on that one.\\n\\nBuilding from source will do the trick though\\n\\n\\n```bash\\n$ pip3 install hg+http://bitbucket.org/pygame/pygame\\n```\\n\\nInstall Pygame specific dependencies\\n\\n```bash\\n$ brew install sdl sdl_image sdl_ttf portmidi libogg libvorbis\\n$ brew install sdl_mixer --with-libvorbis\\n```\\n\\n##### Clone the repo\\n\\n```bash\\n$ git clone https://github.com/tasdikrahman/spaceShooter.git\\n$ cd spaceShooter/ \\n$ python spaceShooter.py\\n```\\n\\n## Contributing\\n\\n[[Back to top]](https://github.com/tasdikrahman/spaceShooter#index)\\n\\nThis game was written in one day, so the coding standards might not be up the mark. Don\\'t be shy to make a Pull request :)\\n\\nFor details, please refer [the Contributing page](https://github.com/tasdikrahman/spaceShooter/blob/master/CONTRIBUTING.rst)\\n\\n### Contributers\\n\\n[[Back to top]](https://github.com/tasdikrahman/spaceShooter#index)\\n\\n- [@bardlean86](https://github.com/bardlean86/) for adding the third missile powerup and the main menu\\n\\n### To-do\\n\\n[[Back to top]](https://github.com/tasdikrahman/spaceShooter#index)\\n\\n- [x] Add the `windows` executable file\\n- [x] Add main menu for the game\\n- [x] Fix [bug](https://github.com/tasdikrahman/spaceShooter/blob/master/spaceShooter.py#L372) which stops the background music from looping \\n- [x] Add support for `WAV` game music file as `ogg` format is not playable as described in [#1](https://github.com/tasdikrahman/spaceShooter/issues/1)\\n- [ ] Add feature to pause to the game.\\n- [ ] add feature to replay the game after all players die\\n- [ ] Add `OS X` executable file as the `Debian` based one fails to execute on it\\n\\n\\n## Issues\\n\\n[[Back to top]](https://github.com/tasdikrahman/spaceShooter#index)\\n\\nYou can report the bugs at the [issue tracker](https://github.com/tasdikrahman/spaceShooter/issues)\\n\\n**OR**\\n\\nYou can [tweet me](https://twitter.com/tasdikrahman) if you can\\'t get it to work. In fact, you should tweet me anyway.\\n\\n## Credits\\n\\nThe game is a fork of the video instructions given by KidsCanCode. I have made several additional enhancements to it. Do check out their [Channel](https://www.youtube.com/channel/UCNaPQ5uLX5iIEHUCLmfAgKg)!\\n\\n## Similar\\n\\n[[Back to top]](https://github.com/tasdikrahman/spaceShooter#index)\\n\\n- [Bullethell.py ](https://github.com/Frederikxyz/bullethell.py) : A fork of [tasdikrahman/spaceShooter](https://github.com/tasdikrahman/spaceShooter) which adds fancy shooting capabilities\\n\\n## License\\n\\n[[Back to top]](https://github.com/tasdikrahman/spaceShooter#index)\\n\\nBuilt with ♥ by [Tasdik Rahman](http://tasdikrahman.me)[(@tasdikrahman)](https://twitter.com/tasdikrahman) under [MIT License](http://tasdikrahman.mit-license.org)\\n\\nYou can find a copy of the License at http://tasdikrahman.mit-license.org/\\n\\n- The images used in the game are taken from [http://opengameart.org/](http://opengameart.org/), more particulary from the [Space shooter content pack](http://opengameart.org/content/space-shooter-redux) from [@kenney](http://opengameart.org/users/kenney).\\n\\nLicense for them is in `Public Domain`\\n\\n- The game sounds were again taken from [http://opengameart.org/](http://opengameart.org/). The game music, [Frozen Jam](http://opengameart.org/content/frozen-jam-seamless-loop) by [tgfcoder](https://twitter.com/tgfcoder) licensed under [CC-BY-3](http://creativecommons.org/licenses/by/3.0/)\\n\\n## Donation\\n\\n[[Back to top]](https://github.com/tasdikrahman/spaceShooter#index)\\n\\nIf you have found my little bits of software being of any use to you, do consider helping me pay my internet bills :)\\n\\n| PayPal | <a href=\"https://paypal.me/tasdik\" target=\"_blank\"><img src=\"https://www.paypalobjects.com/webstatic/mktg/logo/AM_mc_vs_dc_ae.jpg\" alt=\"Donate via PayPal!\" title=\"Donate via PayPal!\" /></a> |\\n|:-------------------------------------------:|:-------------------------------------------------------------:|\\n| Gratipay  | <a href=\"https://gratipay.com/tasdikrahman/\" target=\"_blank\"><img src=\"https://cdn.rawgit.com/gratipay/gratipay-badge/2.3.0/dist/gratipay.png\" alt=\"Support via Gratipay\" title=\"Support via Gratipay\" /></a> |\\n| Patreon | <a href=\"https://www.patreon.com/tasdikrahman\" target=\"_blank\"><img src=\"http://i.imgur.com/ICWPFOs.png\" alt=\"Support me on Patreon\" title=\"Support me on Patreon\" /></a> |\\n| £ (GBP) | <a href=\"https://transferwise.com/pay/d804d854-6862-4127-afdd-4687d64cbd28\" target=\"_blank\"><img src=\"http://i.imgur.com/ARJfowA.png\" alt=\"Donate via TransferWise!\" title=\"Donate via TransferWise!\" /></a> |\\n| € Euros | <a href=\"https://transferwise.com/pay/64c586e3-ec99-4be8-af0b-59241f7b9b79\" target=\"_blank\"><img src=\"http://i.imgur.com/ARJfowA.png\" alt=\"Donate via TransferWise!\" title=\"Donate via TransferWise!\" /></a> |\\n| ₹ (INR)  | <a href=\"https://www.instamojo.com/@tasdikrahman\" target=\"_blank\"><img src=\"https://www.soldermall.com/images/pic-online-payment.jpg\" alt=\"Donate via instamojo\" title=\"Donate via instamojo\" /></a> |\\n'},\n",
       " {'repo': 'space-wizards/space-station-14',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '<p align=\"center\"> <img alt=\"Space Station 14\" width=\"880\" height=\"300\" src=\"https://raw.githubusercontent.com/space-wizards/asset-dump/de329a7898bb716b9d5ba9a0cd07f38e61f1ed05/github-logo.svg\" /></p>\\n\\nSpace Station 14 is a remake of SS13 that runs on [Robust Toolbox](https://github.com/space-wizards/RobustToolbox), our homegrown engine written in C#.\\n\\nThis is the primary repo for Space Station 14. To prevent people forking RobustToolbox, a \"content\" pack is loaded by the client and server. This content pack contains everything needed to play the game on one specific server.\\n\\nIf you want to host or create content for SS14, this is the repo you need. It contains both RobustToolbox and the content pack for development of new content packs.\\n\\n## Links\\n\\n[Website](https://spacestation14.io/) | [Discord](https://discord.ss14.io/) | [Forum](https://forum.spacestation14.io/) | [Steam](https://store.steampowered.com/app/1255460/Space_Station_14/) | [Standalone Download](https://spacestation14.io/about/nightlies/)\\n\\n## Documentation/Wiki\\n\\nOur [docs site](https://docs.spacestation14.io/) has documentation on SS14s content, engine, game design and more. We also have lots of resources for new contributors to the project.\\n\\n## Contributing\\n\\nWe are happy to accept contributions from anybody. Get in Discord if you want to help. We\\'ve got a [list of issues](https://github.com/space-wizards/space-station-14-content/issues) that need to be done and anybody can pick them up. Don\\'t be afraid to ask for help either!\\n\\nWe are not currently accepting translations of the game on our main repository. If you would like to translate the game into another language consider creating a fork or contributing to a fork.\\n\\n## Building\\n\\n1. Clone this repo.\\n2. Run `RUN_THIS.py` to init submodules and download the engine.\\n3. Compile the solution.\\n\\n[More detailed instructions on building the project.](https://docs.spacestation14.io/getting-started/dev-setup)\\n\\n## License\\n\\nAll code for the content repository is licensed under [MIT](https://github.com/space-wizards/space-station-14/blob/master/LICENSE.TXT).\\n\\nMost assets are licensed under [CC-BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/) unless stated otherwise. Assets have their license and the copyright in the metadata file. [Example](https://github.com/space-wizards/space-station-14/blob/master/Resources/Textures/Objects/Tools/crowbar.rsi/meta.json).\\n\\nNote that some assets are licensed under the non-commercial [CC-BY-NC-SA 3.0](https://creativecommons.org/licenses/by-nc-sa/3.0/) or similar non-commercial licenses and will need to be removed if you wish to use this project commercially.\\n'},\n",
       " {'repo': 'spencermountain/spacetime',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '<div align=\"center\">\\n  <div>\\n    <img width=\"277\" alt=\"spacetime logo\" src=\"https://user-images.githubusercontent.com/399657/31140478-80a4269a-a842-11e7-8dbf-b541fe3e87a7.png\">\\n  </div>\\n\\n  <a href=\"https://npmjs.org/package/spacetime\">\\n    <img src=\"https://img.shields.io/npm/v/spacetime.svg?style=flat-square\" />\\n  </a>\\n  <a href=\"https://codecov.io/gh/spencermountain/spacetime\">\\n    <img src=\"https://codecov.io/gh/spencermountain/spacetime/branch/master/graph/badge.svg\" />\\n  </a>\\n  <a href=\"https://bundlephobia.com/result?p=spacetime@latest\">\\n    <img src=\"https://badge-size.herokuapp.com/spencermountain/spacetime/master/builds/spacetime.min.js\" />\\n  </a>\\n</div>\\n\\n<!-- spacer -->\\n<img height=\"50px\" src=\"https://user-images.githubusercontent.com/399657/68221862-17ceb980-ffb8-11e9-87d4-7b30b6488f16.png\"/>\\n\\nIsn\\'t it weird how we can do <i>math</i> in our head, but not <b><i>date math</i></b>?\\n\\n<div align=\"left\">\\n<div >\\n  <img height=\"30px\" src=\"https://user-images.githubusercontent.com/399657/68221862-17ceb980-ffb8-11e9-87d4-7b30b6488f16.png\"/><i>- how many days until the end of the year?</i>\\n</div>\\n<div >\\n  <img height=\"30px\" src=\"https://user-images.githubusercontent.com/399657/68221862-17ceb980-ffb8-11e9-87d4-7b30b6488f16.png\"/><i>-what time was it, 11 hours ago?</i>\\n</div>\\n<div >\\n  <img height=\"30px\" src=\"https://user-images.githubusercontent.com/399657/68221862-17ceb980-ffb8-11e9-87d4-7b30b6488f16.png\"/><i>-is it lunchtime in france?</i>\\n</div>\\n</div>\\n\\n<img height=\"50px\" src=\"https://user-images.githubusercontent.com/399657/68221862-17ceb980-ffb8-11e9-87d4-7b30b6488f16.png\"/>\\n\\nand worse - there is no real **_date calculator_**.\\n\\n<div align=\"center\">\\n  <sub>people end up asking google, and going to weird websites.</sub>\\n</div>\\n\\n<img height=\"10px\" src=\"https://user-images.githubusercontent.com/399657/68221862-17ceb980-ffb8-11e9-87d4-7b30b6488f16.png\"/>\\n\\n<div align=\"center\"><sub>that\\'s bad.</sub></div>\\n\\n<img height=\"25px\" src=\"https://user-images.githubusercontent.com/399657/68221862-17ceb980-ffb8-11e9-87d4-7b30b6488f16.png\"/>\\n\\n<b>spacetime</b> is a date-calculator,\\n\\n<div >\\n  <img height=\"25px\" src=\"https://user-images.githubusercontent.com/399657/68221862-17ceb980-ffb8-11e9-87d4-7b30b6488f16.png\"/><sub>It\\'s very small, and very handy.</sub>\\n</div>\\n\\n```js\\nlet s = spacetime.now()\\n\\ns.diff(s.endOf(\\'year\\'), \\'days\\')\\n// 292\\n\\ns.minus(11, \\'hours\\').time()\\n// 6:50am\\n\\ns = s.now(\\'Europe/Paris\\')\\ns.isAfter(s.time(\\'11:00am\\'))\\n// true 🥐\\n```\\n\\n<img height=\"30px\" src=\"https://user-images.githubusercontent.com/399657/68221862-17ceb980-ffb8-11e9-87d4-7b30b6488f16.png\"/>\\n\\n<div align=\"center\">\\n  <img height=\"50px\" src=\"https://user-images.githubusercontent.com/399657/68221814-05ed1680-ffb8-11e9-8b6b-c7528d163871.png\"/>\\n</div>\\n<img height=\"30px\" src=\"https://user-images.githubusercontent.com/399657/68221862-17ceb980-ffb8-11e9-87d4-7b30b6488f16.png\"/>\\n\\n- calculate time in remote timezones\\n- support **daylight savings**, **leap years**, and **hemispheres**\\n- [Moment-like API](https://beta.observablehq.com/@spencermountain/spacetime-api) _(but immutable)_\\n- Orient time by quarter, season, month, week..\\n- _Zero Dependencies_ - (no _[Intl API](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Intl)_)\\n- weighs about 40kb.\\n- has a cool _[plugin thing](https://observablehq.com/@spencermountain/spacetime)_.\\n\\n<!-- spacer -->\\n<img height=\"30px\" src=\"https://user-images.githubusercontent.com/399657/68221862-17ceb980-ffb8-11e9-87d4-7b30b6488f16.png\"/>\\n\\n```html\\n<script src=\"https://unpkg.com/spacetime\"></script>\\n<script>\\n  var d = spacetime(\\'March 1 2012\\', \\'America/New_York\\')\\n  //set the time\\n  d = d.time(\\'4:20pm\\')\\n\\n  d = d.goto(\\'America/Los_Angeles\\')\\n  d.time()\\n  //\\'1:20pm\\'\\n</script>\\n```\\n\\n<!-- spacer -->\\n<img height=\"30px\" src=\"https://user-images.githubusercontent.com/399657/68221862-17ceb980-ffb8-11e9-87d4-7b30b6488f16.png\"/>\\n\\n`npm install spacetime`\\n\\n```js\\nconst spacetime = require(\\'spacetime\\')\\nlet d = spacetime.now(\\'Europe/Paris\\')\\nd.dayName()\\n//\\'Wednesday\\'\\nd.isAsleep()\\n//true\\n```\\n\\n<sub><i>typescript / babel / deno:</i></sub>\\n\\n```ts\\nimport spacetime from \\'spacetime\\'\\nlet d = spacetime.now()\\nd.format(\\'nice\\')\\n//\\'Apr 1st, 4:32pm\\'\\n```\\n\\n<div align=\"right\">\\n  <a href=\"https://github.com/spencermountain/spacetime/wiki/Typescript\">ts docs</a>\\n</div>\\n\\n<img height=\"50px\" src=\"https://user-images.githubusercontent.com/399657/68221862-17ceb980-ffb8-11e9-87d4-7b30b6488f16.png\"/>\\n\\n<div align=\"center\">\\n  <h3>\\n    <a href=\"https://beta.observablehq.com/@spencermountain/spacetime\">\\n      Demo\\n    </a>\\n    &nbsp; &nbsp; • &nbsp; &nbsp;\\n    <a href=\"https://beta.observablehq.com/@spencermountain/spacetime-api\">\\n      Full API\\n    </a>\\n  </h3>\\n  <img height=\"30px\" src=\"https://user-images.githubusercontent.com/399657/68221862-17ceb980-ffb8-11e9-87d4-7b30b6488f16.png\"/>\\n\\n  <div>\\n    <img width=\"550\" src=\"https://user-images.githubusercontent.com/399657/50862221-1d904a00-1369-11e9-891c-5f4e9fbb9ec0.gif\" />\\n  </div>\\n\\n  <img height=\"40px\" src=\"https://user-images.githubusercontent.com/399657/68221862-17ceb980-ffb8-11e9-87d4-7b30b6488f16.png\"/>\\n\\n </div>\\n\\n<img height=\"30px\" src=\"https://user-images.githubusercontent.com/399657/68221862-17ceb980-ffb8-11e9-87d4-7b30b6488f16.png\"/>\\n\\nplugins:\\n\\n <div align=\"center\">\\n    <a href=\"https://github.com/spencermountain/spacetime/tree/master/plugins/geo\">spacetime-geo</a>\\n    • <a href=\"https://github.com/spencermountain/spacetime/tree/master/plugins/daylight\">spacetime-daylight</a>\\n    • <a href=\"https://github.com/spencermountain/spacetime/tree/master/plugins/age\">spacetime-age</a>\\n  </div>\\n  <div align=\"center\">\\n    <a href=\"https://github.com/spencermountain/scal\">spacetime-calendar</a>\\n    • <a href=\"https://github.com/spencermountain/spacetime/tree/master/plugins/week-of-month\">week-of-month</a>\\n    • <a href=\"https://github.com/spencermountain/spacetime/tree/master/plugins/week-start\">week-start</a>\\n  </div>\\n\\n<img height=\"50px\" src=\"https://user-images.githubusercontent.com/399657/68221862-17ceb980-ffb8-11e9-87d4-7b30b6488f16.png\"/>\\n\\n### Date Inputs:\\n\\nwe can parse _[all the normal stuff](https://github.com/spencermountain/spacetime/wiki/Input)_, and some fancy stuff:\\n\\n```js\\n//epoch\\ns = spacetime(1489520157124)\\n\\n//array [yyyy, m, d] (zero-based months, 1-based days)\\ns = spacetime([2017, 5, 2])\\n\\n//iso\\ns = spacetime(\\'July 2, 2017 5:01:00\\')\\n\\n// All inputs accept a timezone, as 2nd param:\\ns = spacetime(1489520157124, \\'Canada/Pacific\\')\\ns = spacetime(\\'2019/05/15\\', \\'Canada/Pacific\\')\\n\\n// or set the offset right in the date-string (ISO-8601)\\ns = spacetime(\\'2017-04-03T08:00:00-0700\\')\\n// \\'Etc/GMT-7\\'\\n\\n// Some helpers\\ns = spacetime.now()\\ns = spacetime.today() // This morning\\ns = spacetime.tomorrow() // Tomorrow morning\\ns = spacetime.min() // the earliest-possible date (271,821 bc)\\ns = spacetime.max() // the furthest-possible future date (27k years from now)\\n\\n// To get the native Date object back\\n// NOTE: this returns the date in the local browsers timezone\\njsDate = spacetimeDate.toNativeDate()\\n```\\n\\nfor fancier natural-language inputs, use [compromise-dates](https://github.com/spencermountain/compromise/tree/master/plugins/dates).\\n\\n<img height=\"20px\" src=\"https://user-images.githubusercontent.com/399657/68221862-17ceb980-ffb8-11e9-87d4-7b30b6488f16.png\"/>\\n\\n### Get & Set dates:\\n\\nyou can whip things around, but stay intuitive\\n\\n```js\\ns.date() // 14\\ns.year() // 2017\\ns.season() // Spring\\ns = s.hour(5) // Change to 5am\\ns = s.date(15) // Change to the 15th\\n\\ns = s.day(\\'monday\\') // Change to (this week\\'s) monday\\ns = s.day(\\'monday\\', true) // go forward to monday\\ns = s.day(\\'monday\\', false) // go backward to monday\\n\\ns = s.month(\\'march\\') // Change to (this year\\'s) March 1st\\ns = s.quarter(2) // Change to April 1st\\ns.era() // \\'BC\\'/\\'AD\\'\\ns.decade() // 2000\\ns.century() // 21\\n\\n// Percentage-based information\\ns.progress().month = 0.23 // We\\'re a quarter way through the month\\ns.progress().day = 0.48 // Almost noon\\ns.progress().hour = 0.99 // 59 minutes and 59 seconds\\n\\n// Add/subtract methods\\ns = s.add(1, \\'week\\')\\ns = s.add(3, \\'quarters\\')\\ns = s.subtract(2, \\'months\\').add(1, \\'day\\')\\n\\n// start-of/end-of\\ns = s.startOf(\\'day\\') // 12:00am\\ns = s.startOf(\\'month\\') // 12:00am, April 1st\\ns = s.endOf(\\'quarter\\') // 11:59:59pm, June 30th\\n\\ns = s.nearest(\\'hour\\') //round up/down to the hour\\ns = s.nearest(\\'quarter-hour\\') //5:15, 5:30, 5:45..\\ns = s.next(\\'month\\') //start of the next month\\ns = s.last(\\'year\\') //start of the last year\\n\\n// fill-in all dates between a range\\ns.every(\\'week\\', \\'Jan 1st 2020\\') // (in tz of starting-date)\\n\\n//utilities:\\ns.clone() // Make a copy\\ns.isValid() // Sept 32nd → false\\ns.isAwake() // it\\'s between 8am → 10pm\\ns.json() // get values in every unit as key-val object\\n```\\n\\nif it\\'s **_9am on tuesday_**, and you <i>add a week</i>, it will still be 9am on tuesday.\\n... even if some crazy changes happen.\\n\\nsetter methods also support a handy 2nd param that controls whether it should be set forward, or backward.\\n\\n```js\\ns = s.time(\\'4:00pm\\') // 4pm today\\ns = s.time(\\'4:00pm\\', true) // the next 4pm in the future\\ns = s.time(\\'4:00pm\\', false) // the most-recent 4pm\\n\\ns = s.set(\\'march 5th 2020\\')\\ns = s.set(\\'march 4th\\') // 2020 (same year)\\ns = s.set(\\'march 4th\\', true) // 2021\\ns = s.set(\\'march 6th\\', false) // 2019\\n```\\n\\nit\\'s actually a little surprising how helpful this is.\\n\\n<img height=\"20px\" src=\"https://user-images.githubusercontent.com/399657/68221862-17ceb980-ffb8-11e9-87d4-7b30b6488f16.png\"/>\\n\\n### Comparisons:\\n\\n```js\\nlet s = spacetime([2017, 5, 2])\\nlet start = s.subtract(1, \\'milliseconds\\')\\nlet end = s.add(1, \\'milliseconds\\')\\n\\n// gt/lt/equals\\ns.isAfter(d) // True\\ns.isEqual(d) // False\\ns.isBefore(d) // False\\ns.isBetween(start, end, inclusive?) // True\\n\\n// Comparison by unit\\ns.isSame(d, \\'year\\') // True\\ns.isSame(d, \\'date\\') // False\\ns.diff(d, \\'day\\') // 5\\ns.diff(d, \\'month\\') // 0\\n\\n//make a human-readable diff\\nlet before = spacetime([2018, 3, 28])\\nlet now = spacetime([2017, 3, 28]) //one year later\\nnow.since(before)\\n// {diff: { months: 11, days: 30, ...},  rounded: \\'in 12 months\\'  }\\n```\\n\\nall comparisons are done with sensitivity of timezone - **_8am EST_** is < **_8am PST_**.\\n\\n<img height=\"20px\" src=\"https://user-images.githubusercontent.com/399657/68221862-17ceb980-ffb8-11e9-87d4-7b30b6488f16.png\"/>\\n\\n### Timezones:\\n\\nthe best way to describe a timezone is an [IANA code](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones):\\n\\n```js\\n// Roll into a new timezone, at the same moment\\ns = s.goto(\\'Australia/Brisbane\\')\\n```\\n\\nif you want to support relaxed timezone names like `\\'EST\\'`, `Eastern time`, use [timezone-soft](https://github.com/spencermountain/timezone-soft/)\\n\\n```js\\nspacetime.extend(require(\\'timezone-soft\\'))\\n\\ns = s.goto(\\'milwaukee\\') // \\'America/Chicago\\'\\ns = s.goto(\\'-7h\\') // UTC-7\\ns = s.goto(\\'GMT+8\\') // -8h!\\n// (these should be used with some caution)\\n```\\n\\nplay-around with timezones, and their DST-changes:\\n\\n```js\\n//list timezones by their current time\\nspacetime.whereIts(\\'8:30pm\\', \\'9:30pm\\') // [\\'America/Winnipeg\\', \\'America/Yellowknife\\'... ]\\nspacetime.whereIts(\\'9am\\') //(within this hour)\\n\\n// Timezone metadata\\ns.timezone().name // \\'Canada/Eastern\\' (either inferred or explicit)\\ns.hemisphere() // North\\ns.timezone().current.offset // -4 (in hours)\\ns.hasDST() // True\\ns.isDST() // True\\n\\n//list all timezones\\nspacetime.timezones()\\n```\\n\\nyou can flip-around the world pretty quick.\\n\\nspacetime will use your local timezone, by default:\\n\\n`.goto(null)` will pluck your current tz safely from your browser or computer.\\n\\n```js\\nspacetime().time(\\'4:30pm\\').goto(\\'Europe/Paris\\').goto(null).time()\\n// 4:30pm\\n```\\n\\n<img height=\"20px\" src=\"https://user-images.githubusercontent.com/399657/68221862-17ceb980-ffb8-11e9-87d4-7b30b6488f16.png\"/>\\n\\n### Date Formatting:\\n\\nit\\'s _[a pretty-sensible process](https://github.com/spencermountain/spacetime/wiki/Formatting)_ to create nice-looking dates:\\n\\n```js\\n// Date + time formatting\\ns.format(\\'time\\') // \\'5:01am\\'\\ns.format(\\'numeric-uk\\') // 02/03/2017\\ns.format(\\'month\\') // \\'April\\'\\ns.format(\\'month-short\\') // \\'Apr\\'\\ns.format(\\'month-pad\\') // \\'03\\'\\ns.format(\\'iso-month\\') // \\'04\\'\\n\\n//if you want more complex formats, use {}\\'s\\ns.format(\\'{year}-{date-pad}-{month-pad}\\') // \\'2018-02-02\\'\\ns.format(\"{hour} o\\'clock\") // \\'2 o\\'clock\\'\\ns.format(\\'{time}{ampm} sharp\\') // \\'2:30pm sharp\\'\\n\\n//if you prefer, you can also use unix-formatting\\ns.unixFmt(\\'yyyy.MM.dd h:mm a\\') // \\'2017.Nov.16 11:34 AM\\'\\n```\\n\\n<!-- spacer -->\\n<img height=\"50px\" src=\"https://user-images.githubusercontent.com/399657/68221862-17ceb980-ffb8-11e9-87d4-7b30b6488f16.png\"/>\\n<div align=\"center\">\\n  <img src=\"https://user-images.githubusercontent.com/399657/68221814-05ed1680-ffb8-11e9-8b6b-c7528d163871.png\"/>\\n</div>\\n\\n## Limitations & caveats\\n\\n#### ◆ Historical timezone info\\n\\nDST changes move around all the time, and timezones pop-in and out of existence.\\nWe store and use only the latest DST information, and apply it to historical dates.\\n\\n#### ◆ International date line\\n\\n`.goto()` never crosses the date-line. This is mostly the intuitive behaviour.\\n\\nBut if you\\'re in `Fiji` (just west of the date line), and you go to `Midway` (just east of the date line), .goto() will subtract a bunch of hours, instead of just adding one.\\n\\n#### ◆ Destructive changes\\n\\nif it\\'s `2:30pm` and you add a month, it should still be `2:30pm`. Some changes are more destructive than others. Many of thse choices are subjective, but also sensible.\\n\\n#### ◆ 0-based vs 1-based ...\\n\\nfor better or worse we copy the JavaScript spec for 0-based months, and 1-based dates.\\n\\nISO-formatting is different, so keep on your toes.\\n\\nsee [more considerations and gotchas](https://github.com/spencermountain/spacetime/wiki)\\n\\n#### Daylight-savings gotchas\\n\\nWe\\'ve written in detail about how spacetime handles Daylight-savings changes [here](https://observablehq.com/@spencermountain/spacetime-daylight-savings-time?collection=@spencermountain/spacetime)\\n\\nFall DST changes have an hour that is repeated twice. There are a lot of tricky situations that come from this.\\nAdd 10 minutes at `1:55am`, and a spacetime diff may show `-50mins`. Within an hour of this change, some spacetime methods may be off-by-one hour.\\n\\nSpringtime DST changes are generally smoother than Fall ones.\\n\\n<!-- spacer -->\\n<div align=\"center\">\\n  <img height=\"25px\" src=\"https://user-images.githubusercontent.com/399657/68221862-17ceb980-ffb8-11e9-87d4-7b30b6488f16.png\"/>\\n</div>\\n<div align=\"center\">\\n  <img height=\"50px\" src=\"https://user-images.githubusercontent.com/399657/68221632-b9094000-ffb7-11e9-99e0-b48edd6cdf8a.png\"/>\\n</div>\\n\\n### Config:\\n\\n#### Ambiguity warnings:\\n\\njavascript dates use millisecond-epochs, instead of second-epochs, like some other languages.\\nThis is a common bug, and spacetime can warn if you set an epoch within January 1970.\\nto enable:\\n\\n```js\\nlet s = spacetime(123456, \\'UTC\\', {\\n  silent: false\\n})\\ns.log() // \"Jan 1st, 12:02am\"\\n```\\n\\nThere is another situation where you may see a `console.warn` - if you give it a timezone, but then set a ISO-date string with a different offset, like `2017-04-03T08:00:00-0700` (-7hrs UTC offset).\\nIt sets the timezone to UTC-7, but also gives a warning.\\n\\n```js\\nlet s = spacetime(\\'2017-04-03T08:00:00-0700\\', \\'Canada/Eastern\\', {\\n  silent: false\\n})\\ns.timezone().name // \"Etc/GMT-7\"\\n```\\n\\n#### Configure \\'today\\' context:\\n\\nspacetime makes some assumptions about some string inputs:\\n\\n```js\\n// assumes start of month\\nlet s = spacetime(\\'June 1992\\')\\ns.date() // 1\\n\\n// assumes current year\\nlet s = spacetime(\\'June 5th\\')\\ns.year() // 2020 (or whatever it is now)\\n\\n// assumes Jan 1st\\nlet s = spacetime(\\'2030\\')\\ns.month() // \\'January\\'\\n```\\n\\nyou can configure this assumed date (usually for testing) by passing it in as an option:\\n\\n```js\\nlet today = {\\n  month: 3,\\n  date: 4,\\n  year: 1996\\n}\\nlet s = spacetime(\\'June 5th\\', null, { today: today })\\ns.year() // 1996\\n```\\n\\nit also works for `spacetime.now(tz, {today:today})` and others.\\n\\n#### Extending/Plugins:\\n\\nyou can throw any methods onto the Spacetime class you want, with `spacetime.extend()`:\\n\\n```js\\nspacetime.extend({\\n  isHappyHour: function () {\\n    return this.hour() === 16\\n  }\\n})\\n\\nlet s = spacetime.now(\\'Australia/Adelaide\\')\\ns.isHappyHour()\\n//false\\n\\ns = s.time(\\'4:30pm\\')\\ns.isHappyHour()\\n//true\\n```\\n\\n#### DD/MM/YYY interpretation:\\n\\nby default spacetime uses the American interpretation of ambiguous date formats, like javascript does:\\n\\n```js\\nspacetime(\\'12/01/2018\\') //dec 1st\\n\\n// unless it\\'s clear (>12):\\nspacetime(\\'13/01/2018\\') //jan 13th\\n```\\n\\nyou can change this behaviour by passing in a `dmy` option, like this:\\n\\n```js\\nspacetime(\\'12/01/2018\\', null, { dmy: true }) //jan 12th\\n```\\n\\nthis format is more common in [britain, and south america](https://en.wikipedia.org/wiki/Date_format_by_country).\\n\\n#### Custom languages:\\n\\n```js\\nlet s = spacetime.now()\\ns.i18n({\\n  days: {\\n    long: [\\'domingo\\', \\'lunes\\', \\'martes\\', \\'miércoles\\', \\'jueves\\', \\'viernes\\', \\'sábado\\'],\\n    short: [\\'dom\\', \\'lun\\', \\'mar\\', \\'mié\\', \\'jue\\', \\'vie\\', \\'sáb\\']\\n  },\\n  months: {\\n    long: [...],\\n    short: [\\'ene\\', \\'feb\\', \\'mar\\', \\'abr\\', \\'may\\', \\'jun\\', \\'jul\\', \\'ago\\', \\'sep\\', \\'oct\\', \\'nov\\', \\'dic\\'],\\n  },\\n  ampm: {\\n    am: \\' a. m.\\',\\n    pm: \\' a. m.\\'\\n  },\\n  useTitleCase: true // automatically in .format()\\n});\\ns.format(\\'day\\') //\\'Sábado\\'\\n```\\n\\n#### Configure start of week:\\n\\nby default, the start of the week is monday.\\n\\nYou can determine the week by the official country setting, with [spacetime-week](https://github.com/spencermountain/spacetime-week)\\n\\n```js\\nlet s = spacetime.now()\\ns = s.weekStart(\\'sunday\\')\\n\\ns = s.startOf(\\'week\\')\\ns.dayName()\\n//sunday\\n\\ns = s.endOf(\\'week\\')\\ns.dayName()\\n//saturday\\n```\\n\\n<!-- spacer -->\\n<div align=\"center\">\\n  <img height=\"25px\" src=\"https://user-images.githubusercontent.com/399657/68221862-17ceb980-ffb8-11e9-87d4-7b30b6488f16.png\"/>\\n</div>\\n<div align=\"center\">\\n  <img height=\"50px\" src=\"https://user-images.githubusercontent.com/399657/68221824-09809d80-ffb8-11e9-9ef0-6ed3574b0ce8.png\"/>\\n</div>\\n\\n#### See also:\\n\\n- [luxon](https://moment.github.io/luxon/) - a small library from the clever moment people\\n- [date-fns](https://date-fns.org/) - an battle-hardened client-side Date utility\\n- [sugarjs/dates](https://sugarjs.com/dates/) - well-made date fns + timezone math\\n- [Intl.DateTimeFormat](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/DateTimeFormat) - some _[sorta-green](https://caniuse.com/#feat=internationalization)_ in-browser date utilities\\n\\nthank you to the amazing [timeanddate.com](https://www.timeanddate.com/)\\n\\nApache 2.0\\n'},\n",
       " {'repo': 'SpaceTalk/SpaceTalk',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': 'SpaceTalk is an open-source chat app built with Meteor.\\n\\nNote: SpaceTalk is beta software. Most of it should work but it\\'s still a little unpolished and you\\'ll probably find some bugs. Use at your own risk :)\\n\\n### We Need Your Help!\\n\\nA lot of work has already gone into SpaceTalk, but it needs that final push to reach its full potential.\\n\\nSo if you\\'d like to be part of the project, please check out the [roadmap](https://trello.com/b/R9Nh1V3t/spacetalk-roadmap) and [issues](https://github.com/SpaceTalk/SpaceTalk/issues) to see if there\\'s anything you can help with.\\n\\n### Features\\n\\n#### Currently Implemented\\n\\n* Teams (group users into separate teams)\\n* Channels (each group contains multiple channels)\\n  * Remove/edit channels\\n  * Pinned items\\n  * Channel purpose (description) \\n* Users\\n  * Gravatar for profile pictures\\n  * Online status\\n  * Self sign up \\n* Messages\\n  * Emoji, Markdown support\\n  * Edit & trash your own messages\\n  * Direct Messaging between users\\n  * Indication of user typing\\n  * Rich embeds via https://iframely.com/\\n\\n#### On The Way\\n\\n* Pinned messages\\n* Better notifications\\n* Packages structure (for extensibility)\\n* See the [roadmap](https://trello.com/b/R9Nh1V3t/spacetalk-roadmap) for more and get involved with bringing these features to life\\n\\n\\n### Prerequisites\\n\\n* [Git](http://git-scm.com/book/en/v2/Getting-Started-Installing-Git)\\n* [Meteor](https://www.meteor.com/install)\\n\\n### Style Guide & Naming Conventions\\n\\n* We\\'re following [Meteor Style Guide](https://github.com/meteor/meteor/wiki/Meteor-Style-Guide)\\n* Template names: `<template name=\"camelCase\"></template>`\\n* Route names: `dashed-case/routing-perhaps`\\n* File names: `dashed-case.html`, `dashed-case.js`\\n* Custom HTML id / class naming convention: `<div id=\"dashed-case\"></div>` however class names preferred instead of using ids `<div class=\"some-custom-class\"></div>`\\n\\n\\n### Getting started\\n\\nFork the repository with the [top right button](https://github.com/SpaceTalk/SpaceTalk#fork-destination-box) and clone your fork:\\n\\n```\\ngit clone https://github.com/YOURGITHUBUSERNAME/SpaceTalk.git\\n```\\n\\nAdd the remote source to your local clone:\\n\\n```\\ngit remote add upstream https://github.com/SpaceTalk/SpaceTalk.git\\n```\\n\\nStart the app:\\n\\n```\\ncd SpaceTalk\\nmeteor run --settings=settings.json\\n```\\n\\nTo update your clone do a pull:\\n\\n```\\ngit pull upstream master\\n```\\n\\nCommit your changes to your fork, and create Pull Request with [github helper](https://github.com/SpaceTalk/SpaceTalk/compare/master...#)\\n\\n### Guidelines for reviewing Pull Requests\\n\\n1. Code follows the [Meteor Style Guide](https://github.com/meteor/meteor/wiki/Meteor-Style-Guide)\\n2. Code doesn’t break things (The app can still run)\\n\\n### Libraries\\n\\nThis project is in flux at the moment, these are the currently agreed upon client side libraries:\\n\\n* CSS Pre-Processor: [Sass / .scss](http://sass-lang.com/)\\n* Icon Font Library: [FontAwesome](http://fortawesome.github.io/Font-Awesome/)\\n\\n### Packages\\n\\nThis project is in flux at the moment, these are the currently used Meteor packages:\\n\\n* markdown\\n* reactive-var\\n* accounts-password\\n* momentjs:moment\\n* fourseven:scss\\n* copleykj:jquery-autosize\\n* tmeasday:gravatar\\n* meteorhacks:flow-layout\\n* meteorhacks:flow-router\\n* peerlibrary:blaze-components\\n* mizzao:user-status\\n* todda00:friendly-slugs\\n* useraccounts:core\\n* arillo:flow-router-helpers\\n* seriousm:emoji-continued\\n* mrt:tiny-scrollbar\\n* jquery\\n* kevohagan:sweetalert\\n* fortawesome:fontawesome\\n* aldeed:autoform\\n* useraccounts:unstyled\\n* dburles:collection-helpers\\n* iframely:oembed\\n* matb33:collection-hooks\\n* qnub:emojione\\n* mquandalle:jquery-textcomplete\\n* ogourment:settings\\n* tmeasday:presence\\n\\n### Disclaimer\\n\\nThis code was part of a [Meteor Workshop](http://www.meetup.com/Meteor-Goteborg/events/221282857/) that took place on the 14th of May 2015 in Gothenburg, Sweden.\\n\\nWe used it to build a chat application in 25 steps (https://slides.com/timbrandin/meteor-slack) – it was originally designed to look a little bit like Slack.\\n\\nBut it was solely made do demonstrate the efficiency and simple nature of Meteor applications.\\n\\n#### Credits\\n\\nThanks to [@timbrandin](https://twitter.com/timbrandin) who created this material for [a Meteor workshop](http://www.meetup.com/Meteor-Goteborg/events/221282857/).\\n\\n### License\\n\\nNote that SpaceTalk is distributed under the [MIT License](http://opensource.org/licenses/MIT).\\n\\n-------\\n\\nCopyright © 2015 Tim Brandin &amp; SpaceTalk\\n'},\n",
       " {'repo': 'spacewalkproject/spacewalk',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': \"Spacewalk\\n=========\\n\\n![Current Spacewalk release][CurrentReleaseImg]\\n\\n**Spacewalk 2.10 was the last release of Spacewalk project!**\\n\\nSource code for this release can be found in [SPACEWALK-2.10\\nbranch](https://github.com/spacewalkproject/spacewalk/tree/SPACEWALK-2.10).\\n\\nSpacewalk project has been discontinued on May 31 2020.\\n\\n[Master branch](https://github.com/spacewalkproject/spacewalk) contains unfinished experimental\\nattempt to support RHEL8 server and dnf backend which breaks RHEL / CentOS 6 and 7 support.\\n\\nSpacewalk is an open source Linux systems management solution.\\nIt is the upstream community project from which the [Red Hat\\nSatellite 5][RedHatSatellite] and [SUSE Manager][SUSEManager] products\\nare derived. \\n\\nSpacewalk's capabilities include:\\n\\n  * Inventory your systems (hardware and software information)\\n  * Install and update software on your systems\\n  * Collect and distribute your custom software packages into manageable groups\\n  * Provision (kickstart) your systems\\n  * Manage and deploy configuration files to your systems\\n  * Provision virtual guests\\n  * Start/stop/configure virtual guests\\n  * Distribute content across multiple geographical sites in an efficient manner\\n\\nFor information on how to work with our source repository, please visit [the wiki download page][DownloadIt].\\n\\nResources\\n---------\\n\\nFollowing links might be of value in case you are interested:\\n\\n  * [project pages](https://spacewalkproject.github.io/),\\n  * [wiki](https://github.com/spacewalkproject/spacewalk/wiki) ([submit PR](https://github.com/spacewalkproject/spacewalk-wiki/pulls)),\\n  * [issue tracker (Bugzilla)](https://bugzilla.redhat.com/enter_bug.cgi?product=Spacewalk).\\n  * [Spacewalk mailing list and IRC](https://github.com/spacewalkproject/spacewalk/wiki/Communications)\\n\\n[RedHatSatellite]: https://www.redhat.com/products/enterprise-linux/satellite/\\n[SUSEManager]: https://www.suse.com/products/suse-manager/\\n[DownloadIt]: https://github.com/spacewalkproject/spacewalk/wiki/DownloadIt\\n[CurrentReleaseImg]: https://raw.githubusercontent.com/wiki/spacewalkproject/spacewalk/images/210release.png\\n\"},\n",
       " {'repo': 'sparticle999/SpaceCompany',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Overview:\\nSpace Company is a science-fiction incremental game where you start from humble beginnings on Earth, working your way up to travelling between star systems and building Dyson Spheres and pretty much colonising the entire galaxy(ies?)\\n\\nDevelopment has stopped, however, several projects have been created based on this you may wish to try.\\n\\n# Derived projects from members in the community\\n- https://play.google.com/store/apps/details?id=com.freddecgames.ngsc Mobile Port by Freddec\\n- https://ngspacecompany.freddecgames.com/ V1 Web uptaken by Freddec\\n- https://ngsc.freddecgames.com/ V2 Web Freddec\\n- https://github.com/migue1s/SpaceCompanyNative Mobile Port by migue1s\\n- https://github.com/SpiderGamin/SpaceCompany-Desktop Desktop Application by SpiderGamin\\n- \\n# Former Plans for the Future\\n- Check https://www.reddit.com/r/SpaceCompany/wiki/futureplans\\n- When loading game, the tab you left on is the tab you now start on\\n- Random Events\\n- [Change UI from bootstrap to my own]\\n\\n\\n# RoadMap\\n\\nv1.0 The Overlord Update\\nhttps://www.reddit.com/r/SpaceCompany/wiki/futureplans#wiki_v0.6\\n\\n- Adding Lore onto every Building and an introduction to your Space Company\\n- Rebalance all costs to not use as much Lunarite/Gems\\n- Loading screen flavour text changes as you progress\\n- Machine Overview Tab\\n\\nv0.5.1 in progress\\n\\n- (Mass to Energy Conversion)\\n- Buy X buttons\\n- Buy Max/Custom Buttons\\n- Change Letter Formatting To Start At 100K\\n- Rebalance Science\\n- Change Icons (white circle with icon inside)\\n\\n# Changelog:\\n\\n### v0.5.1 (includes all V0.5.0.X)\\n##### Features\\n\\n- Storage Upgrade Discounts\\n- T5 batteries\\n- Fixed Huge Memory Leak\\n- Multiple Spheres\\n- Auto Emc\\n- Redid Rebirth\\n- Respec\\n- Meteorite Tier 3 and 4\\n- Live counter showing how much DM you will get from each section\\n- Fixed Energy Efficiency 25/50 max bug\\n- Heavily Expanded Interstellar Star List\\n\\n##### Small Changes\\n\\n- Made Alpha Centauri distance clear\\n- You cannot rebirth without a sphere\\n- Fixed higher antimatter storages not saving\\n- Reduced framerate from 100fps to 10fps\\n- Build 250 Segments and Dyson Sphere Button\\n- Balanced Ice T5 to have cost just below storage level rather than above it\\n- AutoEmc keeps a 10K bank of plasma to avoid meteorite production depletion due to 0 plasma stored\\n- Buffed Capital Ship\\\\\\' power and defense\\n- Reduced Oil T3 energy cost\\n- Made Science and Resource Efficiency multiplicative with dark matter boost\\n- Forces Swarms to use ShortName formatting for accuracy with DM scaling\\n- Renamed some generic stars (thanks /u/Misacek01)\\n- Inverted the loading screen colours\\n\\n##### Bug Fixed\\n\\n- Fixed Comms wonder not showing interstellar tab\\n- Fixed TARDIS production being incorrect\\n- Fixed Storage Discount not working\\n- Fixed Storage Upgrade overwriting old storages if greater than 6400\\n- Fixed Tier 1 Rocket not showing red costs when inadequate\\n- Fixed Subsequent Rebirths\\n- Fixed Plasma not showing Gain 20\\n- Fixed Wonders showing up as activated after rebirth\\n- Fixed Solar System Showing planets before exploring requirements\\n- Fixed AutoEmc using energy for meteorite\\n- Fixed Fusion Reactor not using enough Hydrogen\\n- Fixed Invasion Chance being NaN when reputation is above 60\\n- Fixed Absorb not working\\n- Fixed Antimatter not being affected by DM Boost\\n- Fixed several UI issues after rebirth with the interstellar tab\\n- Fixed Interstellar notifications of lost ships without actual losses\\n\\n### v0.5.0 The Interstellar Update\\n##### Features\\n\\n- Offline Production\\n- Screen Notifications Graphics Option\\n- Kongregate Leaderboard\\n- 7 New Themes\\n- Random Loading Messages (100)\\n- Communication Wonder\\n- Rocket Wonder\\n- Antimatter Wonder\\n- Portal Room\\n- Stargate\\n- Plasma Storage Units\\n- Buying Multiple Dyson Parts\\n- Option to Hide Gain Buttons\\n- Achievements for Rings and Swarms\\n- Time Until Storage Full Display\\n- Max Emc Conversion Button\\n- Tier 3 Batteries\\n- Tier 4 Labs\\n- Tier 4 Batteries\\n- Custom Company Name\\n- Copy Export to Clipboard\\n- Update Log On Page Load\\n- Battery Efficiency Research\\n- Coloured Destroy Button Option\\n- Hydrazine Catalyst - T3 Rocket Fuel\\n- Interstellar Radar Scanner\\n- Achievement Ranks\\n- Renamed Space Metal as Lunarite\\n- The Wonder Tab hides itself when completed (makes space for more tabs)\\n- Rebuilt Achievement Tooltips\\n- Dark Matter\\n- Hide Completed Tabs Button\\n- Added Astronomical Breakthrough\\n- Seperate Option for Autosave Notifications\\n\\n##### Small Changes\\n\\n- Individual buttons to turn off Plasma and Meteorite Machines\\n- Turn-off switch for all energy producers/consumers\\n- Alternative scientific formatting\\n- Made it clear that Dyson Sections Costs reset when used\\n- \\'Off\\' Option For Autosaving\\n- Science Forced 1 Decimal Until 100\\n- Cleared Up Misconceptions With \\'NB:\\' Notes\\n- Uranium + Plasma Achievements\\n- Changed Some Descriptions\\n- Made Red Bold Costs Also Underlined\\n- Allow decreasing EMC amount with right click\\n- Changed Version Number System to include 4th digit\\n- Made EMC Max By Default\\n- Import Checks For Empty Field To Refuse Load\\n- Fixed Typos\\n- Fixed Interstellar Backwards Compatibility\\n- Changed Dyson Parts Buying To Buy Parts And Build Dyson\\n- Destroy Alcubierre Drive Button\\n- Nerfed Energy Efficiency to be 1000x cheaper, but only go up to 25%\\n- Changed Multibuy researches to show current level instead of next level\\n- Buffed Battery Efficiency to 200 levels instead of 50\\n- Nerfed Rocket Fuel Research Costs\\n- Buffed Hydrazine Production\\n- Achievement Number Formatting\\n- Rocket Fuel Machine Achievements\\n- Refactored EMC (behind the scenes)\\n- Optimised Saving and Loading\\n- Time until storage full now shows until empty if negative gain\\n- Reset achievements\\n- Standardised gainResource() function\\n- Achievement Stars are worth their position\\n\\n##### Bug Fixes\\n\\n- Fixed Bug With Solar System Sidebar\\n- Fixed Unlocked Tabs Statistic Bug\\n- Fixed Rocket Launching Costs Not Being Red\\n- Fixed Antimatter Tab Highlighting Sticking\\n- Fixed Typos\\n- Fixed Rocket Wonder Not Working\\n- Fixed Solar Panel Showing Incorrect Output\\n- Fixed Interstellar Tab Not Loading Values\\n- Fixed Collapse Outer Solar System SideBarTab Visual Bug\\n- Fixed Portal Room Helium Bug\\n- Fixed Whitespace Bug on Plasma Tab\\n- Fixed Infinite Meteorite Bug\\n- Fixed Stargate Not Deducting Resources\\n- Fixed Dyson Sections Cost Bug\\n- Fixed Plasma EMC Display Bug\\n- Fixed Stargate Red Costs Bug\\n- Fixed T3 Battery Lunarite Cost Not Saving\\n- Fixed UI Bug With Certain Themes\\n- Fixed Rocket Building Not Saving\\n- Fixed Antimatter Decimals Not Being Uniform\\n- Fixed Batteries Not Unlocking Without Refresh\\n- Fixed Interstellar Backwards Compatibility\\n- Fixed Browser Compatibility Problem\\n- Fixed Antimatter Not Turning Green On Full Storage\\n- Fixed Hydrazine Research Cost Not Turning Red\\n- Fixed Antimatter Going Above 100k\\n- Fixed Buying Multiple Solar System Rockets\\n- Fixed T4 Science unlocking only after refresh\\n- Fixed Hydrazine Not Getting Resource Efficiency\\n- Fixed Exponential Notation UI\\n- Fixed Typos\\n- Fixed Stargaze not showing up until refresh\\n- Fixed Silicon Achievement not existing\\n- Fixed Logo not animating\\n\\n### v0.4.4\\n##### Features\\n- Dyson Ring\\n- Infinite Research\\n- Overhauled Behind The Scenes Stuff - Made everything data driven\\n- Notifications\\n- Game Now Works in an Inactive Tab\\n- Tier 2 Batteries\\n- Tier 2 Rocket Fuel\\n- Sidebar Nav Compression\\n- Options For Number Format\\n\\n##### Small Changes\\n- Reworked EMC UI\\n- Energy Conversion Changes\\n- Reworded Chemical Plant Description\\n- Reprogrammed Charcoal Production\\n- Changed Laboratory Names and Descriptions\\n- Destruction of Rocket Fuel Machines\\n- Changed Order of Achievements\\n- Prevented Building Multiple Dyson Spheres\\n- Made All Numbers >1000 4 Digits Long\\n- Changed Ice T4 to use Wood\\n- Days on Time Stats\\n- Changed Links in FAQ to open in new tab\\n- Notifications for Achievements and Autosaving and for when Storage is full\\n- Reduced Info Overload on Getting Started Tab\\n- Added LICENCE.txt\\n\\n##### Bug Fixes\\n- Fixed Oil Rig Costs\\n- Fixed Helium T4 Red Costs\\n- Fixed Dyson Section Reset Costs\\n- Fixed Cyborg UI Bug\\n- Fixed Dyson Costs Resetting\\n- Fixed Exploring Wonder Showing \\'!\\' on Resources Tab\\n- Fixed Importing Without Data Wiping Saves\\n- Fixed Highlighting Bugs\\n- Fixed Charcoal Burners Not Using Wood\\n- Fixed Oxidisation UI Bug\\n- Fixed Session Time Not Resetting\\n- Fixed Meteorite Filling Storage Bug\\n- Fixed Typos\\n\\n### v0.4.3\\n- Completed Achievements\\n- Changed Achievements System\\n- 100,000x Conversion Option\\n\\n##### Bug Fixes\\n- Fixed Destruction of Machines Research Red Cost Bug\\n- Fixed Meteorite Wonder Cost Bug\\n- Fixed Titanium T4 Energy Costs\\n- Fixed Dyson Costs Jumping to 3rd from 1st\\n- Fixed Metal Icon not being transparent\\n\\nv0.4.2\\n- Option for bold text on red costs\\n- Nerfed Battery Costs\\n\\nv0.4.1\\n- Bug Fix with Sphere to Swarm Conversion\\n\\nv0.4.0 The Hot and Cold Update\\n- Achievements\\n- Research from Sol Center\\n- Meteorite\\n- Meteorite Tier of resource machines (4th Tier)\\n- Dyson Sections\\n- Dyson Swarm\\n- Dyson Sphere\\n- Per Second Display on Science\\n- Ability to Destroy Machines\\n- Increased Width of Resources List to reduce vertical scrolling\\n- Tiered Laboratories\\n- Batteries\\n- Changed Silicon from an inner planet resource to an earth resource\\n- Altered Cyborg Theme\\n- Stats for Time Keeping\\n- Merged \"More\" and \"Settings\" Tabs\\n- Tier 2 for Plasma and Meteorite\\n\\nv0.3.5\\n- Number Formatting for large numbers\\n- Made Selected Tab Blue\\n\\nv0.3.4\\n- New Solar Theme\\n- New United Theme\\n\\nv0.3.3\\n- Collapsibility for the Solar System Tab\\n- Notifications on tabs when there is something new in them\\n\\nv0.3.2\\n- Made per second text red when negative\\n- Dark Cyborg Theme\\n\\nv0.3.1\\n- Rebalanced Fusion Reactor and Magmatic Dynamo\\n- Rebalanced Tier 3 Machines For Some Resources\\n\\nv0.3.0 The Tech Update [Pushed to Beta]\\n- Sol Scientific Center\\n- Computerized Tier of resource machines\\n- Exploration of the outer planets\\n- Energy-Mass Conversion\\n- Hydrogen and Helium\\n- Ice\\n- Plasma\\n- Tech Wonder\\n- Fusion Reactor\\n- More Statistics\\n- Loading Screen\\n- Fixed Typos\\n- Fixed Wood/Charcoal Bugs\\n\\nv0.2.2\\n- If you do not have enough resources for something, the number will be red\\n- You can collapse resources into earth and space categories\\n- Buffed Charcoal Engines and Solar Panels to stop people falling into a \\'negative energy hole\\'\\n- Nuclear & Magmatic Power\\n- Nerfed Methane Station\\n\\nv0.2.1\\n- Fixed Bugs\\n\\nv0.2.0 The Wondrous Update [Released To The Public]\\n- Wonder Tab\\n- two different wonders - Precious Wonder & Energetic Wonder\\n- ability to upgrade Wonders\\n- Uranium & Lava\\n- Widened resources navigation so that storage could be displayed on one line\\n- Solar Panel and Charcoal Engine Upgrades\\n- Statistics\\n- Settings\\n- Import and Export\\n\\nv0.1.2\\n- Auto-saving\\n- Finished Beginner\\'s Guide\\n\\nv0.1.1\\n- Fixed bugs from the 0.1.0 update\\n\\nv0.1.0 The Space Update\\n- Space travel to The Moon, Venus, Mars and the Asteroid Belt\\n- New resources: Lunarite, Methane Gas, Titanium, Silver, Gold and Silicon\\n- Methane-based power\\n- Commas to all numbers more than 1000\\n- Fixed negative energy\\n- Saving\\n- Donations through paypal\\n- Updated Beginner\\'s Guide\\n\\nv0.0.7\\n- Resource Technology Upgrade to double resource machines output\\n- Nerfed Science\\n- Changed the per second display to be zero if the storage is full\\n\\nv0.0.6\\n- Icons for the rest of the resources\\n- Help / FAQ Tab\\n- Beginner Guide, FAQ and Credits\\n- Made resource numbers scroll up instead of jumping to the current value\\n\\nv0.0.5\\n- Space Tab\\n- Chemical plants that produce rocket fuel\\n- Rocket that uses rocket fuel to launch into space\\n\\nv0.0.4 \\n- Balanced Game More (to not take 2 minutes to complete)\\n- Fixed Wood/sec bug\\n- Made all resource gatherers increase in cost as you buy them\\n- Fixed Resource Machines not using energy\\n- Changed storage upgrades to cost metal as well\\n\\nv0.0.3\\n- Removed cap on energy\\n- Solar Panels\\n- Resource Gathering Machine were added that use a constant supply of energy\\n- Oil was made a component in building machines\\n- Fixed storage costs bug\\n\\nv0.0.2\\n- Merged Crafting and Resources tabs\\n- Reworked Science Techs\\n- Balanced Science Tab\\n- Removed the ability for science to be clicked - it can only be gained slowly\\n- Energy and Charcoal engines\\n- Charcoal\\n- Removed Gas\\n\\nv0.0.1\\n- Basic mechanics and basic Bootstrap theme implemented\\n- Initial release\\n'},\n",
       " {'repo': 'zhixuan-lin/SPACE',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# SPACE\\n\\nThis is an official PyTorch implementation of the SPACE model presented in the following paper:\\n\\n> [SPACE: Unsupervised Object-Oriented Scene Representation via Spatial Attention and Decomposition](https://arxiv.org/abs/2001.02407)  \\n> *{[Zhixuan Lin](https://www.google.com/url?q=https%3A%2F%2Fwww.zhixuanlin.com%2F&sa=D&sntz=1&usg=AFQjCNEh-Y3e9ey472MfGdTmhEUQcLoRug), [Yi-Fu Wu](http://www.google.com/url?q=http%3A%2F%2Fwww.yifuwu.com%2F&sa=D&sntz=1&usg=AFQjCNGq_BQG2SnAYFsvwGuDFkNLUdFU_A), [Skand Vishwanath Peri](http://www.google.com/url?q=http%3A%2F%2Fpvskand.github.io&sa=D&sntz=1&usg=AFQjCNGmDcD9SZNhmXeJIbxwLG4zlqB9tg)}, Weihao Sun, [Gautam Singh](http://www.google.com/url?q=http%3A%2F%2Fsinghgautam.github.io%2F&sa=D&sntz=1&usg=AFQjCNF7UbTUKtzr0VwwYZ7Z2oVdgir0fw), Fei Deng, [Jindong Jiang](https://www.google.com/url?q=https%3A%2F%2Fwww.jindongjiang.me&sa=D&sntz=1&usg=AFQjCNGMRnKNbnqFNIDCDGkb3lziYgUpJQ), [Sungjin Ahn](http://www.google.com/url?q=http%3A%2F%2Fwww.sungjinahn.com%2Fhome&sa=D&sntz=1&usg=AFQjCNFM7arcABqEtSI15Bl6EsrH4Ajm2g)*  \\n> *ICLR 2020*  \\n> [Project page](https://sites.google.com/view/space-project-page)   \\n\\n<img src=\"figures/riverraid.gif\" height=\"150px\"> </img>\\n\\n## General\\n\\nProject directories:\\n\\n* `src`: source code\\n* `data`: where you should put the datasets\\n* `output`: anything the program outputs will be saved here. These include\\n  * `output/checkpoints`: training checkpoints. Also, model weights with the best performance will be saved here\\n  * `output/logs`: tensorboard event files\\n  * `output/eval`: quantitative evaluation results\\n  * `output/demo`: demo images\\n* `scripts`: some useful scripts for downloading things and showing demos\\n* `pretrained`: where to put downloaded pretrained models\\n\\nThis project uses [YACS](https://github.com/rbgirshick/yacs) for managing experiment configurations. Configurations are specified with YAML files. These files are in `src/configs`. We provide five YAML files that correspond to the figures in the paper:\\n\\n* `3d_room_large.yaml`: for the 3D Room Large dataset\\n* `3d_room_small.yaml`: for 3D Room Small dataset\\n* `atari_spaceinvaders.yaml`: for the Space Invaders game\\n* `atari_riverraid.yaml`: for the River Raid game\\n* `atari_joint.yaml`: for joint training on 10 Atari games\\n\\n## Dependencies\\n\\nThis project uses Python 3.7 and PyTorch 1.3.0. \\n\\nCreate a conda environment with Python 3.7 and activate it. Other versions of Python should also be fine:\\n\\n```\\nconda create -n space python=3.7\\nconda activate space\\n```\\n\\nInstall PyTorch 1.3.0:\\n\\n```\\npip install torch==1.3.0+cu100 torchvision==0.4.1+cu100 -f https://download.pytorch.org/whl/torch_stable.html\\n```\\n\\nNote that this requires CUDA 10.0. If you need CUDA 9.2 then change `cu100` to `cu92`. Depending on your cuda version, you may want to install previous versions of PyTorch.  See [here](https://pytorch.org/get-started/previous-versions/).\\n\\nOther requirements are in `requirements.txt` and can be installed with\\n\\n```\\npip install -r requirements.txt\\n```\\n\\n[TensorBoard](https://www.tensorflow.org/tensorboard) is used for training visualization and included in `requirements.txt`. In some cases, TensorBoard without a full TensorFlow installation can show some weird behaviors (not loading logs, etc.). If that happens, consider installing full TensorFlow.\\n\\n## Datasets\\n\\nThe following datasets with Google Drive download links are provided:  \\n\\n* 3D Room Large : [`OBJ3D_LARGE.tar.gz`](https://drive.google.com/open?id=1gE3kr_ZLdsMRr263K2v1HhyfJehhW7Pi) (1.4G)\\n* 3D Room Small: [`OBJ3D_SMALL.tar.gz`](https://drive.google.com/open?id=18Ta1sWCyprv0QPGMUOMMPFWac8KFICMN) (156M)\\n* Atari (with 11 games): [`ATARI.tar.gz`](https://drive.google.com/open?id=1vzFVFhJZDZMkJ8liROtIyzOiUY42r4TZ) (2.2G)\\n\\nDepending on your need, you can download one or more of these datasets. Two download options are available:\\n\\n* **Download with scripts**. Run one or more of the following scripts:\\n\\n  ```sh\\n  # Run one or more of these\\n  sh scripts/download_data_3dlarge.sh\\n  sh scripts/download_data_3dsmall.sh\\n  sh scripts/download_data_atari.sh\\n  ```\\n\\n  Datasets will be downloaded to `data` and decompressed.\\n\\n* **Manual download**. Alternatively, you can manually download them with the provided Google Drive links, put them under the `data` directory, and decompress them with `tar -xzf [FILE].tar.gz`. \\n\\nThe `data` directory should look like this (if you have downloaded all three datasets):\\n\\n```\\ndata\\n├── OBJ3D_LARGE\\n│\\xa0\\xa0 ├── test\\n│\\xa0\\xa0 ├── train\\n│\\xa0\\xa0 └── val\\n├── OBJ3D_SMALL\\n│\\xa0\\xa0 ├── test\\n│\\xa0\\xa0 ├── train\\n│\\xa0\\xa0 └── val\\n└── ATARI\\n \\xa0\\xa0 ├── Asterix-v0\\n \\xa0\\xa0 ├── Atlantis-v0\\n \\xa0\\xa0 ├── Carnival-v0\\n \\xa0\\xa0 ├── DoubleDunk-v0\\n \\xa0\\xa0 ├── Kangaroo-v0\\n \\xa0\\xa0 ├── MontezumaRevenge-v0\\n \\xa0\\xa0 ├── MsPacman-v0\\n \\xa0\\xa0 ├── Pooyan-v0\\n \\xa0\\xa0 ├── Qbert-v0\\n \\xa0\\xa0 ├── Riverraid-v0\\n \\xa0\\xa0 └── SpaceInvaders-v0\\n\\n```\\n\\n\\n## Quick demo with pretrained models\\n\\nTo download pretrained models, two options are available:\\n\\n* **Download with scripts**. Run the following script to download pretrained models:\\n\\n  ```\\n  sh scripts/download_pretrained.sh\\n  ```\\n\\n  Pretrained models will be downloaded to the `pretrained` directory and decompressed. \\n\\n* **Manual download**. Alternatively, you can manually download the files (in one compressed file `pretrained.tar.gz`) with [this Google Drive link](https://drive.google.com/open?id=1gUvLTfy5pKeLa6k3RT8GiEXWiGG8XzzD) (239M), put it under the `pretrained` directory and decompress it with `tar -xzf pretrained.tar.gz`. \\n\\nThe `pretrained` directory should look like this:\\n\\n```\\npretrained\\n├── 3d_room_large.pth\\n├── 3d_room_small.pth\\n├── atari_spaceinvaders.pth\\n├── atari_riverraid.pth\\n└── atari_joint.pth\\n```\\n\\nThen run one of the following to create some visualizations, with either CPU or GPU:\\n\\n```\\n# Use CPU\\nsh scripts/demo_cpu.sh\\n# Use GPU\\nsh scripts/demo_gpu.sh\\n```\\n\\nImages showing foreground objects and background segmentation will be dumped to `output/demo`. If you have downloaded all three datasets, then five images `3d_room_large.png`, `3d_room_small.png`, `atari_spaceinvaders.png`, `atari_riverraid.png` and `atari_joint.png` will be generated in `output/demo`. Otherwise only some of them will be generated.\\n\\nIf you are using a remote server, you can then run \\n\\n```\\npython -m http.server -d output/demo 8080\\n```\\n\\nand then visit `http://[your server\\'s address]:8080` in your local browser to view these images. \\n\\n## Training and Evaluation\\n\\n**First, `cd src`.  Make sure you are in the `src` directory for all commands in this section. All paths referred to are also relative to `src`**.\\n\\nThe general command to run the program is (assuming you are in the `src` directory)\\n\\n```\\npython main.py --task [TASK] --config [PATH TO CONFIG FILE] [OTHER OPTIONS TO OVERWRITE DEFAULT YACS CONFIG...]\\n```\\n\\nDetailed instructions will be given below.\\n\\n**Training**. Run one or more of the following to train the model on the datasets you want:\\n\\n* 3D Room Large:\\n\\n  ```\\n  python main.py --task train --config configs/3d_room_large.yaml resume True device \\'cuda:0\\'\\n  ```\\n\\n* 3D Room Small:\\n\\n  ```\\n  python main.py --task train --config configs/3d_room_small.yaml resume True device \\'cuda:0\\'\\n  ```\\n\\n* River Raid:\\n\\n  ```\\n  python main.py --task train --config configs/atari_riverraid.yaml resume True device \\'cuda:0\\'\\n  ```\\n\\n* Space Invaders:\\n\\n  ```\\n  python main.py --task train --config configs/atari_spaceinvaders.yaml resume True device \\'cuda:0\\'\\n  ```\\n\\n* Joint training on 10 Atari games:\\n\\n  ```\\n  python main.py --task train --config configs/atari_joint.yaml resume True device \\'cuda:0\\'\\n  ```\\n\\nThese start training with GPU 0 (`cuda:0`). There some useful options that you can specify. For example, if you want to use GPU 5, 6, 7, and 8 and resume from checkpoint `../output/checkpoints/3d_room_large/model_000008001.pth`, you can run the following:\\n\\n```\\npython main.py --task train --config configs/3d_room_large.yaml \\\\\\n\\tresume True resume_ckpt \\'../output/checkpoints/3d_room_large/model_000008001.pth\\' \\\\\\n\\tparallel True device \\'cuda:5\\' device_ids \\'[5, 6, 7, 8]\\'\\n```\\n\\nOther available options are specified in `config.py`.\\n\\n**Training visualization**. Run the following\\n\\n```\\n# Run this from the \\'src\\' directory\\ntensorboard --bind_all --logdir \\'../output/logs\\' --port 8848\\n```\\n\\nAnd visit `http://[your server\\'s address]:8848` in your local browser.\\n\\n**Evaluation**. We only have ground truth for the two 3D Room datasets. After training is finished with the specified maximum steps (or you can stop them manually), run the following to evaluate APs and counting accuracy:\\n\\n```\\n# Run one or more of these\\npython main.py --task eval --config configs/3d_room_large.yaml resume True device \\'cuda:0\\'\\npython main.py --task eval --config configs/3d_room_small.yaml resume True device \\'cuda:0\\'\\n```\\n\\nThe model with the best performance (average AP) will be loaded and evaluated. The results will be printed to stdout. They will also be saved to `../output/eval` as JSON files.\\n\\nAlternatively, you can directly run evaluation using the pretrained models that we provided:\\n\\n```\\n# Run one or more of these\\npython main.py --task eval --config configs/3d_room_large.yaml resume True device \\'cuda:0\\' resume_ckpt \\'../pretrained/3d_room_large.pth\\'\\npython main.py --task eval --config configs/3d_room_small.yaml resume True device \\'cuda:0\\' resume_ckpt \\'../pretrained/3d_room_small.pth\\'\\n```\\n\\n## Issues\\n\\n* For some reason we were using BGR images for our Atari dataset and our pretrained models can only handle that. Please convert the images to BGR if you are to test your own Atari images with the provided pretrained models.\\n* There is a chance that SPACE doesn\\'t learn proper background segmentation for the 3D Room Large datasets. Due to the known [PyTorch reproducibity issue](https://pytorch.org/docs/stable/notes/randomness.html), we cannot guarantee each training run will produce exactly the same result even with the same seed. For the 3D Room Large datasets, if the model doesn\\'t seem to be segmenting the background in 10k-15k steps, you may considering changing the seed and rerun (or not even changing the seed, it will be different anyway). Typically after trying 1 or 2 runs you will get a working version.\\n\\n## Use SPACE for other tasks\\n\\nIf you want to apply SPACE to your own task (e.g., for RL), please be careful. Applying SPACE to RL is also our original intent, but we found that the model can sometimes be unstable and sensitive to hyperparameters and training tricks. There are several reasons:\\n\\n1. **The definition of objects and background is ambiguous in many cases**. Atari is one case where objects are often well-defined. But in many other cases, it is not. For more complicated datasets, making SPACE separate foreground and background properly can be something non-trivial. \\n2. **Learning is difficult when object sizes vary a lot**. In SPACE, we need to set a proper prior for object sizes manually and that turn out to be crucial hyperparameter. For example, for the 10 Atari games we tested, objects are small and roughly of the same size. When object sizes vary a lot SPACE may fail.\\n\\nThat said, we are pleased to offer discussions and pointers if you need help (especially when fine-tuning it on your own dataset). We also hope this will facilitate future works that overcome these limitations.\\n\\n## Citation\\n\\nIf you find this code useful for your research, please cite our paper with the following BibTeX entry\\n\\n```\\n@inproceedings{\\n\\tLin2020SPACE,\\n\\ttitle={SPACE: Unsupervised Object-Oriented Scene Representation via Spatial Attention and Decomposition},\\n\\tauthor={Zhixuan Lin and Yi-Fu Wu and Skand Vishwanath Peri and Weihao Sun and Gautam Singh and Fei Deng and Jindong Jiang and Sungjin Ahn},\\n\\tbooktitle={International Conference on Learning Representations},\\n\\tyear={2020},\\n\\turl={https://openreview.net/forum?id=rkl03ySYDH}\\n}\\n```\\n\\n## Acknowledgements\\n\\nThe evaluation code is adapted from the one used in [SPAIR](https://github.com/e2crawfo/auto_yolo). The code structure is inspired (and significantly simplified) by [Mask-RCNN](https://github.com/facebookresearch/maskrcnn-benchmark) (deprecated, with the latest being [Detectron2](https://github.com/facebookresearch/maskrcnn-benchmark)) from Facebook. Google Drive download commands are created with https://gdrive-wget.glitch.me/\\n\\n\\n\\n\\n\\n\\n'},\n",
       " {'repo': 'hakimel/Meny',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Meny\\n\\nA three dimensional and space effecient menu. Meny works best in browsers with support for CSS 3D transforms, although it falls back on 2D animation for older browsers. Supports touch events for mobile devices.\\n\\n[Check out the demo page](http://lab.hakim.se/meny/).\\n\\n\\n## Instructions\\n\\n### 1. Download\\nAdd [meny.js](https://github.com/hakimel/Meny/blob/master/js/meny.js) to your project. The meny.js file is the only thing required, but you could optionally clone the repository if you want to get the default styles.\\n\\nAlternatively you can load the meny.js file from cdnjs, see <https://cdnjs.com/libraries/meny>.\\n\\n### 2. Markup\\nMeny requires two HTML elements to work: a **menu** and the page **contents**. The class names are not used by the library so choose anything you want.\\n\\n```html\\n<body>\\n  <div class=\"meny\">\\n    <!-- your menu items -->\\n  </div>\\n  <div class=\"contents\">\\n    <!-- your page contents -->\\n  </div>\\n</body>\\n```\\n\\nSome rules, notes and best practices to keep in mind in terms of markup and styling:\\n- The **menu** and **contents** should have the same **parent** element.\\n- The background which appears behind the **contents** when Meny is open is not added by the library. You need to set your desired background to the **parent** element. The default style can be found in [demo.css](https://github.com/hakimel/Meny/blob/master/css/demo.css#L23).\\n- The arrow which appears when Meny is closed is also not added by the library, if you want it you can easily copy the styles from the demo.css.\\n- The **menu** container will be automatically resized by the library according to configuration options.\\n- Meny works on scrolling pages as the menu itself is fixed.\\n\\n\\n### 3. Initialize\\nNext you need create an instance of Meny and tell it which HTML elements to use. This should be done after the **meny.min.js** is included on your page. Example using the HTML above:\\n\\n```javascript\\nvar meny = Meny.create({\\n\\t// The element that will be animated in from off screen\\n\\tmenuElement: document.querySelector( \\'.meny\\' ),\\n\\n\\t// The contents that gets pushed aside while Meny is active\\n\\tcontentsElement: document.querySelector( \\'.contents\\' ),\\n\\n\\t// The alignment of the menu (top/right/bottom/left)\\n\\tposition: \\'left\\',\\n\\n\\t// The height of the menu (when using top/bottom position)\\n\\theight: 200,\\n\\n\\t// The width of the menu (when using left/right position)\\n\\twidth: 260,\\n\\n\\t// The angle at which the contents will rotate to.\\n\\tangle: 30,\\n\\n\\t// The mouse distance from menu position which can trigger menu to open.\\n\\tthreshold: 40,\\n\\n\\t// Width(in px) of the thin line you see on screen when menu is in closed position.\\n\\toverlap: 6,\\n\\n\\t// The total time taken by menu animation.\\n\\ttransitionDuration: \\'0.5s\\',\\n\\n\\t// Transition style for menu animations\\n\\ttransitionEasing: \\'ease\\',\\n\\n\\t// Gradient overlay for the contents\\n\\tgradient: \\'rgba(0,0,0,0.20) 0%, rgba(0,0,0,0.65) 100%)\\',\\n\\n\\t// Use mouse movement to automatically open/close\\n\\tmouse: true,\\n\\n\\t// Use touch swipe events to open/close\\n\\ttouch: true\\n});\\n```\\n\\n### 4. API & Events\\nA few handy methods API methods are included, you call these on the instance returned by ```Meny.create``` (see above).\\n\\n```javascript\\nmeny.configure({ mouse: false }); // change settings after initialization\\n\\nmeny.open();\\n\\nmeny.close();\\n\\nmeny.isOpen(); // true/false\\n\\nmeny.destroy(); // revert original DOM state, unbind events\\n```\\n\\nThe wrapper element (parent of the **menu** and **contents**) is decorated with classes based on its state:\\n```css\\n.meny-active\\n.meny-top\\n.meny-right\\n.meny-bottom\\n.meny-left\\n```\\n\\nInstances of Meny dispatch events to notify you of their state:\\n\\n```javascript\\nvar meny = Meny.create( ... ) // see 3. Initialize\\n\\nmeny.addEventListener( \\'open\\', function() {\\n\\n\\t// do something on open\\n\\n} );\\n\\nmeny.addEventListener( \\'close\\', function() {\\n\\n\\t// do something on close\\n\\n} );\\n\\nmeny.addEventListener( \\'opened\\', function() {\\n\\n\\t// do something right after meny is opened and transitions finished\\n\\n} );\\n\\nmeny.addEventListener( \\'closed\\', function() {\\n\\n\\t// do something right after meny is closed and transitions finished\\n\\n} );\\n```\\n\\n\\n## History\\n\\n#### 1.4.0\\n- Adds `opened` and `closed` events\\n- Adds `destroy` API method\\n\\n#### 1.3.0\\n- Add ```touch``` and ```mouse``` config options\\n- Fix error with tap to close when meny is on the left\\n- Add ```configure``` API method for changing settings at runtime\\n\\n#### 1.2.0\\n- Improvements to touch interaction\\n- Setting threshold to 0 disables hover/touch-to-open\\n\\n#### 1.1.0\\n- Instances of Meny now dispatch \\'open\\'/\\'close\\' events\\n\\n#### 1.0.0\\n- 2D animation fallback (works in IE8+)\\n\\n#### 0.9.0\\n- Rewrote the JavaScript\\n- All core styles/transforms are set via JavaScript\\n- Made many options available at initialization\\n- New JavaScript fallback using internal animation method\\n\\n#### 0.3.0\\n- Fallback mode that doesn\\'t rely on transforms\\n\\n#### 0.2.0\\n- Cleaned up CSS\\n- Fix bug where original events for taps on anchors were blocked\\n- It\\'s now possible to reach the meny via tapping as well as swiping\\n\\n#### 0.1.0\\n- Initial release\\n\\n## License\\n\\nMIT licensed\\n\\nCopyright (C) 2014 Hakim El Hattab, http://hakim.se\\n'},\n",
       " {'repo': 'armcha/Space-Navigation-View',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '# Space-Navigation-View\\n[![Android Arsenal](https://img.shields.io/badge/Android%20Arsenal-Space--Navigation--View-green.svg?style=true)](https://android-arsenal.com/details/1/4180)\\n<a href=\\'https://ko-fi.com/A654L70\\' target=\\'_blank\\'><img height=\\'36\\' style=\\'border:0px;height:36px;\\' src=\\'https://az743702.vo.msecnd.net/cdn/kofi1.png?v=f\\' border=\\'0\\' alt=\\'Buy Me a Coffee at ko-fi.com\\' /></a> \\n\\n## Introduction\\n------------\\nSpace Navigation is a library allowing easily integrate fully customizable Google [Spaces][1] like navigation to your app.\\n[1]: https://play.google.com/store/apps/details?id=com.google.android.apps.social.spaces\\n\\n![](screens/mainGif.gif)\\n![](screens/screen4.png)\\n\\n\\nThe current minSDK version is API level 14 Android 4.0 (ICE CREAM SANDWICH).\\n\\nDownload sample [apk][7]\\n[7]: https://github.com/armcha/Space-Navigation-View/raw/master/SpaceNavigationView.apk\\n\\n# YouTube demos \\n\\n## Demo 1    \\n[![Demo 1](https://img.youtube.com/vi/LY-7abfJV2o/2.jpg)](https://www.youtube.com/watch?v=LY-7abfJV2o)    \\n## Demo 2     \\n[![Demo 2](https://img.youtube.com/vi/rA1NMMLJ4TE/2.jpg)](https://www.youtube.com/watch?v=rA1NMMLJ4TE)      \\n\\n# Download magic\\n-----------------------\\n\\n\\nGradle:\\n```groovy\\ncompile \\'com.github.armcha:SpaceNavigationView:1.6.0\\'\\n```\\nMaven:\\n```xml\\n<dependency>\\n  <groupId>com.github.armcha</groupId>\\n  <artifactId>SpaceNavigationView</artifactId>\\n  <version>1.6.0</version>\\n  <type>pom</type>\\n</dependency>\\n```\\n\\n## Setup and usage\\n------------------\\n\\nAdd the Space Navigation view to your layout\\n\\n```xml\\n <FrameLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\\n     xmlns:tools=\"http://schemas.android.com/tools\"\\n     xmlns:app=\"http://schemas.android.com/apk/res-auto\"\\n     android:layout_width=\"match_parent\"\\n     android:layout_height=\"match_parent\">\\n     \\n      <...View\\n             ....\\n             android:layout_marginBottom=\"@dimen/view_bottom_margin\" />\\n             \\n      <com.luseen.spacenavigation.SpaceNavigationView\\n             android:id=\"@+id/space\"\\n             android:layout_width=\"match_parent\"\\n             android:layout_height=\"match_parent\"\\n             android:layout_gravity=\"bottom\"/>\\n             \\n </FrameLayout>\\n```\\n\\nAdd Space Navigation items.\\n\\n```java\\n   SpaceNavigationView spaceNavigationView = (SpaceNavigationView) findViewById(R.id.space);\\n   spaceNavigationView.initWithSaveInstanceState(savedInstanceState);\\n   spaceNavigationView.addSpaceItem(new SpaceItem(\"HOME\", R.drawable.yourDrawable));\\n   spaceNavigationView.addSpaceItem(new SpaceItem(\"SEARCH\", R.drawable.yourDrawable));\\n```\\n\\nUse ```initWithSaveInstanceState(savedInstanceState)``` and override ```onSaveInstanceState``` \\nif you want to keep selected item position and badge on device rotation\\n```java\\n       @Override\\n       protected void onSaveInstanceState(Bundle outState) {\\n           super.onSaveInstanceState(outState);\\n           spaceNavigationView.onSaveInstanceState(outState);\\n       }\\n```\\n\\nSet onClick listener\\n```java\\n   spaceNavigationView.setSpaceOnClickListener(new SpaceOnClickListener() {\\n            @Override\\n            public void onCentreButtonClick() {\\n               Toast.makeText(MainActivity.this,\"onCentreButtonClick\", Toast.LENGTH_SHORT).show();\\n            }\\n\\n            @Override\\n            public void onItemClick(int itemIndex, String itemName) {\\n               Toast.makeText(MainActivity.this, itemIndex + \" \" + itemName, Toast.LENGTH_SHORT).show();\\n            }\\n            \\n             @Override\\n             public void onItemReselected(int itemIndex, String itemName) {\\n               Toast.makeText(MainActivity.this, itemIndex + \" \" + itemName, Toast.LENGTH_SHORT).show();           \\n            }\\n        });\\n```\\n\\nSet onLongClick listener\\n```java\\n    spaceNavigationView.setSpaceOnLongClickListener(new SpaceOnLongClickListener() {\\n            @Override\\n            public void onCentreButtonLongClick() {\\n                Toast.makeText(MainActivity.this,\"onCentreButtonLongClick\", Toast.LENGTH_SHORT).show();\\n            }\\n\\n            @Override\\n            public void onItemLongClick(int itemIndex, String itemName) {\\n                Toast.makeText(MainActivity.this, itemIndex + \" \" + itemName, Toast.LENGTH_SHORT).show();\\n            }\\n        });\\n```\\n\\nCustomize\\n---------\\n\\nCustomize with xml\\n\\n```xml\\n <com.luseen.spacenavigation.SpaceNavigationView\\n        android:id=\"@+id/space\"\\n        android:layout_width=\"match_parent\"\\n        android:layout_height=\"match_parent\"\\n        android:layout_gravity=\"bottom\"\\n        app:active_item_color=\"@color/colorAccent\"\\n        app:centre_button_color=\"@color/centre_button_color\"\\n        app:inactive_item_color=\"@color/white\"\\n        app:space_background_color=\"@color/colorPrimary\"\\n        app:centre_button_icon=\"@drawable/my_drawable\"\\n        app:space_item_icon_size=\"@dimen/space_item_icon_default_size\"\\n        app:space_item_icon_only_size=\"@dimen/space_item_icon_only_size\"\\n        app:space_item_text_size=\"@dimen/space_item_text_default_size\" />\\n```\\n\\n|  Attribute | Description |\\n|---|---|\\n| active_item_color  | item color when selected |\\n| inactive_item_color |  item color when unselected |\\n| centre_button_color | centre circle button color |\\n| space_background_color | space view background color |\\n| space_item_icon_size | item icon size |\\n| space_item_icon_only_size | item icon size on ```showIconOnly()``` mode |\\n| space_item_text_size | item text size |\\n| centre_button_icon | allow changing center icon from layout |\\n\\nChange space navigation background\\n```java\\nspaceNavigationView.setSpaceBackgroundColor(ContextCompat.getColor(this, R.color.yourColor));\\n```\\n\\nChange centre button icon \\n```java\\nspaceNavigationView.setCentreButtonIcon(R.drawable.yourDrawable);\\n```\\n\\nChange centre button background color \\n```java\\nspaceNavigationView.setCentreButtonColor(ContextCompat.getColor(this, R.color.yourColor));\\n```\\n\\nChange selected item text and icon color\\n```java\\nspaceNavigationView.setActiveSpaceItemColor(ContextCompat.getColor(this, R.color.yourColor));\\n```\\n\\nChange unselected item text and icon color\\n```java\\nspaceNavigationView.setInActiveSpaceItemColor(ContextCompat.getColor(this, R.color.yourColor));\\n```\\n\\nChange space item icon size\\n```java\\nspaceNavigationView.setSpaceItemIconSize((int) getResources().getDimension(R.dimen.yourDimen));\\n```\\n\\nChange space item icon size when ```showIconOnly();``` mode activated\\n```java\\nspaceNavigationView.setSpaceItemIconSizeInOnlyIconMode((int) getResources().getDimension(R.dimen.yourDimen));\\n```\\n\\nChange space item text size\\n```java\\nspaceNavigationView.setSpaceItemTextSize((int) getResources().getDimension(R.dimen.yourDimen));\\n```\\n\\nHide items text and show only icons\\n```java\\nspaceNavigationView.showIconOnly();\\n```\\n![](screens/screen2.png)\\n\\nHide items icon and show only texts\\n```java\\nspaceNavigationView.showTextOnly();\\n```\\n![](screens/screen5.png)\\n\\nYou can change selected item programmatically\\n```java\\nspaceNavigationView.changeCurrentItem(int tabIndexToSelect);\\n```\\n\\nShow badge\\n```java\\nspaceNavigationView.showBadgeAtIndex(int itemIndexToShowBadge, int badgeCountText, int badgeBackgroundColor);\\n```\\n![](screens/gif1.gif)\\n\\nHide badge at index\\n```java\\nspaceNavigationView.hideBadgeAtIndex(int itemIndexToHideBadge);\\n```\\n![](screens/gif2.gif)\\n\\nHide all badges\\n```java\\nspaceNavigationView.hideAllBadges();\\n```\\n\\nChange badge text\\n```java\\nspaceNavigationView.changeBadgeTextAtIndex(int itemIndexToChangeBadge, int badgeCountText);\\n```\\n\\nSet your custom font\\n```java\\nspaceNavigationView.setFont(Typeface.createFromAsset(getAssets(), \"your_cutom_font.ttf\"));\\n```\\n\\nSet centre button pressed state color\\n```java\\nspaceNavigationView.setCentreButtonRippleColor(ContextCompat.getColor(this, R.color.yourColor));\\n```\\n\\nNow you can change centre button icon if space navigation view already set up\\n```java\\nspaceNavigationView.changeCenterButtonIcon(R.drawable.yourDrawable);\\n```\\n\\nAlso you can change item text and icon  if space navigation view already set up\\n```java\\nspaceNavigationView.changeItemTextAtPosition(0, \"NEW TEXT\");\\nspaceNavigationView.changeItemIconAtPosition(1, R.drawable.yourDrawable);\\n```\\n\\nNow you can change space navigation view background color if it already set up\\n```java\\n spaceNavigationView.changeSpaceBackgroundColor(ContextCompat.getColor(context,R.color.yourColor));\\n```\\n![](screens/gif3.gif)\\n\\nIf you want to show full badge text or show 9+\\n```java\\nspaceNavigationView.shouldShowFullBadgeText(true);\\n```\\n\\nSet centre button icon color\\n```java\\nspaceNavigationView.setCentreButtonIconColor(ContextCompat.getColor(context,R.color.yourColor));\\n```\\nIf you want to disable default white color filter, just call\\n```java\\nspaceNavigationView.setCentreButtonIconColorFilterEnabled(false);\\n```\\n\\nAdd recycler view scroll behavior\\n```groovy\\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<android.support.design.widget.CoordinatorLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\\n    xmlns:app=\"http://schemas.android.com/apk/res-auto\"\\n    xmlns:tools=\"http://schemas.android.com/tools\"\\n    android:id=\"@+id/main_content\"\\n    android:layout_width=\"match_parent\"\\n    android:layout_height=\"match_parent\"\\n    tools:context=\"com.luseen.spacenavigationview.MainActivity\">\\n\\n    <android.support.v7.widget.RecyclerView\\n        android:id=\"@+id/recyclerView\"\\n        android:layout_width=\"match_parent\"\\n        android:layout_height=\"match_parent\" />\\n\\n    <com.luseen.spacenavigation.SpaceNavigationView\\n        android:id=\"@+id/space\"\\n        android:layout_width=\"match_parent\"\\n        android:layout_height=\"wrap_content\"\\n        android:layout_gravity=\"bottom\"\\n        app:layout_behavior=\"com.luseen.spacenavigation.SpaceNavigationViewBehavior\" />\\n</android.support.design.widget.CoordinatorLayout>\\n```\\n\\n[10]: https://github.com/armcha/Space-Navigation-View/issues/16\\n[11]: https://github.com/armcha/Space-Navigation-View/issues/18\\n[12]: https://github.com/armcha/Space-Navigation-View/issues/17\\n[13]: https://github.com/armcha/Space-Navigation-View/issues/25\\n[14]: https://github.com/armcha/Space-Navigation-View/issues/29\\n[16]: https://github.com/armcha/Space-Navigation-View/issues/34\\n[17]: https://github.com/armcha/Space-Navigation-View/issues/32\\n[20]: https://github.com/armcha/Space-Navigation-View/issues/41\\n[15]: https://github.com/ankitpopli1891\\n[18]: https://github.com/akiraspeirs\\n[19]: https://github.com/nextdimension\\n\\n##Versions\\n\\n##1.6.0\\n* Added saving translation height on rotation. Thanks to [akiraspeirs][18]\\n* Fixed requestLayout being improperly called. Thanks to [akiraspeirs][18]\\n* Fixed inActiveCentreButtonIconColor not being used initially. Thanks to [nextdimension][19]\\n* Fixed issue [#41][20]\\n\\n## 1.5.0\\n* Added SpaceNavigationViewBehavior\\n* Fixed issue [#32][17]\\n\\n## 1.4.2\\n* Fixed issue [#34][16]\\n\\n## 1.4.1\\n* Changing center icon from layout\\n* Fixed issue [#29][14] Thanks to [ankitpopli1891][15]\\n\\n## 1.4.0\\n* Added method do disable centre button default color filter\\n* Fixed issue [#25][13]\\n\\n## 1.3.2\\n* Added method setCentreButtonIconColor [#17][12]\\n\\n## 1.3.1\\n* Added method shouldShowFullBadgeText\\n* Fixed issue [#16][10] , [#18][11]\\n\\n## 1.3.0\\n* Added SpaceOnLongClickListener\\n* Added changeSpaceBackgroundColor method\\n* Fixed rendering problem when view is in edit mode\\n\\n## 1.2.0\\n* Fixed centre button issue\\n* Added API 14+ support\\n\\n## 1.1.0 \\n* Added ```changeItemTextAtPosition, changeItemIconAtPosition, changeCenterButtonIcon,setCentreButtonRippleColor``` methods\\n* Now you can set onItemReselect listener\\n\\n## 1.0.0\\n* Initial release\\n\\n## Apps using the Space Navigation View\\n* [Book Share - Share Ebooks and files](https://play.google.com/store/apps/details?id=com.rvnd.bookshare)\\n* [WiFi FTP Server +File Transfer](https://play.google.com/store/apps/details?id=com.transfer.file)\\n\\nKindly please let me know if you used or planning to use the library in your projects\\n\\n##Project development\\nSome crazy [pics][8]\\n[8]: https://github.com/armcha/Space-Navigation-View/tree/master/development\\n\\n## Contact \\n\\nPull requests are more than welcome.\\nPlease fell free to contact me if there is any problem when using the library.\\n\\n- **Email**: armcha01@gmail.com\\n- **Facebook**: https://web.facebook.com/chatikyana\\n- **Twitter**: https://twitter.com/ArmanChatikyan\\n- **Google +**: https://plus.google.com/112011638040018774140\\n- **Website**: https://armcha.github.io/\\n\\nLicense\\n--------\\n\\n\\n      Space Navigation library for Android\\n      Copyright (c) 2016 Arman Chatikyan (https://github.com/armcha/Space-Navigation-View).\\n      \\n      Licensed under the Apache License, Version 2.0 (the \"License\");\\n      you may not use this file except in compliance with the License.\\n      You may obtain a copy of the License at\\n\\n         http://www.apache.org/licenses/LICENSE-2.0\\n\\n      Unless required by applicable law or agreed to in writing, software\\n      distributed under the License is distributed on an \"AS IS\" BASIS,\\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n      See the License for the specific language governing permissions and\\n      limitations under the License.\\n    \\n'},\n",
       " {'repo': 'MicrosoftDocs/mslearn-tailspin-spacegame-web-models',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': \"\\r\\n# Contributing\\r\\n\\r\\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\\r\\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\\r\\nthe rights to use your contribution. For details, visit https://cla.microsoft.com.\\r\\n\\r\\nWhen you submit a pull request, a CLA-bot will automatically determine whether you need to provide\\r\\na CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions\\r\\nprovided by the bot. You will only need to do this once across all repos using our CLA.\\r\\n\\r\\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\\r\\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\\r\\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\\r\\n\\r\\n# Legal Notices\\r\\n\\r\\nMicrosoft and any contributors grant you a license to the Microsoft documentation and other content\\r\\nin this repository under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode),\\r\\nsee the [LICENSE](LICENSE) file, and grant you a license to any code in the repository under the [MIT License](https://opensource.org/licenses/MIT), see the\\r\\n[LICENSE-CODE](LICENSE-CODE) file.\\r\\n\\r\\nMicrosoft, Windows, Microsoft Azure and/or other Microsoft products and services referenced in the documentation\\r\\nmay be either trademarks or registered trademarks of Microsoft in the United States and/or other countries.\\r\\nThe licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks.\\r\\nMicrosoft's general trademark guidelines can be found at http://go.microsoft.com/fwlink/?LinkID=254653.\\r\\n\\r\\nPrivacy information can be found at https://privacy.microsoft.com/en-us/\\r\\n\\r\\nMicrosoft and any contributors reserve all other rights, whether under their respective copyrights, patents,\\r\\nor trademarks, whether by implication, estoppel or otherwise.\\r\\n\"},\n",
       " {'repo': 'Advanced-Rocketry/AdvancedRocketry',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '# AdvancedRocketry\\r\\nAdvanced Rocketry is a mod about space, exploration, and resources\\r\\n\\r\\n<b>Advanced Rocketry does not support anything other than Forge at this point - be it Fabric or Sponge </b> \\r\\n\\r\\n<b>Please be aware Sponge compatibility issues may never be fixed.</b>\\r\\n\\r\\nDiscord link: https://discord.gg/BbBUmbX\\r\\n\\r\\nCurrent Features:\\r\\n\\r\\n\\r\\n\\r\\n - Rockets that can travel between planets & moons\\r\\n   - Rockets can be configured to use or not use fuel (use fuel by default)\\r\\n   - Can be built from almost any block\\r\\n   - Can be upgraded with different propulsion options\\r\\n - Basic autogenerated planets\\r\\n   - Generated randomly from planet temperature & atmosphere\\r\\n   - Mix of gas giants and rocky planets\\r\\n - Advanced XML options for custom planet creation\\r\\n   - Features all Advanced Rocketry options\\r\\n   - Allows mapping other mods dimensions to Advanced Rocketry planets\\r\\n - Black holes that you can use for power\\r\\n - Space stations\\r\\n   - Can orbit any planet or moon\\r\\n   - Can choose they amount of light the station gets contstantly by rotation\\r\\n - Warp ships, capable of going outside planet/moon and sun/planet systems\\r\\n   - Planet Selection Guidance\\r\\n   - Research system that allows for undiscovered planets\\r\\n     - Requires data and artifact to be able to find planets\\r\\n     - Active beacons also make planets visible\\r\\n - Basic machinery\\r\\n - Many types of hazardous atmospheres\\r\\n   - High-Pressure atmospheres \\r\\n   - Very High Pressure atmospheres\\r\\n   - Hot atmospheres\\r\\n   - Superheated atmospheres\\r\\n   - Vacuum\\r\\n   - Low-Oxygen atmospheres\\r\\n - Data collection of planets (not currently fully featured)\\r\\n - Gravity generators for space stations\\r\\n - Gravity generators for local areas on planets\\r\\n - Satellite system\\r\\n   - Data collection satellites\\r\\n     - Can collect three types of data\\r\\n     - Data is used for planetary discovery & asteroid mining\\r\\n   - Oribtal solar power satellites\\r\\n     - Used with the microwave reciever multiblock for power\\r\\n   - Ore Scanning Satellite\\r\\n   - Biome changing satellites\\r\\n - Asteroid mining\\r\\n   - Automated Harvesting\\r\\n   - Research System\\r\\n      - Random asteroid parameters (size, composition, richness)\\r\\n      - Research can be done to determine properties\\r\\n   - Manual Harvesting\\r\\n      - Player can select as a destination\\r\\n      - temporary dim in created, destroyed when last player leaves\\r\\n - Gas Giant gas/fluid mining\\r\\n - Railguns to transfer goods between planets and stations\\r\\n - Support GTEU, EU, FE, Tesla, and RF as power supplies\\r\\n - Terraforming\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFuture Features:\\r\\n\\r\\n - Rovers (maybe? planetary exploration?)\\r\\n - Planetary Mapping System (feasible?)\\r\\n - More Satellites\\r\\n   - Weather Control Satellite\\r\\n   - Mapping satellite (feasible?)\\r\\n - Clean Rooms (maybe)\\r\\n - Stations can be positioned over certain locations on planets\\r\\n - Telescopes - \\r\\n   - Water Detectors (increases info on %dry) ~ space only\\r\\n   - Chemical Detectors (increases chance of finding planets rich in certain ores) ~ space only\\r\\n   - Atmospheric Detectors (increases info on pressure; when combined with chemical can detect if planet has harvestable/hazardous gasses) ~ ground and space \\r\\n   - Temperature Detector (increases info on planetary temperature) ~ space only\\r\\n   - Star Scanner (increases the likelihood of finding planets orbiting a star; higher level allows for smaller planets) ~ ground and Space\\r\\n   - Radio interferometry (lets you discover black holes and get any data quickly)  ~ ground only\\r\\n - Possibility of rockets requiring life support for the player, without which they die\\r\\n - Larger rocket engines\\r\\n   - T3 engines for nuclear and bipropellant\\r\\n   - _Possible_ T4 engine that is a mini fusion reactor, but unlikely. Up to discrection\\r\\n - Warp core rework\\r\\n   - Requires data to function\\r\\n   - More data can be used to speed the travel time, past the minimum (maybe)\\r\\n   - Model redo + possibly some auxilary models for auxilary computation banks required for longer jumps\\r\\n - Terraformer rework\\r\\n   - Completely redo model and concept\\r\\n   - Split terraformer up into a large center with multiple required buildings\\r\\n   - Up gas costs signficantly\\r\\n'},\n",
       " {'repo': 'Advanced-Rocketry/AdvancedRocketry',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '# AdvancedRocketry\\r\\nAdvanced Rocketry is a mod about space, exploration, and resources\\r\\n\\r\\n<b>Advanced Rocketry does not support anything other than Forge at this point - be it Fabric or Sponge </b> \\r\\n\\r\\n<b>Please be aware Sponge compatibility issues may never be fixed.</b>\\r\\n\\r\\nDiscord link: https://discord.gg/BbBUmbX\\r\\n\\r\\nCurrent Features:\\r\\n\\r\\n\\r\\n\\r\\n - Rockets that can travel between planets & moons\\r\\n   - Rockets can be configured to use or not use fuel (use fuel by default)\\r\\n   - Can be built from almost any block\\r\\n   - Can be upgraded with different propulsion options\\r\\n - Basic autogenerated planets\\r\\n   - Generated randomly from planet temperature & atmosphere\\r\\n   - Mix of gas giants and rocky planets\\r\\n - Advanced XML options for custom planet creation\\r\\n   - Features all Advanced Rocketry options\\r\\n   - Allows mapping other mods dimensions to Advanced Rocketry planets\\r\\n - Black holes that you can use for power\\r\\n - Space stations\\r\\n   - Can orbit any planet or moon\\r\\n   - Can choose they amount of light the station gets contstantly by rotation\\r\\n - Warp ships, capable of going outside planet/moon and sun/planet systems\\r\\n   - Planet Selection Guidance\\r\\n   - Research system that allows for undiscovered planets\\r\\n     - Requires data and artifact to be able to find planets\\r\\n     - Active beacons also make planets visible\\r\\n - Basic machinery\\r\\n - Many types of hazardous atmospheres\\r\\n   - High-Pressure atmospheres \\r\\n   - Very High Pressure atmospheres\\r\\n   - Hot atmospheres\\r\\n   - Superheated atmospheres\\r\\n   - Vacuum\\r\\n   - Low-Oxygen atmospheres\\r\\n - Data collection of planets (not currently fully featured)\\r\\n - Gravity generators for space stations\\r\\n - Gravity generators for local areas on planets\\r\\n - Satellite system\\r\\n   - Data collection satellites\\r\\n     - Can collect three types of data\\r\\n     - Data is used for planetary discovery & asteroid mining\\r\\n   - Oribtal solar power satellites\\r\\n     - Used with the microwave reciever multiblock for power\\r\\n   - Ore Scanning Satellite\\r\\n   - Biome changing satellites\\r\\n - Asteroid mining\\r\\n   - Automated Harvesting\\r\\n   - Research System\\r\\n      - Random asteroid parameters (size, composition, richness)\\r\\n      - Research can be done to determine properties\\r\\n   - Manual Harvesting\\r\\n      - Player can select as a destination\\r\\n      - temporary dim in created, destroyed when last player leaves\\r\\n - Gas Giant gas/fluid mining\\r\\n - Railguns to transfer goods between planets and stations\\r\\n - Support GTEU, EU, FE, Tesla, and RF as power supplies\\r\\n - Terraforming\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFuture Features:\\r\\n\\r\\n - Rovers (maybe? planetary exploration?)\\r\\n - Planetary Mapping System (feasible?)\\r\\n - More Satellites\\r\\n   - Weather Control Satellite\\r\\n   - Mapping satellite (feasible?)\\r\\n - Clean Rooms (maybe)\\r\\n - Stations can be positioned over certain locations on planets\\r\\n - Telescopes - \\r\\n   - Water Detectors (increases info on %dry) ~ space only\\r\\n   - Chemical Detectors (increases chance of finding planets rich in certain ores) ~ space only\\r\\n   - Atmospheric Detectors (increases info on pressure; when combined with chemical can detect if planet has harvestable/hazardous gasses) ~ ground and space \\r\\n   - Temperature Detector (increases info on planetary temperature) ~ space only\\r\\n   - Star Scanner (increases the likelihood of finding planets orbiting a star; higher level allows for smaller planets) ~ ground and Space\\r\\n   - Radio interferometry (lets you discover black holes and get any data quickly)  ~ ground only\\r\\n - Possibility of rockets requiring life support for the player, without which they die\\r\\n - Larger rocket engines\\r\\n   - T3 engines for nuclear and bipropellant\\r\\n   - _Possible_ T4 engine that is a mini fusion reactor, but unlikely. Up to discrection\\r\\n - Warp core rework\\r\\n   - Requires data to function\\r\\n   - More data can be used to speed the travel time, past the minimum (maybe)\\r\\n   - Model redo + possibly some auxilary models for auxilary computation banks required for longer jumps\\r\\n - Terraformer rework\\r\\n   - Completely redo model and concept\\r\\n   - Split terraformer up into a large center with multiple required buildings\\r\\n   - Up gas costs signficantly\\r\\n'},\n",
       " {'repo': 'spacedeck/spacedeck-open',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Spacedeck Open\\n\\n![Spacedeck 6.0 Screenshot](/public/images/sd6-screenshot.png)\\n\\nThis is the free and open source version of Spacedeck, a web based, real time, collaborative whiteboard application with rich media support. Spacedeck was developed in 6 major releases during Autumn 2011 until the end of 2016 and was originally a commercial SaaS. The developers were Lukas F. Hartmann (mntmn) and Martin Güther (magegu).\\n\\nThe spacedeck.com online service was shut down on May 1st 2018. We decided to open-source Spacedeck to allow educational and other organizations who currently rely on Spacedeck to migrate to a self-hosted or local version.\\n\\n[MNT Research GmbH](https://mntre.com) has restarted development of Spacedeck Open in 2020.\\n\\nWe appreciate filed issues, pull requests and general discussion.\\n\\n# Features\\n\\n- Create virtual whiteboards called _Spaces_ with virtually unlimited size\\n- Drag & drop images, videos and audio from your computer or the web\\n- Write and format text with full control over fonts, colors and style\\n- Draw, annotate and highlight with included graphical shapes\\n- Turn your Space into a zooming presentation\\n- Collaborate in realtime with teammates, students or friends\\n- Share Spaces on the web or via email\\n- Export your work as printable PDF or ZIP (currently being fixed, stay tuned)\\n\\n# Use Cases\\n\\n- Education: Virtual classwork with multimedia\\n- Creative: Mood boards, Brainstorming, Design Thinking\\n- Visual note taking and planning\\n\\n# Requirements, Installation\\n\\nSpacedeck requires:\\n\\n- Node.js 10.x: Web Server / API. Download: https://nodejs.org\\n- Graphicsmagick. On non-Linux, Download: http://www.graphicsmagick.org/ On Linux, install via package manager.\\n- Optionally ffmpeg, audiowaveform and ghostscript. See \"Optional Dependencies\" below.\\n\\nTo run Spacedeck, you only need Node.JS 10.x.\\n\\nTo install all node dependencies, run (do this once) after cloning the repository:\\n\\n    npm install\\n\\n# Configuration\\n\\nSee [config/default.json](config/default.json). Set `storage_local_path` for a local sqlite database or `storage_region`, `storage_bucket`, `storage_cdn` and `storage_endpoint` for AWS S3. `mail_provider` may be one of `console` or `smtp`. Also, omit a trailing `/` for the `endpoint`.\\n\\n## Disable DB logs\\n\\n```json\\n...\\n\"db_logs_disabled\": true\\n...\\n```\\n\\n## Configure color swatches\\n\\nAdd a custom array of swatches to your config/default.json.\\n\\n**You should include the swatch transparent (rgba(0,0,0,0)) so users can remove the color applied.**\\n\\n## Configure default colors\\n\\nYou can define text, stroke and fill color in your config/default.json.\\n\\n**You also should include the default colors in your custom swatches palette.**\\n\\n```json\\n...\\n\"spacedeck\": {\\n  \"default_text_color\": \"#E11F26\",\\n  \"default_stroke_color\": \"#9E0F13\",\\n  \"default_fill_color\": \"#64BCCA\",\\n  \"swatches\": [\\n    {\"id\":8, \"hex\":\"#000000\"},\\n    {\"id\":30, \"hex\":\"rgba(0,0,0,0)\"},\\n    {\"id\":31, \"hex\": \"#E11F26\"},\\n    {\"id\":32, \"hex\": \"#9E0F13\"},\\n    {\"id\":33, \"hex\": \"#64BCCA\"},\\n    {\"id\":34, \"hex\": \"#40808A\"},\\n    {\"id\":35, \"hex\": \"#036492\"},\\n    {\"id\":36, \"hex\": \"#005179\"},\\n    {\"id\":37, \"hex\": \"#84427E\"},\\n    {\"id\":38, \"hex\": \"#6C3468\"},\\n    {\"id\":39, \"hex\": \"#F79B84\"},\\n    {\"id\":40, \"hex\": \"#B57362\"},\\n    {\"id\":41, \"hex\": \"#E7D45A\"},\\n    {\"id\":42, \"hex\": \"#ACA044\"}\\n  ]\\n}\\n...\\n```\\n\\n# Run (web server)\\n\\n    node spacedeck.js\\n\\nThen open http://localhost:9666 in a web browser.\\n\\n# Optional Dependencies\\n\\nFor advanced media conversion:\\n\\n- ffmpeg and ffprobe for video/audio conversion. Download: https://www.ffmpeg.org/download.html\\n- audiowaveform for audio waveform rendering. Download: https://github.com/bbcrd/audiowaveform\\n- ghostscript for PDF import. Download: https://www.ghostscript.com/download/gsdnld.html\\n\\n# Data Storage\\n\\nBy default, media files are uploaded to the `storage` folder.\\nThe database is stored in `database.sqlite` by default.\\n\\n# Other databases (Not officially supported)\\n\\n## Postgres\\n\\nAdd the [pg](https://www.npmjs.com/package/pg) module and change the config/default.json to\\n\\n```\\n\"storage_dialect\": \"postgres\",\\n```\\n\\nAdapt the other values as needed\\n\\n```\\n\"storage_host\": \"localhost\",\\n\"storage_database\": \"spacedeck\",\\n\"storage_username\": \"username\",\\n\"storage_password\": \"password\",\\n```\\n\\n# Run with Docker\\n\\n- configure `config/default.json`\\n- adapt your `docker-compose.yml` if needed.\\n- start the container with `docker-compose up`\\n  (use `-d` for background process and `--build` for rebuilding the image)\\n\\n# Hacking\\n\\nTo rebuild the frontend CSS styles:\\n\\n    gulp styles\\n\\n# License\\n\\nThe Spacedeck logo and brand assets are registered trademarks of Spacedeck GmbH. All rights reserved.\\n\\nSpacedeck Open source code is released under the GNU Affero General Public License Version 3 (GNU AGPLv3).\\n\\n    Spacedeck Open - Web-based Collaborative Whiteboard For Rich Media\\n    Copyright (C) 2011-2018 Lukas F. Hartmann, Martin Güther\\n    Icons and original CSS design copyright by Thomas Helbig\\n\\n    This program is free software: you can redistribute it and/or modify\\n    it under the terms of the GNU Affero General Public License as\\n    published by the Free Software Foundation, either version 3 of the\\n    License, or (at your option) any later version.\\n\\n    This program is distributed in the hope that it will be useful,\\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n    GNU Affero General Public License for more details.\\n\\n    You should have received a copy of the GNU Affero General Public License\\n    along with this program.  If not, see <http://www.gnu.org/licenses/>.\\n'},\n",
       " {'repo': 'Andr3wHur5t/react-native-keyboard-spacer',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': \"[![NPM](https://nodei.co/npm-dl/react-native-keyboard-spacer.png?months=3&height=2)](https://nodei.co/npm/react-native-keyboard-spacer/)\\n\\n# react-native-keyboard-spacer\\n\\nPlug and play iOS react-native keyboard spacer view.\\n\\n![image](https://media.giphy.com/media/3oEjHJwLyYg7upTyYo/giphy.gif)\\n## Quick Start\\n\\nInstall View: `npm install --save react-native-keyboard-spacer`\\n\\n## Example Usage\\n\\nThe view will automatically perform a layout animation when the keyboard appears or disappears.\\n\\n```javascript\\nimport KeyboardSpacer from 'react-native-keyboard-spacer';\\nimport React, { Component } from 'react';\\nimport {\\n  AppRegistry,\\n  StyleSheet,\\n  Image,\\n  View,\\n  TextInput\\n} from 'react-native';\\n\\nclass DemoApp extends Component {\\n  render() {\\n    return (\\n      <View style={[{flex: 1}]}>\\n        {/* Some random image to show scaling */}\\n        <Image source={{uri: 'http://img11.deviantart.net/072b/i/2011/206/7/0/the_ocean_cherry_tree_by_tomcadogan-d41nzsz.png', static: true}}\\n                 style={{flex: 1}}/>\\n\\n        {/* The text input to put on top of the keyboard */}\\n        <TextInput style={{left: 0, right: 0, height: 45}}\\n              placeholder={'Enter your text!'}/>\\n\\n        {/* The view that will animate to match the keyboards height */}\\n        <KeyboardSpacer/>\\n      </View>\\n    );\\n  }\\n}\\n\\nAppRegistry.registerComponent('DemoApp', () => DemoApp);\\n```\\n### Properties - Basic\\n\\n| Prop  | Default  | Type | Description |\\n| :------------ |:---------------:| :---------------:| :-----|\\n| topSpacing | 0 | `number` | Add or subtract additional spacing from keyboard height |\\n| animationConfig | [A default animation](https://github.com/Andr3wHur5t/react-native-keyboard-spacer/blob/expose-layout-animations/KeyboardSpacer.js#L14) | `LayoutAnimationConfig` | [LayoutAnimation](https://facebook.github.io/react-native/docs/layoutanimation.html#content) configuration object |\\n\\n### Properties - Methods\\n\\n| Prop  | Params  | Type | Description |\\n| :------------ |:---------------:| :---------------:| :-----|\\n| onToggle | `toggleState` | `function` | onToggle method is called when when keyboard toggles. Two parameters passed through, keyboardState (boolean, true if keyboard shown) and keyboardSpace (height occupied by keyboard) |\\n\"},\n",
       " {'repo': 'HackerPoet/PySpace',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# PySpace\\nGLSL Fractal Ray Marcher in Python\\n\\n## Installation\\n\\n```bash\\npip install -r requirements.txt\\n```\\n\\n## Videos\\nOverview: https://youtu.be/svLzmFuSBhk\\n\\nExamples: https://youtu.be/N8WWodGk9-g\\n\\nODS Demo: https://youtu.be/yJyp7zEGKaU\\n'},\n",
       " {'repo': 'adobe-photoshop/spaces-design',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': 'Photoshop Design Space [![Build Status](https://travis-ci.org/adobe-photoshop/spaces-design.svg?branch=master)](https://travis-ci.org/adobe-photoshop/spaces-design)\\n=================\\n\\n**Design Space is a new way to use Photoshop.** By combining the power of Photoshop with the flexibility of HTML5, Design Space provides a streamlined user interface to Photoshop and new experiences tailored just for design workflows. \\n\\n**Design Space is a work in progress.** We\\'re still building essential features and experimenting with new ideas, and we want your help! Follow us on Twitter at [@psdesign](https://twitter.com/psdesign) for updates from the team. If you find a bug or have an idea for a new feature, please help us by [filing an issue](https://github.com/adobe-photoshop/spaces-design/issues/new). If you\\'re an adventurous developer, you can [download and hack the HTML5 sources](https://github.com/adobe-photoshop/spaces-design/wiki/Design-Space-Development-Setup), and even submit pull requests.\\n\\n![Photoshop Design Space (Preview)](https://s3.amazonaws.com/f.cl.ly/items/3207083Y1i1l0U2R3i2L/Image%202015-10-28%20at%203.49.09%20PM.png \"Photoshop Design Space (Preview)\")\\n\\nContributing\\n------------\\n\\nWe welcome your contributions! Please see [CONTRIBUTING.md](https://github.com/adobe-photoshop/spaces-design/blob/master/CONTRIBUTING.md) for more details.\\n\\nLicense\\n-------\\n\\n(MIT License)\\n\\nCopyright (c) 2014 Adobe Systems Incorporated. All rights reserved.\\n \\nPermission is hereby granted, free of charge, to any person obtaining a\\ncopy of this software and associated documentation files (the \"Software\"), \\nto deal in the Software without restriction, including without limitation \\nthe rights to use, copy, modify, merge, publish, distribute, sublicense, \\nand/or sell copies of the Software, and to permit persons to whom the \\nSoftware is furnished to do so, subject to the following conditions:\\n \\nThe above copyright notice and this permission notice shall be included in\\nall copies or substantial portions of the Software.\\n \\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, \\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER \\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING \\nFROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER \\nDEALINGS IN THE SOFTWARE.\\n\\n**Please see the [LICENSE](https://github.com/adobe-photoshop/spaces-design/blob/master/LICENSE) file at the root of the repository for licensing details on third-party code**\\n\\nThird-Party Code\\n----------------\\n\\nA list of third-party code used by this project is available at https://github.com/adobe-photoshop/spaces-design/wiki/Third-party-code\\n'},\n",
       " {'repo': 'micdoodle8/Galacticraft',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '<h2 align=\"center\">NOTICE</h2>\\n\\nThis project is continued with updates here --> [Galacticraft-Legacy](https://github.com/TeamGalacticraft/Galacticraft-Legacy)\\n\\nGalacticraft 5 is under development here --> [Galacticraft](https://github.com/TeamGalacticraft/Galacticraft)\\n\\n\\n<p align=\"center\"><img src=\"https://cloud.githubusercontent.com/assets/6842258/25335525/4f21552e-28eb-11e7-91bb-de5e1ef602da.jpg\"></p>\\n\\nGalacticraft [![Join our Discord!](https://img.shields.io/discord/449966345665249290.svg?color=blue&label=Discord&logo=discord&style=flat-square)](https://discord.gg/YBVne7R) [![Support the devs!](https://img.shields.io/badge/Patreon-Support-orange.svg?style=flat-square)](https://www.patreon.com/micdoodle8)\\n============\\n\\nAn advanced space exploration mod for Minecraft.\\n\\nFull documentation is at our wiki: [wiki.micdoodle8.com](https://wiki.micdoodle8.com/wiki/Galacticraft)\\n\\n------\\n\\n## Download Galacticraft\\n\\nDownload recent and latest Galacticraft for Minecraft 1.12.2, Minecraft 1.11.2, Minecraft 1.10.2, Minecraft 1.8.9 and Minecraft 1.7.10 at the [Galacticraft download site](https://micdoodle8.com/mods/galacticraft/downloads)\\n\\n------\\n\\n## Contributing\\n\\n### Reporting issues\\n\\nBefore reporting an issue, please read [Notes for Contributors](https://github.com/micdoodle8/Galacticraft/blob/master/CONTRIBUTING.md) and follow the six guidelines given.\\n\\n------\\n### Pull Requests\\n\\nPull Requests to help with Galacticraft development are extremely welcome. [Guide to making a PR](https://gist.github.com/radfast/7ea7577fe2c0fdae1ac90d4b26d6198c)\\n\\nFor Pull Requests to help with translation, see [Notes for Contributors](https://github.com/micdoodle8/Galacticraft/blob/master/CONTRIBUTING.md)\\n\\n------\\n\\n## Mod Developers\\n\\nAPI and deobfuscated version for developers is available through our [downloads](https://micdoodle8.com/mods/galacticraft/downloads) page.  Detailed information on how to set up a development environment for the source code - if you want to use either the Galacticraft API or the full sources - is [here](https://wiki.micdoodle8.com/wiki/GC3_API)\\n\\nTo build, run the command `gradlew build packCoreJar packPlanetsJar packMicCoreJar`\\n \\n------\\n \\n## Modpacks\\n\\nWe welcome inclusion of Galacticraft in modpacks.  [Modpack permission and setup tips](https://wiki.micdoodle8.com/wiki/Modpack_Permission)\\n\\n------\\n \\n## License\\n\\nGalacticraft is open source and free for everyone for non-commercial use.  (But distribution of Galacticraft in modified form is not permitted.)  Full license can be found [here](https://github.com/micdoodle8/Galacticraft/blob/master/LICENSE.txt)\\n'},\n",
       " {'repo': 'leerob/space-invaders',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Space Invaders\\n\\n[![Language](https://img.shields.io/badge/language-python-blue.svg?style=flat)](https://www.python.org)\\n[![Module](https://img.shields.io/badge/module-pygame-brightgreen.svg?style=flat)](http://www.pygame.org/news.html)\\n[![Release](https://img.shields.io/badge/release-v1.0-orange.svg?style=flat)](http://www.leejamesrobinson.com/space-invaders.html)\\n\\n## About\\n\\nSpace Invaders is a two-dimensional fixed shooter game in which the player controls a ship with lasers by moving it horizontally\\nacross the bottom of the screen and firing at descending aliens. The aim is to defeat five rows of ten aliens that move\\nhorizontally back and forth across the screen as they advance towards the bottom of the screen. The player defeats an alien,\\nand earns points, by shooting it with the laser cannon. As more aliens are defeated, the aliens\\' movement and the game\\'s music\\nboth speed up.\\n\\nThe aliens attempt to destroy the ship by firing at it while they approach the bottom of the screen. If they reach the bottom,\\nthe alien invasion is successful and the game ends. A special \"mystery ship\" will occasionally move across the top of the\\nscreen and award bonus points if destroyed. The ship is partially protected by several stationary defense bunkers that are\\ngradually destroyed by projectiles from the aliens and player.\\n\\n<img src=\"http://i.imgur.com/u2mss8o.png\" width=\"300\" height=\"240\" />\\n<img src=\"http://i.imgur.com/mR81p5O.png\" width=\"300\" height=\"240\"/>\\n\\n## How To Play\\n\\n- If you don\\'t have [Python](https://www.python.org/downloads/) or [Pygame](http://www.pygame.org/download.shtml) installed, you can simply double click the .exe file to play the game.\\n  **Note:** _The .exe file needs to stay in the same directory as the sounds, images, and font folders._\\n\\n- If you have the correct version of Python and Pygame installed, you can run the program in the command prompt / terminal.\\n\\n```bash\\ncd SpaceInvaders\\npython spaceinvaders.py\\n```\\n\\n**Note:** If you\\'re using Python 3, replace the command \"python\" with \"python3\"\\n\\n**MacOS Mojave**: You need to use Python 3.7.2 or greater: [Source](https://github.com/pygame/pygame/issues/555)\\n\\n## Demo\\n\\n[![Space Invaders](http://img.youtube.com/vi/_2yUP3WMDRc/0.jpg)](http://www.youtube.com/watch?v=_2yUP3WMDRc)\\n\\n## Notable Forks\\n\\n- [AI research project where four types of agents control the ship and play the game](https://github.com/scott-pickthorn/Space_Invaders)\\n- [NEAT program that evolves to beat the game](https://github.com/lairsonm/neat-in-space-invaders)\\n\\n## Contact\\n\\nThanks for checking out my game and I hope you enjoy it! Feel free to contact me.\\n\\n- Lee Robinson\\n- lrobinson2011@gmail.com\\n'},\n",
       " {'repo': 'dwmkerr/spaceinvaders',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Space Invaders\\n\\nThe classic Space Invaders game written in JavaScript as a learning exercise.\\n\\nNo jQuery or any other third party libraries, just raw JavaScript, CSS and HTML.\\n\\nSee it Live: [https://dwmkerr.github.io/spaceinvaders/](https://dwmkerr.github.io/spaceinvaders/)\\n\\n[![Space Invaders Screenshot](./screenshot.jpg \"Space Invaders Screenshot\")](https://dwmkerr.github.io/spaceinvaders/)\\n\\n## Intro\\n\\n[![Run on Repl.it](https://repl.it/badge/github/dwmkerr/spaceinvaders)](https://repl.it/github/dwmkerr/spaceinvaders)\\n\\nWhat\\'s there to say? It\\'s Space Invaders in JavaScript!\\n\\nCreate the game, give it a `div` to draw to, tell it when the keyboard is mashed and that\\'s all you need to add Space Invaders to a website.\\n\\nThis is a simple learning exercise, so the JavaScript is deliberate kept all one file. There\\'s no linting, testing, CI, or anything like that. If you want to see such patterns in front-end JavaScript, check out something like [angular-modal-service](https://github.com/dwmkerr/angular-modal-service).\\n\\n## Adding Space Invaders to a Web Page\\n\\nFirst, drop the `spaceinvaders.js` file into the website.\\n\\nNow add a canvas to the page.\\n\\n```html\\n<canvas id=\"gameCanvas\"></canvas>\\n```\\n\\nNext, add the Space Invaders game code. You create the game, initialise it with the canvas, start it and make sure you tell it when a key is pressed or released. That\\'s it!\\n\\n```html\\n<script>\\n//  Setup the canvas.\\nvar canvas = document.getElementById(\"gameCanvas\");\\ncanvas.width = 800;\\ncanvas.height = 600;\\n\\n//  Create the game.\\nvar game = new Game();\\n\\n//  Initialise it with the game canvas.\\ngame.initialise(canvas);\\n\\n//  Start the game.\\ngame.start();\\n\\n//  Listen for keyboard events.\\nvar pressedKeys = [];\\nwindow.addEventListener(\"keydown\", function keydown(e) {\\n  var keycode = window.event.keycode || e.which;\\n    if(!pressedKeys[keycode])\\n      pressedKeys[keycode] = true;\\n    //  Supress further processing of left/right/space (37/29/32)\\n    if(keycode == 37 || keycode == 39 || keycode == 32) {\\n      e.preventDefault();\\n    }\\n    game.keyDown(keycode);\\n});\\nwindow.addEventListener(\"keyup\", function keydown(e) {\\n  var keycode = window.event.keycode || e.which;\\n    if(pressedKeys[keycode])\\n      delete pressedKeys[keycode];\\n    game.keyUp(keycode);\\n});\\n</script>\\n```\\n\\n## References\\n\\nOther bits and pieces that are useful can be dropped here.\\n\\n- The sounds came from [http://www.classicgaming.cc/classics/spaceinvaders/sounds.php](http://www.classicgaming.cc/classics/spaceinvaders/sounds.php)\\n\\n## Publishing\\n\\nOn changes to the `master` branch, the GitHub Pages site will be automatically updated.\\n'},\n",
       " {'repo': 'skickar/SpaceAPI',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"# Humans In Space Tracker\\nA project using API's to find how many people are in space, who they are, and then track the spacecraft.\\n\\n\"},\n",
       " {'repo': 'attreyabhatt/Space-Invaders-Pygame',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': ''},\n",
       " {'repo': 'SublimeText/TrailingSpaces',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': 'Trailing Spaces\\n===============\\n\\nA [Sublime Text](http://www.sublimetext.com) plugin that allows you to…\\n\\n**highlight trailing spaces and delete them in a flash!**\\n\\n---\\n\\n- [Synopsis](#synopsis)\\n- [Installation](#installation)\\n\\t- [Alternative installation methods](#alternative-installation-methods)\\n\\t\\t- [From github](#from-github)\\n\\t\\t- [Manually](#manually)\\n- [Usage](#usage)\\n\\t- [Deletion](#deletion)\\n\\t- [Toggling highlighting](#toggling-highlighting)\\n- [Options](#options)\\n\\t- [Changing the highlighting color](#changing-the-highlighting-color)\\n\\t- [Keeping trailing spaces invisible](#keeping-trailing-spaces-invisible)\\n\\t- [Include Current Line](#include-current-line)\\n\\t- [Include Empty Lines](#include-empty-lines)\\n\\t- [Modified Lines Only](#modified-lines-only)\\n\\t- [Trim On Save](#trim-on-save)\\n\\t- [Save After Trim](#save-after-trim)\\n\\t- [Live Matching vs On-demand Matching](#live-matching-vs-on-demand-matching)\\n\\t- [Ignore Syntax](#ignore-syntax)\\n\\t- [For power-users only!](#for-power-users-only)\\n\\t\\t- [Disabled for large files](#disabled-for-large-files)\\n\\t\\t- [The matching pattern](#the-matching-pattern)\\n- [About Sublime Text\\'s built-in features](#about-sublime-texts-built-in-features)\\n\\nSynopsis\\n--------\\n\\nSublime Text provides a way to automate deletion of trailing spaces *upon file\\nsaving* (more on this at the end of this file). Depending on your settings, it\\nmay be more handy to just highlight them and/or delete them by hand, at any\\ntime. This plugin provides just that, and a *lot* of options to fine-tune the\\nway you want to decimate trailing spaces.\\n\\nInstallation\\n------------\\n\\nIt is available through\\n[Sublime Package Control](http://wbond.net/sublime_packages/package_control) and\\nthis is the recommended way of installation (brings configuration instructions,\\nautomatic updates with changelogs…).\\n\\n### Alternative installation methods\\n\\n#### From github\\n\\nYou can install from github if you want, although Package Control automates\\njust that. Go to your `Packages` directory (find out where it is by running\\n`Preferences: Browse Packages` from The _Command Palette_) and clone this repository:\\n\\n    git clone https://github.com/SublimeText/TrailingSpaces.git\\n\\n#### Manually\\n\\n[Download](https://github.com/SublimeText/TrailingSpaces/archive/master.zip)\\nthe plugin as a zip. Copy the *Trailing Spaces* directory to its location\\n(see prior section).\\n\\nUsage\\n-----\\n\\n### Deletion\\n\\nThe main feature you gain from using this plugin is that of deleting all\\ntrailing spaces in the currently edited document. In order to use this\\ndeletion feature, you may either:\\n\\n* click on \"Edit / Trailing Spaces / Delete\";\\n* bind the deletion command to a keyboard shortcut:\\n\\nTo add a key binding, open \"Preferences / Key Bindings - User\" and add:\\n\\n``` js\\n{ \"keys\": [\"ctrl+shift+t\"], \"command\": \"delete_trailing_spaces\" }\\n```\\n\\nWith this setting, pressing <kbd>Ctrl + Shift + t</kbd> will delete all\\ntrailing spaces at once in the current file! For OSX users, quoting wbond:\\n\"When porting a key binding across OSes, it is common for the ctrl key on\\nWindows and Linux to be swapped out for super on OS X\"\\n(eg. use \"super+shift+t\" instead).\\n\\n*Beware*: the binding from this example overrides the default ST\\'s mapping\\nfor reopening last closed file. You can look at the default bindings in\\n\"Preferences / Key Bindings - Default\".\\n\\n### Toggling highlighting\\n\\nAt any time, you can toggle highlighting on and off. You may either:\\n\\n- click on \"Edit / Trailing Spaces / Highlight Regions\"\\n- bind the toggling command to a keyboard shortcut:\\n\\n``` js\\n// I like \"d\", as in \"detect\" (overrides a default binding, though).\\n{ \"keys\": [\"ctrl+shift+d\"], \"command\": \"toggle_trailing_spaces\" }\\n```\\n\\nOptions\\n-------\\n\\nSeveral options are available to customize the plugin\\'s behavior. Those\\nsettings are stored in a configuration file, as JSON. You must use a specific\\nfile: Go to \"Preferences / Package Settings / Trailing Spaces / Settings\" to\\nadd you custom settings.\\n\\nA few of them are also accessible through the \"Edit / Trailing Spaces\" menu.\\nSometimes, editing a setting will require a fresh Sublime Text to be applied\\nproperly, so try relaunching ST before reporting an issue ;)\\n\\nAll settings are global (ie. applied to all opened documents).\\n\\n### Changing the highlighting color\\n\\n*Default: \"invalid\"*\\n\\nYou may change the highlighting color, providing a color scope name such as\\n \"error\", \"comment\"… just like that:\\n\\n``` js\\n{ \"highlight_color\": \"comment\" }\\n```\\n\\nThe scope should be defined in your current theme file. Here is a dummy,\\nfully-fledged example (feel free to cut irrelevant pieces for your settings)\\nof such a custom color scope:\\n\\n``` xml\\n<dict>\\n  <key>name</key>\\n  <string>Invalid - Illegal</string>\\n  <key>scope</key>\\n  <string>invalid.illegal</string>\\n  <key>settings</key>\\n  <dict>\\n    <key>background</key>\\n    <string>#F93232</string>\\n    <key>fontStyle</key>\\n    <string></string>\\n    <key>foreground</key>\\n    <string>#F9F2CE</string>\\n  </dict>\\n</dict>\\n```\\n\\nYou would then use the value of \"invalid.illegal\".\\n\\n### Keeping trailing spaces invisible\\n\\nYou can make trailing spaces \"invisible\" yet still rely on the deletion\\ncommand. To do that, set the highlight scope to an empty string:\\n\\n``` js\\n{ \"highlight_color\": \"\" }\\n```\\n\\nBeware: this is **not** the same as *disabling* the highlighting (see \"On-\\nDemand Matching\" below). With this setting, the plugin still runs when opening\\na file, and in the background afterwards; you just won\\'t see the trailing\\nspaces (they are being highlighted with a \"transparent\" color).\\n\\n### Include Current Line\\n\\n*Default: true*\\n\\nHighlighting of trailing spaces in the currently edited line can be annoying:\\neach time you are about to start a new word, the space you type is matched as\\na trailing spaces. Currently edited line can thus be ignored:\\n\\n``` js\\n{ \"include_current_line\": false }\\n```\\n\\nEven though the trailing spaces are not highlighted on this line, they are\\nstill internally matched and will be delete when firing the deletion command.\\n\\n### Include Empty Lines\\n\\n*Default: true*\\n\\nWhen firing the deletion command, empty lines are matched as trailing regions,\\nand end up being deleted. You can specifically ignore them:\\n\\n``` js\\n{ \"include_empty_lines\": false }\\n```\\n\\nThey will not be highlighted either.\\n\\n### Modified Lines Only\\n\\n*Default: false (reopen ST to update)*\\n\\nWhen firing the deletion command, trailing regions *in the entire document* are\\ndeleted. There are some use-cases when deleting trailing spaces *only on lines\\nyou edited* is smarter; for instance when commiting changes to some third-party\\nsource code.\\n\\nAt any time, you can change which area is covered when deleting trailing\\nregions. You may either:\\n\\n- click on \"Edit / Trailing Spaces / Modified Lines Only\"\\n- specify as a setting:\\n\\n``` js\\n{ \"modified_lines_only\": true }\\n```\\n\\nThere is also a command to toggle this feature on and off. You may thus define\\na key binding:\\n\\n``` js\\n{ \"keys\": [\"pick+a+shortcut\"], \"command\": \"toggle_trailing_spaces_modified_lines_only\" }\\n```\\n\\n### Trim On Save\\n\\n*Default: false*\\n\\nSetting this to `true` will ensure trailing spaces are deleted when you save\\nyour document. It abides by the other settings, such as *Modified Lines Only*.\\n\\n``` js\\n{ \"trim_on_save\": true }\\n```\\n\\n### Save After Trim\\n\\n*Default: false*\\n\\nYou may not want to always trim trailing spaces on save, but the other way\\naround could prove useful. Setting this to `true` will automatically save your\\ndocument after you fire the deletion command:\\n\\n``` js\\n{ \"save_after_trim\": true }\\n```\\n\\nIt is obviously ignored if *Trim On Save* is on.\\n\\n### Live Matching vs On-demand Matching\\n\\n*Default: true (reopen ST to update)*\\n\\nBy default, trailing regions are matched every time you edit the document, and\\nwhen you open it.\\n\\nThis feature is entirely optional and you may set it off: firing the deletion\\ncommand will cause the trailing spaces to be deleted as expected even though\\nthey were not matched prior to your request. If you are afraid of the plugin\\nto cause slowness (for instance, you already installed several *heavy*\\nplugins), you can disable live matching:\\n\\n``` js\\n{ \"enabled\": false }\\n```\\n\\nIn this case, for no trailing regions are matched until you request them to be\\ndeleted, no highlighting occurs—it is in fact disabled, regardless of your\\n\"scope\" setting. If you want to check the trailing spaces regions, you can\\ntoggle highlighting on and off. In this case, it may come in handy to define\\na binding for the toggling command. When \"On-demand Matching\" is on and some\\ntrailing spaces are highlighted, added ones will obviously not be. Toggling\\nhighlight off and on will refresh them.\\n\\n### Ignore Scope\\n\\n*Default: [\"text.find-in-files\", \"source.build_output\", \"source.diff\", \"text.html.markdown\"]*\\n\\nWith this option you can ignore lines being highlighted based on the scope of\\ntheir trailing region.\\n\\nIf at least one scope in the configured list matches a scope in the trailing\\nregion of the line, it won\\'t be highlighted.\\n\\nBy default, the scope under the mouse cursor is shown by pressing\\n`Option+Command+P` (OS X) or `Ctrl+Alt+Shift+P` (Windows, Linux)\\n\\n``` js\\n// Trailing spaces for Find Results, Build output, Diff and Markdown are ignored\\n{ \"scope_ignore\": [\"text.find-in-files\", \"source.build_output\", \"source.diff\", \"text.html.markdown\"] }\\n```\\n\\n### For power-users only!\\n\\n#### Disabled for large files\\n\\nThe plugin is disabled altogether for large files, for it may cause slowness.\\nThe default threshold is around 1 million of characters. This is\\nconfigurable (in \"File Settings - User\") and the unit is number of chars:\\n\\n``` js\\n{ \"file_max_size\": 1000}\\n```\\n\\n#### The matching pattern\\n\\n*Default: [ \\\\t]+*\\n\\nTrailing spaces are line-ending regions containing at least one simple space,\\ntabs, or both. This pattern should be all you ever need, but if you *do* want\\nto abide by another definition to cover edge-cases, go ahead:\\n\\n``` js\\n// *danger* will match newline chars and many other folks\\n\"regexp\": \"[\\\\\\\\s]+\"\\n```\\n\\nAbout Sublime Text\\'s built-in features\\n--------------------------------------\\n\\nTrailing Spaces is designed to be a drop-in replacement of the limited\\n*Trim Whitespace On Save* built-in feature. ST is indeed able to delete\\ntrailing spaces upon saving files, and maybe that\\'s all you need!\\n\\nIn order to enable this behavior, edit \"Preferences / Settings\"\\nto add the following:\\n\\n``` js\\n{ \"trim_trailing_white_space_on_save\": true }\\n```\\n\\nAs Trailing Spaces bypasses this setting, you will have to uninstall it to\\nbenefit from this setting.\\n\\nMade a little less obvious in the documentation are settings to showcase\\nwhitespaces (*not only trailing ones!*):\\n\\n``` js\\n{ \"draw_white_space\": \"all\" }\\n```\\n\\nand to ensure a newline is kept at end of file upon saving:\\n\\n``` js\\n{ \"ensure_newline_at_eof_on_save\": true }\\n```\\n\\nThe former will display *all* whitespaces in your files. There is another value\\nof \"selection\" which display whitespaces under (you got it) your current text\\nselection.\\n'},\n",
       " {'repo': 'zz85/space-radar',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': 'Space Radar Electron\\n====\\nSpaceRadar allows interactive visualization of disk space and memory. It currently supports Sunburst, Treemap, and Flamegraph charts.\\n\\nDownloads\\n==\\nDownload Mac and Windows at the [releases page](https://github.com/zz85/space-radar-electron/releases)\\n\\nFeatures\\n==\\n- space visualizations using sunburst and treemap charts\\n- previews visualization as disk is being scanned\\n- fast (completes disk scanner faster than du)\\n- cross platform (at least on Mac OS X and Windows)\\n- allow drilldown of directories\\n- breadcrumbs and navigation\\n- opens files and directories\\n- analyze disk contents from a remote server (see section [Reading from a file](#reading-file))\\n\\nScreenshots\\n==\\n![space-radar-4](https://cloud.githubusercontent.com/assets/314997/11022585/5c847364-869d-11e5-8079-0a16e7d747e4.gif)\\n\\n![screenshot 2015-11-09 04 45 27](https://cloud.githubusercontent.com/assets/314997/11022582/3cc0bc90-869d-11e5-85c2-e79a0bf7c27f.png)\\n\\n![screenshot 2015-11-09 04 45 36](https://cloud.githubusercontent.com/assets/314997/11022581/33822b50-869d-11e5-9fe6-2db6b7a81505.png)\\n\\n\\nReading from a file <a id=\"reading-file\"></a>\\n==\\nTo create a file to be read from use `du -ak`, for example:\\n- `du -ak /var/log /usr | gzip -c > /tmp/sizes.txt.gz`\\n- `du -ak /opt /home /tmp > /tmp/sizes.txt`\\n\\nCompressed files can be read directly. To detect them, the file name has to end with `.gz`.\\n\\nFuture Enhancements\\n==\\n- more target for scanning\\n- color by file types\\n- filter hidden files\\n- moar!!\\n- let me know what you think\\n\\nFuther Explorations\\n==\\n- More efficient memory usage\\n- More efficient scanning process\\n- 3D visualization\\n\\nHistory\\n==\\n\\nThis project started as quick prototype for me to test drive [electron](https://www.electronjs.org/) (& some es6 syntax), [d3.js](https://d3js.org) and for me to explore the question of \"what\\'s taking up my disk space\". Turns out writing a disk visualization app isn\\'t that simple as I dwell into figuring out how to make disk scanning not block the ui thread, ipc calls go faster, smoother rendering, lesser memory usage, more sensible interactions...\\n\\n\\nWhats New\\n==\\nV5\\n- Import from DU file\\n- Upgrade electron\\n- Flamegraphs (BETA)\\n- Directory Listview\\n- Update libs - Electron, D3\\n\\nV4\\n- Treemap view\\n- Memory monitoring\\n- Mac App look using [Photon](http://photonkit.com)\\n- Context Menus for locating + opening + deleting files / directories\\n- Navigation controls (back/fwd/up)\\n- Switched disk scanning jobs to invisible renderer process\\n\\nVersion 3\\n- App icon finally! Thanks [Jill](http://jilln.com/) for the help with this :)\\n- Many Bug fixes\\n- Disk scanning is moved to a webview process\\n- Investigated various RPC methods. Now uses LocalStorage + FileSystem IPC message passing\\n- Reduce memory usage (and Electron crashes) by not caching key paths\\n- Tested on > 100GB & 2M files\\n- Improvements to user interactivity esp on hover states\\n- To prevent renderer process from hitting heap mem limit (1.5GB), all previous data is null, with dom elements removed to reduce memory pressure\\n- Allow target selection for disk usage scanning\\n- Locate path in Finder\\n- Env Debug Flags\\n\\nVersion 2\\n- Major speed up scanning directories. About 10x from version 1, and almost as fast or faster than du.\\n- Runs disk scanning as a separate headless renderer process\\n- Json is passed back via IPC\\n- Remove Async npm dependency\\n\\nIssues\\n==\\nPlease raise on [github issue tracker](https://github.com/zz85/space-radar-electron/issues) or contact [@blurspline on twitter](http://twitter.com/blurspline)\\n\\nDevelopment\\n==\\n\\nRun\\n\\n```\\nnpm run debug\\n```\\n\\nor\\n\\n```\\nnpm run app\\n```\\n\\nCheck that dependencies are installed, otherwise run (this may take awhile for electron binaries)\\n\\n```\\nnpm run install\\n```\\n\\nThanks\\n==\\n- [Jill](http://jilln.com/) for designing the app logo\\n- Jianwei for his comments on the app\\n- Chee Aun for helping alpha test the app\\n- WM for his talk on Electron that got me started\\n- [Contributors](https://github.com/zz85/space-radar/graphs/contributors)\\n'},\n",
       " {'repo': 'jeyoder/StuffInSpace',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': ''},\n",
       " {'repo': 'adjidieng/ETM',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# ETM\\n\\nThis is code that accompanies the paper titled \"Topic Modeling in Embedding Spaces\" by Adji B. Dieng, Francisco J. R. Ruiz, and David M. Blei. (Arxiv link: https://arxiv.org/abs/1907.04907)\\n\\nETM defines words and topics in the same embedding space. The likelihood of a word under ETM is a Categorical whose natural parameter is given by the dot product between the word embedding and its assigned topic\\'s embedding. ETM is a document model that learns interpretable topics and word embeddings and is robust to large vocabularies that include rare words and stop words.\\n\\n## Dependencies\\nThe major project dependency are :\\n\\n+ python 3.6.7\\n+ pytorch 1.1.0\\n\\nWith or without a virtual environment install you can install the other project requirements with: \\n\\n`pip install -r requirement.txt`\\n## Datasets\\n\\nAll the datasets are pre-processed and can be found below:\\n\\n+ https://bitbucket.org/franrruiz/data_nyt_largev_4/src/master/\\n+ https://bitbucket.org/franrruiz/data_nyt_largev_5/src/master/\\n+ https://bitbucket.org/franrruiz/data_nyt_largev_6/src/master/\\n+ https://bitbucket.org/franrruiz/data_nyt_largev_7/src/master/\\n+ https://bitbucket.org/franrruiz/data_stopwords_largev_2/src/master/ (this one contains stop words and was used to showcase robustness of ETM to stop words.)\\n+ https://bitbucket.org/franrruiz/data_20ng_largev/src/master/\\n\\nAll the scripts to pre-process a given dataset for ETM can be found in the folder \\'scripts\\'. The script for 20NewsGroup is self-contained as it uses scikit-learn. If you want to run ETM on your own dataset, follow the script for New York Times (given as example) called data_nyt.py  \\n\\n## To Run\\n\\nTo learn interpretable embeddings and topics using ETM on the 20NewsGroup dataset, run\\n```\\npython main.py --mode train --dataset 20ng --data_path data/20ng --num_topics 50 --train_embeddings 1 --epochs 1000\\n```\\n\\nTo evaluate perplexity on document completion, topic coherence, topic diversity, and visualize the topics/embeddings run\\n```\\npython main.py --mode eval --dataset 20ng --data_path data/20ng --num_topics 50 --train_embeddings 1 --tc 1 --td 1 --load_from CKPT_PATH\\n```\\n\\nTo learn interpretable topics using ETM with pre-fitted word embeddings (called Labelled-ETM in the paper) on the 20NewsGroup dataset:\\n\\n+ first fit the word embeddings. For example to use simple skipgram you can run\\n```\\npython skipgram.py --data_file PATH_TO_DATA --emb_file PATH_TO_EMBEDDINGS --dim_rho 300 --iters 50 --window_size 4 \\n```\\n\\n+ then run the following \\n```\\npython main.py --mode train --dataset 20ng --data_path data/20ng --emb_path PATH_TO_EMBEDDINGS --num_topics 50 --train_embeddings 0 --epochs 1000\\n```\\n\\n## Citation\\n\\n```\\n@article{dieng2019topic,\\n  title={Topic modeling in embedding spaces},\\n  author={Dieng, Adji B and Ruiz, Francisco J R and Blei, David M},\\n  journal={arXiv preprint arXiv:1907.04907},\\n  year={2019}\\n}\\n```\\n\\n'},\n",
       " {'repo': 'bojone/SPACES',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# SPACES\\n端到端的长文本摘要模型（法研杯2020司法摘要赛道）。\\n\\n博客介绍：https://kexue.fm/archives/8046\\n\\n## 含义\\n\\n我们将我们的模型称为SPACES，它正好是科学空间的域名之一（[https://spaces.ac.cn](https://spaces.ac.cn)），具体含义如下：\\n- **S**：Sparse Softmax；\\n- **P**：Pretrained Language Model；\\n- **A**：Abstractive；\\n- **C**：Copy Mechanism；\\n- **E**：Extractive；\\n- **S**：Special Words。\\n\\n顾名思义，这是一个以词为单位的、包含预训练和Copy机制的“抽取-生成”式摘要模型，里边包含了一些我们对文本生成技术的最新研究成果。\\n\\n## 运行\\n\\n实验环境：tensorflow 1.14 + keras 2.3.1 + bert4keras 0.9.7\\n\\n(如果是Windows，请用bert4keras>=0.9.8)\\n\\n首先请在`snippets.py`中修改相关路径配置，然后再执行下述代码。\\n\\n训练代码：\\n```bash\\n#! /bin/bash\\n\\npython extract_convert.py\\npython extract_vectorize.py\\n\\nfor ((i=0; i<15; i++));\\n    do\\n        python extract_model.py $i\\n    done\\n\\npython seq2seq_convert.py\\npython seq2seq_model.py\\n```\\n\\n预测代码\\n```python\\nfrom final import *\\nsummary = predict(text, topk=3)\\nprint(summary)\\n```\\n\\n## 交流\\n\\nQQ交流群：808623966，微信群请加机器人微信号spaces_ac_cn\\n\\n## 链接\\n\\n- 博客：https://kexue.fm\\n- 追一：https://zhuiyi.ai/\\n- 预训练模型：https://github.com/ZhuiyiTechnology/pretrained-models\\n- WoBERT：https://github.com/ZhuiyiTechnology/WoBERT\\n'},\n",
       " {'repo': 'CoinSpace/CoinSpace',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Coin Wallet\\n\\n[![Build](https://github.com/coinspace/coinspace/actions/workflows/ci.yml/badge.svg)](https://github.com/CoinSpace/CoinSpace/actions/workflows/ci.yml)\\n[![Downloads](https://img.shields.io/github/downloads/coinspace/coinspace/total)](https://github.com/CoinSpace/CoinSpace/releases)\\n[![Version](https://img.shields.io/github/v/release/coinspace/coinspace?label=version)](https://github.com/CoinSpace/CoinSpace/releases)\\n[![License](https://img.shields.io/github/license/CoinSpace/CoinSpace?color=blue)](https://github.com/CoinSpace/CoinSpace/blob/master/LICENSE)\\n[![Twitter](https://img.shields.io/twitter/follow/CoinAppWallet?style=social)](https://twitter.com/intent/follow?screen_name=CoinAppWallet)\\n\\nCoin Wallet is a non-custodial multicurrency wallet for multiple platforms.\\n\\nSupported platforms:\\n- Web\\n- iOS\\n- Android\\n- macOS\\n- Windows\\n- Linux\\n- Tor\\n\\nSupported coins:\\n- Bitcoin\\n- Bitcoin Cash\\n- Bitcoin SV. Removed at block 722524.\\n- Ethereum and ERC20 tokens\\n- Litecoin\\n- XRP\\n- Stellar\\n- EOS\\n- Dogecoin\\n- Dash\\n- Monero\\n- Binance Smart Chain and BEP20 tokens\\n- Cardano\\n- Ethereum Classic\\n- Solana\\n- Avalanche C-Chain and ARC20 tokens\\n- TRON and TRC20 tokens\\n'},\n",
       " {'repo': 'PatHightree/SpaceNavigator',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# SpaceNavigator\\nA Unity3D driver for the SpaceNavigator and other 3DConnexion devices.\\n\\nThis driver lets you fly around your scene and allows you to move items around.  \\nIt can also be used at runtime via scripting.  \\n\\nThe default mode is **Fly** mode and when you\\'re flying around, this driver keeps your horizon horizontal.  \\nSo you don\\'t have to worry about ending up upside down, just go where you want and get some work done.  \\nTo comfortably navigate large areas and minute details, you can easilly switch between 3 customizable sensitivity presets.  \\nTo move stuff around, you can use 2 modes: Telekinesis and GrabMove.  \\nIn **Telekinesis** mode, you can move the stuff you selected with the SpaceNavigator, while your camera stays put.  \\n(this mode can be operated in Camera-, World-, Parent- and Local coordinates)  \\nIn **GrabMove** mode the stuff will be linked to your camera so you can take it with you and position it where you want.  \\nTranslation can be snapped to a grid and rotation can be angle-snapped.  \\n\\nIf you encounter issues, please report them via the project\\'s [Github Issues](https://github.com/PatHightree/SpaceNavigator/issues) page.  \\nIf you have feedback, please use this [thread](http://forum.unity3d.com/threads/182382-SpaceNavigator-driver-OpenSource) on the Unity forums.  \\nThe source code is available on [Github](https://github.com/PatHightree/SpaceNavigator).\\n\\n## What\\'s new in 2.0.0 ?\\n### New foundation, new requirements\\nThe driver\\'s foundation has been rebuilt and it is now compatible with Unity 2019.1 and up.   \\nIt communicates directly with the HID device via Unity\\'s new Input System.  \\nThis means that it **no longer requires the 3DConnexion driver** to be running or even installed.  \\nIt also means that **your project is required to have the new Input System enabled**.  \\nYou can have both the old and new Input System active, see this [tip](#tip_project_settings).  \\n(this [page](https://docs.unity3d.com/Packages/com.unity.inputsystem@1.0/manual/Migration.html) can help with upgrading to the new Input System)\\n\\n### SpaceNavigator driver as a package\\nThe driver is structured as a UPM package and can be added to your project via the Project Manager.  \\nSee the [download](#download) section for details.\\n\\n## <a name=\"download\"></a>Download\\n- The driver can now be downloaded via the Package Manager window\\n  - From git url :\\n    - Click the + button in the top left of the Package Manager window\\n    - Choose *Add package from GIT url...*\\n    - Enter https://github.com/PatHightree/SpaceNavigator.git  \\n      Note, for this you need to have git installed on your system !\\n  - From disk :\\n    - Download the driver from the github [releases page](https://github.com/PatHightree/SpaceNavigator/releases)\\n    - Click the + button in the top left of the Package Manager window\\n    - Choose *Add package from disk...*\\n- ~~The full [package](http://u3d.as/51X) is available on the Unity asset store.~~  \\n  *Todo: Update asset store package to v2.0.0*  \\n  \\nIf you want to install a specific version, download it from the github [releases page](https://github.com/PatHightree/SpaceNavigator/releases) and install it via the *From Disk* method.\\n\\n## <a name=\"installation\"></a>Installation\\n- Add the driver to your project as described in the [Download](#download) section\\n- If your project was not yet using the new Input System, you have 2 options :\\n  - Use both input systems simultaneously \\n    - Close and reopen the project\\n    - A popup will ask you to switch to the new Input System, choose **NO**\\n    - Set *Project Settings/Player/Active Input Handling* to **Both**  \\n    See this [tip](#tip_project_settings)\\n  - Switch to the New Input System\\n    - Close and reopen the project\\n    - A popup will ask you to switch to the new Input System, choose **Yes**\\n    - The project will close and reopen itself\\n    - You will have to modify any code which used the old Input System  \\n      [Here](https://docs.unity3d.com/Packages/com.unity.inputsystem@1.0/manual/Migration.html) is an overview of common old Input usages and their new equivalents \\n- Fly away\\n\\n## Upgrading from 1.x\\nWhen upgrading from a pre-2.0.0 version of the plugin, please follow these steps :\\n- Close the SpaceNavigator editor window\\n- Delete the SpaceNavigator folder from your project\\n- Delete Plugins\\\\3DConnexionWrapperU4.bundle from your project\\n- Add the driver to your project as described in the [Download](#download) section\\n- Install the driver by following the steps in the [Installation](#installation) section\\n\\nIf you delete the folder while the SpaceNavigator window is still open, Unity will throw some errors.\\nWhen this happens, choose the default layout from the layout dropdown in the top right of Unity\\'s UI and everything should return to normal.\\n\\n## Upgrading from 2.x\\nAt this time the Package Manager window does not show all available versions in a git repo.  \\nSo until this changes, the upgrade process consists of removing the old version and installing a new one as described in the [Download](#download) section.\\n\\n## Samples\\nTo install these samples, open the SpaceNavigator package in the Package Manager window and click the Import button in the Samples section.\\n### Runtime Samples\\nThe package also contains a couple of samples of runtime applications :\\n- Fly around.unity: Fly around with a sphere while knocking over some cubes.\\n- Folow curve.unity: Make your torus follow the curve, but don\\'t touch it!  \\n### Input Helper\\nThe Input Helper is a utility to collect all info on your 3DConnexion device.  \\nIf your 3Dconnexion device has a data layout which is not yet supported by the driver,\\nplease file an issue on the [GitHub Issues](https://github.com/PatHightree/SpaceNavigator/issues) page and supply the data this tool collects.  \\nAfter installation you can find the tool in the pull-down menu under _Window/SpaceNavigator/Save all HID descriptors to files_.\\n\\n## Known bugs and limitations\\n- The New Input System package must be active for this driver to work\\n- Grab Mode only works in the camera coordinate system\\n\\n## Credits\\n- Felix Herbst for the input helper and adding scene focus to runtime navigation\\n- William Iturzaeta from Canon Medical Systems USA, for hiring me to make this project compatible with Unity 2020  \\n  The proceeds of this job have been donated to cancer research\\n- Stephen Wolter for further refinement to the mac drift fix \\n- Enrico Tuttobene for contributing the mac drift fix\\n- Kieron Lanning for implementing navigation at runtime\\n- Chase Cobb from Google for motivating me to implement the mac version\\n- Manuela Maier and Dave Buchhoffer (@vsaitoo) for testing and development feedback\\n- Ewoud Wijma for loaning me the Hackingtosh for building the Mac port\\n- Quaternion math by Minahito\\n  http://sunday-lab.blogspot.nl/2008/04/get-pitch-yaw-roll-from-quaternion.html\\n\\n## <a name=\"tip_project_settings\"></a>Tip : You can switch input systems in Project Settings\\n![](Documentation~/ProjectSettings_ActiveInputHandling.png)\\n\\n## Tip : Disable 3DConnexion KMJ Emulator\\nIf you\\'re using the 3dconnexion driver, it comes with a keyboard, mouse, joystick emulator.  \\nPersonally I have never needed it and it interferes with some games and applications.  \\nHere\\'s how to disable it on Windows 10:\\n- Open the device manager by pressing Windows-X and choosing Device Manager\\n- Navigate to Human Interface Devices/HID-compliant game controller\\n- Select it and click the down arrow button\\n- If there are more entries called HID-compliant game controller,  \\n  disable the one which properties say Location: on #Dconnexion KMJ Emulator\\n\\n![](Documentation~/Disable_KMJ_emulator.png)'},\n",
       " {'repo': 'MicrosoftDocs/mslearn-tailspin-spacegame-web-kubernetes',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': \"\\n# Contributing\\n\\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\\nthe rights to use your contribution. For details, visit https://cla.microsoft.com.\\n\\nWhen you submit a pull request, a CLA-bot will automatically determine whether you need to provide\\na CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions\\nprovided by the bot. You will only need to do this once across all repos using our CLA.\\n\\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\\n\\n# Legal Notices\\n\\nMicrosoft and any contributors grant you a license to the Microsoft documentation and other content\\nin this repository under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode),\\nsee the [LICENSE](LICENSE) file, and grant you a license to any code in the repository under the [MIT License](https://opensource.org/licenses/MIT), see the\\n[LICENSE-CODE](LICENSE-CODE) file.\\n\\nMicrosoft, Windows, Microsoft Azure and/or other Microsoft products and services referenced in the documentation\\nmay be either trademarks or registered trademarks of Microsoft in the United States and/or other countries.\\nThe licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks.\\nMicrosoft's general trademark guidelines can be found at http://go.microsoft.com/fwlink/?LinkID=254653.\\n\\nPrivacy information can be found at https://privacy.microsoft.com/en-us/\\n\\nMicrosoft and any contributors reserve all other rights, whether under their respective copyrights, patents,\\nor trademarks, whether by implication, estoppel or otherwise.\\n\"},\n",
       " {'repo': 'usnistgov/PrivacyEngCollabSpace',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Privacy Engineering Collaboration Space\\nThe NIST Privacy Engineering Collaboration Space is an online venue open to the public where practitioners can discover, share, discuss, and improve upon open source tools, solutions, and processes that support privacy engineering and risk management.\\n\\n## Focus Areas\\nWe have launched this space with an initial focus on de-identification and privacy risk assessment tools and use cases, and welcome [feedback](mailto:collabspace@nist.gov) on topics of interest from the community. \\n\\n* **De-identification:** a\\xa0technique or\\xa0process\\xa0applied to a dataset with the goal of preventing or limiting certain types of privacy risks to individuals, protected groups, and establishments, while still allowing for the production of aggregate statistics.\\xa0This focus area includes\\xa0a broad scope of de-identification to allow for noise-introducing techniques such as differential privacy, data masking, and the creation of synthetic datasets that are based on privacy-preserving models.\\n\\n* **Privacy Risk Assessment:** a process that\\xa0helps\\xa0organizations\\xa0to analyze and assess privacy risks\\xa0for individuals arising from the processing of their data. This\\xa0focus area\\xa0includes, but is not limited to, risk models, risk assessment methodologies, and approaches to determining privacy risk factors. \\n\\n## Contribute to the Space \\n\\nContributions come in three categories:\\n\\n1. **Tool:** A tool can be an open source solution or process, ranging from software to frameworks. \\n2. **Use Case:** A use case is an example of an organization processing data about individuals for some explicit purpose(s) (e.g., where a goal is to prevent re-identification of the data during its processing, improve privacy risk assessment practices).\\n3. **Feedback:** Help the community. Provide feedback on tools and use cases.\\n\\nTools and use cases are contributed via pull requests, while feedback is contributed via issues. Contributed tools and use cases can be hosted directly in this repository, or you can host them elsewhere online and link to them from this repository.\\n\\n### How to Contribute Tools and Use Cases\\n\\n1. Fork a copy of USNISTGOV/PrivacyEngCollabSpace to your own organizational or personal space. \\n\\n2. Create a branch in your fork, named specifically for your contribution. \\n\\n3. In your branch: \\n\\n\\tA. Create a new directory within the relevant tool or use case directory: tools/de-identification, tools/risk-assessment, use-cases/de-identification, or use-cases/risk-assessment. Example: *tools/de-identification/[your-contribution-name]*\\n\\n\\tB. Name the directory to describe your contribution. \\n\\n\\tC. Include in the directory a README.md file that follows the relevant [template](https://github.com/usnistgov/PrivacyEngCollabSpace/tree/master/templates). There is a template for a [tool](https://github.com/usnistgov/PrivacyEngCollabSpace/tree/master/templates/tool-template.md) and for a [use case](https://github.com/usnistgov/PrivacyEngCollabSpace/tree/master/templates/use-case-template.md) contribution.\\n\\n\\tD. If hosting a tool in this repository, also include in the directory any pertinent source code files or documentation. \\n\\n\\tE. Update the README.md file of the main directory to which you’re contributing. This README provides an index of that directory\\'s contents. It should include an entry reflecting your contribution. \\n\\n5. Create a [pull request](https://github.com/usnistgov/PrivacyEngCollabSpace/pull/new/master) (PR) from your branch to the master branch in USNISTGOV/PrivacyEngCollabSpace. \\n\\n6. Moderators will then review the PR and may provide comments and suggestions to the contributor. \\n\\n### How to Contribute Feedback \\n\\nSubmit an [issue](https://github.com/usnistgov/PrivacyEngCollabSpace/issues/new) to provide feedback on tools or use cases in the space. Please select appropriate tags related to the feedback. \\n\\n### Additional Contribution Resources\\n\\n**GitHub Help:** If you\\'re having trouble with these instructions, and need more information about GitHub, pull requests, and issues, visit GitHub\\'s Help [page](https://help.github.com/categories/collaborating-with-issues-and-pull-requests/). \\n\\n**Contribution Assistance:** If you\\'re having trouble submitting your contribution to this space, or otherwise would like to send us feedback, [contact us](mailto:collabspace@nist.gov). \\n\\n## Browse Tools and Use Cases\\n\\nInterested in tools or use cases for de-identification and privacy risk assessment? **Browse the contributions [here](https://www.nist.gov/itl/applied-cybersecurity/privacy-engineering/collaboration-space/browse).**\\n\\n## Operating Rules \\n\\nNIST will only accept open source submissions, per the Open Source Initiative’s [definition](https://opensource.org/osd) of “open source”. Upon submission, materials will be public, considered to be open source, and may be altered and shared. \\n\\nThis is a moderated platform. NIST reserves the right to reject, remove, or edit any submission, including anything that: \\n\\n* promotes pay-for services or products;  \\n* includes personally identifiable or business identifiable information according to Department of Commerce Office of Privacy and Open Government [guidelines](http://www.osec.doc.gov/opog/privacy/PII_BII.html); \\n* is inaccurate;  \\n* contains abusive or vulgar content, spam, hate speech, personal attacks, or similar content;\\n* is clearly \"off topic\"; \\n* makes unsupported accusations; or, \\n* contains .exe or .jar file types.* \\n\\n*These file types will not be merged into the NIST repository; instead, NIST may link to these if hosted elsewhere. \\n\\n## Representations and Warranties & Software Use Agreement \\n\\nAny references to commercial entities, products, services, or other nongovernmental organizations or individuals on the site are provided solely for the information of individuals using this page. These references are not intended to reflect the opinion of NIST, the Department of Commerce or the United States, or its officers or employees. Such references are not an official or personal endorsement of any product, person, or service, nor are they intended to imply that the entities, materials, or equipment are necessarily the best available for the purpose. Such references may not be quoted or reproduced for the purpose of stating or implying an endorsement, recommendation, or approval of any product, person, or service. \\n\\nThis platform is provided as a public service. Information, data, and software posted to this platform is “AS IS.” NIST MAKES NO WARRANTY OF ANY KIND, EXPRESS, IMPLIED OR STATUTORY, INCLUDING, WITHOUT LIMITATION, THE IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT AND DATA ACCURACY. NIST does not warrant or make any representations regarding the use of the software or the results thereof, including but not limited to the correctness, accuracy, reliability or usefulness of the software. You are solely responsible for determining the appropriateness of using and distributing the software and you assume all risks associated with its use, including but not limited to the risks and costs of program errors, compliance with applicable laws, damage to or loss of data, programs or equipment, and the unavailability or interruption of operation. This software is not intended to be used in any situation where a failure could cause risk of injury or damage to property. NIST SHALL NOT BE LIABLE AND YOU HEREBY RELEASE NIST FROM LIABILITY FOR ANY INDIRECT, CONSEQUENTIAL, SPECIAL, OR INCIDENTAL DAMAGES (INCLUDING DAMAGES FOR LOSS OF BUSINESS PROFITS, BUSINESS INTERRUPTION, LOSS OF BUSINESS INFORMATION, AND THE LIKE), WHETHER ARISING IN TORT, CONTRACT, OR OTHERWISE, ARISING FROM OR RELATING TO THE SOFTWARE (OR THE USE OF OR INABILITY TO USE THIS SOFTWARE), EVEN IF NIST HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\\n\\n## Moderators \\n\\n### De-Identification Moderators\\n\\n![Joseph Near](https://github.com/usnistgov/PrivacyEngCollabSpace/blob/master/assets/joseph-near.jpg)\\n\\n**Joseph Near [@jnear]:** Joseph Near is an assistant professor of computer science at the University of Vermont. His research interests include data privacy, computer security, and programming languages. Joseph received his B.S. in computer science from Indiana University, and his M.S. and Ph.D. in computer science from MIT.\\n\\n![David Darais](https://github.com/usnistgov/PrivacyEngCollabSpace/blob/master/assets/david-darais.jpg)\\n\\n**David Darais [@davdar]:** David Darais is a Principal Scientist at Galois, Inc. and supports NIST as a moderator for the Privacy Engineering Collaboration Space. David\\'s research focuses on tools for achieving reliable software in critical, security-sensitive, and privacy-sensitive systems. David received his B.S. from the University of Utah, M.S. from Harvard University and Ph.D. from the University of Maryland.\\n\\n### Privacy Risk Management Moderator\\n\\n![Katie Boeckl](https://github.com/usnistgov/PrivacyEngCollabSpace/blob/master/assets/katie-boeckl.jpg)\\n\\n**Katie Boeckl [@kboeckl]:** Katie Boeckl is a privacy risk strategist at NIST. As part of the Privacy Engineering Program, Katie develops privacy risk management guidance, collaborates on the development of international privacy standards, and works to advance tools for privacy engineering and risk management. Katie has a B.A. in English from the University of Maryland, College Park, where she specialized in technology through a digital cultures honors program.\\n\\n## NIST Privacy Engineering Program\\nLearn about NIST\\'s Privacy Engineering Program by visiting our [website](https://www.nist.gov/itl/applied-cybersecurity/privacy-engineering).\\n\\n## Contact \\n\\nContact NIST to submit feedback, including future topics of interest, or for assistance with contributing to the space: [collabspace@nist.gov](mailto:collabspace@nist.gov)\\n'},\n",
       " {'repo': 'IdeaSpaceVR/IdeaSpace',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '<p align=\"center\"><img src=\"public/assets/layouts/app/images/isvr-logo-v2.png\" width=\"100\" alt=\"IdeaSpaceVR - PHP Content management system (CMS) for the virtual reality web (WebVR)\"></p>\\n\\n<p align=\"center\">\\n<a href=\"https://github.com/ideaspacevr/ideaspace/releases\"><img src=\"https://img.shields.io/github/downloads/ideaspacevr/ideaspace/total.svg\"></a>\\n<a href=\"https://github.com/ideaspacevr/ideaspace/releases\"><img src=\"https://img.shields.io/github/license/ideaspacevr/ideaspace.svg\"></a>\\n<a href=\"https://github.com/ideaspacevr/ideaspace/releases\"><img src=\"https://img.shields.io/github/release/ideaspacevr/ideaspace.svg\"></a>\\n<a href=\"https://twitter.com/ideaspacevr\"><img src=\"https://img.shields.io/twitter/follow/ideaspacevr.svg?style=social\"></a>\\n</p>\\n\\n## About IdeaSpaceVR \\n\\n### Mission\\n<em>\"We believe that everyone should be able to easily create and publish interactive 3D and VR web experiences with the focus on content creation and creativity. Software developers should have the possibility to bundle their creations as themes and share them.\"</em>\\n\\n\\n### Content Management System\\nManage your virtual reality spaces and assets like you would manage blog posts. Run it on your own server. All you need is PHP and a database (MySQL, PostgreSQL or MariaDB). \\n\\n### Themes\\nDownload and install themes and create a 3D/VR website. Or create your own theme with the Theme API [Take a look at our theme directory](https://www.ideaspacevr.org/themes). A theme can be anything - a 360 photo tour, a 3D artist portfolio, an interactive video or a blog in VR. \\n\\n### Based on WebXR Device API\\nIdeaSpaceVR is using the WebXR Device API for interactive 3D and VR experiences that is compatible with all modern web browsers on desktop, mobile and VR headsets. Therefore... we love [A-Frame](https://github.com/aframevr/aframe)! \\n\\n## Download Latest Release\\n\\nhttps://www.ideaspacevr.org/download\\n\\n## Documentation\\n\\nhttps://www.ideaspacevr.org/documentation\\n\\n## Contributing\\n\\nhttps://www.ideaspacevr.org/documentation/1.1/contributions\\n\\n## Screenshots\\n\\n![IdeaSpaceVR - Spaces](IdeaSpaceVR-spaces.jpg \"IdeaSpaceVR - Spaces\")\\n\\n![IdeaSpaceVR - Edit Space](IdeaSpaceVR-edit-space.jpg \"IdeaSpaceVR - Edit Space\")\\n\\n![IdeaSpaceVR - Assets](IdeaSpaceVR-assets.jpg \"IdeaSpaceVR - Assets\")\\n\\n![IdeaSpaceVR - Annotations](IdeaSpaceVR-annotations.jpg \"IdeaSpaceVR - Annotations\")\\n\\n![IdeaSpaceVR - Welcome Theme](IdeaSpaceVR-welcome.jpg \"IdeaSpaceVR - Welcome Theme\")\\n\\n![IdeaSpaceVR - Compass Blog Theme](IdeaSpaceVR-compass-blog.jpg \"IdeaSpaceVR - Compass Blog Theme\")\\n\\n## Credits\\n\\nIdeaSpace would not exist without these awesome projects:\\n\\n- https://github.com/aframevr/aframe\\n- https://github.com/mrdoob/three.js\\n- https://github.com/laravel/laravel\\n\\n## Security Vulnerabilities\\n\\nIf you discover a security vulnerability, please do not hesitate to file an issue: https://github.com/IdeaSpaceVR/IdeaSpace/issues\\n\\n## License\\n\\nThe IdeaSpaceVR CMS is open-sourced software licensed under the <a href=\"https://opensource.org/licenses/MIT\" target=\"_blank\">MIT license</a>.\\n\\n\\n\\n'},\n",
       " {'repo': 'KilledByAPixel/SpaceHuggers',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': \"![Space Huggers - A JS13k Game by Frank Force](/screenshot.png)\\n\\nThe empire is spreading like a plague across the galaxy and building outposts on remote planets.\\nYou are an elite rebel soldier tasked with wiping out those bases.\\nExplore strange planets using your tools of destruction and eliminate the invaders!\\nYou have only 10 clones left, 3 more will be replenished after each mission.\\nGood luck, have fun, and give space a hug for me.\\n\\n## This game is only for learning purposes and not intended to be redistributed!\\n\\n# [PLAY SPACE HUGGERS!](https://www.newgrounds.com/portal/view/819609)\\n# [OFFICIAL JS13K BUILD](https://js13kgames.com/entries/space-huggers)\\n# [VIDEO DEMO](https://www.youtube.com/watch?v=6VXrnk18Z4s)\\n\\n# How To Play\\n- Use WASD or D-Pad - Move, jump, and climb\\n- Z or Left click - Shoot - Most things will break, some will burn\\n- X or Middle click - Roll - brief invulnerability, does damage, gives a boost, puts out fire\\n- C or Right click - Grenade - 3 per life, use wisely\\n- You can also use a Xbox or SNES style controller, connect up to 4 for co-op play!\\n- Kill all enemies to complete the level\\n- A radar along the bottom of the screen shows nearby enemies\\n- You start with 10 lives and get 3 more for completing each level\\n- For an optimal play experience please use Chrome in full screen mode\\n- There is no end, but for a challenge, try beating the first 5 levels\\n\\n# Gameplay Tips\\n- Roll to put out fire!\\n- Rolling also does melee damage to enemies\\n- Keep your distance from the specialists (white), they roll and flip often!\\n- You can hold down jump to climb up walls\\n- Jump flip to get more vertical height (roll immediately after jumping)\\n- To reach really high places try a grenade jump!\\n- You can press R to restart the game\\n\\n# Game Features\\n- Run and gun / roguelike hybrid gameplay\\n- 2-4 player jump in local co-op mode\\n- Procedural level generation of great variety and complexity\\n- Levels are fully destructible with persistence\\n- Fire propagation and explosion system\\n- 5 enemy types with a larger variant\\n- 7 different crate/barrel/rock types\\n- 17 sprite textures using a 12 color palette\\n- Checkpoints can be captured for players to respawn there\\n- Multi layer procedurally generated parallax background\\n- Starfield simulation with moving stars, planets, and suns\\n- Particle systems for rain, snow, blood, explosions, weapons, water and more\\n- Native resolution rendering up to 1920x1200\\n- 11 different sound effects with [ZzFX](https://github.com/KilledByAPixel/ZzFX)\\n- Up to 4 player co-op with 4 gamepads!\\n\\n# Engine Features\\n- Custom game engine written during the compo is separate from game code\\n- Super fast rendering system for up to 50,000 objects at 60 fps\\n- Physics engine for axis aligned bounding box rigid body dynamics\\n- Tile based rendering and collision system\\n- Particle effects system\\n- Input processing system for keyboard, mouse, gamepads, and touch\\n- Math library with Vector2, Color and Timer utility classes\\n- Audio with ZzFX has ability to attenuate sounds by distance\\n- Debug visualization system not in JS13K build. (press ~ to enter debug mode)\\n\\n# Enemy Types\\n- Recruit (Green) - A bit shorter, more hesitant, takes only 1 hit\\n- Soldier (Blue) - Average height and ability, takes 2 hits\\n- Captain (Red) - Can climb walls and jumps more often, takes 3 hits\\n- Specialist (White) - Jumps and rolls often, they are ninjas, takes 4 hits\\n- Demolitions Expert (Purple) - Throws grenades and can't catch fire, takes 5 hits\\n- Small chance of a heavy weapons variation that has double health and fires full auto\\n\\n# Object Types\\n- Plastic Crate (Brown) - Burns easily and breaks when fully burnt\\n- Metal Crate & Barrel (Gray) - Is hard to destroy, can't burn\\n- Water Barrel (Blue) - Puts out fires and pushes away objects\\n- Explosive Crate & Barrel (Green) - Burns and explodes after a few seconds\\n- High Explosive Barrel (Red) - Explodes quickly and much larger than normal explosives\\n- Rock (Color Varies) - Heavy and very hard to destroy, can't burn, can crush enemies\\n- Lava Rock (Glowing Red & Orange) - Anything that touches it is lit on fire\\n\\n# Tools Used\\n- [Roadroller](https://github.com/lifthrasiir/roadroller)\\n- [Google Closure Compiler](https://github.com/google/closure-compiler)\\n- [UglifyJS](https://github.com/mishoo/UglifyJS)\\n- [Imagemin](https://github.com/imagemin/imagemin)\\n- [Efficient Compression Tool](https://github.com/fhanau/Efficient-Compression-Tool)\\n- [Advzip](https://www.npmjs.com/package/advzip-bin)\\n- [ZzFX](https://github.com/KilledByAPixel/ZzFX)\\n\\n# How to build the 13k Zip\\n- Run engine\\\\build\\\\setupBuild.bat to install the necessary tools via npm\\n- You will need: google-closure-compiler, uglify, roadroller, imagemin-cli, and advzip\\n- Run engine\\\\build\\\\build.bat, to build app.zip which is the final result\\n- It will also create a file called index.min.html you can use for testing\\n- The zip size may vary by 20 bytes or so due to randomness of roadroller\\n\"},\n",
       " {'repo': 'bitdust/EarthLiveSharp',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': \"Earth Live Sharp\\n===\\n\\n\\n**重要：**[如何使用私有 CDN 作为图片源以提升稳定性](https://github.com/bitdust/EarthLiveSharp/issues/32)  \\n**WARNING:** [How to set CDN as the image source?](https://github.com/bitdust/EarthLiveSharp/issues/32)  \\n\\nFetch real-time images from [himawari-8](http://himawari8.nict.go.jp/)  ([wiki](https://en.wikipedia.org/wiki/Himawari_8))  and set it as your wallpaper  \\nGive you a live view of earth from space.  \\nIt's also recommanded to use [Mantou Earth](https://github.com/oxoooo/earth) on Android.  \\nLinux or OSX users can try Shell/Ruby/Python scripts in [this thread](https://www.v2ex.com/t/241563).\\n## Screenshots\\n![screenshot1](https://cloud.githubusercontent.com/assets/6072743/23821657/7d53554e-0674-11e7-8ccc-260070261967.png)\\n\\n![sample](https://cloud.githubusercontent.com/assets/6072743/11613290/6af013a8-9c56-11e5-8d7e-553cc8226d5a.png)  \\n## Download\\nCheck the [latest release](https://github.com/bitdust/EarthLiveSharp/releases).\\n\\n![license](https://img.shields.io/github/license/bitdust/earthlivesharp.svg)\\n![Build Status](https://travis-ci.org/bitdust/EarthLiveSharp.svg)\\n![downloads](https://img.shields.io/github/downloads/bitdust/earthlivesharp/total.svg)\\n\"},\n",
       " {'repo': 'Mr-B0b/SpaceRunner',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# SpaceRunner\\n\\nThis tool enables the compilation of a C# program that will execute arbitrary PowerShell code, without launching PowerShell processes through the use of runspace.\\n\\nAMSI is patched using [@\\\\_xpn_](https://twitter.com/_xpn_/status/1170852932650262530) and [@_RastaMouse](https://github.com/rasta-mouse/AmsiScanBufferBypass) technique.\\n\\n## Disclaimer\\n\\nThis project can only be used for authorized testing or educational purposes. Using this software against target systems without prior permission is illegal, and any damages from misuse of this software will not be the responsibility of the author.\\n\\n## Compilation\\n\\nTo compile the binary, just type the following command in the repo folder:\\n```\\nC:\\\\Windows\\\\Microsoft.NET\\\\Framework64\\\\v4.0.30319\\\\csc.exe /unsafe /platform:x64 /preferreduilang:en-US /filealign:512 /out:spacerunner.exe /target:exe spacerunner.cs\\n```\\n\\n## Usage\\n\\nExecute the binary with the following parameters:\\n```\\n-i (--input)         Full path to the PowerShell input script file\\n-o (--output)        Full path to the generated output binary file\\n-h (--hide)          Optional, set the specified window\\'s show state -> Default = False\\n-b (--beacon)        Optional, type of script provided (Cobalt Strike beacon) -> Default = PowerShell script\\n-f (--functions)     Optional, set PowerShell function to call (function1,function2,...)\\n-a (--arguments)     Optional, set PowerShell function arguments to pass (\"function1Arg1 function1Arg2 ...#function2Arg1 function2Arg2 ...\")\\n```\\n\\nExamples:  \\n- Compile a [Cobalt Strike](https://www.cobaltstrike.com/) beacon binary:\\n```\\nspacerunner.exe -i bin\\\\beacon.ps1 -o bin\\\\beacon.exe -b -h\\n```\\n- Compile [Inveigh](https://github.com/Kevin-Robertson/Inveigh) binary with `Invoke-Inveigh` function and `\\'-ConsoleOutput Y\\'` arguments parameters:\\n```\\nspacerunner.exe -i bin\\\\Inveigh.ps1 -o bin\\\\inveigh.exe -f Invoke-Inveigh -a \"-ConsoleOutput Y\"\\n```\\n- Compile [Sherlock](https://github.com/rasta-mouse/Sherlock) binary with `Find-AllVulns` function parameter:\\n```\\nspacerunner.exe -i bin\\\\Sherlock.ps1 -o bin\\\\sherlock.exe -f Find-AllVulns\\n```\\n- Compile [PowerUp](https://github.com/PowerShellMafia/PowerSploit/blob/dev/Privesc/PowerUp.ps1) binary with `Find-PathDLLHijack` function parameter:\\n```\\nspacerunner.exe -i bin\\\\Powerup.ps1 -o bin\\\\Powerup.exe -f Find-PathDLLHijack\\n```\\n- Compile [PowerView](https://github.com/PowerShellMafia/PowerSploit/blob/dev/Recon/PowerView.ps1) binary with `Get-DomainGroupMember` function and `-Identity \\'Admins du domaine\\' -Recurse` parameters:\\n```\\nspacerunner.exe -i bin\\\\Powerview.ps1 -o bin\\\\Powerview.exe -f Get-DomainGroupMember -a \"-Identity \\'Admins du domaine\\' -Recurse\"\\n```\\n- Compile [PowerView](https://github.com/PowerShellMafia/PowerSploit/blob/dev/Recon/PowerView.ps1) binary with `Get-DomainGroupMember` and `Get-DomainUser` functions with respective arguments `-Identity \\'Admins du domaine\\' -Recurse` and `user`:\\n```\\nspacerunner.exe -i bin\\\\Powerview.ps1 -o bin\\\\Powerview.exe -f Get-DomainGroupMember,Get-DomainUser -a \"-Identity \\'Admins du domaine\\' -Recurse#\\'user\\'\"\\n```\\n\\n## Credits\\n\\nThis project is based on the work done by :\\n\\n- Adam Chester, @\\\\_xpn_\\n- Casey Smith, @subTee\\n- Lee Christensen, @tifkin_\\n- Matt Graeber, @mattifestation\\n- Rasta Mouse, @_RastaMouse\\n'},\n",
       " {'repo': 'adamian98/pulse',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"# PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models\\nCode accompanying CVPR'20 paper of the same title. Paper link: https://arxiv.org/pdf/2003.03808.pdf\\n\\n## NOTE\\n\\nWe have noticed a lot of concern that PULSE will be used to identify individuals whose faces have been blurred out. We want to emphasize that this is impossible - **PULSE makes imaginary faces of people who do not exist, which should not be confused for real people.** It will **not** help identify or reconstruct the original image.\\n\\nWe also want to address concerns of bias in PULSE. **We have now included a new section in the [paper](https://arxiv.org/pdf/2003.03808.pdf) and an accompanying model card directly addressing this bias.**\\n\\n---\\n\\n![Transformation Preview](./readme_resources/014.jpeg)\\n![Transformation Preview](./readme_resources/034.jpeg)\\n![Transformation Preview](./readme_resources/094.jpeg)\\n\\nTable of Contents\\n=================\\n- [PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models](#pulse-self-supervised-photo-upsampling-via-latent-space-exploration-of-generative-models)\\n- [Table of Contents](#table-of-contents)\\n  - [What does it do?](#what-does-it-do)\\n  - [Usage](#usage)\\n    - [Prereqs](#prereqs)\\n    - [Data](#data)\\n    - [Applying PULSE](#applying-pulse)\\n## What does it do? \\nGiven a low-resolution input image, PULSE searches the outputs of a generative model (here, [StyleGAN](https://github.com/NVlabs/stylegan)) for high-resolution images that are perceptually realistic and downscale correctly.\\n\\n![Transformation Preview](./readme_resources/transformation.gif)\\n\\n## Usage\\n\\nThe main file of interest for applying PULSE is `run.py`. A full list of arguments with descriptions can be found in that file; here we describe those relevant to getting started.\\n\\n### Prereqs\\n\\nYou will need to install cmake first (required for dlib, which is used for face alignment). Currently the code only works with CUDA installed (and therefore requires an appropriate GPU) and has been tested on Linux and Windows. For the full set of required Python packages, create a Conda environment from the provided YAML, e.g.\\n\\n```\\nconda create -f pulse.yml \\n```\\nor (Anaconda on Windows):\\n```\\nconda env create -n pulse -f pulse.yml\\nconda activate pulse\\n```\\n\\nIn some environments (e.g. on Windows), you may have to edit the pulse.yml to remove the version specific hash on each dependency and remove any dependency that still throws an error after running ```conda env create...``` (such as readline)\\n```\\ndependencies\\n  - blas=1.0=mkl\\n  ...\\n```\\nto\\n```\\ndependencies\\n  - blas=1.0\\n ...\\n```\\n\\nFinally, you will need an internet connection the first time you run the code as it will automatically download the relevant pretrained model from Google Drive (if it has already been downloaded, it will use the local copy). In the event that the public Google Drive is out of capacity, add the files to your own Google Drive instead; get the share URL and replace the ID in the https://drive.google.com/uc?=ID links in ```align_face.py``` and ```PULSE.py``` with the new file ids from the share URL given by your own Drive file.\\n \\n\\n### Data\\n\\nBy default, input data for `run.py` should be placed in `./input/` (though this can be modified). However, this assumes faces have already been aligned and downscaled. If you have data that is not already in this form, place it in `realpics` and run `align_face.py` which will automatically do this for you. (Again, all directories can be changed by command line arguments if more convenient.) You will at this stage pic a downscaling factor. \\n\\nNote that if your data begins at a low resolution already, downscaling it further will retain very little information. In this case, you may wish to bicubically upsample (usually, to 1024x1024) and allow `align_face.py` to downscale for you.  \\n\\n### Applying PULSE\\nOnce your data is appropriately formatted, all you need to do is\\n```\\npython run.py\\n```\\nEnjoy!\\n\"},\n",
       " {'repo': 'Arvtesh/UnityFx.Outline',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# UnityFx.Outline\\n\\nChannel | UnityFx.Outline |\\n--------|-----------------|\\nGithub | [![GitHub release](https://img.shields.io/github/release/Arvtesh/UnityFx.Outline.svg?logo=github)](https://github.com/Arvtesh/UnityFx.Outline/releases)\\nNpm (core + built-in RP) | [![Npm release](https://img.shields.io/npm/v/com.unityfx.outline.svg)](https://www.npmjs.com/package/com.unityfx.outline) ![npm](https://img.shields.io/npm/dt/com.unityfx.outline)\\nNpm (Post-processing v2) | [![Npm release](https://img.shields.io/npm/v/com.unityfx.outline.postprocessing.svg)](https://www.npmjs.com/package/com.unityfx.outline.postprocessing) ![npm](https://img.shields.io/npm/dt/com.unityfx.outline.postprocessing)\\nNpm (URP) | [![Npm release](https://img.shields.io/npm/v/com.unityfx.outline.urp.svg)](https://www.npmjs.com/package/com.unityfx.outline.urp) ![npm](https://img.shields.io/npm/dt/com.unityfx.outline.urp)\\nNpm (HDRP) | TODO\\n\\n**Requires Unity 2018.4 or higher.**<br/>\\n**Compatible with [Unity Post-processing Stack v2](https://github.com/Unity-Technologies/PostProcessing/tree/v2).**<br/>\\n**Compatible with [Universal Render Pipeline](https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@8.1/manual/index.html).**</br>\\n**Compatible with [XR](https://docs.unity3d.com/Manual/XR.html) (Multi Pass, Single Pass Instanced).**\\n\\n**Please ask any questions and leave feedback at the [Unity forums](https://forum.unity.com/threads/screen-space-outline-effect-for-unity-free.836908/).**\\n\\n## Synopsis\\n![Outline demo](Docs/OutlineSamples.png \"Outline demo\")\\n![Outline demo](Docs/MotusOutline.png \"Outline demo\")\\n\\n*UnityFx.Outline* implements configurable per-object and per-camera outlines. Both solid and blurred outline modes are supported (Gauss blur). The outlines can be easily customized either through scripts or with Unity editor (both in edit-time or runtime).\\n\\nImplementation is based on Unity [command buffers](https://docs.unity3d.com/ScriptReference/Rendering.CommandBuffer.html), compatible with [Unity Post-processing Stack v2](https://github.com/Unity-Technologies/PostProcessing/tree/v2) and [Universal Render Pipeline](https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@8.1/manual/index.html), extendable and has no external dependencies.\\n\\nSupported outline parameters are:\\n- Color;\\n- Width (in pixels);\\n- Type (solid or blurred);\\n- Intensity (for blurred outlines);\\n- Depth testing;\\n- Alpha testing.\\n\\nSupported platforms:\\n- Windows/Mac standalone;\\n- Android;\\n- iOS;\\n- WebGL;\\n- Other platforms (untested).\\n\\nPlease see [CHANGELOG](CHANGELOG.md) for information on recent changes.\\n\\n## Getting Started\\n### Prerequisites\\nYou may need the following software installed in order to build/use the library:\\n- [Unity3d 2018.4+](https://store.unity.com/).\\n\\n### Getting the code\\nYou can get the code by cloning the github repository using your preffered git client UI or you can do it from command line as follows:\\n```cmd\\ngit clone https://github.com/Arvtesh/UnityFx.Outline.git\\n```\\n\\n### Npm packages\\n[![NPM](https://nodei.co/npm/com.unityfx.outline.png)](https://www.npmjs.com/package/com.unityfx.outline)<br/>\\n[![NPM](https://nodei.co/npm/com.unityfx.outline.postprocessing.png)](https://www.npmjs.com/package/com.unityfx.outline.postprocessing)<br/>\\n[![NPM](https://nodei.co/npm/com.unityfx.outline.urp.png)](https://www.npmjs.com/package/com.unityfx.outline.urp)<br/>\\n\\nNpm core package is available at [npmjs.com](https://www.npmjs.com/package/com.unityfx.outline). There are dedicated packages for [Post-processing Stack v2](https://github.com/Unity-Technologies/PostProcessing/tree/v2) and [Universal Render Pipeline](https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@8.0/manual/index.html). To use the packages, add the following line to dependencies section of your `manifest.json`. Unity should download and link the package automatically:\\n```json\\n{\\n  \"scopedRegistries\": [\\n    {\\n      \"name\": \"Arvtesh\",\\n      \"url\": \"https://registry.npmjs.org/\",\\n      \"scopes\": [\\n        \"com.unityfx\"\\n      ]\\n    }\\n  ],\\n  \"dependencies\": {\\n    \"com.unityfx.outline\": \"0.8.5\",\\n    \"com.unityfx.outline.urp\": \"0.5.0\",\\n  }\\n}\\n```\\n\\n## Usage\\nInstall the package and import the namespace:\\n```csharp\\nusing UnityFx.Outline;\\n```\\n\\n### Per-camera outlines (built-in RP)\\n![Outline demo](Docs/OutlineEffectInspector.png \"OutlineEffect Inspector\")\\n\\nAdd `OutlineEffect` script to a camera that should render outlines. Then add and configure as many layers as you need. An outline layer is a group of game objects that share the same outline settings:\\n```csharp\\nvar outlineEffect = Camera.main.GetComponent<OutlineEffect>();\\nvar layer = new OutlineLayer(\"MyOutlines\");\\n\\nlayer.OutlineColor = Color.red;\\nlayer.OutlineWidth = 7;\\nlayer.OutlineRenderMode = OutlineRenderFlags.Blurred;\\nlayer.Add(myGo);\\n\\noutlineEffect.OutlineLayers.Add(layer);\\n```\\nor\\n```csharp\\nvar outlineEffect = Camera.main.GetComponent<OutlineEffect>();\\n\\n// This adds layer 0 (if it is not there) and then adds myGo.\\noutlineEffect.AddGameObject(myGo);\\n\\n// Now setup the layer.\\nvar layer = outlineEffect[0];\\n\\nlayer.OutlineColor = Color.red;\\nlayer.OutlineWidth = 7;\\nlayer.OutlineRenderMode = OutlineRenderFlags.Blurred;\\nlayer.Add(myGo);\\n```\\n\\nThis can be done at runtime or while editing a scene. If you choose to assign the script in runtime make sure `OutlineEffect.OutlineResources` is initialized. Disabling `OutlineEffect` script disables outlining for the camera (and frees all resources used).\\n\\nMultiple `OutlineEffect` scripts can share outline layers rendered. To achieve that assign the same layer set to all `OutlineEffect` instances:\\n\\n```csharp\\nvar effect1 = camera1.GetComponent<OutlineEffect>();\\nvar effect2 = camera2.GetComponent<OutlineEffect>();\\n\\n// Make effect1 share its layers with effect2.\\neffect1.ShareLayersWith(effect2);\\n```\\n\\n### Per-object outlines (built-in RP)\\n![Outline demo](Docs/OutlineBehaviourInspector.png \"OutlineBehaviour Inspector\")\\n\\nAdd `OutlineBehaviour` script to objects that should be outlined (in edit mode or in runtime). Make sure `OutlineBehaviour.OutlineResources` is initialized. You can customize outline settings either via Unity inspector or via script. Objects with `OutlineBehaviour` assigned render outlines in all cameras.\\n\\n```csharp\\nvar outlineBehaviour = GetComponent<OutlineBehaviour>();\\n\\n// Make sure to set this is OutlineBehaviour was added at runtime.\\noutlineBehaviour.OutlineResources = myResources;\\n\\noutlineBehaviour.OutlineColor = Color.green;\\noutlineBehaviour.OutlineWidth = 2;\\noutlineBehaviour.OutlineIntensity = 10;\\n```\\n\\n### Depth testing\\nBy default depth testing is disabled when rendering outlines. This behaviour can be overriden by setting `EnableDepthTesting` flag of `Rander Flags` (either via scripting API or with editor).\\n```csharp\\nvar outlineSettings = GetComponent<OutlineBehaviour>();\\n\\noutlineSettings.OutlineColor = Color.green;\\noutlineSettings.OutlineWidth = 2;\\noutlineSettings.OutlineRenderMode = OutlineRenderFlags.Blurred | OutlineRenderFlags.EnableDepthTesting;\\n```\\n\\n### Alpha testing\\nBy default alpha testing is disabled when rendering outlines. This behaviour can be overriden by setting `EnableAlphaTesting` flag of `Rander Flags` (either via scripting API or with editor).\\n```csharp\\noutlineSettings.OutlineRenderMode = OutlineRenderFlags.EnableAlphaTesting;\\n```\\n\\n### Ignore layers\\nWhen adding a `GameObject` to outline collection it is often desirable to ignore child renderers in specific layers (for instance, `TransparentFX`). This can be achieved by settings the `IgnoreLayers` mask in outline settings (or through corresponding API).\\n```csharp\\nvar outlineSettings = GetComponent<OutlineBehaviour>();\\noutlineSettings.IgnoreLayerMask = LayerMask.GetMask(\"TransparentFX\", \"UI\");\\n```\\n\\n### Extensibility\\nThere are a number of helper classes that can be used for writing highly customized outline implementations (if neither `OutlineBehaviour` nor `OutlineEffect` does not suit your needs).\\nAll outline implementations use following helpers:\\n- `OutlineRenderer` is basically a wrapper around `CommandBuffer` for low-level outline rendering.\\n- `OutlineSettings` is a set of outline settings.\\n\\nUsing these helpers is quite easy to create new outline tools. For instance, the following code renders a blue outline around object the script is attached to in `myCamera`:\\n\\n```csharp\\nvar commandBuffer = new CommandBuffer();\\nvar renderers = GetComponentsInChildren<Renderer>();\\n\\n// Any implementation of `IOutlineSettings` interface can be used here instead of `OutlineSettings`.\\nvar settings = ScriptableObject.CreateInstance<OutlineSettings>();\\n\\nsettings.OutlineColor = Color.blue;\\nsettings.OutlineWidth = 12;\\n\\n// Get outline assets instance. In real app this usually comes from MonoBehaviour\\'s serialized fields.\\nvar resources = GetMyResources();\\n\\nusing (var renderer = new OutlineRenderer(commandBuffer, resources))\\n{\\n  renderer.Render(renderers, settings, myCamera.actualRenderingPath);\\n}\\n\\nmyCamera.AddCommandBuffer(OutlineRenderer.RenderEvent, commandBuffer);\\n```\\n\\n## Integration with Unity post-processing v2.\\n[![NPM](https://nodei.co/npm/com.unityfx.outline.postprocessing.png)](https://www.npmjs.com/package/com.unityfx.outline.postprocessing)\\n\\nInstall the package, add `Outline` effect to `PostProcessProfile`\\'s overrides list. Configure the effect parameters, make sure outline resources and layer collection are set:\\n\\n![Post processing outlinesettings](Docs/PpOutlineSettings.png \"Post processing outlinesettings\")\\n\\nAssign the configured `PostProcessProfile` to `PostProcessVolume` and that\\'s it!\\n\\nMore info on writing custom post processing effects can be found [here](https://docs.unity3d.com/Packages/com.unity.postprocessing@2.3/manual/Writing-Custom-Effects.html).\\n\\n## Integration with Universal Render Pipeline (URP).\\n[![NPM](https://nodei.co/npm/com.unityfx.outline.urp.png)](https://www.npmjs.com/package/com.unityfx.outline.urp)\\n\\nInstall the package, add `OutlineFeature` to `ScriptableRendererData`\\'s list of features. Configure the feature parameters (make sure outline resources reference is set). Outline objects can be selected by layer or explixitly using `OutlineLayerCollection`:\\n\\n![URP outline settings](Docs/UrpOutlineSettings.png \"URP outline settings\")\\n\\nEnable depth texture rendering in `UniversalRenderPipelineAsset` and that\\'s it!\\n\\n### Integration with High Definition Render Pipeline (HDRP).\\n[![NPM](https://nodei.co/npm/com.unityfx.outline.hdrp.png)](https://www.npmjs.com/package/com.unityfx.outline.hdrp)\\n\\nTODO\\n\\n## Motivation\\nThe project was initially created to help author with his [Unity3d](https://unity3d.com) projects. There are not many reusable open-source examples of it, so here it is. Hope it will be useful for someone.\\n\\n## Documentation\\nPlease see the links below for extended information on the product:\\n- [Unity forums](https://forum.unity.com/threads/screen-space-outline-effect-for-unity-free.836908/).\\n- [CHANGELOG](CHANGELOG.md).\\n- [SUPPORT](.github/SUPPORT.md).\\n\\n## Useful links\\n- [A great outline tutorial](https://willweissman.wordpress.com/tutorials/shaders/unity-shaderlab-object-outlines/).\\n- [Command buffers tutorial](https://lindenreid.wordpress.com/2018/09/13/using-command-buffers-in-unity-selective-bloom/).\\n- [Gaussian blur tutorial](https://www.ronja-tutorials.com/2018/08/27/postprocessing-blur.html).\\n- [Excellent post-processing tutorial](https://catlikecoding.com/unity/tutorials/scriptable-render-pipeline/post-processing/).\\n\\n## Contributing\\nPlease see [contributing guide](.github/CONTRIBUTING.md) for details.\\n\\n## Versioning\\nThe project uses [SemVer](https://semver.org/) versioning pattern. For the versions available, see [tags in this repository](https://github.com/Arvtesh/UnityFx.Outline/tags).\\n\\n## License\\nPlease see the [![license](https://img.shields.io/github/license/Arvtesh/UnityFx.Outline.svg)](LICENSE.md) for details.\\n'},\n",
       " {'repo': 'krpc/krpc',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': 'kRPC [![Build Status](https://travis-ci.com/krpc/krpc.svg?branch=master)](https://travis-ci.com/krpc/krpc)\\n====\\n\\nkRPC allows you to control Kerbal Space Program from scripts running outside of\\nthe game, and comes with client libraries for many popular languages.\\n\\n* [Documentation](https://krpc.github.io/krpc)\\n* [Forum release thread](http://forum.kerbalspaceprogram.com/index.php?/topic/130742-105-krpc-remote-control-your-ships-using-python-c-c-lua-v021-10th-feb-2016/)\\n\\nLinks for Developers\\n--------------------\\n\\n* [Forum development thread](https://forum.kerbalspaceprogram.com/index.php?/topic/62902-14113x122-krpc-remote-procedure-call-server-v045-17th-march-2018/)\\n* [Travis CI page](https://travis-ci.com/krpc/krpc)\\n* [Travis CI build outputs](http://krpc.s3-website-us-east-1.amazonaws.com/deploy/)\\n* [Release guide](Release-Guide.md)\\n'},\n",
       " {'repo': 'llSourcell/Landing-a-SpaceX-Falcon-Heavy-Rocket',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"# PPO\\n\\n\\n# Overview\\n\\nThis is the code for [this](https://youtu.be/09OMoGqHexQ) video on Youtube by Siraj Raval. Download OpenAI's Gym and Tensorflow as dependencies. \\n\\nPPO implementation for OpenAI gym environment based on Unity ML Agents: https://github.com/Unity-Technologies/ml-agents\\n\\n\\nNotable changes include:\\n  * Ability to continuously display progress with non-stochastic policy during training\\n  * Works with OpenAI environments\\n  * Option to record episodes\\n  * State normalization for given number of frames\\n  * Frame skip\\n  * Faster reward discounting etc.\\n\\n\\n# Credits\\n\\n\\nCredits for this code go to [embersarc](https://github.com/EmbersArc/PPO). I've merely created a wrapper to get people started. \\n\"},\n",
       " {'repo': 'fozziethebeat/S-Space',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': \"The S-Space Package is a collection of algorithms for building Semantic Spaces\\nas well as a highly-scalable library for designing new distributional semantics\\nalgorithms. Distributional algorithms process text corpora and represent the\\nsemantic for words as high dimensional feature vectors. These approaches are\\nknown by many names, such as word spaces, semantic spaces, or distributed\\nsemantics and rest upon the Distributional Hypothesis: words that appear in\\nsimilar contexts have similar meanings.\\n\\nThe research and development is being done by the Natural Language Processing\\ngroup at UCLA led by David Jurgens and Keith Stevens, under the advisory of Dr.\\nMichael Dyer.\\n\\nSee the [Getting Started](../../wiki/GettingStarted) page for\\na quick introduction on how to use the S-Space package, see the [Package\\nOverview](../../wiki/PackageLayout) for information on the\\ncode and available features, or dive right into the\\n[Javadoc](http://fozziethebeat.github.com/S-Space/apidocs/) to see what's\\navailable now.  For any questions, please contact us via our mailing lists:\\n[S-Space-Users][1] and [S-Space-Research-Dev][2].\\n\\n  [1]:mailto:s-space-users@googlegroups.com\\n  [2]:mailto:s-space-research-dev@googlegroups.com\\n\"},\n",
       " {'repo': 'spacebudz/spacebudz',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '<p align=\"center\">\\n  <img width=\"100px\" src=\"./src/images/brand/logo.png\" align=\"center\" />\\n  <h1 align=\"center\">SpaceBudz</h1>\\n  <p align=\"center\">SpaceBudz is an NFT collection on Cardano consisting of 10,000 unique little astronauts. We make use of Cardano\\'s multi asset ledger for the NFTs and Plutus validators for the marketplace.\\nThis repository contains the full marketplace implementation including the frontend interface.\\nOur official website: <a href=\"https://spacebudz.io\">spacebudz.io</a></p>\\n\\n  <p align=\"center\">\\n    <img src=\"https://img.shields.io/github/commit-activity/m/SpaceBudz/spacebudz?style=for-the-badge\" />\\n    <img src=\"https://img.shields.io/github/license/SpaceBudz/spacebudz?style=for-the-badge\" />\\n    <a href=\"https://twitter.com/spacebudzNFT\">\\n      <img src=\"https://img.shields.io/twitter/follow/spacebudzNFT?style=for-the-badge&logo=twitter\" />\\n    </a>\\n  </p>\\n\\n</p>\\n\\n### Validity\\n\\nTo make sure you have a real SpaceBud the policy id must match the following:\\n**`d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc`**\\n\\nYou can find the according policy script in `./collection_data/minting_policy.json`\\n\\nThe contract address for the official SpaceBudz marketplace:\\n**`addr1wyzynye0nksztrfzpsulsq7whr3vgh7uvp0gm4p0x42ckkqqq6kxq`**\\n\\n### Metadata\\n\\nWe follow [CIP-25](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0025/CIP-0025.md), the NFT metadata standard on Cardano, which was created by SpaceBudz.\\n\\nImages are stored on IPFS and Arweave and you find the image link to a SpaceBud inside the metadata.\\n\\n### Marketplace\\n\\nThe marketplace can be run by members of the community. They can host the marketplace with their own custom interface and earn 0.4% per trade.\\n\\nWe have a seperate module inside this repository for the marketplace with the full source code.\\n\\nCheck it out [here](./src/cardano/market/).\\n\\n### Community tools\\n\\n[Here](https://spacebudz.io/communityTools) you can find helpful and useful tools created by the community.\\n\\nYou have created something for SpaceBudz and it\\'s not in the list?\\nMake a PR!\\n\\nAdd your tool to the registry under `./src/data/toolsRegistry.json` with the following format:\\n```\\n  {\\n    name: string,\\n    description: string (max 70 characters),\\n    image: string (relative path to image),\\n    url: string (e.g. https://spacebudz.io)\\n  }\\n```\\nThe `image` property contains the relative path to the image: `../image/toolsRegistry/{image}`. Place the actual image under `./src/images/toolsRegistry/`.\\nThe image should be in landscape mode (e.g. 600px width, 400px height).\\nYou could use the other tools in the registry as template in case something is unclear.\\n\\n\\n'},\n",
       " {'repo': 'lvshaoge/ebao.space',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '## Welcome to GitHub Pages\\n\\nYou can use the [editor on GitHub](https://github.com/lvshaoge/tree/edit/master/README.md) to maintain and preview the content for your website in Markdown files.\\n\\nWhenever you commit to this repository, GitHub Pages will run [Jekyll](https://jekyllrb.com/) to rebuild the pages in your site, from the content in your Markdown files.\\n\\n### Markdown\\n\\nMarkdown is a lightweight and easy-to-use syntax for styling your writing. It includes conventions for\\n\\n```markdown\\nSyntax highlighted code block\\n\\n# Header 1\\n## Header 2\\n### Header 3\\n\\n- Bulleted\\n- List\\n\\n1. Numbered\\n2. List\\n\\n**Bold** and _Italic_ and `Code` text\\n\\n[Link](url) and ![Image](src)\\n```\\n\\nFor more details see [GitHub Flavored Markdown](https://guides.github.com/features/mastering-markdown/).\\n\\n### Jekyll Themes\\n\\nYour Pages site will use the layout and styles from the Jekyll theme you have selected in your [repository settings](https://github.com/lvshaoge/tree/settings). The name of this theme is saved in the Jekyll `_config.yml` configuration file.\\n\\n### Support or Contact\\n\\nHaving trouble with Pages? Check out our [documentation](https://help.github.com/categories/github-pages-basics/) or [contact support](https://github.com/contact) and we’ll help you sort it out.\\n'},\n",
       " {'repo': 'LunaMultiplayer/LunaMultiplayer',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '<p align=\"center\">\\n    <img src=\"../master/External/logo.png\" alt=\"Luna multiplayer logo\"/>\\n    <a href=\"https://www.youtube.com/watch?v=rmJL_c-EJK8\"><img src=\"https://img.youtube.com/vi/rmJL_c-EJK8/0.jpg\" alt=\"Video\" height=\"187\" width=\"250\"/></a>    \\n    <a href=\"https://www.youtube.com/watch?v=gf6xyLnpnoM\"><img src=\"https://img.youtube.com/vi/gf6xyLnpnoM/0.jpg\" alt=\"Video\" height=\"187\" width=\"250\"/></a>\\n</p>\\n\\n<p align=\"center\">\\n    <a href=\"https://paypal.me/gavazquez\"><img src=\"https://img.shields.io/badge/paypal-donate-yellow.svg?style=flat&logo=paypal\" alt=\"PayPal\"/></a>\\n    <a href=\"https://discord.gg/wKVMhWQ\"><img src=\"https://img.shields.io/discord/378456662392045571.svg?style=flat&logo=discord&label=discord\" alt=\"Chat on discord\"/></a>\\n    <a href=\"../../releases\"><img src=\"https://img.shields.io/github/release/lunamultiplayer/lunamultiplayer.svg?style=flat&logo=github&logoColor=white\" alt=\"Latest release\" /></a>\\n    <a href=\"../../releases\"><img src=\"https://img.shields.io/github/downloads/lunamultiplayer/lunamultiplayer/total.svg?style=flat&logo=github&logoColor=white\" alt=\"Total downloads\" /></a>\\n</p>\\n\\n<p align=\"center\">\\n    <a href=\"https://forum.kerbalspaceprogram.com/index.php?/topic/168271-131-luna-multiplayer-lmp-alpha\"><img src=\"https://img.shields.io/badge/KSP%20Forum-Post-4265f4.svg?style=flat\" alt=\"KSP forum post\"/></a>\\n    <a href=\"https://github.com/LunaMultiplayer/LunaMultiplayerUpdater\"><img src=\"https://img.shields.io/badge/Automatic-Updater-4265f4.svg?style=flat\" alt=\"Latest build updater\"/></a>\\n</p>\\n\\n---\\n\\n<p align=\"center\">\\n  <a href=\"../../releases/latest\"><img src=\"../master/External/downloadIcon.png\" alt=\"Download\" height=\"85\" width=\"300\"/></a>\\n  <a href=\"../../wiki\"><img src=\"../master/External/documentationIcon.png\" alt=\"Documentation\" height=\"85\" width=\"353\"/></a>\\n</p>\\n\\n---\\n\\n# Luna Multiplayer Mod (LMP)\\n\\n*Multiplayer mod for [Kerbal Space Program (KSP)](https://kerbalspaceprogram.com)*\\n\\n### Main features:\\n\\n- [x] Clean and optimized code, based on systems and windows which makes it easier to read and modify.\\n- [x] Multi threaded.\\n- [x] [NTP](https://en.wikipedia.org/wiki/Network_Time_Protocol) protocol to sync the time between clients and the server.\\n- [x] [UDP](https://en.wikipedia.org/wiki/User_Datagram_Protocol) based using the [Lidgren](https://github.com/lidgren/lidgren-network-gen3) library for reliable UDP message handling.\\n- [x] [Interpolation](http://www.gabrielgambetta.com/entity-interpolation.html) so the vessels won\\'t jump when there are bad network conditions.\\n- [x] Multilanguage.\\n- [x] [Nat-punchtrough](../../wiki/Master-server) feature so a server doesn\\'t need to open ports on it\\'s router.\\n- [x] [IPv6](https://en.wikipedia.org/wiki/IPv6) support for client<->server connections, allowing connection setup even behind symmetric IPv4 NAT\\n- [x] Servers displayed within the mod.\\n- [x] Settings saved as XML.\\n- [x] [UPnP](https://en.wikipedia.org/wiki/Universal_Plug_and_Play) support for servers and [master servers](../../wiki/Master-server)\\n- [x] Better creation of network messages so they are easier to modify and serialize.\\n- [x] Every network message is cached in order to reduce the garbage collector spikes.\\n- [x] Based on tasks instead of threads.\\n- [x] Supports career and science modes (funds, science, strategies, etc are shared between all players).\\n- [x] Cached [QuickLZ](http://www.quicklz.com) for fast compression without generating garbage.\\n- [ ] Support for groups/companies inside career and science modes.\\n\\nPlease check the [wiki](../../wiki) to see how to [install](../../wiki/How-to-install-LMP), [run](../../wiki/How-to-play-with-LMP), [build](../../wiki/How-to-compile-LMP) or [debug](../../wiki/Debugging-in-Visual-studio) LMP among other things\\n\\n---\\n### Troubleshooting:\\n\\nPlease visit [this page](../../wiki/Troubleshooting) in the wiki to solve the most common issues with LMP \\n[![Analytics](https://ga-beacon.appspot.com/UA-118326748-1/Home?pixel&useReferer)](https://github.com/igrigorik/ga-beacon)\\n\\n---\\n### Contributing:\\n\\nConsider [donating trough paypal](https://paypal.me/gavazquez) if you like this project. \\nIt will encourage us to do future releases, fix bugs and add new features :star:\\n\\nPlease write the code as you were going to leave it, return after 1 year and you\\'d have to understand what you wrote.  \\nIt\\'s **very** important that the code is clean and documented so in case someone leaves, another programmer could take and maintain it. Bear in mind that **nobody** likes to take a project where it\\'s code looks like a dumpster.\\n\\nThere\\'s also a test project in case you want to add tests to your code.\\n\\n---\\n### Servers:\\n\\nYou can check [how many servers are up](../../wiki/Master-server-status) and running either in [Release](../../wiki/How-to-get-the-latest-version-of-LMP) or in [Nightly](../../wiki/How-to-get-nightly-builds) versions through our [master servers](../../wiki/Master-server)\\n\\n| Master server | Release | Nightly |\\n| ------------  | ------- |-------- |\\n[Dagger](https://github.com/gavazquez) | [![Release servers](https://img.shields.io/website-up-down-brightgreen-red/http/servers.lunamultiplayer.com:8701.svg?label=status)](http://servers.lunamultiplayer.com:8701) | [![Nightly servers](https://img.shields.io/website-up-down-brightgreen-red/http/servers.lunamultiplayer.com:8751.svg?label=status)](http://servers.lunamultiplayer.com:8751) |\\nTekbot | [![Release servers](https://img.shields.io/website-up-down-brightgreen-red/http/168.119.90.137:8701.svg?label=status)](http://168.119.90.137:8701) | [![Nightly servers](https://img.shields.io/website-up-down-brightgreen-red/http/168.119.90.137:8751.svg?label=status)](http://168.119.90.137:8751) |\\n[Angryjoshi](https://github.com/Angryjoshi) | [![Release servers](https://img.shields.io/website-up-down-brightgreen-red/http/lmp.anschuetznet.de:8701.svg?label=status)](http://lmp.anschuetznet.de:8701) | [![Nightly servers](https://img.shields.io/website-up-down-brightgreen-red/http/lmp.anschuetznet.de:8751.svg?label=status)](http://lmp.anschuetznet.de:8751) |\\n[Bloodfallen](https://github.com/Bloodfallen) | [![Release servers](https://img.shields.io/website-up-down-brightgreen-red/http/direct.imexile.moe:8701.svg?label=status)](http://direct.imexile.moe:8701) | [![Nightly servers](https://img.shields.io/website-up-down-brightgreen-red/http/direct.imexile.moe:8703.svg?label=status)](http://direct.imexile.moe:8703)|\\n\\n---\\n### Status:\\n\\n|   Branch   |   Build  |   Tests  |  Last commit  |   Activity    |    Commits    |\\n| ---------- | -------- | -------- | ------------- | ------------- | ------------- |\\n| **master** |[![AppVeyor](https://img.shields.io/appveyor/ci/gavazquez/lunamultiplayer/master.svg?style=flat&logo=appveyor)](https://ci.appveyor.com/project/gavazquez/lunamultiplayer/branch/master) | [![AppVeyor Tests](https://img.shields.io/appveyor/tests/gavazquez/lunamultiplayer/master.svg?style=flat&logo=appveyor)](https://ci.appveyor.com/project/gavazquez/lunamultiplayer/branch/master/tests) | [![Last commit](https://img.shields.io/github/last-commit/lunamultiplayer/lunamultiplayer/master.svg?style=flat&logo=github&logoColor=white)](../../commits/master) | [![Commit activity](https://img.shields.io/github/commit-activity/y/lunamultiplayer/lunamultiplayer.svg?style=flat&logo=github&logoColor=white)](../../commits/master) | [![Commits since release](https://img.shields.io/github/commits-since/lunamultiplayer/lunamultiplayer/latest.svg?style=flat&logo=github&logoColor=white)](../../commits/master)\\n\\n<p align=\"center\">\\n    <a href=\"https://ci.appveyor.com/project/gavazquez/lunamultiplayer/history\"><img src=\"https://buildstats.info/appveyor/chart/gavazquez/lunamultiplayer?buildCount=100\" alt=\"Build history\"/></a>\\n</p>\\n\\n---\\n\\n<p align=\"center\">\\n  <a href=\"mailto:gavazquez@gmail.com\"><img src=\"https://img.shields.io/badge/email-gavazquez@gmail.com-blue.svg?style=flat\" alt=\"Email: gavazquez@gmail.com\" /></a>\\n  <a href=\"./LICENSE\"><img src=\"https://img.shields.io/github/license/lunamultiplayer/LunaMultiPlayer.svg\" alt=\"License\" /></a>\\n</p>\\n'},\n",
       " {'repo': 'hainproject/hain',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Hain\\n[![Build status](https://ci.appveyor.com/api/projects/status/l4p8r613wckaiqm6?svg=true)](https://ci.appveyor.com/project/appetizermonster/hain)\\n[![Build Status](https://travis-ci.org/hainproject/hain.svg)](https://travis-ci.org/hainproject/hain)\\n[![Join the chat at https://gitter.im/appetizermonster/hain](https://badges.gitter.im/appetizermonster/hain.svg)](https://gitter.im/appetizermonster/hain?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\\n\\nAn <kbd>alt</kbd>+<kbd>space</kbd> launcher for Windows, built with Electron.\\n\\nI always dreamed of an alternative to Alfred on Windows, that is made with JavaScript.\\nso, I made it.\\n\\n<p align=\"center\">\\n  <img src=\"docs/images/demo.gif\" width=\"600\"/>\\n</p>\\n\\n## Vision\\n\\nIt\\'s a launcher with strict syntax (like terminal programs), it\\'s not targeting to interpret natural language.  \\nI believe the strict syntax can provide more powerful and fast response than to interpret natural language.\\n\\n## Features\\n\\n* Searching Executable files very fast with Fuzzy Matching\\n* Plugins in Pure JavaScript\\n\\n## Downloads\\n\\nGo to [Releases](https://github.com/hainproject/hain/releases), then you can download prebuilt binaries.\\n\\n## Usage\\nRun and press <kbd>alt</kbd>+<kbd>space</kbd> anywhere.\\n\\n## Themes\\nSee [THEMES.md](THEMES.md)\\n\\n## How to make Plugins\\n\\nSee [Plugin Documentation](http://hainproject.github.io/hain/docs/)\\n\\n## Development Requirements\\n\\n- Node.js v8.9.x\\n- npm v5.6.x\\n\\n> See `engines` property in `package.json`\\n\\n## Install/Build from Source\\n\\n```shell\\n# Clone this repo\\ngit clone https://github.com/hainproject/hain.git\\n# Go into the repo\\ncd hain\\n# Install dependencies\\nnpm install\\n```\\n\\n### Run from Source\\n\\n```shell\\nnpm run dev\\n```\\n\\n### Build for Windows\\n\\n```shell\\nnpm run build\\n```\\n\\n### Build for Linux\\n\\n```shell\\nnpm run build-debian\\n```\\n\\n### Build for macOS\\n\\n```shell\\ngulp build-darwin\\n```\\n\\n## Contributing\\nSee [CONTRIBUTING.md](CONTRIBUTING.md)\\n\\n## Credits\\nThe name \"Hain\" is named by Hyunseop Lee, it means \"a Servant\" in Korean.  \\nThe app icon & gif are designed by Yunsung Lee.  \\nIt uses [npmsearch.com](https://github.com/solids/npmsearch) for searching packages for now.  \\n\\n## License\\nMIT\\n'},\n",
       " {'repo': 'trailheadapps/easy-spaces-lwc',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Easy Spaces Lightning Web Components Sample Application\\n\\n[![CI Workflow](https://github.com/trailheadapps/easy-spaces-lwc/workflows/CI/badge.svg)](https://github.com/trailheadapps/easy-spaces-lwc/actions?query=workflow%3ACI) [![Packaging Workflow](https://github.com/trailheadapps/easy-spaces-lwc/workflows/Packaging/badge.svg)](https://github.com/trailheadapps/easy-spaces-lwc/actions?query=workflow%3APackaging) [![codecov](https://codecov.io/gh/trailheadapps/easy-spaces-lwc/branch/develop/graph/badge.svg)](https://codecov.io/gh/trailheadapps/easy-spaces-lwc)\\n\\n> IMPORTANT: This is the new Lightning Web Components version of the Easy Spaces sample application. If you are looking for the Aura version, click [here](https://github.com/trailheadapps/easy-spaces).\\n\\n![Easy Spaces Logo](./docs/EasySpacesWithText.png)\\n\\nEasy Spaces is a fictional event management company that creates and manages custom pop-up spaces for companies and individuals. Easy Spaces helps customers create temporary spaces like cafés, game rooms or themed rooms for special occasions in their offices and homes.\\n\\n<div>\\n   <img src=\"https://developer.salesforce.com/resource/images/trailhead/badges/projects/trailhead_project_quick-start-explore-the-easyspaces-sample-app.png\" align=\"left\" alt=\"Trailhead Badge\" height=\"40px\" width=\"40px\" style=\"padding-right: 0.5em;\"/>\\n   <p style=\"padding-top:0.5em;\">\\n      Learn more about this app by completing the <a href=\"https://trailhead.salesforce.com/en/content/learn/projects/quick-start-explore-the-easyspaces-sample-app\" >Quick Start: Explore the Easy Spaces Sample App</a> Trailhead project.\\n   </p>\\n</div>\\n\\n[![Thumbnail](./docs/space_designer.png)](https://youtu.be/ZwvegTLx9kk)\\n\\n> This sample application is designed to run on Salesforce Platform. If you want to experience Lightning Web Components on any platform, please visit https://lwc.dev, and try out our Lightning Web Components sample application [LWC Recipes OSS](https://github.com/trailheadapps/lwc-recipes-oss).\\n\\n## Table of Contents\\n\\n-   Installation Instructions\\n\\n    -   [Installing Easy Spaces using a Scratch Org](#installing-easy-spaces-using-a-scratch-org)\\n    -   [Installing Easy Spaces using Unlocked Packages](#installing-easy-spaces-using-unlocked-packages)\\n    -   [Completing the Installation](#completing-the-installation)\\n\\n-   [Optional installation instructions](#optional-installation-instructions)\\n-   [Features](#features)\\n-   [Code Highlights](#code-highlights)\\n\\n## Installation Instructions\\n\\nThere are two ways to install Easy Spaces:\\n\\n-   [Using a Scratch Org](#installing-easy-spaces-using-a-scratch-org): This is the recommended installation option. Use this option if you are a developer who wants to experience the app and the code.\\n-   [Using Unlocked Packages](#installing-easy-spaces-using-unlocked-packages): This option allows anybody to experience the sample app without installing a local development environment.\\n\\n### Installing Easy Spaces using a Scratch Org\\n\\n1. Set up your environment. Follow the steps in the [Quick Start: Lightning Web Components](https://trailhead.salesforce.com/content/learn/projects/quick-start-lightning-web-components/) Trailhead project. The steps include:\\n\\n    - Enable Dev Hub in your Trailhead Playground\\n    - Install Salesforce CLI\\n    - Install Visual Studio Code\\n    - Install the Visual Studio Code Salesforce extensions, including the Lightning Web Components extension\\n\\n1. If you haven\\'t already done so, authenticate with your hub org and provide it with an alias (**myhuborg** in the command below):\\n\\n    ```\\n    sfdx auth:web:login -d -a myhuborg\\n    ```\\n\\n1. Clone this repository:\\n\\n    ```\\n    git clone https://github.com/trailheadapps/easy-spaces-lwc\\n    cd easy-spaces-lwc\\n    ```\\n\\n1. Create a scratch org and provide it with an alias (**easyspaces** in the command below):\\n\\n    ```\\n    sfdx force:org:create -s -f config/project-scratch-def.json -a easyspaces\\n    ```\\n\\n1. Push source to your scratch org:\\n\\n    ```\\n    sfdx force:source:push\\n    ```\\n\\n1. Assign two Easy Spaces permission sets to the default user:\\n\\n    ```\\n    sfdx force:user:permset:assign -n EasySpacesObjects\\n    sfdx force:user:permset:assign -n SpaceManagementApp\\n    ```\\n\\n1. (Optional) Assign the `Walkthroughs` permission set to the default user.\\n\\n    > Note: this will enable your user to use In-App Guidance Walkthroughs, allowing you to be taken through a guided tour of the sample app. The Walkthroughs permission set gets auto-created with In-App guidance activation.\\n\\n    ```\\n    sfdx force:user:permset:assign -n Walkthroughs\\n    ```\\n\\n1. Import sample data:\\n\\n    ```\\n    sfdx force:data:tree:import -p ./data/Plan1.json\\n    sfdx force:data:tree:import -p ./data/Plan2.json\\n    ```\\n\\n1. Open the scratch org:\\n\\n    ```\\n    sfdx force:org:open\\n    ```\\n\\n1. In **Setup**, navigate to **Themes and Branding**\\n\\n1. Activate the **Easy Spaces** theme\\n\\n### Installing Easy Spaces using Unlocked Packages\\n\\nFollow this set of instructions if you want to deploy the app to a more permanent environment than a Scratch org or if you don\\'t want to install the local developement tools. You can use a non source-tracked orgs such as a free [Developer Edition Org](https://developer.salesforce.com/signup) or a [Trailhead Playground](https://trailhead.salesforce.com/).\\n\\nMake sure to start from a brand-new environment to avoid conflicts with previous work you may have done.\\n\\n1. Log in to your org\\n\\n1. Click [this link](https://login.salesforce.com/packaging/installPackage.apexp?p0=04t4W000002Lm9QQAS) to install the **es-base-objects** package and choose **Install for All Users**.\\n\\n1. Click [this link](https://login.salesforce.com/packaging/installPackage.apexp?p0=04t4W000002LmAOQA0) to install the **es-base-styles** package and choose **Install for All Users**.\\n\\n1. Click [this link](https://login.salesforce.com/packaging/installPackage.apexp?p0=04t4W000002LmAJQA0) to install the **es-base-code** package and choose **Install for All Users**.\\n\\n1. Click [this link](https://login.salesforce.com/packaging/installPackage.apexp?p0=04t4W000002Lm9kQAC) to install the **es-space-mgmt** package and choose **Install for All Users**.\\n\\n1. From the command line, enter the following commands to clone this repository. You need to do this to get the metadata, code and files with sample data on your computer:\\n\\n    ```\\n    git clone https://github.com/trailheadapps/easy-spaces-lwc\\n    cd easy-spaces-lwc\\n    ```\\n\\n1. Assign EasySpacesObjects permission set by following below steps:\\n\\n    - Go to **Setup > Users > Permission Sets**.\\n    - Click **EasySpacesObjects**.\\n    - Click **Manage Assignments**.\\n    - Click **Add Assignments**.\\n    - Select your username and click **Assign**.\\n\\n1. Assign SpaceManagementApp permission set by following below steps:\\n\\n    - Go to **Setup > Users > Permission Sets**.\\n    - Click **SpaceManagementApp**.\\n    - Click **Manage Assignments**.\\n    - Click **Add Assignments**.\\n    - Select your username and click **Assign**.\\n\\n1. Import Lead data:\\n\\n    - In **Setup**, type **Data Import** in the Quick Find box and click **Data Import Wizard**.\\n    - Click **Launch Wizard**.\\n    - Click the **Standard objects** tab, click **Leads**, and click **Add New Records**.\\n    - Drag **Lead_Data.csv** from the data folder of this project to the upload area.\\n    - Click **Next**, **Next**, and **Start Import**.\\n\\n1. Import Contact data:\\n\\n    - In **Setup**, type **Data Import** in the Quick Find box and click **Data Import Wizard**.\\n    - Click **Launch Wizard**.\\n    - Click the **Standard objects** tab, click **Accounts and Contacts**, and click **Add New Records**.\\n    - Drag **Contact_Data.csv** from the data folder of this project to the upload area.\\n    - Click **Next**, **Next**, and **Start Import**.\\n\\n1. Import Market data:\\n\\n    - In **Setup**, type **Data Import** in the Quick Find box and click **Data Import Wizard**.\\n    - Click **Launch Wizard**.\\n    - Click the **Custom objects** tab, click **Markets**, and click **Add New Records**.\\n    - Drag **Market_Data.csv** from the data folder of this project to the upload area.\\n    - Click **Next**, **Next**, and **Start Import**.\\n\\n1. Import Spaces data:\\n\\n    - In **Setup**, type **Data Import** in the Quick Find box and click **Data Import Wizard**.\\n    - Click **Launch Wizard**.\\n    - Click the **Custom objects** tab, click **Spaces**, and click **Add New Records**.\\n    - In the **Add New Records** menu, under _Which Market field in your file specifies the Master/Detail relationship?_ select **Market Name**\\n    - Drag **Space_Data.csv** from the data folder of this project to the upload area.\\n    - Click **Next**, **Next**, and **Start Import**.\\n\\n1. In **Setup**, navigate to **Themes and Branding**\\n\\n1. Activate the **Easy Spaces** theme\\n\\n#### Explore the application\\n\\n1. In **App Launcher**, click **View all** then select the **Space Management** app.\\n\\n1. Note: Before trying to work with the Spaces Designer, use the **Reservation Manager** to draft a few reservations.\\n\\n1. Have fun exploring!\\n\\n## Optional Installation Instructions\\n\\nThis repository contains several files that are relevant if you want to integrate modern web development tooling to your Salesforce development processes, or to your continuous integration/continuous deployment processes.\\n\\n### Code formatting\\n\\n[Prettier](https://prettier.io/) is a code formatter used to ensure consistent formatting across your code base. To use Prettier with Visual Studio Code, install [this extension](https://marketplace.visualstudio.com/items?itemName=esbenp.prettier-vscode) from the Visual Studio Code Marketplace. The [.prettierignore](/.prettierignore) and [.prettierrc](/.prettierrc) files are provided as part of this repository to control the behavior of the Prettier formatter.\\n\\n> **Warning**\\n> The current Apex Prettier plugin version requires that you install Java 11 or above.\\n\\n### Code linting\\n\\n[ESLint](https://eslint.org/) is a popular JavaScript linting tool used to identify stylistic errors and erroneous constructs. To use ESLint with Visual Studio Code, install [this extension](https://marketplace.visualstudio.com/items?itemName=salesforce.salesforcedx-vscode-lwc) from the Visual Studio Code Marketplace. The [.eslintignore](/.eslintignore) file is provided as part of this repository to exclude specific files from the linting process in the context of Lighning Web Components development.\\n\\n### Pre-commit hook\\n\\nThis repository also comes with a [package.json](./package.json) file that makes it easy to set up a pre-commit hook that enforces code formatting and linting by running Prettier and ESLint every time you `git commit` changes.\\n\\nTo set up the formatting and linting pre-commit hook:\\n\\n1. Install [Node.js](https://nodejs.org) if you haven\\'t already done so\\n1. Run `npm install` in your project\\'s root folder to install the ESLint and Prettier modules (Note: Mac users should verify that Xcode command line tools are installed before running this command.)\\n\\nPrettier and ESLint will now run automatically every time you commit changes. The commit will fail if linting errors are detected. You can also run the formatting and linting from the command line using the following commands (check out [package.json](./package.json) for the full list):\\n\\n```\\nnpm run lint\\nnpm run prettier\\n```\\n\\n## Features\\n\\nA quick overview of the features you can explore in Easy Spaces:\\n\\n-   Modular app design and Unlocked Packages\\n-   Lightning Console APIs & Background Refresh Methods\\n-   Salesforce Flow\\n\\n    -   Dynamic flow interview components\\n    -   Custom flow screen components\\n    -   Local Action components\\n\\n-   Custom Lightning Page Templates\\n-   Lightning Theming\\n-   Custom Metadata Types\\n\\n## Code Highlights\\n\\n### Dynamic Flows and Local Action Components\\n\\nThe **spaceDesigner** and **reservationHelper** Aura components render flow interviews dynamically, by using the **lightning:flow** base component. You can see the **reservationHelperForm** and **spaceDesignForm** Lightning web components at work as screens in these dynamic flows. Both of these components use **lightning\\\\_\\\\_FlowScreen** as the target in the component’s <targets> tag, and **FlowNavigationNextEvent** event from the **lightning/flowSupport** module to control flow navigation actions.\\n\\nSee this [blog post](https://developer.salesforce.com/blogs/2018/06/announcing-the-easy-spaces-app.html) for more detail about custom flow navigation and dynamic flow interviews.\\n\\nThese components used as Salesforce Flow screens also use a convention in the markup of their meta files, to help developers better track how properties are being used by flow interviews. See the [reservationHelperForm](./es-space-mgmt/main/default/lwc/reservationHelperForm/reservationHelperForm.js-meta.xml) and [spaceDesignForm](./es-space-mgmt/main/default/lwc/spaceDesignForm/spaceDesignForm.js-meta.xml) component meta files for examples.\\n\\n### Object-Agnostic Design\\n\\nThe **customerList** and **customerTile** Lightning web components can display information from Contact objects or Lead objects. In the Reservation Manager app page, both Lead and Contact variations are used in order to create a unified workspace:\\n\\n![Two instances of customerList component on canvas in Lightning App Builder](./docs/reservation_manager.png)\\n\\nThe customerList Lightning web component uses a design attribute to allow for users working in Lightning App Builder to control which object an instance of the Lightning web component should display:\\n\\n![customerList component design attribute as picklist in Lightning App Builder](./docs/customerList_design.png)\\n\\nThis is just one example of object-agnostic design at work in Easy Spaces. See this [blog post](https://developer.salesforce.com/blogs/2018/06/announcing-the-easy-spaces-app.html) for more detail about this pattern.\\n\\n### Console Navigation and Background Refresh\\n\\nEasy Spaces uses the Lightning Console JavaScript API and the Lightning web component NavigationMixin to control user navigation between tabs and subtabs. You can see the lightning:workspaceAPI component at work in the **openRecordAction** Aura component, which enables flow interviews to navigate users to a new subtab. The **reservationHelperAura** and **spaceDesignerAura** components use the lightning:navigationItemAPI component to refresh custom Lightning Page tabs in the background as a user works. You can get more detail about using the Workspace API in your components in this [blog post](https://developer.salesforce.com/blogs/2018/06/announcing-the-easy-spaces-app.html).\\n\\nLightning web components use the NavigationMixin control navigation behavior. You can see the NavigationMixin at work in the [relatedSpaces](./es-space-mgmt/main/default/lwc/relatedSpaces/relatedSpaces.js), [customerTile](./es-space-mgmt/main/default/lwc/customerTile/customerTile.js) and [reservationTile](./es-space-mgmt/main/default/lwc/reservationTile/reservationTile.js) Lightning web components. The **customerList** and **reservationList** Lightning web components use Lightning Data Service wired methods to display data from Apex. This allows the platform to handle provisioning and managing a client-side cache. These components programmatically refresh that wired data, based on user interactions with sibling Aura components. Check out the code in [reservationList](./es-space-mgmt/main/default/lwc/reservationList/reservationList.js) and [customerList](./es-space-mgmt/main/default/lwc/customerList/customerList.js) Lightning web component Javascript files.\\n\\n### Modular Design and Unlocked Packaging\\n\\nEasy Spaces illustrates how to organize application metadata into granular units or modules. This approach is reflected in the design patterns at work throughout the application, like the use of object-agnostic components. But you\\'ll also see this at work in the structure of the Easy Spaces repo itself.\\n\\nThe Easy Spaces application is made of several, interdependent unlocked packages. The dependecies between the Easy Spaces packages are listed in the [sfdx-project.json](./sfdx-project.json) file for this repo.\\n\\nYou can also explore the contents of each package by looking at the related package folder within this repo. The `path` attribute entries in the sfdx-project.json show which folder contains the metadata for a particular package.\\n\\nFor more about how the Easy Spaces metadata is organized into package modules, check out [this post](https://developer.salesforce.com/blogs/2018/06/working-with-modular-development-and-unlocked-packages-part-2.html).\\n\\n## Code Tours\\n\\nCode Tours are guided walkthroughs that will help you understand the app code better. To be able to run them, install the [CodeTour VSCode extension](https://marketplace.visualstudio.com/items?itemName=vsls-contrib.codetour).\\n'},\n",
       " {'repo': 'sgermosen/Spacegame', 'language': 'Java', 'readme_contents': ''},\n",
       " {'repo': 'Spacebrew/spacebrew',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': 'Spacebrew Server\\n================\\n\\nA dynamically re-routable software toolkit for choreographing interactive spaces. Visit http://www.spacebrew.cc to learn more about spacebrew. On our site we feature a bunch of example apps and tutorials to help you get started. You\\'ll also find a blog where we feature spacebrew projects and events.  \\n  \\n@version: \\t\\t0.4.0  \\n@date:\\t\\t\\tApril 10, 2014\\n@contributors: \\t\\tLAB at Rockwell Group, Quin Kennedy, Brett Renfer, Josh Walton, James Tichenor, Julio Terra   \\n  \\nGetting Started\\n---------------  \\n  \\n### 1. Install Dependencies  \\n* Download and install [Node.js](http://nodejs.org)  \\n* Clone the repo from github  \\n* Install the dependencies using node packaged modules   \\n    - `npm install`\\n  \\n### 2. Run the Server  \\n* Open terminal and navigate to the base directory of the spacebrew server  \\n* Run the server by using `node node_server_forever.js`  \\n  \\n`node_server_forever.js` vs `node_server.js`\\nThe first of these two files runs node using the forever-monitor node utility. This utility relaunches the spacebrew server if it crashes and it saves logs of the standard output from the spacebrew server to log files in the data/logs directory.\\n\\n### 3. Connect Client Apps  \\n* Open the [spacebrew_button example](http://spacebrew.github.io/spacebrew.js/spacebrew_button/index.html?server=localhost&name=button2) - make sure that the `server=` in the query string points to the appropriate host. Customize the `name=` element in the query string to change your apps name.  \\n* Open the [spacebrew admin interface](http://spacebrew.github.io/spacebrew/admin/admin.html?server=localhost) in another browser window - again, make sure that the `server=` in the query string points to the appropriate host.  \\n* Start connecting apps and routing data.   \\n  \\nSpacebrew Server Options\\n------------------------ \\nHere is an overview of the command line options that the spacebrew server accepts:\\n```\\n--port (-p): set the port of the spacebrew server (default 9000)\\n--help (-h): print help text (which is what you are reading here)\\n--close (-c): force close clients that don\\'t respond to pings\\n--ping: enable pinging of clients to track who is potentially disconnected (default)\\n--noping: opposite of --ping\\n--timeout (-t): minimum number of ms to wait for response pong before force closing (implies --close, default 10000 [10 seconds])\\n--persist: saves route configurations that are set via any admin interface\\n--nopersist: opposite of --persist\\n--log (-l): sets logging to info level\\n--loglevel: set logging to info, debug, warn, error\\n--pinginterval: the number of ms between pings (implies --ping, default 1000 [1 second])\\n--secure: launch the server with https/wss support. See certificate details in the secure/ directory.\\n```\\n\\nHere are a few examples of how to launch the app using command line options:\\n```\\n\\tnode node_forever_server.js -p 9011 -t 1000 --pinginterval 1000\\n\\tnode node_server.js --nopersist --loglevel warn\\n```\\n\\nOther Services\\n-------------- \\n\\n### HTTP Link\\n\\nThe HTTP Link (`http_link.js`) is a Node.js app which acts essentially as an HTTP <-> Websocket bridge for Spacebrew. Only `GET` requests are supported currently, so all commands are read from the query string. Responses are provided as JSON.\\n\\nThe HTTP Link allows you to use HTTP-only devices, such as the [Electric Imp](http://electricimp.com/), within the Spacebrew environment. \\n\\n1. Register a client by sending a `config` query string key which contains the same json structure as would be sent over Websockets \\n    - `http://localhost:9092/?config={\"config\":{\"name\":\"test\",\"publish\":{\"messages\":[{\"name\":\"output\",\"type\":\"string\"},{\"name\":\"out\",\"type\":\"string\"}]},\"subscribe\":{\"messages\":[{\"name\":\"input\",\"type\":\"string\"}]}}}`\\n    - this is the human-readable version, don\\'t forget to URL encode the data first\\n* The HTTP Link will respond with a `clientID` that you will use in the future to refer your client.\\n* You can send messages into the Spacebrew environment by sending a `publish` query string key which contains an array of messages you wish to publish\\n    - `http://localhost:9092/?clientID=0&publish=[{\"message\":{\"clientName\":\"test\",\"name\":\"output\",\"type\":\"string\",\"value\":\"hello!\"}},{\"message\":{\"clientName\":\"test\",\"name\":\"output\",\"type\":\"string\",\"value\":\"good bye.\"}}]`\\n    - in this case we are sending 2 messages\\n* You can retrieve sent messages by including `poll=true` in the query string. This will return an array of all messages that have been received by the HTTP Link for your client since the last poll:\\n    - `http://localhost:9092/?clientID=0&poll=true`\\n* By default, only one message is queued per subscriber. If you wish to queue more, you can send a `bufferSize` along with your subscriber specifications\\n    - `http://localhost:9092/?clientID=0&config={\"config\":{\"name\":\"test\",\"publish\":{\"messages\":[{\"name\":\"output\",\"type\":\"string\"},{\"name\":\"out\",\"type\":\"string\"}]},\"subscribe\":{\"messages\":[{\"name\":\"input\",\"type\":\"string\",\"bufferSize\":3}]}}}`\\n    - this example also shows how you can send a config update\\n* By default the HTTP Link will remove your client after 5 minutes if there is no queries associated with it. You can change this at any time by specifying a custom `timeout` in seconds\\n    - `http://localhost:9092/?clientID=0&poll=true&timeout=3600`\\n    - this example polls for input and also sets a 1-hour timeout\\n\\n### Command Line Persistent Admin\\n\\nThe Persistent Admin (`node_persistent_admin.js`) is a command line Node.js app which makes sure certain specified publishers and subscribers always stay routed to one-another.\\n\\nAfter starting the Persistent Admin (`node node_persistent_admin.js` in the command line/terminal) you can type `help` to get an overview of the various commands available. Basically you can:\\n\\n* `ls` to get a list of currently-protected routes\\n* `add myClient,pubOne,theirClient,subscriberUno` to connect the `pubOne` publisher associated with client `myClient` to the `subscriberUno` subscriber associated with client `theirClient`\\n* `save` to save to disk\\n* `load` to load from disk (it will automatically load when starting up)\\n* `remove 0` to remove the zero\\'th route from protection (when you list the routes via `ls`, the indices listed before each route are what should be used for the remove command)\\n* `exit` to quit the Persistent Admin\\n* the `add` command can also be used with regular expressions such as `add myClient,.*,theirClient,.*` to connect all publishers from `myClient` with all compatible subscribers in `theirClient`\\n\\n\\n=============\\n#### LICENSE\\nThe MIT License (MIT)\\nCopyright © 2012 LAB at Rockwell Group, http://www.rockwellgroup.com/lab\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n'},\n",
       " {'repo': 'HanSolo/SpaceFX',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '## SpaceFX \\n\\n<br>\\nA simple tiny space game written in JavaFX.\\n\\nThe \"dev\" branch contains new developments that might come to the game later on. The \"simpleversion\" branch is like the name says more simple and should also run on\\nembedded devices etc. \\nThe \"master\" branch now contains some more fun things like enemies attacking in waves, a level boss that needs more hits to kill, enemies that drop bombs and an\\nenemy boss that also fires rockets.\\n\\n\\nIf you want to see/play it in a browser you can check it out at the [jpro demo page](https://demos.jpro.one/spacefx.html)\\n\\n\\nDonations are welcome at [Paypal](https://paypal.me/hans0l0)\\n\\n### Desktop and Mobile\\n<p>Desktop<br><img src=\"https://raw.githubusercontent.com/HanSolo/SpaceFX/master/SpaceFX_Desktop.jpg\" width=50%></img></p>\\n<p>iOS<br><img src=\"https://raw.githubusercontent.com/HanSolo/SpaceFX/master/SpaceFX_iOS.jpg\" width=50%></img></p>\\n<p>Android<br><img src=\"https://raw.githubusercontent.com/HanSolo/SpaceFX/master/SpaceFX_Android.jpg\" width=50%></img></p>\\n<p>Web<br><img src=\"https://raw.githubusercontent.com/HanSolo/SpaceFX/master/SpaceFX_Web.jpg\" width=50%></img></p>\\n\\n### Youtube video\\nI\\'ve recorded a little [video](https://youtu.be/Kc0lv3R5VG0) that shows the game in action.\\n\\n\\n### Requirements for building a native package\\nIf you would like to build a native package you should have at least JDK 13 installed\\non your machine. Make sure you run on Java13 and do a `gradle clean build jar` on the \\ncommand line and execute the build app script e.g. `bash build_app.sh`. If everything \\nworked ok you will find the native app in the folder `/build/installer`.\\nTo build a native package you will need the early access release of the \\njpackage tool. Please find more info [here](https://github.com/dlemmermann/JPackageScriptFX).\\n\\n\\n### master branch\\nThis branch is using gradle for the build and it needs JDK 17 with OpenJFX 17.0.0.1.\\nOn my machines I use [Zulu](https://cdn.azul.com/zulu/bin/) for the JDK 17 installation and\\n[OpenJFX](https://openjfx.io/) for the JavaFX installation.\\nYou should be able to fork the branch and open the build.gradle file in your favourite IDE as a project to run it from the code.\\nTo compile it you need to make sure you are on JDK13 and OpenJFX is installed, then execute the following on the command line.\\n\\nOS X:\\n```\\ncd /PATH/TO/SpaceFX\\n\\n./gradlew clean build\\n\\nbash ./build_app.sh\\n```\\n\\nWindows:\\n```\\ncd \\\\PATH\\\\TO\\\\SpaceFX\\n\\n.\\\\gradlew.bat clean build\\n\\n.\\\\build_app.bat\\n```\\n\\nLinux:\\n```\\ncd /PATH/TO/SpaceFX\\n\\n./gradlew clean build\\n\\nbash ./build_app_linux.sh\\n```\\n\\n\\nAfter that you will find the runnable jar file in\\n```\\n/PATH/TO/SpaceFX/build/libs\\n```\\nand the bundle created by the jpackage tool from JDK17 in\\n```\\n/PATH/TO/SpaceFX/build/installer\\n```\\n\\n### mobile branch\\nThis branch uses maven for the build and it needs at least JDK 11. In principle you simply have to follow the instructions\\nover at [github](https://github.com/gluonhq/client-samples) from Gluon to make it run on iOS and Android.\\nIf you have the setup your system as mentioned on the github page with the Gluon examples you can build/deploy SpaceFX to your\\ndevice as follows. To build/run it on iOS you need to run it on a Mac and to build/run it on Android you need to run it on Linux.\\nFirst of all make sure you are on JDK11 and that you have installed all things described on github.\\n\\niOS:\\nMake sure your iPhone is registered as a developer device at [apple](https://developer.apple.com/) and is plugged into your Mac.\\n```\\nexport JAVA_HOME=$GRAALVM_HOME\\n\\ncd /PATH/TO/SpaceFX\\n\\nmvn clean -Pios client:build\\n\\nmvn -Pios client:run\\n```\\n\\nThe iOS spacefx.app file can be found at \\n```\\n/PATH/TO/SpaceFX/target/client/arm64-ios/\\n```\\n\\n\\nOSX:\\n```\\nexport JAVA_HOME=$GRAALVM_HOME\\n\\ncd /PATH/TO/SpaceFX\\n\\nmvn clean client:build\\n```\\n\\nThe OSX spacefx binary can be found at\\n```\\n/PATH/TO/SpaceFX/target/client/x86_64-darwin\\n```\\n\\n\\nAndroid:\\nMake sure your Android phone is a developer phone and is plugged into your Linux machine.\\n```\\nexport JAVA_HOME=$GRAALVM_HOME\\n\\ncd /PATH/TO/SpaceFX\\n\\nmvn clean -Pandroid client:build\\n\\nmvn -Pandroid client:run\\n```\\n\\nThe Android spacefx.apk file can be found at\\n```\\n/PATH/TO/SpaceFX/target/client/aarch64-android/gvm/apk/bin\\n```\\n\\n\\nLinux:\\n```\\nexport JAVA_HOME=$GRAALVM_HOME\\n\\ncd /PATH/TO/SpaceFX\\n\\nmvn clean client:build\\n```\\n\\nThe Linux binary can be found at\\n```\\n/PATH/TO/SpaceFX/target/x86_64-linux\\n```\\n\\nKeep in mind that at the moment there is no support for sound when using the GraalVM/Substrate combo.\\n\\n###Attention:\\nBecause we use different build tools for the master and mobile branch at the moment it can lead to problems\\nin your favourite IDE when switching branches. To avoid those problems just keep the following steps in mind:\\n\\nWhen switching from the master to the mobile branch you should follow these simple steps:\\n- remove the project from your IDE\\n- close your IDE\\n- switch branch from master to mobile\\n- remove files/folders like .gradle, .idea, /build and /logs\\n- start your IDE\\n- open the pom.xml file as a project\\n\\nWhen switching from mobile to the master branch you should follow these simple steps:\\n- remove the project from your IDE\\n- close your IDE\\n- switch branch from mobile to master\\n- start your IDE\\n- open the build.gradle file as a project\\n\\n\\n### Run SpaceFX in the browser using jpro\\nTo run SpaceFX in the browser you will need to set the used JDK to 11 in\\nthe build.gradle file. In the future you will also be able to use JDK 13 etc.\\nMore info on how to run a JavaFX application in the browser can be found\\nhere [jpro](https://www.jpro.one/?page=docs/current/1.1/).\\n\\nTo make SpaceFX run in your browser you need to be on the master branch and execute the following steps:\\nOpen the build.gradle file in an editor and comment out the line:\\n```\\n//mainClassName = \"$moduleName/eu.hansolo.spacefx.SpaceFX\"\\n```\\nNow enable the line:\\n```\\nmainClassName = \"eu.hansolo.spacefx.SpaceFX\"\\n```\\n\\nOn the command line execute:\\n```\\ncd /PATH/TO/SpaceFX\\n\\n./gradlew jproRun\\n```\\n\\nOpen a browser and go to ```localhost:8080```\\n'},\n",
       " {'repo': 'mozilla/hubs',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# [Mozilla Hubs](https://hubs.mozilla.com/)\\n\\n[![License: MPL 2.0](https://img.shields.io/badge/License-MPL%202.0-brightgreen.svg)](https://opensource.org/licenses/MPL-2.0) [![Build Status](https://travis-ci.org/mozilla/hubs.svg?branch=master)](https://travis-ci.org/mozilla/hubs) [![Discord](https://img.shields.io/discord/498741086295031808)](https://discord.gg/CzAbuGu)\\n\\nThe client-side code for [Mozilla Hubs](https://hubs.mozilla.com/), an online 3D collaboration platform that works for desktop, mobile, and VR platforms.\\n\\n[Learn more about Hubs](https://hubs.mozilla.com/docs/welcome.html)\\n\\n## Getting Started\\n\\nIf you would like to run Hubs on your own servers, check out [Hubs Cloud](https://hubs.mozilla.com/docs/hubs-cloud-intro.html).\\n\\nIf you would like to deploy a custom client to your existing Hubs Cloud instance please refer to [this guide](https://hubs.mozilla.com/docs/hubs-cloud-custom-clients.html).\\n\\nIf you would like to contribute to the main fork of the Hubs client please see the [contributor guide](./CONTRIBUTING.md).\\n\\nIf you just want to check out how Hubs works and make your own modifications continue on to our Quick Start Guide.\\n\\n### Quick Start\\n\\n[Install NodeJS](https://nodejs.org) if you haven\\'t already. We use 16.16.0 on our build servers. If you work on multiple javascript projects it may be useful to use something like [NVM](https://github.com/nvm-sh/nvm) to manage multiple versions of node for you.\\n\\nRun the following commands:\\n\\n```bash\\ngit clone https://github.com/mozilla/hubs.git\\ncd hubs\\n# nvm use v16.16.0 # if using NVM\\nnpm ci\\nnpm run dev\\n```\\n\\nThe backend dev server is configured with CORS to only accept connections from \"hubs.local:8080\", so you will need to access it from that host. To do this, you likely want to add \"hubs.local\" and \"hubs-proxy.local\" to the [local \"hosts\" file](https://phoenixnap.com/kb/how-to-edit-hosts-file-in-windows-mac-or-linux) on your computer:\\n\\n```\\n127.0.0.1\\thubs.local\\n127.0.0.1\\thubs-proxy.local\\n```\\n\\nThen visit https://hubs.local:8080 (note: HTTPS is required, you\\'ll need to accept the warning for the self-signed SSL certificate)\\n\\n> Note: When running the Hubs client locally, you will still connect to the development versions of our [Janus WebRTC](https://github.com/mozilla/janus-plugin-sfu) and [reticulum](https://github.com/mozilla/reticulum) servers. These servers do not allow being accessed outside of localhost. If you want to host your own Hubs servers, please check out [Hubs Cloud](https://hubs.mozilla.com/docs/hubs-cloud-intro.html).\\n\\n## Documentation\\n\\nThe Hubs documentation can be found [here](https://hubs.mozilla.com/docs).\\n\\n## Community\\n\\nJoin us on our [Discord Server](https://discord.gg/CzAbuGu) or [follow us on Twitter](https://twitter.com/MozillaHubs).\\n\\n## Contributing\\n\\nRead our [contributor guide](./CONTRIBUTING.md) to learn how you can submit bug reports, feature requests, and pull requests.\\n\\nWe\\'re also looking for help with localization. The Hubs redesign has a lot of new text and we need help from people like you to translate it. Follow the [localization docs](./src/assets/locales/README.md) to get started.\\n\\nContributors are expected to abide by the project\\'s [Code of Conduct](./CODE_OF_CONDUCT.md) and to be respectful of the project and people working on it.\\n\\n## Additional Resources\\n\\n* [Reticulum](https://github.com/mozilla/reticulum) - Phoenix-based backend for managing state and presence.\\n* [NAF Janus Adapter](https://github.com/mozilla/naf-janus-adapter) - A [Networked A-Frame](https://github.com/networked-aframe) adapter for the Janus SFU service.\\n* [Janus Gateway](https://github.com/meetecho/janus-gateway) - A WebRTC proxy used for centralizing network traffic in this client.\\n* [Janus SFU Plugin](https://github.com/mozilla/janus-plugin-sfu) - Plugins for Janus which enables it to act as a SFU.\\n* [Hubs-Ops](https://github.com/mozilla/hubs-ops) - Infrastructure as code + management tools for running necessary backend services on AWS.\\n\\n## Privacy\\n\\nMozilla and Hubs believe that privacy is fundamental to a healthy internet. Read our [privacy policy](https://www.mozilla.org/en-US/privacy/hubs/) for more info.\\n\\n\\n## License\\n\\nHubs is licensed with the [Mozilla Public License 2.0](./LICENSE)\\n'},\n",
       " {'repo': 'ilyagru/Space-Snake',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Space Snake\\n\\nA Desktop game built with [Electron-vue](https://github.com/SimulatedGREG/electron-vue) template.\\n\\n![Screenshot](/Screenshot.png?raw=true \"Screenshot\")\\n\\n[![Packagist](https://img.shields.io/packagist/l/doctrine/orm.svg)](https://github.com/ilyagru/Space-Snake/blob/master/LICENSE)\\n[![](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)]()\\n[![js-standard-style](https://img.shields.io/badge/code%20style-standard-brightgreen.svg)](http://standardjs.com/)\\n[![](https://img.shields.io/badge/tests-not%20tested-red.svg)]()\\n[![](https://img.shields.io/badge/platform-macOS%20%7C%20Windows%20%7C%20Linux-blue.svg)]()\\n[![](https://img.shields.io/badge/download-releases-brightgreen.svg)](https://github.com/ilyagru/Space-Snake/releases)\\n[![MadeWithVueJs.com shield](https://madewithvuejs.com/storage/repo-shields/173-shield.svg)](https://madewithvuejs.com/p/space-snake/shield-link)\\n\\n## Description\\n\\nA desktop game Space Snake.\\n- Built with [Electron](https://electron.atom.io/) and [Vue.js](https://vuejs.org/).\\n- State Management – [Vuex](https://github.com/vuejs/vuex).\\n- Routing – [Vue-router](https://github.com/vuejs/vue-router).\\n- Bundling – [Webpack](https://webpack.github.io/).\\n- Styles – [SASS](http://sass-lang.com/).\\n- Tests – [Karma](https://karma-runner.github.io/1.0/index.html) + [Mocha](https://mochajs.org/).\\n\\n\\n## Build Setup\\n\\n``` bash\\n# install dependencies\\nnpm install\\n\\n# serve with hot reload at localhost:9080\\nnpm run dev\\n\\n# build electron app for production\\nnpm run build\\n\\n# lint all JS/Vue component files in `app/src`\\nnpm run lint\\n\\n# run webpack in production\\nnpm run pack\\n```\\nMore information can be found [here](https://simulatedgreg.gitbooks.io/electron-vue/content/docs/npm_scripts.html).\\n\\n---\\n\\nThis project was generated from [electron-vue](https://github.com/SimulatedGREG/electron-vue) using [vue-cli](https://github.com/vuejs/vue-cli). Documentation about this project can be found [here](https://simulatedgreg.gitbooks.io/electron-vue/content/index.html).\\n\\n\\nTODO:\\n\\n- [x] Settings (speed(level))\\n- [x] Leaderboards\\n- [x] Save data\\n- [x] Logo, icon\\n- [x] Idea\\n- [x] Styles, animation\\n- [ ] Tests\\n- [x] UI/UX\\n- [x] Clean code\\n- [x] Downloadable bundles\\n\\n## License\\n\\n[MIT](https://github.com/ilyagru/Space-Snake/blob/master/LICENSE)\\n'},\n",
       " {'repo': 'mschwager/fierce',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"# Fierce\\n\\n[![CI](https://github.com/mschwager/fierce/actions/workflows/ci.yml/badge.svg)](https://github.com/mschwager/fierce/actions/workflows/ci.yml)\\n[![Coverage Status](https://coveralls.io/repos/github/mschwager/fierce/badge.svg?branch=master)](https://coveralls.io/github/mschwager/fierce?branch=master)\\n[![Dlint](https://github.com/mschwager/fierce/actions/workflows/dlint.yml/badge.svg)](https://github.com/mschwager/fierce/actions/workflows/dlint.yml)\\n[![Python Versions](https://img.shields.io/pypi/pyversions/fierce.svg)](https://img.shields.io/pypi/pyversions/fierce.svg)\\n[![PyPI Version](https://img.shields.io/pypi/v/fierce.svg)](https://img.shields.io/pypi/v/fierce.svg)\\n\\nFierce is a `DNS` reconnaissance tool for locating non-contiguous IP space.\\n\\nUseful links:\\n\\n* [Domain Name System (DNS)](https://en.wikipedia.org/wiki/Domain_Name_System)\\n  * [Domain Names - Concepts and Facilities](https://tools.ietf.org/html/rfc1034)\\n  * [Domain Names - Implementation and Specification](https://tools.ietf.org/html/rfc1035)\\n  * [Threat Analysis of the Domain Name System (DNS)](https://tools.ietf.org/html/rfc3833)\\n* [Name Servers (NS)](https://en.wikipedia.org/wiki/Domain_Name_System#Name_servers)\\n* [State of Authority Record (SOA)](https://en.wikipedia.org/wiki/List_of_DNS_record_types#SOA)\\n* [Zone Transfer](https://en.wikipedia.org/wiki/DNS_zone_transfer)\\n  * [DNS Zone Transfer Protocol (AXFR)](https://tools.ietf.org/html/rfc5936)\\n  * [Incremental Zone Transfer in DNS (IXFR)](https://tools.ietf.org/html/rfc1995)\\n* [Wildcard DNS Record](https://en.wikipedia.org/wiki/Wildcard_DNS_record)\\n\\n# Overview\\n\\nFirst, credit where credit is due, `fierce` was\\n[originally written](https://github.com/mschwager/fierce/blob/master/scripts/fierce.pl)\\nby RSnake along with others at http://ha.ckers.org/. This is simply a\\nconversion to Python 3 to simplify and modernize the codebase.\\n\\nThe original description was very apt, so I'll include it here:\\n\\n> Fierce is a semi-lightweight scanner that helps locate non-contiguous\\n> IP space and hostnames against specified domains. It's really meant\\n> as a pre-cursor to nmap, unicornscan, nessus, nikto, etc, since all \\n> of those require that you already know what IP space you are looking \\n> for. This does not perform exploitation and does not scan the whole \\n> internet indiscriminately. It is meant specifically to locate likely \\n> targets both inside and outside a corporate network. Because it uses \\n> DNS primarily you will often find mis-configured networks that leak \\n> internal address space. That's especially useful in targeted malware.\\n\\n# Installing\\n\\n```\\n$ python -m pip install fierce\\n$ fierce -h\\n```\\n\\nOR\\n\\n```\\n$ git clone https://github.com/mschwager/fierce.git\\n$ cd fierce\\n$ python -m pip install -r requirements.txt\\n$ python fierce/fierce.py -h\\n```\\n\\n*Requires Python 3.*\\n\\n# Using\\n\\nLet's start with something basic:\\n\\n```\\n$ fierce --domain google.com --subdomains accounts admin ads\\n```\\n\\nTraverse IPs near discovered domains to search for contiguous blocks with the\\n`--traverse` flag:\\n\\n```\\n$ fierce --domain facebook.com --subdomains admin --traverse 10\\n```\\n\\nLimit nearby IP traversal to certain domains with the `--search` flag:\\n\\n```\\n$ fierce --domain facebook.com --subdomains admin --search fb.com fb.net\\n```\\n\\nAttempt an `HTTP` connection on domains discovered with the `--connect` flag:\\n\\n```\\n$ fierce --domain stackoverflow.com --subdomains mail --connect\\n```\\n\\nExchange speed for breadth with the `--wide` flag, which looks for nearby\\ndomains on all IPs of the [/24](https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing#IPv4_CIDR_blocks)\\nof a discovered domain:\\n\\n```\\n$ fierce --domain facebook.com --wide\\n```\\n\\nZone transfers are rare these days, but they give us the keys to the DNS castle.\\n[zonetransfer.me](https://digi.ninja/projects/zonetransferme.php) is a very\\nuseful service for testing for and learning about zone transfers:\\n\\n```\\n$ fierce --domain zonetransfer.me\\n```\\n\\nTo save the results to a file for later use we can simply redirect output:\\n\\n```\\n$ fierce --domain zonetransfer.me > output.txt\\n```\\n\\nInternal networks will often have large blocks of contiguous IP space assigned.\\nWe can scan those as well:\\n\\n```\\n$ fierce --dns-servers 10.0.0.1 --range 10.0.0.0/24\\n```\\n\\nCheck out `--help` for further information:\\n\\n```\\n$ fierce --help\\n```\\n\\n# Developing\\n\\nFirst, install development packages:\\n\\n```\\n$ python -m pip install -r requirements.txt\\n$ python -m pip install -r requirements-dev.txt\\n$ python -m pip install -e .\\n```\\n\\n## Testing\\n\\n```\\n$ pytest\\n```\\n\\n## Linting\\n\\n```\\n$ flake8\\n```\\n\\n## Coverage\\n\\n```\\n$ pytest --cov\\n```\\n\"},\n",
       " {'repo': 'TeamPorcupine/ProjectPorcupine',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# ProjectPorcupine [![Build Status](https://travis-ci.org/TeamPorcupine/ProjectPorcupine.svg?branch=master)](https://travis-ci.org/TeamPorcupine/ProjectPorcupine)\\n\\n# **NOTE** We have now moved over to a new repository due to change in management, find it [here](https://github.com/OrderOfThePorcupine/ProjectPorcupine)\\n\\n### Project Porcupine: A Base-Building Game...in Space!\\n\\n![Preview Thumbnail](https://cloud.githubusercontent.com/assets/22880786/19826387/7ad0f0d2-9dd4-11e6-92f3-eb47b395ac63.png)\\n\\n[About](#about)  \\n[Copyright & Licensing](#copyright--licensing)  \\n[Contributing](#contributing)  \\n[Vote on Proposed Features](#vote-on-proposed-features)  \\n[Community](#community)  \\n[Contact](#contact) \\n\\n## About\\n\\nProject Porcupine was created to serve two purposes:\\n\\n1. To act as a tutorial to teach people how to make a full,\\n  complex, and multi-featured game inside of Unity (as opposed\\n  to the more typical one-off, single-feature tutorials that\\n  are more common).\\n\\n2. To provide a basic skeleton for any game that requires a\\n  tile-based map with self-governing agents (i.e. characters)\\n  as well as highly customizable objects (i.e. XML/Lua defined\\n  furniture.)  To this end, we would be making a program themed as\\n  a starbase-construction game -- though there\\'s no reason that \\n  someone couldn\\'t produce something with a wildly different theme\\n  or purpose (including not being a base-building game at all).\\n\\n**However, our true ultimate goal is to produce a community-developed\\nbase building game in a similar style to Dwarf Fortress or RimWorld!**\\n\\nIf you want to watch the tutorials that cover the entirety of\\ncreating this project from scratch, please visit:\\n\\n * <https://www.youtube.com/user/quill18creates/playlists>\\n\\nProject Porcupine was created by Martin \"quill18\" Glaude, whose work\\nwas supported via Patreon:\\n\\n * <https://www.patreon.com/quill18creates>\\n\\n## Copyright & Licensing\\n\\nThe base project code is copyrighted by Martin \"quill18\" Glaude and\\nis covered by multiple licenses.\\n\\nAll program code (i.e. C#, Lua, XML) is licensed under GPL v3.0 unless otherwise\\nspecified.  Please see the \"LICENSE\" file for more information.\\n\\nAll non-code assets (e.g. art, sound) is licensed under CC BY-NC-SA 3.0\\n(Attribution-NonCommercial-ShareAlike 3.0 Unported) unless otherwise specified.\\n\\nThe original tutorial project files, which feature no community-contributed code,\\nare licensed under the MIT License and can be found here:\\n * <http://quill18.com/porcupine/project_files/>\\n \\nAudio engine : [FMOD](http://www.fmod.com/) by Firelight Technologies\\n\\nLogger: Based on UnityDebugger by Valian (at <https://github.com/Valian/UnityDebugger>), modified source code available at <https://github.com/koosemose/UnityDebugger>\\n\\n## Contributing\\n\\nPlease check the [CONTRIBUTING.md](CONTRIBUTING.md) file for contribution instructions and guidelines.\\n\\nFor further information, such as Roadmaps, explanations of systems and features, Standards and Conventions, and all your Git needs and troubleshooting see the [Wiki](https://github.com/TeamPorcupine/ProjectPorcupine/wiki)\\n\\nMake sure that you are using Unity 5.4.2 [Windows](https://unity3d.com/get-unity/download?thank-you=update&download_nid=43049&os=Win) | [Mac](https://unity3d.com/get-unity/download?thank-you=update&download_nid=43049&os=Mac)\\n\\n## Community\\n\\n* [Unoffical Discord Channel ](https://discord.gg/68hkpSA)\\n* [Unoffical Subreddit](https://reddit.com/r/ProjectPorcupine)\\n\\n## Contact\\n\\nYou can contact Quill18 by Twitter (@quill18) or email:\\n    quill18@quill18.com\\n\\nHowever, please note that Quill receives a lot of email and may\\nnot be able to respond to everyone in a timely manner.\\n\\n'},\n",
       " {'repo': 'UnityTechnologies/SpaceShooterECS',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# Space Shooter ECS\\n\\n**Description**\\n\\nThis repo contains the Space Shooter game with ECS implementation developed by Intel and Unity.\\nA video walkthrough can be found [Here](https://youtu.be/WLfhUKp2gag). This project is meant to be a \"Rosetta Stone\" for understanding how to get started with the ECS structure in Unity. \\n\\nThe basic examples can be found under Assets/Simple Examples while the final result can be found in Assets/Scenes.\\n\\n**Contact**\\n\\nIf you have any questions about this simple example, please feek free to contact Mike Geig:\\n- Email: mike[at]unity3d.com\\n- Twitter: [@mikegeig](https://twitter.com/mikegeig)\\n'},\n",
       " {'repo': 'ferram4/Ferram-Aerospace-Research',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': 'Ferram Aerospace Research v0.15.9.1 \"Liepmann\"\\n=========================\\nAerodynamics model for Kerbal Space Program\\n\\n   Serious thanks:\\n\\t\\t\\t\\t* a.g., for tons of bugfixes and code-refactorings   \\n\\t\\t\\t\\t* stupid_chris, for the RealChuteLite implementation\\n            \\t\\t\\t* Taverius, for correcting a ton of incorrect values  \\n\\t\\t\\t\\t* Tetryds, for finding lots of bugs and issues and not letting me get away with them, and work on example crafts\\n            \\t\\t\\t* sarbian, for refactoring code for working with MechJeb, and the Module Manager updates  \\n            \\t\\t\\t* ialdabaoth (who is awesome), who originally created Module Manager  \\n                        \\t* Regex, for adding RPM support  \\n\\t\\t\\t\\t* DaMichel, for some ferramGraph updates and some control surface-related features  \\n            \\t\\t\\t* Duxwing, for copy editing the readme  \\n   \\n   CompatibilityChecker by Majiir, BSD 2-clause http://opensource.org/licenses/BSD-2-Clause\\n\\n   Part.cfg changes powered by sarbian & ialdabaoth\\'s ModuleManager plugin; used with permission  \\n\\thttp://forum.kerbalspaceprogram.com/threads/55219\\n\\n   ModularFLightIntegrator by Sarbian, Starwaster and Ferram4, MIT: http://opensource.org/licenses/MIT\\n\\thttp://forum.kerbalspaceprogram.com/threads/118088\\n\\n   Toolbar integration powered by blizzy78\\'s Toolbar plugin; used with permission  \\n\\thttp://forum.kerbalspaceprogram.com/threads/60863\\n\\nSource available at: https://github.com/ferram4/Ferram-Aerospace-Research\\n\\n----------------------------------------------\\n---------------- INSTALLATION ----------------\\n----------------------------------------------\\n\\nInstall by merging the GameData folder in the zip with the GameData folder in your KSP install\\n\\nModuleManager and ModularFlightIntegrator are REQUIRED for FAR to work properly.  Failing to copy this over will result in strange errors.\\n\\n-----------------------------------------------------\\n---------------- LEGACY WING CONFIGS ----------------\\n-----------------------------------------------------\\n\\nSample Part.cfg:\\n\\nFor wings\\n-----------------------------------\\n```\\nMODULE  \\n{  \\n\\tname = FARControllableSurface / FARWingAerodynamicModel  \\n\\tb_2 = 0.5\\t\\t\\t\\t//distance from wing root to tip; semi-span  \\n\\tMAC = 0.5\\t\\t\\t\\t//Mean Aerodynamic Chord  \\n\\tnonSideAttach = 0\\t\\t\\t//0 for canard-like / normal wing pieces, 1 for ctrlsurfaces attached to the back of other wing parts  \\n\\tTaperRatio = 0.7\\t\\t\\t//Ratio of tip chord to root chord generally < 1, must be > 0  \\n\\tMidChordSweep = 25\\t\\t\\t//Sweep angle in degrees; measured down the center of the span / midchord position  \\n\\tmaxdeflect = 15\\t\\t\\t\\t//Default maximum deflection value; only used by FARControlableSurface  \\n\\tcontrolSurfacePivot = 1, 0, 0;\\t\\t//Local vector that obj_ctrlSrf pivots about; defaults to 1, 0, 0 (right)  \\n\\tctrlSurfFrac = 0.2\\t\\t\\t//Value from 0-1, percentage of the part that is a flap; only used by FARControlableSurface  \\n}\\n```\\n\\nFor control surfaces, use above but replace FARWingAerodynamicModel with FARControllableSurface and add maxdeflect value\\n\\nSet all the other winglet/control surface values to zero\\n\\n\\nCHANGELOG\\n=======================================================\\n0.15.9.1V \"Liepmann\"------------------------------------  \\n\\nUpdate for KSP 1.3.1 (though not strictly necessary)  \\nUpdate to MM 3.0.4 for KSP 1.3.1  \\n\\nAdded ability to override structural stress values for aerodynamic failures on a per-part basis  \\nSwitch to applying forces through part.AddForce rather than rb.AddForce to allow Principia to handle gravity within atmospheres  \\nAdded functions to KSPAPI to check the status of any vessel\\'s voxelization  \\n\\nFix issues with all RealChuteLite chutes having the same exact drag properties  \\nFix RealChuteLite GUI not displaying any information  \\nRemove unnecessary stock lifting body effects on pods  \\n\\n0.15.9V \"Liebe\"------------------------------------  \\n\\nUpdate for KSP 1.3  \\nUpdate to MM 2.8.1  \\n\\nInclude support for localization  \\nInclude German (by terorie), Russian (by pand5461), and Chinese (by Nigh) translations\\n\\nFix NaN errors with Trajectories  \\nFix some issues with identifying KSPWheel Adjustable Landing Gear as gear  \\n\\n\\n0.15.8.1V \"Lewis\"------------------------------------  \\n\\nBugfix patch for KSP 1.2.2  \\n\\nFix Flight GUI button activated/not activated being backwards  \\nDon\\'t revoxelize for several B9 and AJE animation modules to reduce lag, thanks blowfish  \\nFix game crashing when a vessel landed in water is loaded  \\n\\n0.15.8V \"de Laval\"------------------------------------  \\n\\nCompatibility for KSP 1.2.2 (finally)  \\nUpdate to MFI 1.2.4  \\nUpdate to MM 2.7.6  \\n\\nLots of compatibility changes thanks to Alexander Abramov  \\nReduce memory use and garbage production in GUI thanks to soulsource and Virindi-AC  \\n\\nFix GUI button multiplication  \\nFix stock drag arrows to be useful again  \\nFix voxelization errors with some intake parts  \\nFix FARAction group settings not saving  \\nFix landing gear main axis dtermination  \\nFix voxel errors with some stock parts  \\n\\nMade ignorable transforms for voxelization customizable via config  \\n\\n\\n0.15.7.2V \"Lanchester\"------------------------------------  \\n\\nFix a serious bug in v0.15.7 and v0.15.7.1 where chutes would not provide any drag  \\n\\n0.15.7.1V \"Kutta\"------------------------------------  \\n\\nUpdate to MFI 1.1.6 to fix an incompatibility with Kopernicus and the earlier version  \\nUpdate CompatibilityChecker version  \\nUpdate license  \\n\\nFix an issue where voxels could be incredibly asymmetric on symmetric crafts  \\n\\n\\n0.15.7V \"K�chemann\"------------------------------------  \\n\\nUpdate to ModuleManager 2.6.25  \\nUpdate for KSP 1.1.3 compatibility  \\n\\nImplement higher resolution sub-voxel voxelization method  \\nAllow switching between high and low res sub-voxel methods  \\nOptimize voxel shell generation, particularly for high triangle count meshes  \\nIncrease the resistance to sideways aerostructural failures for many fuselage and rocket parts  \\n\\nFix voxelization error that would lead to transparent mesh objects being voxelized  \\nFix voxelization errors that could lead to incomplete voxelization of some stock procedural fairing shapes  \\n\\n0.15.6.5V \"Knudsen\"------------------------------------  \\n\\nUpdate to ModularFlightIntegrator 1.1.4  \\nFix a serious issue where wings would provide no forces and forces would be distributed incorrectly across vehicles  \\nFix an issue where wing symmetry counterparts would not have equal masses  \\nFix non-zero convective heat flux on shielded parts  \\n\\n0.15.6.4V \"Kleinhans\"------------------------------------  \\n\\nFix a no-drag issue with asteroids  \\nFix a physics breaking issue with Tweakscaled wing parts, thanks pellinor  \\nFix GUI window positions not loading on vessel spawn  \\nFix distribution of forces on parts; no change in total force and torque applied to vessel, just to which parts  \\nFix slightly negative drag on rearward-facing vehicles at high Knudsen numbers  \\n\\n0.15.6.3V \"Kindelberger\"------------------------------------\\n\\nRecompile for KSP 1.1.2 compatibility  \\nBundle ModuleManager 2.6.24 for 1.1.2 compatibility  \\n\\nFix a critical error that would cause KerbalEVAs to have no aerodynamic forces applied to them  \\n\\n0.15.6.2V \"Kartveli\"------------------------------------\\n\\nEnsure KSP 1.1.1 compatiblity  \\nUpgrade to ModuleManager 2.6.23  \\n\\nFix new landing gear interfering with main axis determination  \\nFix RealChute / RealChuteLite interaction breaking stock chute behavior, thanks to stupid_chris  \\nFix mass-calc error for wing-mass-strength that resulted in all planes gaining unhealthy amounts of weight  \\nAttempt to make debug-compatibility actually work, thanks to NathanKell  \\n\\n0.15.6.1V \"von K�rm�n\"------------------------------------\\n\\nFix a critical CPU usage bug that resulted in voxelization threads SpinWaiting forever, monopolizing the processor  \\nFix parachutes without RealChute configs not applying forces when FAR + RC are installed, thanks to stupid_chris  \\nFix ModuleManager database reload function hanging halfway through, breaking the game, thanks to stupid_chris  \\n\\n0.15.6V \"Jones\"------------------------------------  \\n\\nUpdate to KSP 1.1  \\nUpdate to bundle ModuleManager 2.6.22  \\nUpdate to bundle ModularFlightIntegrator 1.1.3  \\n\\nUpdates to RealChuteLite, thanks to stupid_chris  \\nCompatibility changes for use of KSP debuggers, thanks to neouy  \\n\\nIncrease aerodynamic damping for fuselages to somewhat more realistic levels  \\nFix a serious issue that disabled the majority of conduction between parts  \\n\\nDisable win64 locking\\n\\n0.15.5.7V \"Johnson\"------------------------------------  \\n\\nTweak pitch and roll damping of fuselages to make more logical sense; excessive roll damping at high dynamic pressures for wingless vehicles has been fixed  \\nChange units for specific excess power in the Flight Data readout to be W/kg on the basis that it makes more logical sense than m^2/s^3  \\n\\nFix a critical error that prevented voxelizations of Kerbals or any vehicles that had Kerbals riding in a command seat  \\n\\n\\n0.15.5.6V \"Jacobs\"------------------------------------  \\n\\nUpdate to MM 2.6.18\\n\\nFix more negative sonic drag issues  \\nFix unrealistically low sonic drag  \\nFix failure to load saved FAR data in flight  \\nFix unrealistically high numbers in indicated airspeed at higher Mach numbers  \\n\\nLower critical Mach number for slender vehicles with sudden bulges and waviness in their cross-section  \\n\\n\\n0.15.5.5V \"Hugoniot\"------------------------------------  \\n\\nFix an inconsistency in calculations of sonic drag  \\nFix possibility of sonic drag resulting in negative drag coefficients on very blunt shapes  \\nGenerally increase sonic drag of blunt objects, generally decrease drag of slender objects  \\n\\nFix water drag failing to function under complete submersion  \\nFix rare error where Procedural Fairings will not properly voxelize  \\nFix GetCurrentDensity method (for external mods) to return result consistent with simulation  \\nFix overheat interaction on load with ModuleCoreHeat  \\nFix FAR breaking on attempts to load Training or Scenario scenes  \\nFix spoilers and flaps not updating with settings in the editor  \\n\\n\\n0.15.5.4V \"Hoerner\"------------------------------------  \\n\\nAdjust water drag for better splashdown performance  \\nFix a serious voxelization issue with ModuleJettison, most notable in leading to no-drag reentries  \\nFix an issue where 3rd-party voxelization updates could sometimes break the editor GUI and CoL  \\nFix a serious issue that could lead to spontaneous crashes on aero initialization (either VAB / SPH CoL, editor GUI, or going to flight)  \\n\\n\\n0.15.5.3V \"von Helmholtz\"------------------------------------  \\n\\nUpgrade to MM 2.6.13  \\n\\nRealChuteLite consistency with RealChute calcs and optimizations thanks to stupid_chris  \\nImplement dynamic smoothing calculations based on relative \"filledness\" of voxel; should help reduce effect of voxel-resolution-induced smoothing on larger vehicles  \\nTweaks to critical Mach calculations  \\n\\nFix \"silent\" KSP update breaking hydrodynamic drag  \\nFix some voxelization irregularities  \\nFix control surface flap settings not appearing if the settings are turned on in flight  \\nFix some other control surface in-flight changes oddities  \\n\\nFix Firehound MS example craft action groups not acting in symmetry  \\nAdded E42 example craft by tetryds  \\n\\n\\n0.15.5.2V \"Helmbold\"------------------------------------  \\n\\nCompatibility with KSP 1.0.5  \\nUpgrade to MFI 1.1.2  \\n\\nOptimizations of runtime aerodynamic calculations  \\nCut background memory usage for voxels to ~60% of previous value  \\nIncreases in consistency of properties with similar voxel shapes  \\n\\nFull support for new stock hydrodynamic drag  \\nVoxel model used in calculating radiative influx from celestial bodies  \\nReduction in first-load inconsistency in editor  \\nMore varied support for intake ducting setups, including support for stock Goliath engine  \\nTweaks to intake drag at low airbreather throttles\\n\\nEditor GUI header cleanup  \\nDropdowns notated with down triangles for clarity  \\nAdded AoA Arrow to make AoAs for static analysis sweeps and stability deriv sims clearer  \\n\\nFix for voxelization issues with degenerate triangles  \\nFix for voxelization issues with meshes with 0 triangles  \\nFix for B9 pWings not solidifying properly  \\nFix editor race condition in displaying sonic drag for vehicles  \\nFix for multiple vehicle aerodynamic NREs that could break aero  \\nFix for vehicle aerodynamics breaking under certain vessel-part configurations  \\n\\nUpdated FAR Firehound MS, FAR SkyEye, FAR Montauk Shuttle to be more useful in KSP 1.0.5\\n\\n\\n0.15.5.1V \"Hayes\"------------------------------------  \\n\\nUpgrade to MM 2.6.8  \\n\\nFix some legacy wing interaction issues  \\nFix drag properties drifting slowly over multiple voxelization events due to numerical errors  \\nFix parts being occluded when main axis is in a strange orientation  \\nFix in-flight control surface tweaks not applying to symmetry counterparts  \\nFix KerbalEVAs working with Vanguard Parachutes  \\n\\nFix for a critical error where detached boosters, weapons, debris, etc. would not have drag properties  \\n\\n\\n0.15.5V \"Haack\"------------------------------------  \\n\\nUpgrade to MM 2.6.7  \\nFix for some RealChute issues by DaMichel  \\nAnimation ignoring for voxelization by Blowfish  \\nAddition of air brakes for yaw control (RudderBrakes) adopted from original code contributed by HoneyFox  \\n\\nReduction in memory garbage created during voxelization; this should reduce the impact of voxelization somewhat, especially hitching.  Note: some hitching may still occur with vehicles with many wing parts due to legacy wing code  \\nRuntime performance optimizations for ram drag and general aero calculations  \\nHighly optimized bounds checking for voxelization  \\n\\nKerbals handled by voxel model now (using a simple primitive shape, presence or absence of helmet doesn\\'t matter)  \\nControl surface parameters available for tweaking in flight  \\nControl surface tweakables given open/close buttons for sections to reduce clutter  \\nRevert BDArmory bombs and missiles to stock model after launch to improve tracking, stability and predictions  \\n\\nFixed main axis issue with BDArmory parts and similarly designed parts  \\nFixed a long-standing issue in wing aspect ratio calcs (they were double what they should have been)  \\nFixed ram drag variation with throttle not accepting AJE jets as valid jets for that purpose  \\nFixed drag not using the calculated critical Mach number for the beginning of the drag transonic rise  \\nFixed possible NRE issues during steady destruction of vessel with parts being destroyed  \\nFixed issue where increased number of wing parts would cause wing mass adjustment to increase mass without bound; mass is now bounded, but more concentrated in root parts than wingtip parts  \\n\\nTweaked subsonic drag downwards to get more accurate results with fixed AJE props\\nTweaked wing mass downwards slightly  \\nTweaked wing strength power to result in greater strength from lower-mass wings  \\n\\n\\n0.15.4.1V \"Goldstein\"------------------------------------  \\n\\nRe-implementation of aero viz coloration, thanks to mjn33  \\n\\nReduction in garbage produced by voxelization, prep for further garbage reductions  \\n\\nFixed NaN issue with KAX electric props  \\nFixed drag-breaking NRE during rapid disintegrations  \\nFixed some issues with Blizzy Toolbar icons  \\nFixed exacerbation of stock heating bug  \\nFixed control surfaces not updating direction during staging-related CoM shifts  \\n\\n0.15.4V \"Glauert\"------------------------------------  \\n\\nUpdate to MM 2.6.6  \\nUpdate to MFI 1.1.1, fixes gimbaling bug below 750 m/s on short vehicles  \\nUpdate win64 check code to MM method  \\n\\nAdded internal ducted area feature:  \\n\\t* Area ducted through a vehicle from intakes to airbreathing engines will be removed from cross-section  \\n\\t* Adjusts area ruling to properly model air that flows through the vehicle as opposed to around it  \\n\\t* Does not count for airflow through switch-backing or reversing ducts; no benefits for intakes that feed upstream engines  \\n\\t* Supports stock intake part + airbreathing engine part setups, AJE intake part + airbreathing engine part setups, and combined intake + engine part setups  \\n\\nSlight improvement to Flight Data readouts from mjn33  \\nToggle gear button now states \"Raise\" or \"Lower\" gear for clarity  \\n\\nFixed serious issue where exposed area was not updated for thermal calculations  \\nFixed some blunt shapes having NaN drag in the transonic regime  \\nFixed some blunt, thin-plate shapes having negative drag in the transonic regime  \\nFixed serious issue where long, skinny vehicles would have incorrect 2nd derivatives and incorrect transonic drag as a result  \\nFixed NRE with Launch Clamps  \\nFixed NRE with animations that are removed from a part (for whatever reason)  \\n\\nCleaned up unused values from FARAeroData.cfg  \\nAdded support for planets to be identified by planet name, not just index; combining these in a FARAeroData MM patch is likely to cause overwrites, don\\'t do it  \\nAdded ability to read and set flap and spoiler states from FARAPI  \\nFixed Firespitter gear not responding to Toggle Gear button  \\nAdded support for adjustable landing gear in Toggle Gear button  \\nStopgap fix to unintended voxelization of USI Warp Drive bubbles  \\n\\n\\n0.15.3.1V \"Garabedian\"------------------------------------  \\n\\nCompatibility with KSP v1.0.3 and thermal changes  \\nFix one last error with voxelization breaking due to reverts\\n\\nAdd ability to make parts not count for main axis determination; fix structural panels interfering with proper main axis determination  \\n\\n0.15.3V \"Froude\"------------------------------------  \\n\\nUpdate to MM 2.6.5 for greater nyan nyan  \\nAllow display of pressure coefficient (under assumption of axisymmetric flow) over the vehicle  \\nTweak subsonic drag to be lower for slender shapes  \\n\\nFixed voxelization breaking due to combined memory leak + hard memory limit for voxelization after many editor -> flight cycles  \\nFixed some race conditions in voxelization that could break aero properties  \\nFixed deadlock in threadpool if many voxelization events triggered simultaneously  \\nFixed possibility of deadlock if voxelization settings were updated  \\n\\nFixed voxelization errors for some cargo bays and other parts  \\nFixed voxelization errors for pWings; includes support for any parts making use of mirrorAxis  \\n\\nFixed some longstanding wing interaction issues, including permanent stalled wings  \\nFixed a newer issue with wing shielding on symmetry counterparts  \\n  \\nSome main axis determination improvements  \\nFixed an where certain user atmospheric settings would not take  \\n\\n0.15.2V \"Ferri\"------------------------------------  \\n\\nImproved voxelization accuracy  \\nChanged CoL code again to try and make it more useful  \\nCleaned up some unnecessary calculations  \\n\\nFixed voxelization breaking after many voxelization events; this fixes no-drag situations  \\nFixed deployed spoilers not producing drag if mounted flush with vehicle  \\nFixed some main axis issues  \\nFixed improper heating area for atmospheric heat  \\n\\nFixed interaction with KIS breaking things  \\nFixed some data not saving  \\nFixed exceptions during EVA  \\n\\n0.15.1V \"Fanno\"------------------------------------  \\n\\nFixed improper voxelization of debris and vehicles dropped from existing vessel, including effects on stock \"occlusion\" system  \\nFixed improper determination of vehicle main axis  \\nFixed Kerbal EVAs having no drag  \\nFixed exceptions where outirght disintegration could prevent some vehicles from having aerodynamics applied  \\n\\nAdded upper cap on memory allocated for voxelization  \\n\\nChanged calculation of CoL to make more sense  \\nFixed error in determining AoA for nominal flight in Stability Derivative GUI  \\nHid yellow aero moment arrows by default in aero overlay to reduce user confusion  \\nFixed lift / drag arrows remaining on wings that become shielded when aero overlay is open  \\n\\nSwitched to a cleaner method of setting internal speedometers  \\nDisable control surfaces auto-response below 5 m/s to prevent wacky flailing during load / when stopped  \\n\\nChange compatibility settings to reject KSP 1.0.0, which is not compatible with RealChuteLite  \\nUpdated save-load method to save more reliably and not throw exceptions  \\n\\n\\n0.15V \"Euler\"------------------------------------  \\n\\nCompatibility with KSP 1.0, 1.0.1, and 1.0.2  \\nUpgraded to MM 2.6.3  \\nIntroduction of ModularFlightIntegrator for interfacing with KSP drag / heating systems without interference with other mods\\n\\nReplaced previous part-based drag model with new vessel-centered, voxel-powered model:  \\n\\t* Generates voxel model of vehicle using part meshes, accounting for part clipping  \\n\\t* Drag is calculated for vehicle as a whole, rather than linear combination of parts  \\n\\t* Payload fairings and cargo bays are emergent from code and do not require special treatment with configs  \\n\\t* Area ruling of vehicles is accounted for; unsmooth area distributions will result in very high drag at and above Mach 1  \\n\\t* Body lift accounts for vehicle shape in determining potential and viscous flow contributions  \\n\\t* Areas exposed to outside used for stock heating calculations  \\n\\nPerformance optimizations in legacy wing model  \\nJet engine windmilling drag accounted for at intakes  \\n\\nEditor GUI improvements including:  \\n\\t* Greater clarity in AoA / Mach sweep tab  \\n\\t* Stability deriv GUI math modified for improved accuracy  \\n\\t* Stability deriv simulation tweaked to fix some minor issues in displaying and calculating response  \\n\\t* Addition of a Transonic Design tab that displays cross-section distribution and drag at Mach 1 for area ruling purposes  \\n\\nParachute methods have been replaced with RealChuteLite implementation by stupid_chris:  \\n\\t* Less severe parachute deployment  \\n\\t* Parachutes melt / break in high Mach number flows  \\n\\t* No interference with RealChute  \\n\\nChanges to FARAPI to get information faster  \\n\\t\\nFARBasicDragModel, FARPayloadFairingModule, FARCargoBayModule are now obsolete and removed from the codebase  \\nExtensive reorganizing of source to reduce spaghetti and improve maintainability  \\n\\nModifications to Firehound and Colibri to function with new flight model  \\nAddition of Blitzableiter and SkyEye example crafts  \\n\\nA 1.5x increase to all stock gimbal ranges  \\n\\n0.14.7V------------------------------------  \\nFeatures:  \\nRaised stalled-wing drag up to proper maximum levels  \\nAdjusted intake drag to be lower  \\nImproved method of dealing with very high vertex count parts for geometry purposes  \\nUpgraded to MM 2.5.13  \\nIncluded FAR Colibri, a VTOL by Tetryds as an example craft  \\n\\nBugfixes:  \\nFixed an issue preventing loading custom-defined FARBasicDragModels  \\n\\n0.14.7V------------------------------------  \\nFeatures:  \\nRaised stalled-wing drag up to proper maximum levels  \\nAdjusted intake drag to be lower  \\nImproved method of dealing with very high vertex count parts for geometry purposes  \\nUpgraded to MM 2.5.13  \\nIncluded FAR Colibri, a VTOL by Tetryds as an example craft  \\n\\nBugfixes:  \\nFixed an issue preventing loading custom-defined FARBasicDragModels\\n\\n\\n0.14.6V------------------------------------  \\nFeatures:  \\nModified skin friction variation with M and Re to closer to that expected by using the Knudsen number  \\nChanged saving and loading method to allow better behavior when settings need to be cleaned during updates, especially for automated installs  \\nModified aerodynamic failures for water landings for compatibility with upcoming BetterBuoyancy  \\nOption for aerodynamic failures to result in explosions at the joint during failure.  \\nSerious reworking to handle edge cases with lightly-clipped parts and their effects on blunt body drag (read: when people clip heatshields into the bottom of Mk1 pods and cause problems)  \\nUpgrade to MM 2.5.6\\n\\nBugfixes:  \\nFixed an issue that prevented Trajectories from functioning  \\nFixed blunt body drag errors with AJE  \\nFixed issues involving editor GUI and control surface deflections  \\nFixed edge cases involving attach-node blunt body drag being applied when it shouldn\\'t have  \\nFixed issues with command pods containing intakes\\n\\n0.14.5.1V------------------------------------  \\nFeatures:  \\nAdd Reynolds Number readout to main flight GUI\\n\\nTweaks:  \\nAdjust skin friction drag for rarefied atmosphere\\n\\nBugfixes:  \\nFix Stab Deriv GUI from breaking for altitudes above atmosphere  \\nFix flaps and spoilers not functioning with negative deflections\\n\\n\\n0.14.5V------------------------------------  \\nFeatures:  \\nSkin friction drag now varies with Reynolds number; this means much higher skin friction drags at higher altitudes  \\nAdded simple attempt at handling hydrodynamic effects; not detailed, but objects in oceans move much less  \\nAdded color changing options for colorblind users  \\nTweak flap and spoiler deflection functions  \\nGive spoilers faster deflection coefficients  \\nUpdate to ModuleManager 2.5.4\\n\\nBugfixes:  \\nRemoved spontaneous aero-spline warp drive in some Linux64 versions\\n\\n0.14.4.1v------------------------------------  \\nFeatures:  \\nAdded changes to blunt body drag to make command pods more stable on reentry  \\nAttempt to account for most inaccurate effects of part clipping  \\n\\n0.14.4v------------------------------------\\nFeatures:  \\nDefault ActionGroups now controlled throuhg dropdown menus rather than string entry  \\nStability Deriv tab now takes entry in terms of planet, altitude and Mach Number, not density, temperature and Mach number  \\nStability Deriv tab now accounts for reduced gravity due to high speeds\\n\\nContributed by HoneyFox:  \\n\\tPitch damper now has an additional gain for greater tuning  \\n\\tControl surfaces can now be set to deflect in response to local AoA changes  \\n\\tControl surfaces are not On/Off for a given control direction; can be scaled from -100% to 100% for each  \\n\\nContributed by Bitronic:  \\n\\tFull Tweakscale Support\\n\\nBugFixes:  \\nFixed no shielding with some payload fairings (particularly resized procedural fairings)  \\nFixed aero tinting blocking tinting from other mods\\n\\t\\n\\n\\n0.14.3.2v------------------------------------  \\nFeatures:  \\nContributed by Da Michel:  \\n\\tAirspeed settings change readouts in cockpits  \\n\\nBugfixes:  \\nFixed serious issues with the wing interaction code  \\nFixed an issue where wind velocity was applied in the opposite direction that was expected  \\n\\n\\n0.14.3.1v------------------------------------  \\nFeatures:  \\nImproved performance in editor and flight for vessel configuration changes  \\nFliht GUI appears in mapview  \\n\\nBugfixes:  \\nFixed neverending stall resulting from wing interactions with sudden changes in velocity vector direction  \\nFixed flight GUI issues when passing another vehicle  \\n\\n\\n0.14.3v------------------------------------\\nFeatures:\\nRefactored wing interaction code:  \\n\\tWing interactions should be smoother  \\n\\tCode should be less processor intensive  \\n\\nUpgrade to ModuleManager v2.5.1  \\nAdded stall visualization to aero force visualization  \\nAdded ability to scale wing mass up or down for additional strength / weight savings (addedby NathanKell)   \\nImproved cargo bay and payload fairing detection algorithm  \\n\\nTweaks:  \\nReduced intake drag  \\nDecreased wing mass per area slightly  \\n\\nBugfixes:\\nFixed aero visualization leaving parachutes glowing brightly  \\nFixed some critical errors for when config files do not have values listed  \\nFixed an issue with AppLauncher buttons multiplying when KSP fails at loading a particular vessel\\n\\n0.14.2v------------------------------------\\nFeatures:\\n0.25 compatibility, with stock support for SP+ parts  \\nUpgrade CompatibilityChecker  \\nDisable functions on CompatibilityChecker warnings\\n\\nPrototype aero force visualization in flight  \\nRemoved vector from CoL indicator to reduce confusion  \\nMore Get functions for the FARAPI  \\nEstimated range and endurance readouts in the Flight Data UI  \\nSee and dump FAR module data in the VAB / SPH using the Editor GUI  \\nSome runtime optimizations  \\n\\nContributed by Da Michel:  \\n\\tImplement separate deflection speeds for flaps / spoilers  \\n\\tAllow preferred default action groups for spoilers / flaps  \\nContributed by regex:  \\n\\tAdd some RPM integration  \\nContributed by Ippo:  \\n\\tFARWind class for 3rd-party wind implementation\\n\\n\\nBugfixes:\\nFixed some vessel-switching FAR GUI issues  \\nFixed control surface reversal on undocking or backwards root part selection  \\nFixed some issues involving CoL position with wings when dealing with parts that have multiple colliders  \\nFixed some payload fairing and cargo bay part detection issues \\n'},\n",
       " {'repo': 'kubowania/space-invaders',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# space-invaders\\nA vanilla JavaScript game with HTML and CSS\\n\\nThis walkthrough has been updated. Please see the newest version of the full walkthrough [here](https://youtu.be/3Nz4Yp7Y_uA) \\n\\nI have kept the styling at a bare miniumum for you to go wild and make it your own. Please tag me as I would LOVE to see your game!!!\\n\\nSpace Invaders is a simple grid-based game in which you as the shooter have to shoot down as many alien invaders before they get to you. We are going to build a 15 x 15 grid square with 20 invaders to shoot. \\n\\nIn this repo, I will be putting extra focus on for loops and classLists in JavaScript. If you want to learn how to use these loops and classLists effectively, please have a look at my code.\\n\\n\\n### We are also going to be looking at:\\n* for loops\\n* addEventListener\\n* classList\\n* document.querySelector\\n* Timeout\\n* switch case\\n\\nAs always I have kept the styling at a bare minimum for you to go wild and make it your own.\\n\\n\\n### MIT Licence\\n\\nCopyright (c) 2020 Ania Kubow\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\\n\\n*Translation: Ofcourse you can use this for you project! Just make sure to say where you got this from :)\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n'},\n",
       " {'repo': 'hzl123456/SpacesItemDecoration',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '# SpacesItemDecoration\\n自定义RecyclerView.ItemDecoration，实现RecyclerView的分割线效果\\n\\n简书地址：http://www.jianshu.com/p/3b860938e503\\n'},\n",
       " {'repo': 'shahar603/SpaceXtract',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# SpaceXtract\\n\\n![SpaceXtract gif](https://github.com/shahar603/SpaceXtract/blob/master/docs/SpaceXtract.gif)\\n\\n\\nExtraction and analysis of telemetry from launch provders\\' webcasts (like SpaceX and RocketLab).\\nThis module is built for Python 3. You\\'ll need [OpenCV](http://opencv.org/), [NumPy](http://www.numpy.org/), [Streamlink](https://streamlink.github.io/) and [FFMpeg](https://ffmpeg.org/). Video sources are local files or HLS compliant streams.\\n\\n \\n\\n\\nInstalling the required modules\\n==========================\\n\\n\\n\\nAll the required modules can be installed using pip in the following manner:\\n\\n```\\npip install -r requirements.txt\\n```\\n\\nOr manualy by installing the individul modules:\\n\\n```\\npip install numpy\\npip install opencv-python\\npip install streamlink\\npip install matplotlib\\n```\\n\\nYou will need [FFMpeg](https://ffmpeg.org/) to be installed and to be in ```PATH```\\n\\n\\nUsage\\n=========\\n\\nTo capture telemetry from SpaceX\\'s webcasts clone this repository and run ```python get_telemetry_spacex.py``` with the webcast (video file).\\n\\n\\nHere\\'s the output of the ```--help``` option:\\n\\n```\\nusage: get_telemetry_spacex.py [-h] [-c CAPTURE_PATH] [-d DESTINATION_PATH]\\n                               [-T LAUNCH_TIME] [-o] [-f]\\n\\nExtract telemetry for SpaceX\\'s launch videos.\\n\\noptional arguments:\\n  -h, --help            show this help message and exit\\n  -c CAPTURE_PATH, --capture CAPTURE_PATH\\n                        Path (url or local) of the desired video\\n  -d DESTINATION_PATH, --destination DESTINATION_PATH\\n                        Path to the file that will contain the output\\n  -T LAUNCH_TIME, --time LAUNCH_TIME\\n                        Time from launch of the video to the time of the\\n                        launch (in seconds). If not given and not live, the\\n                        capture is set to the launch. If live, the capture\\n                        starts from the beginning of the stream\\n  -o                    If given results will be printed to stdout\\n  -f                    Force override of output file\\n```\\n\\n\\n\\n\\nExtraction of telemetry from other sources\\n==============\\n\\n```get_telemetry_spacex.py``` uses the [SpaceXtract](https://github.com/shahar603/SpaceXtract/tree/master/src/SpaceXtract) package.\\n\\n[SpaceXtract](https://github.com/shahar603/SpaceXtract/tree/master/src/SpaceXtract) is a package that performes fast OCR by searching and parsing only the data the user needs.\\nTo do that it uses JSON configuration files (their format is specified [below]()).\\n\\n[SpaceXtract](https://github.com/shahar603/SpaceXtract/tree/master/src/SpaceXtract) uses the general_extract.py script to perform OCR.\\n\\ngeneral_extract.py contains two classes, ```BaseExtract``` and ```RelativeExtract```.\\n\\n\\n```RelativeExtract``` is a subclass of ```BaseExtract```, the main difference between the two is that BaseExtract performes\\nOCR on a fixed region of interest on screen. In contrast, RelativeExtract tracks the region of interest even when it moves and changes.\\n\\n\\n\\n\\nConfiguration files\\n------------------\\n\\nThe configuration file is a JSON file which contains a dictionary (keys - string, value - a list of size 4) that tell BaseExtract and it\\'s subclasses where to perform\\nOCR, what characters to search, how clear are the characters and how many characters to expect to find.\\n\\nThe format of the file is the following:\\n\\n```javascript\\n{\\n    \"field_1\": [\\n        [top, bottom, left, right],\\n        \\n        [\"path_to_template_1.png\", \"path_to_template_2.png\", ...],\\n        \\n        threshold,\\n        \\n        [expected_length_1, expected_length_2, ...]\\n    ],\\n    \\n    \"field_2\" : [\\n        ...\\n    ],\\n    \\n    ...\\n    \\n}\\n```\\n\\n\\n* \"field_1\" - The name of the field to search.\\n\\n* [top, bottom, left, right] - A list that specify the area to perform OCR. top, bottom, left and right\\nare in ratio to screen size.\\n\\nFor example: The list [0.1, 0.9, 0.4, 0.6] on a 1920x1080 image captures a rectangle with dimentions:\\n\\n(Rectangle_Width, Rectangle_Height) = (Screen_Width * (right - left), Screen_Height * (bottom - top)) = (1920*(0.6-0.4), 1080*(0.9-0.1)) = (384, 864)\\n\\nThe location of the rectangle is specific to the Extractor class used and is specified [below]().\\n\\n\\n* [\"path_to_template_1.png\", \"path_to_template_2.png\", ...] - A list of pathes to templates (images) to look for in the image.\\nThe images can be colored, but they are converted to grayscale and only prominent features in the image (like edges) are used to detect characters.\\n\\n\\n* threshold - Minimum confidence required to detect a character.\\n\\n\\n* [expected_length_1, expected_length_2, ...] - A list of optional lengths of the output. \\n\\n\\n\\n\\nRectangle Location\\n-----------------\\n\\n\\n## BaseExtractor\\n\\nThe (top, left) corner of the rectangle is the same as the (top, left) value specified in the configuration file.\\n\\n## RelativeExtractor\\n\\nRelativeExtractor uses an *anchor*. A template whos location is used as a reference for the other fields.\\n\\nIf the anchor top left corner is (anchor_top, anchor_left), then the (top, left) corner of the rectangle RelativeExtract performes OCR in is (anchor_top+top, anchor_left+left).\\n\\n\\nThe anchor is specified in the configuration file as follows:\\n\\n```javascript\\n    \"anchor\": [\\n        null,\\n        [\\n            \"path_to_the_anchor.png\"\\n        ],\\n        threshold,\\n        []\\n    ] \\n```\\n\\n\\n\\nUsage\\n-----------\\n\\nTo use BaseExtract and RelativeExtract first import general_extract.py\\n\\n```python\\nimport general_extract\\n```\\n\\nThen create a BaseExtract instance.\\n\\n```python\\nsession = general_extract.BaseExtract(configuration_file_content)\\n```\\n\\nFor a given OpenCV ```frame```, extract ```\\'my_field\\'``` using ```extract_number```:\\n\\n```python\\nmy_field = session.extract_number(frame, \\'my_field\\')\\n```\\n\\n```\\'my_field\\'``` is a number that contains the indecies of the templates as defined in ```configuration_file_content[\\'my_field\\'][1]``` (Path to template list).\\n'},\n",
       " {'repo': 'EquiFox/KsDumper',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': \"# KsDumper\\n![Demo](https://i.imgur.com/6XyMDxa.gif)\\n\\nI always had an interest in reverse engineering. A few days ago I wanted to look at some game internals for fun, but it was packed & protected by EAC (EasyAntiCheat).\\nThis means its handle were stripped and I was unable to dump the process from Ring3. I decided to try to make a custom driver that would allow me to copy the process memory without using OpenProcess.\\nI knew nothing about Windows kernel, PE file structure, so I spent a lot of time reading articles and forums to make this project.\\n\\n## Features\\n- Dump any process main module using a kernel driver (both x86 and x64)\\n- Rebuild PE32/PE64 header and sections\\n- Works on protected system processes & processes with stripped handles (anti-cheats)\\n\\n**Note**: Import table isn't rebuilt.\\n\\n## Usage\\nBefore using KsDumperClient, the KsDumper driver needs to be loaded.\\n\\nIt is unsigned so you need to load it however you want. I'm using drvmap for Win10.\\nEverything is provided in this release if you want to use it aswell.\\n\\n- Run `Driver/LoadCapcom.bat` as Admin. Don't press any key or close the window yet !\\n- Run `Driver/LoadUnsignedDriver.bat` as Admin.\\n- Press enter in the `LoadCapcom` cmd to unload the driver.\\n- Run `KsDumperClient.exe`.\\n- Profit !\\n\\n**Note**: The driver stays loaded until you reboot, so if you close KsDumperClient.exe, you can just reopen it !  \\n**Note2**: Even though it can dump both x86 & x64 processes, this has to run on x64 Windows.\\n\\n## Disclaimer\\nThis project was a way for me to learn about Windows kernel, PE file structure and kernel-user space interactions. It has been made available for informational and educational purposes only.\\n\\nConsidering the nature of this project, it is highly recommended to run it in a `Virtual Environment`. I am not responsible for any crash or damage that could happen to your system.\\n\\n**Important**: This tool makes no attempt at hiding itself. If you target protected games, the anti-cheat might flag this as a cheat and ban you after a while. Use a `Virtual Environment` !\\n\\n## References\\n- https://github.com/not-wlan/drvmap\\n- https://github.com/Zer0Mem0ry/KernelBhop\\n- https://github.com/NtQuery/Scylla/\\n- http://terminus.rewolf.pl/terminus/\\n- https://www.unknowncheats.me/\\n\\n## Compile Yourself\\n- Requires Visual Studio 2017\\n- Requires Windows Driver Kit (WDK)\\n- Requires .NET 4.6.1\\n\"},\n",
       " {'repo': 'SpaceNetChallenge/SpaceNet_Off_Nadir_Solutions',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '<p align=\"center\">\\n<a href=\"https://spacenet.ai\"><img src=\"sn_logo.png\" width=\"350\" alt=\"SpaceNet LLC\"></a>\\n</p>\\n<h1 align=\"center\">SpaceNet Challenge Round 4: Off-Nadir Buildings</h1>\\n<h2 align=\"center\">Competitor solutions</h2>\\n<br>\\n\\n## Summary\\nThe five subdirectories in this repository comprise the code for the winning solutions of SpaceNet 4: Off-Nadir Building Detection hosted by TopCoder. Each subdirectory contains the competitors\\' written descriptions of their solution to the challenge. See the blog post on CosmiQ Works\\' blog [The DownlinQ](https://medium.com/the-downlinq/the-spacenet-challenge-off-nadir-buildings-introducing-the-winners-b60f2b700266) for an additional summary.\\n\\n## Coming soon\\n- Links to download the competitors\\' trained models\\n\\nQuestions about SpaceNet? Check out our website at https://spacenet.ai.\\n'},\n",
       " {'repo': 'legoandmars/SpaceMonke',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': \"# Space Monke\\n\\nA PC Gorilla Tag mod that makes your jumps super huge.\\n## Installation\\n\\n### Automatic installation\\nIf you don't want to manually install, you can install this mod with the [Monke Mod Manager](https://github.com/DeadlyKitten/MonkeModManager/releases/latest)\\n### Manual Installation\\n\\nIf your game isn't modded with BepinEx, DO THAT FIRST! Simply go to the [latest BepinEx release](https://github.com/BepInEx/BepInEx/releases) and extract BepinEx_x64_VERSION.zip directly into your game's folder, then run the game once to install BepinEx properly.\\n\\nThis mod requires Utilla, so download it here: https://github.com/legoandmars/Utilla/releases/latest - Make sure to follow the installation instructions!\\n\\nNext, go to the [latest release of this mod](https://github.com/legoandmars/SpaceMonke/releases/latest) and extract it directly into your game's folder. Make sure it's extracted directly into your game's folder and not into a subfolder!\\n\\n## Configuration\\n\\nIf you'd like to change how extreme the jump effect is, run the game once to generate a config found in `Gorilla Tag/BepInEx/config/SpaceMonke.cfg`\\n\\nThe config should be mostly self-explanatory, here's a quick rundown of what it does:\\n```\\n[Configuration]\\n\\n# How much to multiply the jump height/distance by. 10 = 10x higher jumps\\n# Default value: 10\\nJumpMultiplier = 10\\n```\\n\\n## For Developers\\nThis project is built with C# using .NET Standard.\\n\\nFor references, create a Libs folder in the same folder as the project solution. Inside of this folder you'll need to copy:\\n\\n```\\n0Harmony.dll\\nBepInEx.dll\\nBepInEx.Harmony.dll\\n``` \\nfrom `Gorilla Tag\\\\BepInEx\\\\core`,\\n```\\nUtilla.dll\\n``` \\nfrom `Gorilla Tag\\\\BepInEx\\\\plugins`, and\\n```\\nAssembly-CSharp.dll\\nUnityEngine.dll\\nUnityEngine.CoreModule.dll\\n``` \\nfrom `Gorilla Tag\\\\Gorilla Tag_Data\\\\Managed`.\\n\\n## Disclaimers\\nThis product is not affiliated with Gorilla Tag or Another Axiom LLC and is not endorsed or otherwise sponsored by Another Axiom LLC. Portions of the materials contained herein are property of Another Axiom LLC. ©2021 Another Axiom LLC.\\n\"},\n",
       " {'repo': 'MicrosoftDocs/mslearn-tailspin-spacegame-web-azure-functions',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '\\n## Contributing\\n\\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\\n\\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\\nprovided by the bot. You will only need to do this once across all repos using our CLA.\\n\\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.'},\n",
       " {'repo': 'rishipurwar1/coding-space',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# 🚀 CODINGSPACE\\n\\nCodingSpace is the platform where we aim to provide lots of different real-world UI-based challenges that can help developers to improve their web and mobile development skills.\\n\\n![Coding space](https://i.imgur.com/EPPICCO.gif)\\n\\n## 👨\\u200d💻 Demo\\n\\n<a href=\"https://github.com/rishipurwar1/coding-space\" target=\"blank\">\\n<img src=\"https://img.shields.io/website?url=https://coding-space.vercel.app&logo=github&style=flat-square\" />\\n</a>\\n\\nTry out the website : [CodingSpace](https://coding-space.vercel.app)\\n\\n## 👨\\u200d🔧 Tech Stack\\n\\n![React](https://img.shields.io/badge/react-%2320232a.svg?style=for-the-badge&logo=react&logoColor=%2361DAFB)\\n![Firebase](https://img.shields.io/badge/firebase-%23039BE5.svg?style=for-the-badge&logo=firebase)\\n![TailwindCSS](https://img.shields.io/badge/tailwindcss-%2338B2AC.svg?style=for-the-badge&logo=tailwind-css&logoColor=white)\\n\\n### 👇 Prerequisites\\n\\nBefore installation, please make sure you have already installed following tools:\\n\\n- [NodeJs](https://nodejs.org/en/download/)\\n- [Git](https://git-scm.com/downloads)\\n\\nYou also need to create a Firebase Project.\\nYou can do so by following [this guide](https://firebase.google.com/docs/web/setup). After creating a Firebase project, store the config info somewhere.\\n\\n## 🛠️ Installation Steps\\n\\n1. Fork [this](https://github.com/rishipurwar1/coding-space) repository.\\n\\n2. Clone your forked copy of the project.\\n\\n```bash\\ngit clone https://github.com/<your-github-username>/coding-space.git\\n```\\n\\n3. Change the working directory\\n\\n```bash\\ncd coding-space\\n```\\n\\n4. Create a `.env` file at the root of the project folder\\n\\n```bash\\ntouch .env\\n```\\n\\n5. Enter your Firebase Config in `.env` like this\\n\\n```bash\\nREACT_APP_API_KEY=\"YOUR_FIREBASE_API_KEY\"\\nREACT_APP_AUTH_DOMAIN=\"YOUR_FIREBASE_AUTH_DOMAIN\"\\nREACT_APP_PROJECT_ID=\"YOUR_FIREBASE_PROJECT_ID\"\\nREACT_APP_STORAGE_BUCKET=\"YOUR_FIREBASE_STORAGE_BUCKET\"\\nREACT_APP_MESSAGING_SENDER_ID=\"YOUR_FIREBASE_MESSAGING_SENDER_ID\"\\nREACT_APP_APP_ID=\"YOUR_FIREBASE_APP_ID\"\\nREACT_APP_MEASUREMENT_ID = \"YOUR_FIREBASE_MEASUREMENT_ID\"\\n```\\n\\n6. Install dependencies\\n\\n```bash\\nnpm install\\n```\\n\\n7. Run the app\\n\\n```bash\\nnpm start\\n```\\n\\n🌟 You are all set!\\n\\n## 👨\\u200d💻 Contributing\\n\\nContributions are what make the open source community such an amazing place to learn, inspire, and create. Any contributions you make are **greatly appreciated**.\\n\\n1. Fork the Project\\n2. Create your Feature Branch (`git checkout -b feature`)\\n3. Commit your Changes (`git commit -m \\'Add some AmazingFeature\\'`)\\n4. Push to the Branch (`git push origin feature`)\\n5. Open a Pull Request\\n\\n## 👉 Join our Discord Community\\n\\nYou can join our Discord Community, here is the [invite link](https://discord.gg/FYSQUEw6xP).\\n\\n## 📇 Contact\\n\\nRishi Purwar - [@thefierycoder](https://twitter.com/thefierycoder)\\n\\nProject Link: [https://github.com/rishipurwar1/coding-space](https://github.com/rishipurwar1/coding-space)\\n\\n## Code of Conduct\\n\\nWe follow the [Code of Conduct](CODE_OF_CONDUCT.md) of the [CodingSpace](https://www.codingspace.codes) Community.\\n\\n## License\\n\\nThis project is licensed under the [MIT License](LICENSE).\\n'},\n",
       " {'repo': 'space-wizards/RobustToolbox',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': \"![Robust Toolbox](https://raw.githubusercontent.com/space-wizards/asset-dump/3dd3078e49e3a7e06709a6e0fc6e3223d8d44ca2/robust.png)\\n\\nRobust Toolbox is an engine primarily being developed for [Space Station 14](https://github.com/space-wizards/space-station-14), although we're working on making it usable for both [singleplayer](https://github.com/space-wizards/RobustToolboxTemplateSingleplayer) and [multiplayer](https://github.com/space-wizards/RobustToolboxTemplate) projects.\\n\\nUse the [content repo](https://github.com/space-wizards/space-station-14) for actual development, even if you're modifying the engine itself.\\n\\n## Project Links\\n\\n[Website](https://spacestation14.io/) | [Discord](https://discord.gg/t2jac3p) | [Forum](https://forum.spacestation14.io/) | [Steam](https://store.steampowered.com/app/1255460/Space_Station_14/) | [Standalone Download](https://spacestation14.io/about/nightlies/)\\n\\n## Documentation/Wiki\\n\\nThe [wiki](https://docs.spacestation14.io/) has documentation on SS14s content, engine, game design and more. We also have lots of resources for new contributors to the project.\\n\\n## Contributing\\n\\nWe are happy to accept contributions from anybody. Get in Discord or IRC if you want to help. We've got a [list of issues](https://github.com/space-wizards/RobustToolbox/issues) that need to be done and anybody can pick them up. Don't be afraid to ask for help either!\\n\\n## Building\\n\\nThis repository is the **engine** part of SS14. It's the base engine all SS14 servers will be built on. As such, it does not start on its own: it needs the [content repo](https://github.com/space-wizards/space-station-14). Think of Robust Toolbox as BYOND in the context of Space Station 13.\\n\\n## Legal Info\\n\\nSee [legal.md](https://github.com/space-wizards/RobustToolbox/blob/master/legal.md) for licenses and copyright.\\n\"},\n",
       " {'repo': 'cryptotwinsbr/spacecrypto-bot',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# 🚀 Space Crypto auto click bot ready-to-use 🚀\\n\\n[![Author](https://img.shields.io/badge/author-cryptotwins-blue)]() ![python](https://img.shields.io/badge/python-%5E%203-green) \\n\\nThis is a free python bot program that crosses you to farm with auto click in space crypto NFT game, having fun :)\\n\\n![space](https://user-images.githubusercontent.com/98666682/155437986-4aa4cf43-f03b-4329-813a-047ae5cbdee4.gif)\\n\\n# Quer saber mais sobre versão v1.08 MULT ACCOUNT?\\n\\nhttps://www.youtube.com/watch?v=Bkoc6xSM7f8&t=95s&ab_channel=VaideNFT \\n\\n# Versão oficial (Official version) 1.07\\n# Novos updates \\n\\nLista novos updates versão 1.07 - versão otimizada.\\n\\n- Possivel alterar resolução por parametro para as pre-definidas:\\n  #Selecione a resulução, disponiveis:\\n    #1 - 1366x768\\n    #2 - 1680x1050\\n    #3 - 1920x1080\\n    resolution: 1\\n\\n- Visualização de recompensas em arquivo de log \"rewards.log\":\\n- Selecione naves por filtro de maior munição automaticamente por parametro \"set_filter_max_ammo\":\\n- Otimização de performance na seleção de naves:\\n- Otimização da velocidade do mouse por parametro \"move_speed_mouse\":\\n- Otimização do tempo entre clicks por parametro \"time_click\":\\n- Adicionado parametro para após surrender selecionar se quer ou não que va para tela de naves:\\n- Refresh na pagina caso o bot não encontre nenhuma atividade após 10 minutos (possivel ser alterado tempo por parametro).:\\n- Opção de enviar apenas naves comuns (em testes).:\\n- Correções de bugs \\n\\n\\n---\\n\\n# Perguntas frequentes (FAQ)\\n\\n•    Serve para multi account?\\n \\nNo momento apenas single account.\\n\\n•  Quantas naves preciso?\\n\\nA partir de uma você já consegue jogar.\\n\\n•  Roda em qual navegador?\\n\\nRecomendamos chrome, firefox, edge e brave (navegadores já testados e funcionando)\\n\\n•  Em qual s.o.\\n\\nWindows e Linux.\\n\\n•  Qual versão do python?\\n\\nA partir da versão 3.0+.\\n\\n•  É pago?\\n\\nTotalmente gratuito, porem é possivel dar aquela força para continuarmos produzindo e melhorando o bot para você ter noites melhores de sono e melhor aproveitamento do farm 😊\\n\\n Metamask wallet (BNB/SPG/BUSD/BCOIN): 0x73933b679F940ea7352c3895852501e3044FE855\\n\\n My key pix: 5f3d220c-a2a3-4db2-bfb2-30ae0533e240\\n\\n•  Qual resolução recomendada?\\n\\nRecomendamos 1366x768 que é a mais estavel atualmente, porem estamos disponibilizando novas pastas com outros tipos de resolução.\\n\\n\\n\\n\\n# Algumas dicas que podem ajudar:\\n\\n \"[WARNING] Nenhuma acao encontrada\"\\n\\n1 - Verifiquem o zoom do seu navegador ( Tem que estar em 100% )\\n\\n2 - Verifiquem a barra de favoritos se está ativa (sim isso pode diminuir a resolução do navegador), nossas imagens foram printadas na resolução 1366 x 768 com a barra de favoritos, então recomendamos para quem está com problema habilitar os favoritos e testar, para habilitar os favoritos de qualquer navegador use as teclas \"crlt + shift + B\"\\n\\n3 - Se você está usando a resolução da sua maquina conforme a pasta do bot deixe sempre o navegador no tamanho máximo da tela\\n\\n4 - Se você está usando a extensão \"Window resizer\" o tamanho do seu navegador diminuirá, mas preste atenção em deixa-lo totalmente à mostra na tua tela (não corte metade do navegador)\\n\\n5 - Caso nenhum item resolva tente deixar apenas um monitor ativo.\\n\\nCaso você identifique que o bot está funcionando porem para em algum ponto não clica em algum botão do jogo, faça o processo de recortar a imagem que não está sendo clicada e coloque-a dentro da pasta referente a sua resolução.\\n\\n90% dos casos que me chamam esses pontos resolvem, espero que te ajude, caso você executou todos esses passos sem sucesso chama a gente que vamos te ajudar.\\n\\nArquivo de configuração (todas os parametros que podem ser utilizados como estrategia e configuração no jogo):\\n\\n![image](https://user-images.githubusercontent.com/98666682/155436295-25fa023a-f1f6-4359-a54f-76421a3c68d9.png)\\n\\n\\n## Videos youtube passo a passo e funcionalidades nova versão\\n\\nhttps://www.youtube.com/watch?v=kC3z3DvVgEo&t=3s&ab_channel=VaideNFT\\n\\nhttps://www.youtube.com/watch?v=MjL8hnsKf7c&ab_channel=CryptoM.A.D.\\n\\n## About Project\\n\\n\\nThis is a open source project inspired on bombcrypto-bot that was a success auto click bot that helped me a lot.\\n\\nSo, I decided to create a new auto click bot for the new NFT game space crypto. (It is not soo easy) I hope you like \\n\\nTo maintain the improvments and this auto click bot free, please help me with any value, have fun :)\\n\\n- Metamask wallet (BNB/SPG/BUSD/BCOIN):  `0x73933b679F940ea7352c3895852501e3044FE855`\\n- My key pix: `5f3d220c-a2a3-4db2-bfb2-30ae0533e240`\\n\\nQRCode pix:\\n\\n![image](https://user-images.githubusercontent.com/98666682/151678042-ad125099-297c-4c5d-a5f3-92b083733b55.png)\\n\\n\\nFeel free to give your feedback ;)\\n---\\nWe added a folder (screen_test) with images that you can test the bot, just open the image with your bot active.\\n\\n*Observation: we reccomend to use just 1 screen to have a bot totally functional\\n\\n## Warning\\n\\nWe are not responsible for any penalties incurred by those who use the autoclick bot, use at your own risk.\\n\\n## Aviso \\n\\nNós não nos responsabilizarmos por eventuais penalidades sofridas por quem usar o bot de autoclick, use por sua própria conta e risco.\\n\\n---\\n## Summary\\n\\n<!--ts-->\\n\\n- [About Project](#about-project)\\n- [Software Requirements](#software-requirements)\\n- [How to Run](#how-to-run)\\n  - [Install dependencies](#install-dependencies)\\n  - [Run auto click bot](#run-auto-click-bot)\\n- [Thank you](#thank-you)\\n  <!--te-->\\n  </br>\\n\\n\\n## Software Requirements\\n\\n- PYTHON **3+**\\n  </br>\\n\\n\\n## How to run\\n\\n### Install dependencies\\n\\nIn order for you to run, you need to install the dependencies in advance. \\n\\nDownload [python site](https://www.python.org/downloads/) \\n\\n![image](https://user-images.githubusercontent.com/98666682/151677437-87fea683-60dd-495a-a2e4-4ec28bb7a04c.png)\\n\\nIn the installation check the option and complete the install.\\n\\n![image](https://user-images.githubusercontent.com/98666682/151677529-96ed2731-3ac9-412c-bd7f-9b629cee8ebb.png)\\n\\n\\nDownload the project on your machine\\n\\n![image](https://user-images.githubusercontent.com/98666682/151677578-60b29bad-6c67-4e73-9c3b-65a0398e76fc.png)\\n\\nExtract your zip and copy the directory, example:\\n\\n![image](https://user-images.githubusercontent.com/98666682/151678334-874b554b-723e-47d3-bc5e-63bceb43ae27.png)\\n\\n### Run auto click bot\\n\\nAfter install and download the dependences, run the following commands:\\n\\n\\n```bash\\n# Access tha project folder:\\n cd <path copyed before> \\n Sample:\\n cd C:\\\\<your path here>\\\\space-crypto-bot-main\\n```\\n![image](https://user-images.githubusercontent.com/98666682/151678504-5062fd0b-c20f-4162-aea6-4ad263e67da1.png)\\n\\n\\n\\n\\nInstall the dependences, execute this commands:\\n\\n````bash\\n# It will run\\npip install -r requirements.txt\\n````\\n\\nNow you just need to run by following the command:\\n\\n````bash\\n# run the auto click bot\\npython spacecrypto-bot.py\\n````\\n\\n\\n</br>\\n\\n### Thank you! \\n\\nI hope it can help you, don\\'t forget to strengthen it by making a donation :) any amount helps us to keep updated to help you in the best way\\n\\n- Metamask wallet (BNB/SPG/BUSD/BCOIN):  `0x73933b679F940ea7352c3895852501e3044FE855`\\n- My key pix: `5f3d220c-a2a3-4db2-bfb2-30ae0533e240`\\n\\nQRCode pix:\\n\\n![image](https://user-images.githubusercontent.com/98666682/151678042-ad125099-297c-4c5d-a5f3-92b083733b55.png)\\n\\n\\nHave a nice farm and a good sleep night! :)\\n\\n\\n</br>\\n</br>\\n'},\n",
       " {'repo': 'typpo/spacekit',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# spacekit\\n[![Build Status](https://travis-ci.com/typpo/spacekit.svg?branch=master)](https://travis-ci.com/typpo/spacekit)\\n[![npm](https://img.shields.io/npm/v/spacekit.js)](https://www.npmjs.com/package/spacekit.js)\\n\\nSpacekit is a JavaScript library for creating interactive 3D space visualizations - whether of the Earth/moon system, solar system, or beyond.\\n\\nYou can check out an editable live example on [jsfiddle](https://jsfiddle.net/typpo/x9nv8jg0/6/), or look at a variety of live examples on [SpaceReference.org](https://www.spacereference.org/solar-system#ob=2001-einstein-1973-eb,7672-hawking-1995-uo2,2709-sagan-1982-fh).  This library generalizes work that is currently used on [Asterank](https://www.asterank.com/), [Meteor Showers](https://www.meteorshowers.org/), [Ancient Earth](https://dinosaurpictures.org/ancient-earth), [and](https://www.ianww.com/ceres/) [many](https://www.asterank.com/exoplanets) [other](https://www.ianww.com/pluto/) [things](https://www.ianww.com/moonviz/) into a single open-source 3D engine for space that is both accurate and visually stunning.\\n\\nSee the **[full documentation](https://typpo.github.io/spacekit/)**\\n\\nNote that this library is a work in progress and the API might change!\\n\\n[![spacekit examples](https://i.imgur.com/u48FCjJ.jpg)](https://typpo.github.io/spacekit/)\\n\\n# Usage\\n\\nInstall via npm:\\n\\n```\\nnpm install spacekit.js\\n```\\n\\nAnd then use `require` or `import`:\\n\\n```js\\nconst Spacekit = require(\\'spacekit.js\\');\\n// or\\nimport Spacekit from \\'spacekit.js\\';\\n```\\n\\nYou can also [download a raw build](https://github.com/typpo/spacekit/tree/master/build) or use the latest build in a script tag:\\n```html\\n<script src=\"https://typpo.github.io/spacekit/build/spacekit.js\"></script>\\n```\\n\\n# Terminology and components\\n\\n`Simulation`: the main container for your visualization.  A simulation is comprised by a `Camera` plus whatever you choose to put in it. See [documentation](https://typpo.github.io/spacekit/docs/class/src/Simulation.js~Simulation.html) for full options.\\n```javascript\\nconst sim = new Spacekit.Simulation(document.getElementById(\\'my-container\\'), {\\n // Required\\n basePath: \\'../path/to/asset\\',\\n // Optional\\n camera: {\\n   initialPosition: [0, -10, 5],\\n   enableDrift: false,\\n },\\n debug: {\\n   showAxes: false,\\n   showGrid: false,\\n   showStats: false,\\n },\\n});\\n```\\n\\n`Skybox`: the image background of the visualization.  The \"universe\" of the visualization is contained within a large sphere, so \"skysphere\" may be a better (less conventional) way to describe it.  Some skybox assets are provided, including starry milky way background from ESA and NASA Tycho. See [documentation](https://typpo.github.io/spacekit/variable/index.html#static-variable-SkyboxPresets) for full preset options.\\n```javascript\\n// Use an existing skybox preset.\\nconst skybox = sim.createSkybox(Spacekit.SkyboxPresets.NASA_TYCHO);\\n\\n// Add a skybox preset\\nconst skybox = sim.createSkybox({\\n  textureUrl: \\'../path/to/image.png\\'\\n});\\n```\\n\\n`Stars`: an alternative to a skybox.  Instead of showing an image, this class loads real star data and positions the stars accordingly in the simulation.  Usually this is more performant but less visually stunning.\\n```javascript\\n// Use an existing skybox preset.\\nconst skybox = sim.createStars({minSize /* optional */: 0.75 /* default */});\\n\\n// Add a skybox preset\\nconst skybox = sim.createSkybox({\\n  textureUrl: \\'../path/to/image.png\\'\\n});\\n```\\n\\n`SpaceObject`: an object that can be added to the visualization (SpaceObjects can sometimes be referred to as simply \"Object\").  SpaceObjects can orbit, rotate, etc.  Subclasses include `RotatingObject` (has a defined spin axis), `ShapeObject` (has a 3D shapefile), and `SphereObject` (is spherical, like the Earth).\\n```javascript\\n// Create objects using presets. The presets include scientific ephem params and/or position.\\nconst sun = viz.createObject(\\'sun\\', Spacekit.SpaceObjectPresets.SUN);\\nviz.createObject(\\'mercury\\', Spacekit.SpaceObjectPresets.MERCURY);\\nviz.createObject(\\'venus\\', Spacekit.SpaceObjectPresets.VENUS);\\n\\n// Create a stationary object at [3, 1, -5] position.\\nconst obj = viz.createObject(\\'myobj\\', {\\n  position: [3, 1, -5],\\n};\\n\\n// Create an object that orbits.\\n\\n// Ephem is a class representing Kepler ephemerides, which defines the trajectory of astronomical objects as well\\n// as artificial satellites in the sky, i.e., the position (and possibly velocity) over time.\\nconst ephem = new Spacekit.Ephem({\\n  epoch: 2458600.5,\\n  a: 5.38533,\\n  e: 0.19893,\\n  i: 22.11137,\\n  om: 294.42992,\\n  w: 314.28890,\\n  ma: 229.14238,\\n}, \\'deg\\');\\n\\nconst asteroid = sim.createObject(\\'Asteroid Aci\\', {\\n  ephem,\\n});\\n\\n// Create a shape object\\nconst obj = viz.createShape(\\'myobj\\', {\\n  position: [3, 1, -5],\\n  shape: {\\n    // Example shape file -\\n    // http://astro.troja.mff.cuni.cz/projects/asteroids3D/web.php?page=db_asteroid_detail&asteroid_id=1046\\n    shapeUrl: \\'../path/to/shape.obj\\', // Cacus\\n  },\\n  rotation: {\\n    lambdaDeg: 251,\\n    betaDeg: -63,\\n    period: 3.755067,\\n    yorp: 1.9e-8,\\n    phi0: 0,\\n    jd0: 2443568.0,\\n  },\\n  debug: {\\n    showAxes: true,\\n  },\\n});\\n\\n// Create a sphere object\\nsim.createSphere(\\'earth\\', {\\n  textureUrl: \\'./earth_66mya.jpg\\',\\n  radius: 2 /* default to 1 */\\n  debug: {\\n    showAxes: true,\\n  },\\n});\\n```\\n\\n`KeplerParticles`: an optimized class for creating many particles that follow Kepler orbits.  These particles don\\'t have a specific shape or size.  Instead, they share a 2D texture.  This is useful for when you want to show many objects at once, such as the asteroid belt.\\n\\n# Dependencies\\n\\nSpacekit relies on some image and data assets that are not included in the Javascript file.\\n\\nBy default, these dependencies are hosted on the spacekit site (typpo.github.io/spacekit).  If you want to host these assets yourself, you can set the `Simulation`\\'s `basePath` parameter to a folder that contains these files:\\n\\n  - [Spacekit asset directory](https://github.com/typpo/spacekit/tree/master/src/assets)\\n  - [Spacekit data directory](https://github.com/typpo/spacekit/tree/master/src/data)\\n\\nFor example:\\n\\n```\\nconst viz = new Spacekit.Simulation({\\n  basePath: \\'https://mysite.com/static/spacekit\\',\\n});\\n```\\n\\nIf you want to contribute to this project, you will also need to install Python (2.7 or 3).\\n\\n# Running an Example\\n\\nRunning `./server.sh` will start a basic Python webserver.  Go to http://localhost:8001/examples/index.html to load a simple example.\\n\\nIf you\\'re making changes to the code, run `yarn build` to update the build outputs.  `yarn build:watch` will continuously watch for your changes and update the build and also host a server on localhost:8001 (so you don\\'t have to start the Python server separately).\\n\\n# Usage\\n\\nSee the [examples](https://github.com/typpo/spacekit/tree/master/examples) directory for full usage examples.  For now, here\\'s some example code that will build an interactive visualization of a couple planets:\\n\\n```javascript\\n// Create the visualization and put it in our div.\\nconst viz = new Spacekit.Simulation(document.getElementById(\\'main-container\\'), {\\n  assetPath: \\'../src/assets\\',\\n});\\n\\n// Create a skybox using NASA TYCHO artwork.\\nconst skybox = viz.createSkybox(Spacekit.SkyboxPresets.NASA_TYCHO);\\n\\n// Create our first object - the sun - using a preset space object.\\nconst sun = viz.createObject(\\'sun\\', Spacekit.SpaceObjectPresets.SUN);\\n\\n// Then add some planets\\nviz.createObject(\\'mercury\\', Spacekit.SpaceObjectPresets.MERCURY);\\nviz.createObject(\\'venus\\', Spacekit.SpaceObjectPresets.VENUS);\\nviz.createObject(\\'earth\\', Spacekit.SpaceObjectPresets.EARTH);\\nviz.createObject(\\'mars\\', Spacekit.SpaceObjectPresets.MARS);\\nviz.createObject(\\'jupiter\\', Spacekit.SpaceObjectPresets.JUPITER);\\nviz.createObject(\\'saturn\\', Spacekit.SpaceObjectPresets.SATURN);\\nviz.createObject(\\'uranus\\', Spacekit.SpaceObjectPresets.URANUS);\\nviz.createObject(\\'neptune\\', Spacekit.SpaceObjectPresets.NEPTUNE);\\n```\\n\\n![example](https://i.imgur.com/WseTJidl.jpg)\\n'},\n",
       " {'repo': 'automl/ConfigSpace',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# ConfigSpace\\n\\nA simple Python/Cython module implementing a domain specific language to manage \\nconfiguration spaces for algorithm configuration and hyperparameter optimization tasks.  \\nDistributed under BSD 3-clause, see LICENSE except all files in the directory\\nConfigSpace.nx, which are copied from the networkx package and licensed\\nunder a BSD license.\\n\\nThe documentation can be found at [https://automl.github.io/ConfigSpace/main/](https://automl.github.io/ConfigSpace/main/).\\nFurther examples can be found in the [SMAC documentation](https://automl.github.io/SMAC3/main/pages/examples/index.html).\\n\\n\\n## Minimum Example\\n\\n```python\\nfrom ConfigSpace import ConfigurationSpace\\n\\ncs = ConfigurationSpace(\\n    name=\"myspace\",\\n    space={\\n        \"a\": (0.1, 1.5),  # UniformFloat\\n        \"b\": (2, 10),  # UniformInt\\n        \"c\": [\"mouse\", \"cat\", \"dog\"],  # Categorical\\n    },\\n)\\n\\nconfigs = cs.sample_configuration(2)\\n```\\n\\n\\n## Citing the ConfigSpace\\n\\n```bibtex\\n@article{\\n    title   = {BOAH: A Tool Suite for Multi-Fidelity Bayesian Optimization & Analysis of Hyperparameters},\\n    author  = {M. Lindauer and K. Eggensperger and M. Feurer and A. Biedenkapp and J. Marben and P. Müller and F. Hutter},\\n    journal = {arXiv:1908.06756 {[cs.LG]}},\\n    date    = {2019},\\n}\\n```\\n'},\n",
       " {'repo': 'KSP-SpaceDock/SpaceDock',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# SpaceDock\\n\\nWebsite engine for Kerbal Space Program mods.\\n\\nhttps://spacedock.info\\n\\n## Contributing\\n\\nPlease see [CONTRIBUTING.md](./CONTRIBUTING.md)\\n\\n## Installation\\n\\nThis describes a bare-metal setup. For a local development setup using Docker, see https://github.com/KSP-SpaceDock/SpaceDock/wiki/Development-Guide#running-with-docker.\\n\\nQuick overview:\\n\\n1. Install and set up the dependencies\\n2. Clone SpaceDock repository\\n3. Activate the virtualenv\\n4. Install pip requirements\\n5. Build frontend\\n6. Configure SpaceDock\\n7. SQL\\n8. Site configuration\\n\\n**Install the dependencies**\\n\\nYou\\'ll need these things:\\n(Names taken from Ubuntu\\'s package repository)\\n\\n* python3, python3-dev for uwsgi, python3-pip, python3-virtualenv\\n* nodejs, npm\\n* postgresql (or postgresql-client if the database is on another server)\\n* redis-tools\\n\\nUse the packages your OS provides, or build them from source.\\nFor an up to date NodeJS distribution, see https://nodejs.org/en/download/current/\\nand https://github.com/nodesource/distributions/blob/master/README.md\\n\\n**Set up services**\\n\\nDo a quick sanity check on all of those things.\\n\\n    $ python3 --version\\n      Python 3.8.10\\n    $ node --version\\n      v16.6.1\\n    $ npm --version\\n      7.20.3\\n    $ pip --version\\n      pip 21.2.3\\n    $ virtualenv --version\\n      virtualenv 20.0.17\\n    $ psql --version\\n      psql (PostgreSQL) 12.7 (Ubuntu 12.7-0ubuntu0.20.04.1)\\n    $ redis-cli --version\\n      redis-cli 5.0.7\\n\\nYMMV if you use versions that differ from these.\\n\\nPrepare a connection string that looks like this when you\\'re done, prepare PostgreSQL accordingly:\\n\\n    postgresql://username:password@hostname:port/database\\n\\nThe connection string for localhost can look like this:\\n\\n    postgresql://postgres@localhost/spacedock\\n\\nSpaceDock needs to be able to create/alter/insert/update/delete in the database\\nyou give it.\\n\\nYou also need to start up Redis on the default port if you want to send emails.\\n\\n**Clone SpaceDock**\\n\\nFind a place you want the code to live.\\n\\n    $ git clone git://github.com/KSP-SpaceDock/SpaceDock.git\\n    $ cd SpaceDock\\n\\n**Activate virtualenv**\\n\\n    $ virtualenv -p python3 .\\n    $ source bin/activate\\n\\nIf you are on a system where `python3` is not the name of your\\nPython executable, add `--python=/path/to/python3` to the virtualenv command to fix that.\\n\\n**pip requirements**\\n\\nIf you use systemd/spacedock.target or Docker, this will be done automatically for you.\\n\\n    $ pip install -r requirements.txt\\n\\n**Frontend**\\n\\n    $ ./build-frontend.sh\\n\\n**Configure SpaceDock**\\n\\n    $ cp config.ini.example config.ini\\n    $ cp alembic.ini.example alembic.ini\\n    $ cp logging.ini.example logging.ini\\n\\nEdit config.ini and alembic.ini to your liking.\\n\\n**Postgres Configuration**\\n\\nDepending on your environment, you may need to tell postgres to trust localhost connections. This setting is in the pg_hba.conf file, usually located in /etc/postgresql/[version]/main/.\\nAn example of what the config should look like:\\n\\n    local   all    all                    trust\\n    host    all    all    127.0.0.1/32    trust\\n    host    all    all    ::1/128         trust    #may or may not be needed for IPv6 aware installs\\n\\n**Site Configuration**\\n\\nWhat you do from here depends on your site-specific configuration. If you just\\nwant to run the site for development, you can source the virtualenv and run\\n\\n    python app.py\\n\\nTo run it in production, you probably want to use gunicorn behind an nginx proxy.\\nThere\\'s a sample nginx config in the configs/ directory here, but you\\'ll probably\\nwant to tweak it to suit your needs. Here\\'s how you can run gunicorn, put this in\\nyour init scripts:\\n\\n    /path/to/SpaceDock/bin/gunicorn app:app -b 127.0.0.1:8000\\n\\nThe `-b` parameter specifies an endpoint to use. You probably want to bind this to\\nlocalhost and proxy through from nginx. I\\'d also suggest blocking the port you\\nchoose from external access. It\\'s not that gunicorn is *bad*, it\\'s just that nginx\\nis better.\\n\\nTo get an admin user you have to register a user first and then run this (replace &lt;username&gt; with your username):\\n\\n\\tsource bin/activate\\n\\tpython\\n\\n\\tfrom KerbalStuff.objects import *\\n\\tfrom KerbalStuff.database import db\\n\\tu = User.query.filter(User.username == \"<username>\").first()\\n\\tu.admin = True\\n\\tu.confirmation = None\\n\\tdb.commit()\\n\\n\\nWhen running in a production environment, run `python app.py` at least once and\\nthen read the SQL stuff below before you let it go for good.\\n\\n## Emails\\n\\nIf you want to send emails (like registration confirmation, mod updates, etc),\\nyou need to have redis running and then start the KerbalStuff mailer daemon.\\nYou can run it like so:\\n\\n    celery -A KerbalStuff.celery:app worker --loglevel=info\\n\\nOf course, this only works if you\\'ve filled out the smtp options in `config.ini`\\nand you have sourced the virtualenv.\\n\\n## SQL Stuff\\n\\nWe use alembic for schema migrations between versions. The first time you run the\\napplication, the schema will be created. However, you need to tell alembic about\\nit. Run the application at least once, then:\\n\\n    $ cd /path/to/SpaceDock/\\n    $ source bin/activate\\n    $ python\\n    >>> from alembic.config import Config\\n    >>> from alembic import command\\n    >>> alembic_cfg = Config(\"alembic.ini\")\\n    >>> command.stamp(alembic_cfg, \"head\")\\n    >>> exit()\\n\\nCongrats, you\\'ve got a schema in place. Run `alembic upgrade head` after pulling\\nthe code to update your schema to the latest version. Do this before you restart\\nthe site.\\n'},\n",
       " {'repo': 'bradtraversy/spacex_launch_stats',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# SpaceX Launch Stats\\r\\n\\r\\n> React, GraphQL, Apollo app that uses the SpaceX API to display launches\\r\\n\\r\\n## Quick Start\\r\\n\\r\\n```bash\\r\\n# Install dependencies (server & client)\\r\\nnpm install\\r\\ncd client && npm install\\r\\n\\r\\n# Run server & client (:3000 & :5000)\\r\\nnpm run dev\\r\\n\\r\\n# Server only (:5000)\\r\\nnpm run server\\r\\n\\r\\n# Client only (:3000)\\r\\nnpm run client\\r\\n\\r\\n# Build for production (Builds into server ./public)\\r\\ncd client && npm run build\\r\\n\\r\\n# Graphiql - http://localhost:5000/graphql\\r\\n```\\r\\n\\r\\n## App Info\\r\\n\\r\\n### Author\\r\\n\\r\\nBrad Traversy\\r\\n[Traversy Media](http://www.traversymedia.com)\\r\\n\\r\\n### Version\\r\\n\\r\\n1.0.0\\r\\n\\r\\n### License\\r\\n\\r\\nThis project is licensed under the MIT License\\r\\n'},\n",
       " {'repo': 'amzn/pecos',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# PECOS - Predictions for Enormous and Correlated Output Spaces\\n\\n[![PyPi Latest Release](https://img.shields.io/pypi/v/libpecos)](https://img.shields.io/pypi/v/libpecos)\\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](./LICENSE)\\n\\nPECOS is a versatile and modular machine learning (ML) framework for fast learning and inference on problems with large output spaces, such as extreme multi-label ranking (XMR) and large-scale retrieval.\\nPECOS\\' design is intentionally agnostic to the specific nature of the inputs and outputs as it is envisioned to be a general-purpose framework for multiple distinct applications.\\n\\nGiven an input, PECOS identifies a small set (10-100) of relevant outputs from amongst an extremely large (~100MM) candidate set and ranks these outputs in terms of relevance. \\n\\n\\n### Features\\n\\n#### Extreme Multi-label Ranking and Classification\\n* X-Linear ([`pecos.xmc.xlinear`](pecos/xmc/xlinear/README.md)): recursive linear models learning to traverse an input from the root of a hierarchical label tree to a few leaf node clusters, and return top-k relevant labels within the clusters as predictions. See more details in the [PECOS paper (Yu et al., 2020)](https://arxiv.org/pdf/2010.05878.pdf).\\n  + fast real-time inference in C++\\n  + can handle 100MM output space\\n\\n* XR-Transformer ([`pecos.xmc.xtransformer`](pecos/xmc/xtransformer/README.md)): Transformer based XMC framework that fine-tunes pre-trained transformers recursively on multi-resolution objectives. It can be used to generate top-k relevant labels for a given instance or simply as a fine-tuning engine for task aware embeddings. See technical details in [XR-Transformer paper (Zhang et al., 2021)](https://arxiv.org/pdf/2110.00685.pdf).\\n  + easy to extend with many pre-trained Transformer models from [huggingface transformers](https://github.com/huggingface/transformers).\\n  + establishes the State-of-the-art on public XMC benchmarks.\\n\\n* ANN Search with HNSW ([`pecos.ann.hnsw`](pecos/ann/hnsw/README.md)): a PECOS Approximated Nearest Neighbor (ANN) search module that implements the Hierarchical Navigable Small World Graphs (HNSW) algorithm ([`Malkov et al., TPAMI 2018`](https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf)).\\n  + Supports both sparse and dense input features\\n  +  SIMD optimization for both dense/sparse distance computation\\n  +  Supports thread-safe graph construction in parallel on multi-core shared memory machines\\n  +  Supports thread-safe Searchers to do inference in parallel, which reduces inference overhead\\n\\n\\n## Requirements and Installation\\n\\n* Python (>=3.6, <=3.9)\\n* Pip (>=19.3)\\n\\nSee other dependencies in [`setup.py`](https://github.com/amzn/pecos/blob/mainline/setup.py#L135)\\nYou should install PECOS in a [virtual environment](https://docs.python.org/3/library/venv.html).\\nIf you\\'re unfamiliar with Python virtual environments, check out the [user guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).\\n\\n### Supporting Platforms\\n* Ubuntu 18.04 and 20.04\\n* Amazon Linux 2\\n\\n### Installation from Wheel\\n\\n\\nPECOS can be installed using pip as follows:\\n```bash\\npython3 -m pip install libpecos\\n```\\n\\n### Installation from Source\\n\\n#### Prerequisite builder tools\\n* For Ubuntu (18.04, 20.04):\\n``` bash\\nsudo apt-get update && sudo apt-get install -y build-essential git python3 python3-distutils python3-venv\\n```\\n* For Amazon Linux 2 Image:\\n``` bash\\nsudo yum -y install python3 python3-devel python3-distutils python3-venv && sudo yum -y groupinstall \\'Development Tools\\' \\n```\\nOne needs to install at least one BLAS library to compile PECOS, e.g. `OpenBLAS`:\\n* For Ubuntu (18.04, 20.04):\\n``` bash\\nsudo apt-get install -y libopenblas-dev\\n```\\n* For Amazon Linux 2 Image and AMI:\\n``` bash\\nsudo amazon-linux-extras install epel -y\\nsudo yum install openblas-devel -y\\n```\\n\\n#### Install and develop locally\\n```bash\\ngit clone https://github.com/amzn/pecos\\ncd pecos\\npython3 -m pip install --editable ./\\n```\\n\\n\\n## Quick Tour\\nTo have a glimpse of how PECOS works, here is a quick tour of using PECOS API for the XMR problem.\\n\\n### Toy Example\\nThe eXtreme Multi-label Ranking (XMR) problem is defined by two matrices\\n* instance-to-feature matrix `X`, of shape `N by D` in [`SciPy CSR format`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html)\\n* instance-to-label matrix `Y`, of shape `N by L` in [`SciPy CSR format`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html)\\n\\nSome toy data matrices are available in the [`tst-data`](https://github.com/amzn/pecos/tree/mainline/test/tst-data/xmc/xlinear) folder. \\n\\nPECOS constructs a hierarchical label tree and learns linear models recursively (e.g., XR-Linear):\\n```python\\n>>> from pecos.xmc.xlinear.model import XLinearModel\\n>>> from pecos.xmc import Indexer, LabelEmbeddingFactory\\n\\n# Build hierarchical label tree and train a XR-Linear model\\n>>> label_feat = LabelEmbeddingFactory.create(Y, X)\\n>>> cluster_chain = Indexer.gen(label_feat)\\n>>> model = XLinearModel.train(X, Y, C=cluster_chain)\\n>>> model.save(\"./save-models\")\\n```\\n\\nAfter learning the model, we do prediction and evaluation \\n```python\\n>>> from pecos.utils import smat_util\\n>>> Yt_pred = model.predict(Xt)\\n# print precision and recall at k=10\\n>>> print(smat_util.Metrics.generate(Yt, Yt_pred))\\n```\\n\\nPECOS also offers optimized C++ implementation for fast real-time inference\\n```python\\n>>> model = XLinearModel.load(\"./save-models\", is_predict_only=True)\\n>>> for i in range(X_tst.shape[0]):\\n>>>   y_tst_pred = model.predict(X_tst[i], threads=1)\\n```\\n\\n\\n## Citation\\n\\nIf you find PECOS useful, please consider citing the following paper:\\n\\n* [PECOS: Prediction for Enormous and Correlated Output Spaces (Yu et al., JMLR 2022)](https://arxiv.org/pdf/2010.05878.pdf) [[bib]](./bibtex/yu2020pecos.bib)\\n\\nSome papers from our group using PECOS:\\n\\n* [FINGER: Fast Inference for Graph-based Approximate Nearest Neighbor Search (Chen et al., ArXiv 2022)](https://arxiv.org/pdf/2206.11408.pdf) [[bib]](./bibtex/)\\n\\n* [Relevance under the Iceberg: Reasonable Prediction for Extreme Multi-label Classification (Jiang et al., SIGIR 2022)](https://dl.acm.org/doi/abs/10.1145/3477495.3531767) [[bib]](./bibtex/)\\n\\n* [Extreme Zero-Shot Learning for Extreme Text Classification (Xiong et al., NAACL 2022)](https://aclanthology.org/2022.naacl-main.399.pdf) [[bib]](./bibtex/)\\n\\n* [Node Feature Extraction by Self-Supervised Multi-scale Neighborhood Prediction (Chien et al., ICLR 2022)](https://arxiv.org/pdf/2111.00064.pdf) [[bib]](./bibtex/chien2021node.bib)\\n\\n* [Accelerating Inference for Sparse Extreme Multi-Label Ranking Trees (Etter et al., WWW 2022)](https://arxiv.org/pdf/2106.02697.pdf) [[bib]](./bibtex/etter2021accelerating.bib)\\n\\n* [Fast Multi-Resolution Transformer Fine-tuning for Extreme Multi-label Text Classification (Zhang et al., NeurIPS 2021)](https://arxiv.org/pdf/2110.00685.pdf) [[bib]](./bibtex/zhang2021fast.bib)\\n\\n* [Label Disentanglement in Partition-based Extreme Multilabel Classification (Liu et al., NeurIPS 2021)](https://arxiv.org/pdf/2106.12751.pdf) [[bib]](./bibtex/liu2021label.bib)\\n\\n* [Enabling Efficiency-Precision Trade-offs for Label Trees in Extreme Classification (Baharav et al., CIKM 2021)](https://arxiv.org/pdf/2106.00730.pdf) [[bib]](./bibtex/baharav2021enabling.bib)\\n\\n* [Extreme Multi-label Learning for Semantic Matching in Product Search (Chang et al., KDD 2021)](https://arxiv.org/pdf/2106.12657.pdf) [[bib]](./bibtex/chang2021extreme.bib)\\n\\n* [Session-Aware Query Auto-completion using Extreme Multi-label Ranking (Yadav et al., KDD 2021)](https://arxiv.org/pdf/2012.07654.pdf)  [[bib]](./bibtex/yadav2021session.bib)\\n\\n* [Top-k eXtreme Contextual Bandits with Arm Hierarchy (Sen et al., ICML 2021)](https://arxiv.org/pdf/2102.07800.pdf) [[bib]](./bibtex/sen2021top.bib)\\n\\n* [Taming pretrained transformers for extreme multi-label text classification (Chang et al., KDD 2020)](https://arxiv.org/pdf/1905.02331.pdf) [[bib]](./bibtex/chang2020taming.bib)\\n\\n\\n## License\\n\\nCopyright (2021) Amazon.com, Inc.\\n \\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n \\n    http://www.apache.org/licenses/LICENSE-2.0\\n \\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\\n\\n'},\n",
       " {'repo': 'OCASM/SSMS',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': 'Screen Space Multiple Scattering for Unity\\n==========================================\\nRepurposes [Keijiro Takahashi\\'s KinoBloom][KinoBloom] to mimic [this.][CAVE] It comes with a modified version of the Global Fog effect found inside Unity\\'s Standard Assets package.\\n\\nUnity thread with more info: https://forum.unity3d.com/threads/screen-space-multiple-scattering.446647/\\n\\nExamples\\n--------\\n\\nNo fog:\\n![ScreenshotA1][ImageA1] \\nFog:\\n![ScreenshotA2][ImageA2]\\nFog + SMSS:\\n![ScreenshotA3][ImageA3]\\nFog + SMSS + Bloom:\\n![ScreenshotA4][ImageA4]\\n\\nNo fog:\\n![ScreenshotB1][ImageB1]\\nHeight fog:\\n![ScreenshotB2][ImageB2]\\nHeight fog + SMSS:\\n![ScreenshotB3][ImageB3]\\n\\nLicense\\n-------\\n\\nCopyright (C) 2015, 2016 Keijiro Takahashi, OCASM\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy of\\nthis software and associated documentation files (the \"Software\"), to deal in\\nthe Software without restriction, including without limitation the rights to\\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\\nthe Software, and to permit persons to whom the Software is furnished to do so,\\nsubject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\\nFOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\\nCOPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\\nIN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\\nCONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n\\n[KinoBloom]: https://github.com/keijiro/KinoBloom\\n[CAVE]: http://www.cs.columbia.edu/CAVE/projects/ptping_media/\\n\\n[ImageA1]: http://i.imgur.com/UbIjTt7.jpg\\n[ImageA2]: http://i.imgur.com/hgJ4Beo.png\\n[ImageA3]: http://i.imgur.com/6ykjXOI.png\\n[ImageA4]: http://i.imgur.com/fPkvPFQ.png\\n\\n[ImageB1]: http://i.imgur.com/AFxiAIG.png\\n[ImageB2]: http://i.imgur.com/qcZCJpF.png\\n[ImageB3]: http://i.imgur.com/nEIF2B3.png\\n'},\n",
       " {'repo': 'charlesq34/pointnet2',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '### PointNet++: *Deep Hierarchical Feature Learning on Point Sets in a Metric Space*\\nCreated by <a href=\"http://charlesrqi.com\" target=\"_blank\">Charles R. Qi</a>, <a href=\"http://stanford.edu/~ericyi\">Li (Eric) Yi</a>, <a href=\"http://ai.stanford.edu/~haosu/\" target=\"_blank\">Hao Su</a>, <a href=\"http://geometry.stanford.edu/member/guibas/\" target=\"_blank\">Leonidas J. Guibas</a> from Stanford University.\\n\\n![prediction example](https://github.com/charlesq34/pointnet2/blob/master/doc/teaser.jpg)\\n\\n### Citation\\nIf you find our work useful in your research, please consider citing:\\n\\n        @article{qi2017pointnetplusplus,\\n          title={PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space},\\n          author={Qi, Charles R and Yi, Li and Su, Hao and Guibas, Leonidas J},\\n          journal={arXiv preprint arXiv:1706.02413},\\n          year={2017}\\n        }\\n\\n### Introduction\\nThis work is based on our NIPS\\'17 paper. You can find arXiv version of the paper <a href=\"https://arxiv.org/pdf/1706.02413.pdf\">here</a> or check <a href=\"http://stanford.edu/~rqi/pointnet2\">project webpage</a> for a quick overview. PointNet++ is a follow-up project that builds on and extends <a href=\"https://github.com/charlesq34/pointnet\">PointNet</a>. It is version 2.0 of the PointNet architecture.\\n\\nPointNet (the v1 model) either transforms features of *individual points* independently or process global features of the *entire point set*. However, in many cases there are well defined distance metrics such as Euclidean distance for 3D point clouds collected by 3D sensors or geodesic distance for manifolds like isometric shape surfaces. In PointNet++ we want to respect *spatial localities* of those point sets. PointNet++ learns hierarchical features with increasing scales of contexts, just like that in convolutional neural networks. Besides, we also observe one challenge that is not present in convnets (with images) -- non-uniform densities in natural point clouds. To deal with those non-uniform densities, we further propose special layers that are able to intelligently aggregate information from different scales.\\n\\nIn this repository we release code and data for our PointNet++ classification and segmentation networks as well as a few utility scripts for training, testing and data processing and visualization.\\n\\n### Installation\\n\\nInstall <a href=\"https://www.tensorflow.org/install/\">TensorFlow</a>. The code is tested under TF1.2 GPU version and Python 2.7 (version 3 should also work) on Ubuntu 14.04. There are also some dependencies for a few Python libraries for data processing and visualizations like `cv2`, `h5py` etc. It\\'s highly recommended that you have access to GPUs.\\n\\n#### Compile Customized TF Operators\\nThe TF operators are included under `tf_ops`, you need to compile them (check `tf_xxx_compile.sh` under each ops subfolder) first. Update `nvcc` and `python` path if necessary. The code is tested under TF1.2.0. If you are using earlier version it\\'s possible that you need to remove the `-D_GLIBCXX_USE_CXX11_ABI=0` flag in g++ command in order to compile correctly.\\n\\nTo compile the operators in TF version >=1.4, you need to modify the compile scripts slightly.\\n\\nFirst, find Tensorflow include and library paths.\\n\\n        TF_INC=$(python -c \\'import tensorflow as tf; print(tf.sysconfig.get_include())\\')\\n        TF_LIB=$(python -c \\'import tensorflow as tf; print(tf.sysconfig.get_lib())\\')\\n        \\nThen, add flags of `-I$TF_INC/external/nsync/public -L$TF_LIB -ltensorflow_framework` to the `g++` commands.\\n\\n### Usage\\n\\n#### Shape Classification\\n\\nTo train a PointNet++ model to classify ModelNet40 shapes (using point clouds with XYZ coordinates):\\n\\n        python train.py\\n\\nTo see all optional arguments for training:\\n\\n        python train.py -h\\n\\nIf you have multiple GPUs on your machine, you can also run the multi-GPU version training (our implementation is similar to the tensorflow <a href=\"https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10\">cifar10 tutorial</a>):\\n\\n        CUDA_VISIBLE_DEVICES=0,1 python train_multi_gpu.py --num_gpus 2\\n\\nAfter training, to evaluate the classification accuracies (with optional multi-angle voting):\\n\\n        python evaluate.py --num_votes 12 \\n\\n<i>Side Note:</i> For the XYZ+normal experiment reported in our paper: (1) 5000 points are used and (2) a further random data dropout augmentation is used during training (see commented line after `augment_batch_data` in `train.py` and (3) the model architecture is updated such that the `nsample=128` in the first two set abstraction levels, which is suited for the larger point density in 5000-point samplings.\\n\\nTo use normal features for classification: You can get our sampled point clouds of ModelNet40 (XYZ and normal from mesh, 10k points per shape) <a href=\"https://shapenet.cs.stanford.edu/media/modelnet40_normal_resampled.zip\">here (1.6GB)</a>. Move the uncompressed data folder to `data/modelnet40_normal_resampled`\\n\\n#### Object Part Segmentation\\n\\nTo train a model to segment object parts for ShapeNet models:\\n\\n        cd part_seg\\n        python train.py\\n\\nPreprocessed ShapeNetPart dataset (XYZ, normal and part labels) can be found <a href=\"https://shapenet.cs.stanford.edu/media/shapenetcore_partanno_segmentation_benchmark_v0_normal.zip\">here (674MB)</a>. Move the uncompressed data folder to `data/shapenetcore_partanno_segmentation_benchmark_v0_normal`\\n\\n#### Semantic Scene Parsing\\n\\nSee `scannet/README` and `scannet/train.py` for details.\\n\\n#### Visualization Tools\\nWe have provided a handy point cloud visualization tool under `utils`. Run `sh compile_render_balls_so.sh` to compile it and then you can try the demo with `python show3d_balls.py` The original code is from <a href=\"http://github.com/fanhqme/PointSetGeneration\">here</a>.\\n\\n#### Prepare Your Own Data\\nYou can refer to <a href=\"https://github.com/charlesq34/3dmodel_feature/blob/master/io/write_hdf5.py\">here</a> on how to prepare your own HDF5 files for either classification or segmentation. Or you can refer to `modelnet_dataset.py` on how to read raw data files and prepare mini-batches from them. A more advanced way is to use TensorFlow\\'s dataset APIs, for which you can find more documentations <a href=\"https://www.tensorflow.org/programmers_guide/datasets\">here</a>.\\n\\n### License\\nOur code is released under MIT License (see LICENSE file for details).\\n\\n### Updates\\n* 02/23/2018: Added support for multi-gpu training for the classification task.\\n* 02/23/2018: Adopted a new way for data loading. No longer require manual data downloading to train a classification network.\\n* 02/06/2018: Added sample training code for ScanNet semantic segmentation.\\n\\n### Related Projects\\n\\n* <a href=\"http://stanford.edu/~rqi/pointnet\" target=\"_blank\">PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</a> by Qi et al. (CVPR 2017 Oral Presentation). Code and data released in <a href=\"https://github.com/charlesq34/pointnet\">GitHub</a>.\\n* <a href=\"https://arxiv.org/abs/1711.08488\" target=\"_blank\">Frustum PointNets for 3D Object Detection from RGB-D Data</a> by Qi et al. (CVPR 2018) A novel framework for 3D object detection with RGB-D data. Based on 2D boxes from a 2D object detector on RGB images, we extrude the depth maps in 2D boxes to point clouds in 3D space and then realize instance segmentation and 3D bounding box estimation using PointNet/PointNet++. The method proposed has achieved first place on KITTI 3D object detection benchmark on all categories (last checked on 11/30/2017). Code and data release TBD.\\n'},\n",
       " {'repo': 'lordofduct/spacepuppy-unity-framework',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# TODO OBSOLETE - See Spacepuppy Unity Framework 4.0\\nhttps://github.com/lordofduct/spacepuppy-unity-framework-4.0\\n\\nThis version of spacepuppy is now obsolete. We\\'ve moved to a newer version of Unity that supports the newer C# versions in the compiler.\\n\\nThis framework will remain for posterity sake.\\n\\n# spacepuppy-unity-framework\\nA framework of reusable objects with the Unity game engine version 5.x.\\n\\nThis project is intended to be compiled in Visual Studio and then place the compiled dll\\'s into the project. When compiling you will need to ensure you reference the UnityEngine and UnityEditor dll\\'s for your version of unity. I include a version list in the Resources folder for the version of unity I\\'m currently using.\\n\\n# Quick Import\\n\\nSelect the latest stable version from the builds section of the git page. Or if you want the bleeding edge build locate the files in the Builds/OpenSource directory of the repo.\\n\\nDrag the contents of this into a folder anywhere in your project and it\\'ll work out of the box, I prefer the \\'Assets/3rdParty/SpacepuppyUnityFramework\\' folder. The included meta file configures the scripts execution order automatically. Caution, this meta file\\'s guid is what controls the references to scripts in the library, deleting or modifying it will ruin any script references.\\n\\nOnce added make sure to go to \"Spacepuppy->Settings->Sync TagData\".\\n\\nThis will add the required \\'MultiTag\\' and \\'Root\\' tags, as well as sync any of your existing tags to a resource that can be read at runtime. Any time you add new tags you must sync the tag data again by either clicking this same menu item, or going into /Assets/Resources/TagData and clicking \"Sync TagData\" in the inspector there.\\n\\nEarlier version (1.0 and earlier) will require adding these tags manually before syncing.\\n\\n# Custom Building\\n\\nYou\\'ll need to create a folder in Assets for the dll\\'s, I personally use Assets/3rdParty/SpaceuppyUnityFramework. The SpacepuppyUnityFramework dll and .pdb go in there (a .mdb will also be generated by unity, this will allow for code stepping when debugging). I then create a folder Assets/3rdParty/SpaceuppyUnityFramework/Editor and place the SpacepuppyUnityFrameworkEditor dll and .pdb there.\\n\\nYou will also want to go to the PlayerSettings and set the Api compatability level to .Net 2.0, and not 2.0 subset.\\n\\nNext you\\'ll want to set the execution order of 3 classes in the framework. Of course, if you have meta files turned on, you could just include the supplied SpacepuppyUnityFramework.dll.meta file instead (found in the Resources folder, just place in the same folder as the dll). Those 3 classes are:\\n\\ncom.spacepuppy.GameLoopEntry : -32000\\n\\ncom.spacepuppy.Hooks.EarlyExecutionUpdateEventHooks : -31990\\n\\ncom.spacepuppy.Utils.SpecializedCoponents.EarlyParentSetter : -31980\\n\\ncom.spacepuppy.Hooks.TardyExecutionUpdateEventHooks : 32000\\n\\ncom.spacepuppy.SPEntity: 32000\\n\\nLastly, make sure to go to \"Spacepuppy->Settings->Sync TagData\".\\n\\nThis will add the required \\'MultiTag\\' and \\'Root\\' tags, as well as sync any of your existing tags to a resource that can be read at runtime. Any time you add new tags you must sync the tag data again by either clicking this same menu item, or going into /Assets/Resources/TagData and clicking \"Sync TagData\" in the inspector there.\\n\\nNow you\\'re fully set up.\\n\\n# License\\nCopyright (c) 2015, Dylan Engelman, Jupiter Lighthouse Studio\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n'},\n",
       " {'repo': 'ColinLeung-NiloCat/UnityURP-MobileScreenSpacePlanarReflection',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# UnityURP-MobileScreenSpacePlanarReflection\\n A simple and fast ScreenSpacePlanarReflection(SSPR) as a standalone reusable RendererFeature in URP.  \\n Can run on PC/console/vulkan android, other platforms not tested but should work if compute shader is supported. \\n - See runtime video here: https://youtu.be/Cy46A8EyX4Q\\n - download pre-built .apk here: https://drive.google.com/file/d/14Z_Gjb1ADz8RhcBgAFpa96dm-oQuOyQx/view?usp=sharing  \\n \\n SSPR ON\\n ![screenshot](https://i.imgur.com/cNaVHLK.png)\\n SSPR OFF\\n ![screenshot](https://i.imgur.com/0WCIcTM.png)\\n SSPR ON\\n ![screenshot](https://i.imgur.com/XvudHkR.png)\\n SSPR OFF\\n ![screenshot](https://i.imgur.com/AZ08hZ8.png)\\n \\nOn Adreno630 GPU android mobile device(almost all 2018/2019 flagship android mobiles), Toggle SSPR ON/OFF:\\n - cost <1ms to update 128 height SSPR RT\\n - cost <1ms to update 256 height SSPR RT\\n - cost 1~2ms to update 512 height SSPR RT  \\n \\nOn Adreno612 GPU android mobile device(Samsung Galaxy A70), Toggle SSPR ON/OFF:\\n - cost <1ms to update 128 height SSPR RT\\n - cost 1~2ms to update 256 height SSPR RT\\n - cost 4~5ms to update 512 height SSPR RT\\n \\n On Adreno506 GPU android mobile device(Lenovo S5), Toggle SSPR ON/OFF:\\n - cost ~1ms to update 128 height SSPR RT\\n - cost 3~4ms to update 256 height SSPR RT\\n - cost 8~9ms to update 512 height SSPR RT\\n \\n Where are the important files?\\n-------------------\\n There are only 4 important code files, all inside a folder \"Assets \\\\ _MobileSSPR \\\\ ReusableCore\".  \\nhttps://github.com/ColinLeung-NiloCat/UnityURP-MobileScreenSpacePlanarReflection/tree/master/Assets/_MobileSSPR/ReusableCore  \\n Other files are for demo only, not important.\\n \\n Can it run on mobile?\\n-------------------\\n Tested on ~10 android devices(all support Vulkan).\\n If your android device support Vulkan, result should be correct and rendering should be fast enough. (OpenGLES3.2 is not enough, must support Vulkan!)\\n   \\n *We have received a report that this SSPR is not working on MaliT760 GPU android (Galaxy S6), but we don\\'t have this device to reproduce it\\n \\n How to try this inside my own URP project?\\n -------------------\\n - copy \"Assets \\\\ _MobileSSPR \\\\ ReusableCore\" folder to your project (contains 4 important code files)\\n - turn on \"Depth Texture\" in all your project\\'s URP\\'s setting\\n - turn on \"Opaque Texture\" in all your project\\'s URP\\'s setting\\n - Add MobileSSPRRendererFeature(RendererFeature) to your forward renderer asset\\n   \\n - create a new plane game object in your scene (set world space pos y = 0.01)\\n - create a material using MobileSSPRExampleShader.shader\\n - assign this material to your new plane\\n   \\n - DONE! You should see correct reflection both in scene and game window\\n\\n I can see some small flickering in reflection / can\\'t see any reflection\\n -------------------\\nIt is not expected! Please report your device name in Issues, thanks!\\n \\n Notes\\n -------------------  \\nIt is not safe to use InterlockedMin() and RenderTexture color format \"RInt\" on android/iOS compute shader(see -> https://zhuanlan.zhihu.com/p/150890059). \\nInstead, we will use RenderTexture color format RFloat / ARGBHalf to produce similar result.\\n \\n Editor\\n -------------------\\n2020.3.33f1 LTS\\n\\nImplementation reference\\n-------------------\\n- http://remi-genin.fr/blog/screen-space-plane-indexed-reflection-in-ghost-recon-wildlands/\\n- http://advances.realtimerendering.com/s2017/PixelProjectedReflectionsAC_v_1.92_withNotes.pdf\\n- https://zhuanlan.zhihu.com/p/150890059\\n- https://github.com/Steven-Cannavan/URP_ScreenSpacePlanarReflections\\n- UE4 source - PostProcessPixelProjectedReflectionMobile.usf (UE4 4.26)\\n\\nChange log\\n-------------------\\n- 2020-08-23: add iOS/OSX support (with the help of MusouCrow)\\n- 2022-05-02: upgrade project to Unity2020.3.33f1, merged a bug fix in MobileSSPRComputeShader.compute\\n'},\n",
       " {'repo': 'olgarose/ParkingLot',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Parking Space Detection in OpenCV\\nFor a fun weekend project, I decided to play around with the OpenCV (Open Source Computer Vision) library in python.\\n\\nOpenCV is an extensive open source library (available in python, Java, and C++) that\\'s used for image analysis and is pretty neat.\\n\\nThe lofty goal for my OpenCV experiment was to take any static image or video of a parking lot and be able to automatically detect whenever a parking space was available or occupied.\\n\\nThrough research and exploration, I discovered how lofty of a goal that was (at least for the scope of a weekend). What I was able accomplish was to detect how many spots were available in a parking lot, with just a bit of upfront work by the user.\\n\\nThis page is a walkthrough of my process and what I learned along the way.\\n\\nI\\'ll start with an overview, then talk about my process, and end with some ideas for future work.\\n\\n## Overview\\n[![Unedited parking lot](https://s3-us-west-2.amazonaws.com/parkinglot-opencv/parking_shot.png)](https://www.youtube.com/watch?v=SszV59YBn_o)\\n\\nThe above link takes you to a video of the parking space detection program in action.\\n\\nTo run:\\n```python\\npython main.py --image images/parking_lot_1.png --data data/coordinates_1.yml --video videos/parking_lot_1.mp4 --start-frame 400\\n```\\n\\nProgram flow is as follows:\\n- User inputs file name for a video, a still image from the video, and a path for the output file of parking space coordinates.\\n- User clicks 4 corners for each spot they want tracked. Presses \\'q\\' when all desired spots are marked.\\n- Video begins with the user provided boxes overlayed the video. Occupied spots initialized with red boxes, available spots with green.\\n    - Car leaves a space, the red box turns green.\\n    - Car drives into a free space, the green box turns red.\\n\\nThe data on the entering and exiting of these cars can be used for a number of purposes: closest spot detection, analytics on parking lot usage, and for those counters outside of parking garages that tell you how many cars are on each level (to name a few).\\n\\nThis project was my first tour through computer vision, so to get it working in a weekend, I went the \"express learning\" route. That consisted of auditing this [Computer Vision and Image Analytics course](https://www.edx.org/course/computer-vision-and-image-analysis), reading through [OpenCV documentation](https://docs.opencv.org/2.4/modules/refman.html), querying the net, and toggling OpenCV function parameters to see what happened. Overall, a lot of learning and a ton of fun.\\n\\n## Process\\n### The beginning\\nMy first thought was how can I tell whether a parking space is empty?\\n\\nWell, if a space is empty, it would be the color of the pavement. Otherwise, it wouldn\\'t be.\\n\\nI also knew that I needed a way to mark the boundaries of the space, so that I could return the number of spots available.\\n\\nLet\\'s grab an image and head to the OpenCV docs!\\n\\n### Line Detection\\nTo detect the parking spots, I knew I could take advantage of the lines demarking the boundaries.\\n\\nThe Hough Transform is a popular feature extraction technique for detecting lines. OpenCV encapsulates the math of the Hough Transform into HoughLines(). Further abstraction in captured in HoughLinesP(), which is the probabilistic model of creating lines with the points that HoughLines() returns. For more info, check out the [OpenCV Hough Lines tutorial.](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_houghlines/py_houghlines.html)\\n\\nThe following is a walkthrough to prepare an image to detect lines with the Hough Transform. Links point to OpenCV documentation for each function. Arguments for each function are given as keyword args for clarity.\\n\\n[Reading](https://docs.opencv.org/master/d4/da8/group__imgcodecs.html#ga288b8b3da0892bd651fce07b3bbd3a56) in this image:\\n```python\\nimg = cv2.imread(filename=\\'examples/hough_lines/p_lots.jpg\\')\\n```\\n![Org_hough](https://s3-us-west-2.amazonaws.com/parkinglot-opencv/org.png)\\n\\n\\n\\nI [converted it to gray scale](https://docs.opencv.org/master/d7/d1b/group__imgproc__misc.html#ga397ae87e1288a81d2363b61574eb8cab) to reduce the info in the photo:\\n```python\\ngray = cv2.cvtColor(src=img, code=cv2.COLOR_BGR2GRAY)\\n```\\n\\n![Gray_hough](https://s3-us-west-2.amazonaws.com/parkinglot-opencv/s_gray.png)\\n\\n\\n\\nGave it a good [Gaussian blur](https://docs.opencv.org/master/d4/d86/group__imgproc__filter.html#gaabe8c836e97159a9193fb0b11ac52cf1) to remove even more unnecessary noise:\\n```python\\nblur_gray = cv2.GaussianBlur(src=gray, ksize=(5, 5), sigmaX=0)\\n```\\n![Blur_hough](https://s3-us-west-2.amazonaws.com/parkinglot-opencv/s_blur.png)\\n\\n\\n\\nDetected the edges with [Canny](https://docs.opencv.org/master/dd/d1a/group__imgproc__feature.html#ga04723e007ed888ddf11d9ba04e2232de):\\n```python\\nedges = cv2.Canny(image=blur_gray, threshold1=50, threshold1=150, apertureSize=3)\\n```\\n![Canny_hough](https://s3-us-west-2.amazonaws.com/parkinglot-opencv/s_canny.png)\\n\\n\\nAnd then, a few behind-the-scenes rhos and thetas later, we have our [Hough Line](https://docs.opencv.org/master/dd/d1a/group__imgproc__feature.html#ga8618180a5948286384e3b7ca02f6feeb) results.\\n\\n```python\\nlines = cv2.HoughLinesP(image=edges, rho=1, theta=np.pi/180, threshold=80, minLineLength=15, maxLineGap=5)\\nfor x1,y1,x2,y2 in lines[0]:\\n    cv2.line(img,(x1,y1),(x2,y2),(0,255,0),2)\\n```\\n![Hough_transform](https://s3-us-west-2.amazonaws.com/parkinglot-opencv/s_line.png)\\n\\n\\n\\n\\nWell that wasn\\'t quite what I expected.\\n\\nI experimented a bit with the hough line, but toggling the parameters kept getting me the same one line.\\n\\nA bit of digging and I found a [promising post on StackOverflow](https://stackoverflow.com/questions/45322630/how-to-detect-lines-in-opencv)\\n\\nAfter following the directions of the top answer, I got this:\\n\\n![SO_transform](https://s3-us-west-2.amazonaws.com/parkinglot-opencv/stack_overflow_lines.png)\\n\\n\\nWhich gave me more lines, but I still had to figure out which lines were part of the parking space and which weren\\'t. Then, I would also need to detect when a car moved from a spot.\\n\\nI was running into a challenge; with this approach, I needed an empty parking lot to overlay with an image of a non-empty lot. Which would also call for a mask to cover unimportant information (trees, light posts, etc.)\\n\\nGiven my scope for the weekend, it was time to find another approach.\\n\\n### Drawing Rectangles\\n\\nIf my program wasn\\'t able to detect parking spots on it\\'s own, maybe it was reasonable to expect that the user give positions for each of the parking spots.\\n\\nNow, the goal was to find a way to click on the parking lot image and to store the 4 points that made up a parking space for all of the spaces in the lot.\\n\\nI discovered that I could do this using a [mouse as a \"paintbrush\"](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_gui/py_mouse_handling/py_mouse_handling.html)\\n\\nAfter some calculations for the center of the rectangle (to label each space), I got this:\\n\\n![Drawn Rectangles](https://s3-us-west-2.amazonaws.com/parkinglot-opencv/draw_rectangles.png)\\n\\n### Finishing touches\\n\\nAfter drawing the rectangles, all there was left to do was examine the area of each rectangle to see if there was a car in there or not.\\n\\nBy taking each (filtered and blurred) rectangle, determining the area, and doing an average on the pixels, I was able to tell when there wasn\\'t a car in the spot if the average was high (more dark pixels). I changed the color of the bounding box accordingly and viola, a parking detection program!\\n\\nThe code for drawing the rectangles and motion detection is pretty generic. It\\'s seperated out into classes and should be reusable outside of the context of a parking lot. I have tested this with two different parking lot videos and it worked pretty well. I plan to make other improvements and try to seperate OpenCV references to make code easier to test. I\\'m open to ideas and feedback.\\n\\nCheck out [the code](https://github.com/olgarose/ParkingLot) for more!\\n\\n## Future work\\n- Hook up a webcam to a Raspberry Pi and have live parking monitoring at home!\\n- [Transform parking lot video to have overview perspective](http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_geometric_transformations/py_geometric_transformations.html) (for clearer rectangles)\\n- Experiment with [HOG descriptors](https://gurus.pyimagesearch.com/lesson-sample-histogram-of-oriented-gradients-and-car-logo-recognition/) to detect people or other objects of interest\\n\\n\\n'},\n",
       " {'repo': 'ddd-by-examples/library',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '[![CircleCI](https://circleci.com/gh/ddd-by-examples/library.svg?style=svg)](https://circleci.com/gh/ddd-by-examples/library)\\n[![Code Coverage](https://codecov.io/gh/ddd-by-examples/library/branch/master/graph/badge.svg)](https://codecov.io/gh/ddd-by-examples/library)\\n\\n# Table of contents\\n\\n1. [About](#about)\\n2. [Domain description](#domain-description)\\n3. [General assumptions](#general-assumptions)  \\n    3.1 [Process discovery](#process-discovery)  \\n    3.2 [Project structure and architecture](#project-structure-and-architecture)    \\n    3.3 [Aggregates](#aggregates)  \\n    3.4 [Events](#events)  \\n    3.4.1 [Events in Repositories](#events-in-repositories)   \\n    3.5 [ArchUnit](#archunit)  \\n    3.6 [Functional thinking](#functional-thinking)  \\n    3.7 [No ORM](#no-orm)  \\n    3.8 [Architecture-code gap](#architecture-code-gap)  \\n    3.9 [Model-code gap](#model-code-gap)   \\n    3.10 [Spring](#spring)  \\n    3.11 [Tests](#tests)  \\n4. [How to contribute](#how-to-contribute)\\n5. [References](#references)\\n\\n## About\\n\\nThis is a project of a library, driven by real [business requirements](#domain-description).\\nWe use techniques strongly connected with Domain Driven Design, Behavior-Driven Development,\\nEvent Storming, User Story Mapping. \\n\\n## Domain description\\n\\nA public library allows patrons to place books on hold at its various library branches.\\nAvailable books can be placed on hold only by one patron at any given point in time.\\nBooks are either circulating or restricted, and can have retrieval or usage fees.\\nA restricted book can only be held by a researcher patron. A regular patron is limited\\nto five holds at any given moment, while a researcher patron is allowed an unlimited number\\nof holds. An open-ended book hold is active until the patron checks out the book, at which time it\\nis completed. A closed-ended book hold that is not completed within a fixed number of \\ndays after it was requested will expire. This check is done at the beginning of a day by \\ntaking a look at daily sheet with expiring holds. Only a researcher patron can request\\nan open-ended hold duration. Any patron with more than two overdue checkouts at a library\\nbranch will get a rejection if trying a hold at that same library branch. A book can be\\nchecked out for up to 60 days. Check for overdue checkouts is done by taking a look at\\ndaily sheet with overdue checkouts. Patron interacts with his/her current holds, checkouts, etc.\\nby taking a look at patron profile. Patron profile looks like a daily sheet, but the\\ninformation there is limited to one patron and is not necessarily daily. Currently a\\npatron can see current holds (not canceled nor expired) and current checkouts (including overdue).\\nAlso, he/she is able to hold a book and cancel a hold.\\n\\nHow actually a patron knows which books are there to lend? Library has its catalogue of\\nbooks where books are added together with their specific instances. A specific book\\ninstance of a book can be added only if there is book with matching ISBN already in\\nthe catalogue.  Book must have non-empty title and price. At the time of adding an instance\\nwe decide whether it will be Circulating or Restricted. This enables\\nus to have book with same ISBN as circulated and restricted at the same time (for instance,\\nthere is a book signed by the author that we want to keep as Restricted)\\n\\n## General assumptions\\n\\n### Process discovery\\n\\nThe first thing we started with was domain exploration with the help of Big Picture EventStorming.\\nThe description you found in the previous chapter, landed on our virtual wall:    \\n![Event Storming Domain description](docs/images/eventstorming-domain-desc.png)   \\nThe EventStorming session led us to numerous discoveries, modeled with the sticky notes:  \\n![Event Storming Big Picture](docs/images/eventstorming-big-picture.jpg)   \\nDuring the session we discovered following definitions:  \\n![Event Storming Definitions](docs/images/eventstorming-definitions.png)    \\n\\nThis made us think of real life scenarios that might happen. We discovered them described with the help of\\nthe **Example mapping**:  \\n![Example mapping](docs/images/example-mapping.png)  \\n\\nThis in turn became the base for our *Design Level* sessions, where we analyzed each example:  \\n![Example mapping](docs/images/eventstorming-design-level.jpg)  \\n\\nPlease follow the links below to get more details on each of the mentioned steps:\\n- [Big Picture EventStorming](./docs/big-picture.md)\\n- [Example Mapping](docs/example-mapping.md)\\n- [Design Level EventStorming](docs/design-level.md)\\n\\n### Project structure and architecture\\nAt the very beginning, not to overcomplicate the project, we decided to assign each bounded context\\nto a separate package, which means that the system is a modular monolith. There are no obstacles, though,\\nto put contexts into maven modules or finally into microservices.\\n\\nBounded contexts should (amongst others) introduce autonomy in the sense of architecture. Thus, each module\\nencapsulating the context has its own local architecture aligned to problem complexity.\\nIn the case of a context, where we identified true business logic (**lending**) we introduced a domain model\\nthat is a simplified (for the purpose of the project) abstraction of the reality and utilized\\nhexagonal architecture. In the case of a context, that during Event Storming turned out to lack any complex\\ndomain logic, we applied CRUD-like local architecture.  \\n\\n![Architecture](docs/images/architecture-big-picture.png) \\n\\nIf we are talking about hexagonal architecture, it lets us separate domain and application logic from\\nframeworks (and infrastructure). What do we gain with this approach? Firstly, we can unit test most important\\npart of the application - **business logic** - usually without the need to stub any dependency.\\nSecondly, we create ourselves an opportunity to adjust infrastructure layer without the worry of\\nbreaking the core functionality. In the infrastructure layer we intensively use Spring Framework\\nas probably the most mature and powerful application framework with an incredible test support.\\nMore information about how we use Spring you will find [here](#spring).\\n\\nAs we already mentioned, the architecture was driven by Event Storming sessions. Apart from identifying\\ncontexts and their complexity, we could also make a decision that we separate read and write models (CQRS).\\nAs an example you can have a look at **Patron Profiles** and *Daily Sheets*.\\n\\n### Aggregates\\nAggregates discovered during Event Storming sessions communicate with each other with events. There is\\na contention, though, should they be consistent immediately or eventually? As aggregates in general\\ndetermine business boundaries, eventual consistency sounds like a better choice, but choices in software\\nare never costless. Providing eventual consistency requires some infrastructural tools, like message broker\\nor event store. That\\'s why we could (and did) start with immediate consistency.\\n\\n> Good architecture is the one which postpones all important decisions\\n\\n... that\\'s why we made it easy to change the consistency model, providing tests for each option, including\\nbasic implementations based on **DomainEvents** interface, which can be adjusted to our needs and\\ntoolset in future. Let\\'s have a look at following examples:\\n\\n* Immediate consistency\\n    ```groovy\\n    def \\'should synchronize Patron, Book and DailySheet with events\\'() {\\n        given:\\n            bookRepository.save(book)\\n        and:\\n            patronRepo.publish(patronCreated())\\n        when:\\n            patronRepo.publish(placedOnHold(book))\\n        then:\\n            patronShouldBeFoundInDatabaseWithOneBookOnHold(patronId)\\n        and:\\n            bookReactedToPlacedOnHoldEvent()\\n        and:\\n            dailySheetIsUpdated()\\n    }\\n    \\n    boolean bookReactedToPlacedOnHoldEvent() {\\n        return bookRepository.findBy(book.bookId).get() instanceof BookOnHold\\n    }\\n    \\n    boolean dailySheetIsUpdated() {\\n        return new JdbcTemplate(datasource).query(\"select count(*) from holds_sheet s where s.hold_by_patron_id = ?\",\\n                [patronId.patronId] as Object[],\\n                new ColumnMapRowMapper()).get(0)\\n                .get(\"COUNT(*)\") == 1\\n    }\\n    ```\\n   _Please note that here we are just reading from database right after events are being published_\\n   \\n   Simple implementation of the event bus is based on Spring application events:\\n    ```java\\n    @AllArgsConstructor\\n    public class JustForwardDomainEventPublisher implements DomainEvents {\\n    \\n        private final ApplicationEventPublisher applicationEventPublisher;\\n    \\n        @Override\\n        public void publish(DomainEvent event) {\\n            applicationEventPublisher.publishEvent(event);\\n        }\\n    }\\n    ```\\n\\n* Eventual consistency\\n    ```groovy\\n    def \\'should synchronize Patron, Book and DailySheet with events\\'() {\\n        given:\\n            bookRepository.save(book)\\n        and:\\n            patronRepo.publish(patronCreated())\\n        when:\\n            patronRepo.publish(placedOnHold(book))\\n        then:\\n            patronShouldBeFoundInDatabaseWithOneBookOnHold(patronId)\\n        and:\\n            bookReactedToPlacedOnHoldEvent()\\n        and:\\n            dailySheetIsUpdated()\\n    }\\n    \\n    void bookReactedToPlacedOnHoldEvent() {\\n        pollingConditions.eventually {\\n            assert bookRepository.findBy(book.bookId).get() instanceof BookOnHold\\n        }\\n    }\\n    \\n    void dailySheetIsUpdated() {\\n        pollingConditions.eventually {\\n            assert countOfHoldsInDailySheet() == 1\\n        }\\n    }\\n    ```\\n    _Please note that the test looks exactly the same as previous one, but now we utilized Groovy\\'s\\n    **PollingConditions** to perform asynchronous functionality tests_\\n\\n    Sample implementation of event bus is following:\\n    \\n    ```java\\n    @AllArgsConstructor\\n    public class StoreAndForwardDomainEventPublisher implements DomainEvents {\\n    \\n        private final JustForwardDomainEventPublisher justForwardDomainEventPublisher;\\n        private final EventsStorage eventsStorage;\\n    \\n        @Override\\n        public void publish(DomainEvent event) {\\n            eventsStorage.save(event);\\n        }\\n    \\n        @Scheduled(fixedRate = 3000L)\\n        @Transactional\\n        public void publishAllPeriodically() {\\n            List<DomainEvent> domainEvents = eventsStorage.toPublish();\\n            domainEvents.forEach(justForwardDomainEventPublisher::publish);\\n            eventsStorage.published(domainEvents);\\n        }\\n    }\\n    ```\\n\\nTo clarify, we should always aim for aggregates that can handle a business operation atomically\\n(transactionally if you like), so each aggregate should be as independent and decoupled from other\\naggregates as possible. Thus, eventual consistency is promoted. As we already mentioned, it comes\\nwith some tradeoffs, so from the pragmatic point of view immediate consistency is also a choice.\\nYou might ask yourself a question now: _What if I don\\'t have any events yet?_. Well, a pragmatic\\napproach would be to encapsulate the communication between aggregates in a _Service-like_ class,\\nwhere you could call proper aggregates line by line explicitly.\\n\\n### Events\\nTalking about inter-aggregate communication, we must remember that events reduce coupling, but don\\'t remove\\nit completely. Thus, it is very vital to share(publish) only those events, that are necessary for other\\naggregates to exist and function. Otherwise there is a threat that the level of coupling will increase\\nintroducing **feature envy**, because other aggregates might start using those events to perform actions\\nthey are not supposed to perform. A solution to this problem could be the distinction of domain events\\nand integration events, which will be described here soon.  \\n\\n### Events in Repositories \\nRepositories are one of the most popular design pattern. They abstract our domain model from data layer. \\nIn other words, they deal with state. That said, a common use-case is when we pass a new state to our repository,\\nso that it gets persisted. It may look like so:\\n\\n```java\\npublic class BusinessService {\\n   \\n    private final PatronRepository patronRepository;\\n    \\n    void businessMethod(PatronId patronId) {\\n        Patron patron = patronRepository.findById(patronId);\\n        //do sth\\n        patronRepository.save(patron);\\n    }\\n}\\n```\\n\\nConceptually, between 1st and 3rd line of that business method we change state of our Patron from A to B. \\nThis change might be calculated by dirty checking or we might just override entire Patron state in the database. \\nThird option is _Let\\'s make implicit explicit_ and actually call this state change A->B an **event**. \\nAfter all, event-driven architecture is all about promoting state changes as domain events.\\n\\nThanks to this our domain model may become immutable and just return events as results of invoking a command like so:\\n\\n```java\\npublic BookPlacedOnHold placeOnHold(AvailableBook book) {\\n      ...\\n}\\n```\\n\\nAnd our repository might operate directly on events like so:\\n\\n```java\\npublic interface PatronRepository {\\n     void save(PatronEvent event) {\\n}\\n```\\n\\n### ArchUnit\\n\\nOne of the main components of a successful project is technical leadership that lets the team go in the right\\ndirection. Nevertheless, there are tools that can support teams in keeping the code clean and protect the\\narchitecture, so that the project won\\'t become a Big Ball of Mud, and thus will be pleasant to develop and\\nto maintain. The first option, the one we proposed, is [ArchUnit](https://www.archunit.org/) - a Java architecture\\ntest tool. ArchUnit lets you write unit tests of your architecture, so that it is always consistent with initial\\nvision. Maven modules could be an alternative as well, but let\\'s focus on the former.\\n\\nIn terms of hexagonal architecture, it is essential to ensure, that we do not mix different levels of\\nabstraction (hexagon levels):\\n```java \\n@ArchTest\\npublic static final ArchRule model_should_not_depend_on_infrastructure =\\n    noClasses()\\n        .that()\\n        .resideInAPackage(\"..model..\")\\n        .should()\\n        .dependOnClassesThat()\\n        .resideInAPackage(\"..infrastructure..\");\\n```      \\nand that frameworks do not affect the domain model  \\n```java\\n@ArchTest\\npublic static final ArchRule model_should_not_depend_on_spring =\\n    noClasses()\\n        .that()\\n        .resideInAPackage(\"..io.pillopl.library.lending..model..\")\\n        .should()\\n        .dependOnClassesThat()\\n        .resideInAPackage(\"org.springframework..\");\\n```    \\n\\n### Functional thinking\\nWhen you look at the code you might find a scent of functional programming. Although we do not follow\\na _clean_ FP, we try to think of business processes as pipelines or workflows, utilizing functional style through\\nfollowing concepts.\\n\\n_Please note that this is not a reference project for FP._\\n\\n#### Immutable objects\\nEach class that represents a business concept is immutable, thanks to which we:\\n* provide full encapsulation and objects\\' states protection,\\n* secure objects for multithreaded access,\\n* control all side effects much clearer. \\n\\n#### Pure functions\\nWe model domain operations, discovered in Design Level Event Storming, as pure functions, and declare them in\\nboth domain and application layers in the form of Java\\'s functional interfaces. Their implementations are placed\\nin infrastructure layer as ordinary methods with side effects. Thanks to this approach we can follow the abstraction\\nof ubiquitous language explicitly, and keep this abstraction implementation-agnostic. As an example, you could have\\na look at `FindAvailableBook` interface and its implementation:\\n\\n```java\\n@FunctionalInterface\\npublic interface FindAvailableBook {\\n\\n    Option<AvailableBook> findAvailableBookBy(BookId bookId);\\n}\\n```\\n\\n```java\\n@AllArgsConstructor\\nclass BookDatabaseRepository implements FindAvailableBook {\\n\\n    private final JdbcTemplate jdbcTemplate;\\n\\n    @Override\\n    public Option<AvailableBook> findAvailableBookBy(BookId bookId) {\\n        return Match(findBy(bookId)).of(\\n                Case($Some($(instanceOf(AvailableBook.class))), Option::of),\\n                Case($(), Option::none)\\n        );\\n    }  \\n\\n    Option<Book> findBy(BookId bookId) {\\n        return findBookById(bookId)\\n                .map(BookDatabaseEntity::toDomainModel);\\n    }\\n\\n    private Option<BookDatabaseEntity> findBookById(BookId bookId) {\\n        return Try\\n                .ofSupplier(() -> of(jdbcTemplate.queryForObject(\"SELECT b.* FROM book_database_entity b WHERE b.book_id = ?\",\\n                                      new BeanPropertyRowMapper<>(BookDatabaseEntity.class), bookId.getBookId())))\\n                .getOrElse(none());\\n    }  \\n} \\n```\\n    \\n#### Type system\\n_Type system - like_ modelling - we modelled each domain object\\'s state discovered during EventStorming as separate\\nclasses: `AvailableBook`, `BookOnHold`, `CheckedOutBook`. With this approach we provide much clearer abstraction than\\nhaving a single `Book` class with an enum-based state management. Moving the logic to these specific classes brings\\nSingle Responsibility Principle to a different level. Moreover, instead of checking invariants in every business method\\nwe leave the role to the compiler. As an example, please consider following scenario: _you can place on hold only a book\\nthat is currently available_. We could have done it in a following way:\\n```java\\npublic Either<BookHoldFailed, BookPlacedOnHoldEvents> placeOnHold(Book book) {\\n  if (book.status == AVAILABLE) {  \\n      ...\\n  }\\n}\\n```\\nbut we use the _type system_ and declare method of following signature\\n```java\\npublic Either<BookHoldFailed, BookPlacedOnHoldEvents> placeOnHold(AvailableBook book) {\\n      ...\\n}\\n```  \\nThe more errors we discover at compile time the better.\\n\\nYet another advantage of applying such type system is that we can represent business flows and state transitions\\nwith functions much easier. As an example, following functions:\\n```\\nplaceOnHold: AvailableBook -> BookHoldFailed | BookPlacedOnHold\\ncancelHold: BookOnHold -> BookHoldCancelingFailed | BookHoldCanceled\\n``` \\nare much more concise and descriptive than these:\\n```\\nplaceOnHold: Book -> BookHoldFailed | BookPlacedOnHold\\ncancelHold: Book -> BookHoldCancelingFailed | BookHoldCanceled\\n```\\nas here we have a lot of constraints hidden within function implementations.\\n\\nMoreover if you think of your domain as a set of operations (functions) that are being executed on business objects\\n(aggregates) you don\\'t think of any execution model (like async processing). It is fine, because you don\\'t have to.\\nDomain functions are free from I/O operations, async, and other side-effects-prone things, which are put into the\\ninfrastructure layer. Thanks to this, we can easily test them without mocking mentioned parts. \\n\\n#### Monads\\nBusiness methods might have different results. One might return a value or a `null`, throw an exception when something\\nunexpected happens or just return different objects under different circumstances. All those situations are typical\\nto object-oriented languages like Java, but do not fit into functional style. We are dealing with this issues\\nwith monads (monadic containers provided by [Vavr](https://www.vavr.io)):\\n* When a method returns optional value, we use the `Option` monad:\\n\\n    ```java\\n    Option<Book> findBy(BookId bookId) {\\n        ...\\n    }\\n    ```\\n\\n* When a method might return one of two possible values, we use the `Either` monad:\\n\\n    ```java\\n    Either<BookHoldFailed, BookPlacedOnHoldEvents> placeOnHold(AvailableBook book) {\\n        ...\\n    }\\n    ```\\n\\n* When an exception might occur, we use `Try` monad:\\n\\n    ```java\\n    Try<Result> placeOnHold(@NonNull PlaceOnHoldCommand command) {\\n        ...\\n    }\\n    ```\\n\\nThanks to this, we can follow the functional programming style, but we also enrich our domain language and\\nmake our code much more readable for the clients.\\n\\n#### Pattern Matching\\nDepending on a type of a given book object we often need to perform different actions. Series of if/else or switch/case statements\\ncould be a choice, but it is the pattern matching that provides the most conciseness and flexibility. With the code\\nlike below we can check numerous patterns against objects and access their constituents, so our code has a minimal dose\\nof language-construct noise:\\n```java\\nprivate Book handleBookPlacedOnHold(Book book, BookPlacedOnHold bookPlacedOnHold) {\\n    return API.Match(book).of(\\n        Case($(instanceOf(AvailableBook.class)), availableBook -> availableBook.handle(bookPlacedOnHold)),\\n        Case($(instanceOf(BookOnHold.class)), bookOnHold -> raiseDuplicateHoldFoundEvent(bookOnHold, bookPlacedOnHold)),\\n        Case($(), () -> book)\\n    );\\n}\\n```\\n\\n### (No) ORM\\nIf you run `mvn dependency:tree` you won\\'t find any JPA implementation. Although we think that ORM solutions (like Hibernate)\\nare very powerful and useful, we decided not to use them, as we wouldn\\'t utilize their features. What features are\\ntalking about? Lazy loading, caching, dirty checking. Why don\\'t we need them? We want to have more control\\nover SQL queries and minimize the object-relational impedance mismatch ourselves. Moreover, thanks to relatively\\nsmall aggregates, containing as little data as it is required to protect the invariants, we don\\'t need the\\nlazy loading mechanism either.\\nWith Hexagonal Architecture we have the ability to separate domain and persistence models and test them\\nindependently. Moreover, we can also introduce different persistence strategies for different aggregates. \\nIn this project, we utilize both plain SQL queries and `JdbcTemplate` and use new and very promising \\nproject called Spring Data JDBC, that is free from the JPA-related overhead mentioned before.\\nPlease find below an example of a repository:\\n\\n```java\\ninterface PatronEntityRepository extends CrudRepository<PatronDatabaseEntity, Long> {\\n\\n    @Query(\"SELECT p.* FROM patron_database_entity p where p.patron_id = :patronId\")\\n    PatronDatabaseEntity findByPatronId(@Param(\"patronId\") UUID patronId);\\n\\n}\\n```\\n\\nAt the same time we propose other way of persisting aggregates, with plain SQL queries and `JdbcTemplate`:  \\n\\n```java\\n@AllArgsConstructor\\nclass BookDatabaseRepository implements BookRepository, FindAvailableBook, FindBookOnHold {\\n\\n    private final JdbcTemplate jdbcTemplate;\\n\\n    @Override\\n    public Option<Book> findBy(BookId bookId) {\\n        return findBookById(bookId)\\n                .map(BookDatabaseEntity::toDomainModel);\\n    }\\n\\n    private Option<BookDatabaseEntity> findBookById(BookId bookId) {\\n        return Try\\n                .ofSupplier(() -> of(jdbcTemplate.queryForObject(\"SELECT b.* FROM book_database_entity b WHERE b.book_id = ?\",\\n                                     new BeanPropertyRowMapper<>(BookDatabaseEntity.class), bookId.getBookId())))\\n                .getOrElse(none());\\n    }\\n    \\n    ...\\n}\\n```\\n_Please note that despite having the ability to choose different persistence implementations for aggregates\\nit is recommended to stick to one option within the app/team_ \\n    \\n### Architecture-code gap\\nWe put a lot of attention to keep the consistency between the overall architecture (including diagrams)\\nand the code structure. Having identified bounded contexts we could organize them in modules (packages, to\\nbe more specific). Thanks to this we gain the famous microservices\\' autonomy, while having a monolithic\\napplication. Each package has well defined public API, encapsulating all implementation details by using\\npackage-protected or private scopes.\\n\\nJust by looking at the package structure:\\n\\n```\\n└── library\\n    ├── catalogue\\n    ├── commons\\n    │\\xa0\\xa0 ├── aggregates\\n    │\\xa0\\xa0 ├── commands\\n    │\\xa0\\xa0 └── events\\n    │\\xa0\\xa0     └── publisher\\n    └── lending\\n        ├── book\\n        │\\xa0\\xa0 ├── application\\n        │\\xa0\\xa0 ├── infrastructure\\n        │\\xa0\\xa0 └── model\\n        ├── dailysheet\\n        │\\xa0\\xa0 ├── infrastructure\\n        │\\xa0\\xa0 └── model\\n        ├── librarybranch\\n        │\\xa0\\xa0 └── model\\n        ├── patron\\n        │\\xa0\\xa0 ├── application\\n        │\\xa0\\xa0 ├── infrastructure\\n        │\\xa0\\xa0 └── model\\n        └── patronprofile\\n            ├── infrastructure\\n            ├── model\\n            └── web\\n```\\nyou can see that the architecture is screaming that it has two bounded contexts: **catalogue**\\nand **lending**. Moreover, the **lending context** is built around five business objects: **book**,\\n**dailysheet**, **librarybranch**, **patron**, and **patronprofile**, while **catalogue** has no subpackages,\\nwhich suggests that it might be a CRUD with no complex logic inside. Please find the architecture diagram\\nbelow.\\n\\n![Component diagram](docs/c4/component-diagram.png)\\n\\nYet another advantage of this approach comparing to packaging by layer for example is that in order to \\ndeliver a functionality you would usually need to do it in one package only, which is the aforementioned\\nautonomy. This autonomy, then, could be transferred to the level of application as soon as we split our\\n_context-packages_ into separate microservices. Following this considerations, autonomy can be given away\\nto a product team that can take care of the whole business area end-to-end.\\n\\n### Model-code gap\\nIn our project we do our best to reduce _model-code gap_ to bare minimum. It means we try to put equal attention\\nto both the model and the code and keep them consistent. Below you will find some examples.\\n\\n#### Placing on hold\\n![Placing on hold](docs/images/placing_on_hold.jpg)\\n\\nStarting with the easiest part, below you will find the model classes corresponding to depicted command and events:\\n\\n```java\\n@Value\\nclass PlaceOnHoldCommand {\\n    ...\\n}\\n```\\n```java\\n@Value\\nclass BookPlacedOnHold implements PatronEvent {\\n    ...\\n}\\n```\\n```java\\n@Value\\nclass MaximumNumberOfHoldsReached implements PatronEvent {\\n    ...    \\n}\\n```\\n```java\\n@Value\\nclass BookHoldFailed implements PatronEvent {\\n    ...\\n}\\n```\\n\\nWe know it might not look impressive now, but if you have a look at the implementation of an aggregate,\\nyou will see that the code reflects not only the aggregate name, but also the whole scenario of `PlaceOnHold` \\ncommand handling. Let us uncover the details:\\n\\n```java\\npublic class Patron {\\n\\n    public Either<BookHoldFailed, BookPlacedOnHoldEvents> placeOnHold(AvailableBook book) {\\n        return placeOnHold(book, HoldDuration.openEnded());\\n    }\\n    \\n    ...\\n}    \\n```\\n\\nThe signature of `placeOnHold` method screams, that it is possible to place a book on hold only when it\\nis available (more information about protecting invariants by compiler you will find in [Type system section](#type-system)).\\nMoreover, if you try to place available book on hold it can **either** fail (`BookHoldFailed`) or produce some events -\\nwhat events?\\n\\n```java\\n@Value\\nclass BookPlacedOnHoldEvents implements PatronEvent {\\n    @NonNull UUID eventId = UUID.randomUUID();\\n    @NonNull UUID patronId;\\n    @NonNull BookPlacedOnHold bookPlacedOnHold;\\n    @NonNull Option<MaximumNumberOfHoldsReached> maximumNumberOfHoldsReached;\\n\\n    @Override\\n    public Instant getWhen() {\\n        return bookPlacedOnHold.when;\\n    }\\n\\n    public static BookPlacedOnHoldEvents events(BookPlacedOnHold bookPlacedOnHold) {\\n        return new BookPlacedOnHoldEvents(bookPlacedOnHold.getPatronId(), bookPlacedOnHold, Option.none());\\n    }\\n\\n    public static BookPlacedOnHoldEvents events(BookPlacedOnHold bookPlacedOnHold, MaximumNumberOfHoldsReached maximumNumberOfHoldsReached) {\\n        return new BookPlacedOnHoldEvents(bookPlacedOnHold.patronId, bookPlacedOnHold, Option.of(maximumNumberOfHoldsReached));\\n    }\\n\\n    public List<DomainEvent> normalize() {\\n        return List.<DomainEvent>of(bookPlacedOnHold).appendAll(maximumNumberOfHoldsReached.toList());\\n    }\\n}\\n```\\n\\n`BookPlacedOnHoldEvents` is a container for `BookPlacedOnHold` event, and - if patron has 5 book placed on hold already -\\n`MaximumNumberOfHoldsReached` (please mind the `Option` monad). You can see now how perfectly the code reflects\\nthe model.\\n\\nIt is not everything, though. In the picture above you can also see a big rectangular yellow card with rules (policies)\\nthat define the conditions that need to be fulfilled in order to get the given result. All those rules are implemented \\nas functions **either** allowing or rejecting the hold:\\n\\n![Restricted book policy](docs/images/placing-on-hold-policy-restricted.png)\\n```java\\nPlacingOnHoldPolicy onlyResearcherPatronsCanHoldRestrictedBooksPolicy = (AvailableBook toHold, Patron patron, HoldDuration holdDuration) -> {\\n    if (toHold.isRestricted() && patron.isRegular()) {\\n        return left(Rejection.withReason(\"Regular patrons cannot hold restricted books\"));\\n    }\\n    return right(new Allowance());\\n};\\n```\\n\\n![Overdue checkouts policy](docs/images/placing-on-hold-policy-overdue.png)\\n\\n```java\\nPlacingOnHoldPolicy overdueCheckoutsRejectionPolicy = (AvailableBook toHold, Patron patron, HoldDuration holdDuration) -> {\\n    if (patron.overdueCheckoutsAt(toHold.getLibraryBranch()) >= OverdueCheckouts.MAX_COUNT_OF_OVERDUE_RESOURCES) {\\n        return left(Rejection.withReason(\"cannot place on hold when there are overdue checkouts\"));\\n    }\\n    return right(new Allowance());\\n};\\n```\\n\\n![Max number of holds policy](docs/images/placing-on-hold-policy-max.png)\\n\\n```java\\nPlacingOnHoldPolicy regularPatronMaximumNumberOfHoldsPolicy = (AvailableBook toHold, Patron patron, HoldDuration holdDuration) -> {\\n    if (patron.isRegular() && patron.numberOfHolds() >= PatronHolds.MAX_NUMBER_OF_HOLDS) {\\n        return left(Rejection.withReason(\"patron cannot hold more books\"));\\n    }\\n    return right(new Allowance());\\n};\\n```\\n\\n![Open ended hold policy](docs/images/placing-on-hold-policy-open-ended.png)\\n\\n```java\\nPlacingOnHoldPolicy onlyResearcherPatronsCanPlaceOpenEndedHolds = (AvailableBook toHold, Patron patron, HoldDuration holdDuration) -> {\\n    if (patron.isRegular() && holdDuration.isOpenEnded()) {\\n        return left(Rejection.withReason(\"regular patron cannot place open ended holds\"));\\n    }\\n    return right(new Allowance());\\n};\\n```\\n\\n#### Spring\\nSpring Framework seems to be the most popular Java framework ever used. Unfortunately it is also quite common\\nto overuse its features in the business code. What you find in this project is that the domain packages\\nare fully focused on modelling business problems, and are free from any DI, which makes it easy to\\nunit-test it which is invaluable in terms of code reliability and maintainability. It does not mean,\\nthough, that we do not use Spring Framework - we do. Below you will find some details:\\n- Each bounded context has its own independent application context. It means that we removed the runtime\\ncoupling, which is a step towards extracting modules (and microservices). How did we do that? Let\\'s have\\na look:\\n    ```java\\n    @SpringBootConfiguration\\n    @EnableAutoConfiguration\\n    public class LibraryApplication {\\n    \\n        public static void main(String[] args) {\\n            new SpringApplicationBuilder()\\n                    .parent(LibraryApplication.class)\\n                    .child(LendingConfig.class).web(WebApplicationType.SERVLET)\\n                    .sibling(CatalogueConfiguration.class).web(WebApplicationType.NONE)\\n                    .run(args);\\n        }\\n    }\\n    ```\\n- As you could see above, we also try not to use component scan wherever possible. Instead we utilize\\n`@Configuration` classes where we define module specific beans in the infrastructure layer. Those\\nconfiguration classes are explicitly declared in the main application class.\\n\\n### Tests\\nTests are written in a BDD manner, expressing stories defined with Example Mapping.\\nIt means we utilize both TDD and Domain Language discovered with Event Storming. \\n\\nWe also made an effort to show how to create a DSL, that enables to write\\ntests as if they were sentences taken from the domain descriptions. Please\\nfind an example below:\\n\\n```groovy\\ndef \\'should make book available when hold canceled\\'() {\\n    given:\\n        BookDSL bookOnHold = aCirculatingBook() with anyBookId() locatedIn anyBranch() placedOnHoldBy anyPatron()\\n    and:\\n        PatronEvent.BookHoldCanceled bookHoldCanceledEvent = the bookOnHold isCancelledBy anyPatron()\\n\\n    when:\\n        AvailableBook availableBook = the bookOnHold reactsTo bookHoldCanceledEvent\\n    then:\\n        availableBook.bookId == bookOnHold.bookId\\n        availableBook.libraryBranch == bookOnHold.libraryBranchId\\n        availableBook.version == bookOnHold.version\\n}\\n``` \\n_Please also note the **when** block, where we manifest the fact that books react to \\ncancellation event_\\n\\n## How to contribute\\n\\nThe project is still under construction, so if you like it enough to collaborate, just let us\\nknow or simply create a Pull Request.\\n\\n\\n## How to Build\\n\\n### Requirements\\n\\n* Java 11\\n* Maven\\n\\n### Quickstart\\n\\nYou can run the library app by simply typing the following:\\n\\n```console\\n$ mvn spring-boot:run\\n...\\n...\\n2019-04-03 15:55:39.162  INFO 18957 --- [           main] o.s.b.a.e.web.EndpointLinksResolver      : Exposing 2 endpoint(s) beneath base path \\'/actuator\\'\\n2019-04-03 15:55:39.425  INFO 18957 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port(s): 8080 (http) with context path \\'\\'\\n2019-04-03 15:55:39.428  INFO 18957 --- [           main] io.pillopl.library.LibraryApplication    : Started LibraryApplication in 5.999 seconds (JVM running for 23.018)\\n\\n```\\n\\n### Build a Jar package\\n\\nYou can build a jar with maven like so:\\n\\n```console\\n$ mvn clean package\\n...\\n...\\n[INFO] Building jar: /home/pczarkowski/development/spring/library/target/library-0.0.1-SNAPSHOT.jar\\n[INFO] ------------------------------------------------------------------------\\n[INFO] BUILD SUCCESS\\n[INFO] ------------------------------------------------------------------------\\n```\\n\\n### Build with Docker\\n\\nIf you\\'ve already built the jar file you can run:\\n\\n```console\\ndocker build -t spring/library .\\n```\\n\\nOtherwise you can build the jar file using the multistage dockerfile:\\n\\n```console\\ndocker build -t spring/library -f Dockerfile.build .\\n```\\n\\nEither way once built you can run it like so:\\n\\n```console\\n$ docker run -ti --rm --name spring-library -p 8080:8080 spring/library\\n```\\n\\n### Production ready metrics and visualization\\nTo run the application as well as Prometheus and Grafana dashboard for visualizing metrics you can run all services:\\n\\n```console\\n$ docker-compose up\\n```\\n\\nIf everything goes well, you can access the following services at given location:\\n* http://localhost:8080/actuator/prometheus - published Micrometer metrics\\n* http://localhost:9090 - Prometheus dashboard\\n* http://localhost:3000 - Grafana dashboard\\n\\nIn order to see some metrics, you must create a dashboard. Go to `Create` -> `Import` and select attached `jvm-micrometer_rev8.json`. File has been pulled from \\n`https://grafana.com/grafana/dashboards/4701`.\\n\\nPlease note application will be run with `local` Spring profile to setup some initial data.\\n\\n## References\\n\\n1. [Introducing EventStorming](https://leanpub.com/introducing_eventstorming) by Alberto Brandolini\\n2. [Domain Modelling Made Functional](https://pragprog.com/book/swdddf/domain-modeling-made-functional) by Scott Wlaschin\\n3. [Software Architecture for Developers](https://softwarearchitecturefordevelopers.com) by Simon Brown\\n4. [Clean Architecture](https://www.amazon.com/Clean-Architecture-Craftsmans-Software-Structure/dp/0134494164) by Robert C. Martin\\n5. [Domain-Driven Design: Tackling Complexity in the Heart of Software](https://www.amazon.com/Domain-Driven-Design-Tackling-Complexity-Software/dp/0321125215) by Eric Evans\\n'},\n",
       " {'repo': 'yangsiyu007/SpaceNetExploration',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"*This repository was originally at https://github.com/aiforearth/SpaceNetExploration*. For other Microsoft AI for Earth repositories, search for the topic `#aiforearth` on GitHub or visit them [here](https://github.com/search?l=&q=user%3Amicrosoft+topic%3Aaiforearth&type=Repositories).\\n\\n# Building Footprint Extraction\\n\\n## Overview\\nThis repository contains a walkthrough demonstrating how to perform semantic segmentation using convolutional neural networks (CNNs) on satellite images to extract the footprints of buildings. We show how to carry out the procedure on an Azure Deep Learning Virtual Machine (DLVM), which are GPU-enabled and have all major frameworks pre-installed so you can start model training straight-away. We use a subset of the data and labels from the [SpaceNet Challenge](http://explore.digitalglobe.com/spacenet), an online repository of freely available satellite imagery released to encourage the application of machine learning to geospatial data.\\n\\nThe blog post that first announced this sample project is [here](https://azure.microsoft.com/en-us/blog/how-to-extract-building-footprints-from-satellite-images-using-deep-learning/) on the Azure Blog.\\n\\n\\n## Data\\n\\n### SpaceNet Building Footprint Extraction Dataset\\nThe code in this repository was developed for training a semantic segmentation model (currently two variants of the U-Net are implemented) on the Vegas set of the SpaceNet building footprint extraction [data](https://spacenetchallenge.github.io/). This makes the sample code clearer, but it can be easily extended to take in training data from the four other locations.\\n\\nThe organizers release a portion of this data as training data and the rest are held out for the purpose of the competitions they hold. For the experiments discussed here, we split the official training set 70:15:15 into our own training, validation and test sets. These are 39 GB in size as raw images in TIFF format with labels.\\n\\n\\n### Generate Input from Raw Data\\nInstruction for downloading the SpaceNet data can be found on their [website](https://spacenetchallenge.github.io/). The authors provide a set of utilities to convert the raw images to a format that semantic segmentation models can take as input. The utilities are in this [repo](https://github.com/SpaceNetChallenge/utilities). Most of the functionalities you will need are in the `python` folder. Please read their instructions on the repo's [README](https://github.com/SpaceNetChallenge/utilities) to understand all the tools and parameters available. After using `python/createDataSpaceNet.py` from the utilities repo to process the raw data, the input image and its label look like the following:\\n\\n![Example of input image and its label](./visuals/sample_input_pair.png)\\n\\n\\n## Environment Setup\\n### Provision an Azure Deep Learning Virtual Machine\\nYou could train your models on a Deep Learning Virtual Machine ([DLVM](https://azuremarketplace.microsoft.com/en-ca/marketplace/apps/microsoft-ads.dsvm-deep-learning)) on Azure to get started quickly, where all the major deep learning frameworks, including PyTorch used in this repo, are installed and ready to use. These VMs are configured specifically for use with GPUs. Instructions for provisioning can be found [here](https://docs.microsoft.com/en-us/azure/machine-learning/data-science-virtual-machine/provision-deep-learning-dsvm). The code here has been used on a Ubuntu Linux DLVM, but you should be able to use it on a Windows DLVM with minor modifications to the commands such as those setting environment variable values. The commands on this page are for running in a Linux shell.\\n\\n\\n### Additional Packages to Install\\nThere are two additional packages for the polygonization of the result of the CNN model so that our results can be compared to the original labels, which are expressed in a polygon data type. You can install these using `pip`:\\n\\n```\\npip install rasterio\\npip install shapely\\n```\\n\\n\\n### Data Storage Options\\nFor quick experimentations you could download your data to the OS disk, but this makes data transfer and sharing costly when you scale out.\\n\\nThere are several options for storing the data while you perform computation on them in Azure. Here's a piece of [documentation](https://docs.microsoft.com/en-us/azure/storage/common/storage-decide-blobs-files-disks) to guide you through choosing among these, and here are the [pricing](https://azure.microsoft.com/en-us/pricing/details/storage/) information.\\n\\nIf you are not planning on training models distributedly across several machines, you could attach a data disk to your VM. See [instructions](https://docs.microsoft.com/en-us/azure/virtual-machines/linux/attach-disk-portal) on attaching a data disk to a Linux VM. You can later re-attach this data disk to a more powerful VM, but it can only be attached to one machine at a time.\\n\\nFor both Azure [Blob Storage](https://azure.microsoft.com/en-us/services/storage/blobs/) and [File Share](https://azure.microsoft.com/en-us/services/storage/files/), you can browse the files stored from any computer using the [Storage Explorer](https://azure.microsoft.com/en-us/features/storage-explorer/) desktop app. Both blob storage containers and file shares can be mounted on your VM so you can use them as if they were local disks. See instructions for mounting [blob storage](https://docs.microsoft.com/en-us/azure/storage/blobs/storage-how-to-mount-container-linux) and [file shares](https://docs.microsoft.com/en-us/azure/storage/files/storage-how-to-use-files-linux). Note however that such file systems have different performance for writing and deleting files than local file systems. Please refer to Azure Storage [performance targets](https://docs.microsoft.com/en-us/azure/storage/common/storage-scalability-targets?toc=%2fazure%2fstorage%2fqueues%2ftoc.json) for more information.\\n\\n\\n\\n## Model Training\\nWe tackle the problem of outlining building footprints in satellite images by applying a semantic segmentation model to first classify each pixel as background, building, or boundary of buildings. The [U-Net](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/) is used for this task. There are two variants of the U-Net implemented in the [models](./models) directory, differing by the sizes of filters used. The baseline U-Net is a similar version as used by the winner of the SpaceNet Building Footprint competition [XD\\\\_XD](https://github.com/SpaceNetChallenge/BuildingDetectors_Round2/tree/master/1-XD_XD). We referenced several open source implementations, noted in the relevant files.\\n\\nCode for training the model is in the [pipeline](./pipeline) directory. The training script is `train.py` and all the paths to input/output, parameters and other arguments are specified in `train_config.py`, which you can modify and experiment with. The default configuration has `total_epochs` set to 15 to run training for 15 epochs, which takes about an hour in total on a VM with a P100 GPU (SKU NC6s_v2 on Azure). For the sample image above, the result of the segmentation model is as follows at epoch 3, 5, 7 and 10:\\n\\n![Example of input image and its label](./visuals/training_progress.png)\\n\\n\\n\\n## Generate Polygons of the Building Footprints\\nStandard graphics techniques are used to convert contiguous blobs of building pixels identified by the segmentation model, using libraries [Rasterio](https://github.com/mapbox/rasterio) and [Shapely](https://github.com/Toblerity/Shapely). The script `pipeline/polygonize.py` performs this procedure, and you can change various parameters in `polygonize_config.py` in the same directory. The most important parameter influencing the performance of the model is `min_polygon_area`, which is the area in squared pixels below which blobs of building pixels are discarded, reducing the noise in our results. Increasing this threshold decreases the number of false positive footprint proposals.\\n\\n\\n## Evaluation\\n\\nThe evaluation metric used by the SpaceNet Challenge is the F1 score, where a footprint proposal is counted as a true positive if its intersection over union ([IoU](https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/)) with the ground truth polygon is above 0.5.\\n\\nYou can of course employ your own metric to suit your application, but if you would like to use the SpaceNet utilities to compute the F1 score based on polygons of building footprints, you need to first combine the annotations for each image in geojson format into a csv with `python/createCSVFromGEOJSON.py` from the utilities [repo](https://github.com/SpaceNetChallenge/utilities). In the root directory of `utilities`, run\\n\\n```\\npython python/createCSVFromGEOJSON.py -imgDir /tutorial_data/val/RGB-PanSharpen -geoDir /tutorial_data/val/geojson/buildings -o ground_truth.csv --CreateProposalFile\\n```\\n\\n\\nThen you can use `python/evaluateScene.py` to compute the F1 score, giving the ground truth csv produced from the last command and the csv output `proposals.csv` produced by `pipeline/polygonize.py` in this repo:\\n\\n```\\npython python/evaluateScene.py ground_truth.csv proposal.csv\\n```\\n\\n\\n## Related Materials\\n\\nBing team's [announcement](https://blogs.bing.com/maps/2018-06/microsoft-releases-125-million-building-footprints-in-the-us-as-open-data) that they released a large quantity of building footprints in the US in support of the Open Street Map community, and [article](https://github.com/Microsoft/USBuildingFootprints) briefly describing their method of extracting them.\\n\\nVery helpful [blog post](http://jeffwen.com/2018/02/23/road_extraction) and [code](https://github.com/jeffwen/road_building_extraction) on road extraction from satellite images by Jeff Wen on a different dataset. We also took inspiration in structuring the training pipeline from this repo.\\n\\nSpaceNet [road extraction](https://spacenetchallenge.github.io/Competitions/Competition3.html) challenge.\\n\\n[Tutorial](https://github.com/Azure/pixel_level_land_classification) on pixel-level land cover classification using semantic segmentation in CNTK on Azure.\\n\\n\\n\\n\"},\n",
       " {'repo': 'wwwtyro/space-3d',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# space-3d\\nQuickly generate procedural 3D space scenes in your browser with WebGL\\n'},\n",
       " {'repo': 'CobaltWolf/Bluedog-Design-Bureau',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# Bluedog-Design-Bureau\\nStockalike parts pack for Kerbal Space Program\\n\\nWhat is this mod?\\n\\nBDB adds parts. Lots of parts. Just over 1000 parts, including launchers, spacecraft, and probes, all in a ReStockalike style. The focus lies primarily on the US space program up through the Apollo program, though some modern parts exist. They are not perfect replicas, but rather take inspiration directly from existing designs. Care has been taken to research and include pieces for projects that were proposed but never happened, to increase the possibilities available to the player. Parts are made in a lego frame of mind, and screenshots of your unique franken creations are always appreciated! The mod is not recommended for new players, as the mod adds a good deal of clutter to the VAB lists. The mod also adds new standard sizes (such a 0.9375m, 1.5m, 1.875m, 3.125m and 5.625m diameter) of parts in order to more accurately scale to KSP.\\n\\nNow, 1000 parts sounds like it would kill your PC. While the mod certainly is large, the memory overhead is likely lower than you would expect. Care has been taken to ensure the mod is efficient, with methods such as enforcing a strict texel density (a technique I have only used since mid-2017, so the older assets might not be as efficient) and texture atlasing to cram several parts onto each texture sheet.\\n\\nBDB makes heavy use of B9PartSwitch\\'s advanced features including features such as\\n- Texture and mesh switching\\n- BDB\\'s own custom fuel switcher to allow different fuel types in all tanks\\n- Engine config switching which can include stats, model and plume switches\\n- Part upgrade handling so that older configurations of engines can still be selected after unlocking the upgrade for accurate replicas\\n\\nFor those familiar with KSP mods, BDB can be considered a ReStockalike equivalent of FASA, and a US counterpart to Tantares\\' soviet rockets.\\n\\nCurrently, the mod includes (but is not limited to):\\n- Mercury spacecraft\\n- Gemini spacecraft\\n- Gemini Ferry\\n- Big Gemini 69\\' (nice)\\n- Big Gemini (Saturn)\\n- Manned Orbiting Laboratory (MOL) and derivative station parts\\n- Modular space station parts based on never built Gemini proposals\\n- Apollo Block I and II spacecraft, as featured in REAL LIFE!\\n- Apollo Block III, III+, IV, and V spacecraft, from Eyes Turned Skywards\\n- AARDV \"Aardvark\" autonomous resupply vehicle, also from Eyes\\n- Modular 1.875m and 1.5m station parts for Gemini-era space stations\\n- Skylab station parts\\n- Spacelab (from Eyes) station parts\\n- Parts for the Manned Venus Flyby mission\\n- Apollo-Soyuz Test Project Docking Module and the APAS 75 docking port\\n- Apollo Boilerplate CM/SM\\n- Vanguard rocket and probe\\n- Scout rocket\\n- Redstone rocket, with Sergeant clusters and Explorer 1\\n- Juno II, IV rockets\\n- Thor-Delta rocket (Almost all variants can be made and including Castor SRMs)\\n- All Able and Delta upper stages with engines.\\n- Never built Delta HOSS cryogenic upper stage.\\n- Delta 2 and 3 rockets (with a variety of GEM SRBs)\\n- 4 meter Delta Cryogenic Second Stage, modeled by Blowfish.\\n- Delta 4 rocket including 5m Delta Cryogenic Second Stage.\\n- Atlas rocket (everything from Atlas A up through Atlas V 551)\\n- Agena upper stage (All variants, plus some never built ones)\\n- Vega (old Kerolox upper stage for Atlas)\\n- Centaur D, G, G\\', T, II & III upper stages\\n- Centaur Kickstage\\n- Titan 1, 2, 3, 4 rockets, with SRMs and Transtage\\n- Config switching for the LR87s, with alternate kerolox versions and plumes to match!\\n- Config switching for the H1, the RS27, and many more! (All config switching provided courtesy of B9 Part Switch)\\n- LDC Titan\\n- Peacekeeper/Minotaur SRB based launcher\\n- Various STAR kick motors\\n- Athena SRB based launcher\\n- Saturn 1, 1B, V\\n- Saturn 1C and Saturn Multibody from Eyes\\n- TIMBER WIND particle bed reactor nuclear engines (TW 75 by Maffif)\\n- Never built cryogenic engines such as the M-1, XLR129, RS30 and RL20.\\n- Keyhole satellites with Corona return capsule (KH1, KH4, KH4B, KH7, KH8 and KH9 Hexagon)\\n\\n-- Huge number of probes including --\\n\\n- AIMP/IMP\\n- Alouette\\n- ANNA 1B\\n- Beacon Explorer\\n- Biosat\\n- Clementine, ISAS\\n- Courier, Relay & Telstar\\n- Explorer 1, 6, 7, 8, 11, S45, S46\\n- Helios\\n- IDCSP\\n- Lunar Orbiter\\n- Mariner 10\\n- Nimbus (parts for most variants)\\n- Orbiting Astronomical Observatory (OAO 1,2 & 3)\\n- Orbiting Frog Otolith\\n- Orbiting Geophysical Observatory\\n- Orbiting Solar Observatory\\n- Pegasus Micrometeorite Satellite (Saturn I-based)\\n- Pioneer 1, 4\\n- Pioneer P3\\n- Pioneer 5\\n- Pioneer 6\\n- Pioneer 10/11\\n- Pioneer SUAE (Saturn Uranus Atmospheric Entry)\\n- Pioneer Outer Planets Orbiter\\n- Ranger block I, II, III, Ranger B, Mariner 2 and Ranger Lander.\\n- Radio Astronomy Explorer A/B\\n- SOLRAD/GRAB/POPPY\\n- Strawman 1-4\\n- Transit 2-3, ANNA A/B\\n- Transit 4\\n- Transit 5A, 5C, 5-Oscar\\n- Transit 5BN\\n- Transit 5E\\n- Transit 5 SOOS (Stacked Oscar on Scout)\\n- TIROS-1\\n- New probe cores\\n- New antennas and dishes\\n- New science experiments\\n- Challenging custom BDB contracts that work with Contract Configurator!\\n- Custom Kerbal Suits (Apollo Skylab A7L) by Bejee10\\n- Simple Adjustable Fairings for high quality historical fairings.\\n- PlumeParty based rocket plume FX for the stock particle system including FX exclusive to BDB provided by Jade of Maar!\\n- Mesh based high quality, high performing plumes powered by Waterfall (by Nertea).\\n- High quality realplumes built with Plume Party and custom FX by Zorg!\\n- Full engine ignitor configs by _Rock3tMan\\n- In total, over 1000 parts and many features for those parts!\\n\\nPlanned content includes massive Saturn rockets including unflown variants, and parts based on proposed projects for Apollo.\\n\\n\\nBluedog Design Bureau by Matthew (CobaltWolf) Mlodzienski is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\\n'},\n",
       " {'repo': 'zlynn1990/SpaceSim',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# SpaceSim\\r\\n\\r\\nSpaceSim is an n-body simulation of the solar system complete with stars, planets, moons, and spacecraft. It is currently setup to simulate various SpaceX launch profiles with vehicles including the Dragon, Falcon 9, and Falcon Heavy.\\r\\n\\r\\n## Installation\\r\\n\\r\\nIf you just want to run the simulation, download the latest build found under the builds directory. Unzip the build and launch SpaceSim.exe to get started. Otherwise, you can clone this repository and browse the source code.\\r\\n\\r\\nIn order to have the best graphically experience, your video card must support double precision openCL kernels. If support isn\\'t detected the program will fallback to basic GDI rendering.\\r\\n\\r\\nThe program accepts two command line arguments. If you want to run the game in windowed mode add a \"-w\". To launch a custom flight profile use the profile\\'s name like \"Falcon Heavy 35000kg\".\\r\\n\\r\\n## Controls\\r\\n\\r\\nThe control scheme is similar to Kerbal Space Program.\\r\\n\\r\\n| Button | Action |\\r\\n| ------ | ----------- |\\r\\n| Enter  | Starts the simulation. |\\r\\n| Escape | Exits the simulation. |\\r\\n| Scroll | Changes the zoom factor. |\\r\\n| [ | Switch focus to previous body. |\\r\\n| ] | Switch focus to next body. |\\r\\n| , | Reduce simulation speed. |\\r\\n| . | Increase simulation speed. |\\r\\n\\r\\n## Flight Profiles\\r\\n\\r\\nFlight profiles are found in the \\'flight profiles\\' root directory. To add a new flight profile create a folder under that directory with a unique name. Flight profiles consist of various xml files with controls for the vehicles components. The control format allows for 5 basic commands (ignition, shutdown, throttle, stage, and orient). The payload.xml file specifies the payload properties like DryMass and PropellantMass. Use the included flight profile as a guide for creating new profiles.\\r\\n\\r\\nSee the instructions above for launching custom profiles.'},\n",
       " {'repo': 'NTaylorMullen/ShootR',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': \"ShootR\\n======\\n\\nMultiplayer space ship game powered by [SignalR] (http://www.asp.net/signalr) and [EndGate] (http://endgate.net/).\\n\\nTo play ShootR visit http://shootr.signalr.net/.\\n\\nIn order to run ShootR locally you must do the following:  \\n  1. If you have VS2012: Install the latest TypeScript version. http://www.microsoft.com/en-us/download/details.aspx?id=34790  \\n  2. Pull down ShootR source.  \\n  3. Checkout master.  \\n  4. Learn [SignalR] (http://www.asp.net/signalr).  \\n  5. Learn [EndGate] (http://endgate.net).  \\n  6. Have fun =]  \\n\\nFeel free to stop by and say hi at http://jabbr.net/#/rooms/ShootR.\\n\\n**SignalR Source**: https://github.com/SignalR/SignalR  \\n**EndGate Source**: https://github.com/ntaylormullen/endgate  \\n\\n###Building for the First Time###\\nThe latest versions of Visual Studio are now cofigured to restore missing packages in solutions that you open, however, ShootR uses packages from a custom package source. \\nTo restore packages in the ShootR solution, you'll need to make sure that your installation of Visual Studio is configured to [restore missing packages] (http://docs.nuget.org/docs/workflows/using-nuget-without-committing-packages). \\n\\nYou'll also need to add the nightly build feed from the Asp.Net web stack to your NuGet package sources, which is located here: http://www.myget.org/F/aspnetwebstacknightly/.\\nYou can learn more about adding a custom feed in Visual Studio on the [Nuget docs site] (http://docs.nuget.org/docs/creating-packages/hosting-your-own-nuget-feeds).\\n\\n\\n\"},\n",
       " {'repo': 'facebookresearch/sound-spaces',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '![](res/logo.png)\\n--------------------------------------------------------------------------------\\nSoundSpaces is a realistic acoustic simulation platform for audio-visual embodied AI research. From audio-visual navigation, audio-visual exploration to echolocation and audio-visual floor plan reconstruction, this platform expands embodied vision research to a broader scope of topics.\\n\\n<p align=\"center\"><a href=\"https://youtu.be/4uiptTUyq30\">\\n  <img src=\"res/soundspaces-demo.gif\"  height=\"400\"></a>\\n<br>\\nClick on the gif to view the video. Listen with headphones to hear the spatial sound properly!\\n</p>\\n\\n[comment]: <> ([<img src=\"https://i.imgur.com/BdhXQaZ.png\" width=\"80%\">]&#40;https://youtu.be/4uiptTUyq30&#41;)\\n[comment]: <> (Presentation videos can be found at our [project page]&#40;http://vision.cs.utexas.edu/projects/audio_visual_navigation/&#41;.)\\n\\n## Motivation\\nMoving around in the world is naturally a multisensory experience, but today\\'s embodied agents are deaf---restricted to solely their visual perception of the environment. We introduce audio-visual navigation for complex, acoustically and visually realistic 3D environments. We further build *SoundSpaces*: a first-of-its-kind dataset of audio renderings based on geometrical acoustic simulations for two sets of publicly available 3D environments (Matterport3D and Replica), and we instrument [Habitat](https://github.com/facebookresearch/habitat-api/blob/master/README.md) to support the new sensor, making it possible to insert arbitrary sound sources in an array of real-world scanned environments.\\n\\n## Citing SoundSpaces\\nIf you use the SoundSpaces platform in your research, please cite the following [paper](https://arxiv.org/pdf/1912.11474.pdf):\\n```\\n@inproceedings{chen22soundspaces2,\\n  title     =     {SoundSpaces 2.0: A Simulation Platform for Visual-Acoustic Learning},\\n  author    =     {Changan Chen and Carl Schissler and Sanchit Garg and Philip Kobernik and Alexander Clegg and Paul Calamia and Dhruv Batra and Philip W Robinson and Kristen Grauman},\\n  booktitle =     {NeurIPS 2022 Datasets and Benchmarks Track},\\n  year      =     {2022}\\n}\\n@inproceedings{chen20soundspaces,\\n  title     =     {SoundSpaces: Audio-Visual Navigaton in 3D Environments},\\n  author    =     {Changan Chen and Unnat Jain and Carl Schissler and Sebastia Vicenc Amengual Gari and Ziad Al-Halah and Vamsi Krishna Ithapu and Philip Robinson and Kristen Grauman},\\n  booktitle =     {ECCV},\\n  year      =     {2020}\\n}\\n```\\nIf you use any of the 3D scene assets (Matterport3D, Replica, HM3D, Gibson, etc.), please make sure you cite these papers as well!\\n\\n## Installation \\n1. Install [habitat-lab v0.2.1](https://github.com/facebookresearch/habitat-lab) and [habitat-sim from this commit](https://github.com/facebookresearch/habitat-sim/tree/80f8e31140eaf50fe6c5ab488525ae1bdf250bd9)\\n2. Install this repo into pip by running the following command:\\n```\\npip install -e .\\n```\\n3. To use SoundSpaces 1.0, follow instructions on the [dataset](soundspaces/README.md) page to download the rendered audio data and datasets.\\n4. ***[New]*** To use SoundSpaces 2.0, add ```--audio``` flag while [building Habitat-Sim from the source](https://github.com/facebookresearch/habitat-sim/blob/80f8e31140eaf50fe6c5ab488525ae1bdf250bd9/BUILD_FROM_SOURCE.md).\\nUse this [Habitat-Sim](https://github.com/facebookresearch/habitat-sim/tree/RLRAudioPropagationUpdate) branch for the latest features. \\n\\n## Usage\\nThis repo renders audio-visual observations with high acoustic and spatial correspondence. \\nIt supports various visual-acoustic learning tasks, including audio-visual embodied navigation, acoustics prediction from egocentric observations, etc.\\nIn this repo, we provide code for training and evaluating audio-visual navigation agents. \\nFor other downstream tasks, please check out each paper\\'s respective repo, \\ne.g., [visual acoustic matching](https://github.com/facebookresearch/visual-acoustic-matching) \\nand [audio-visual dereverberation](https://github.com/facebookresearch/learning-audio-visual-dereverberation).\\n\\nBelow we show some example commands for training and evaluating AudioGoal with depth sensor on Replica. \\n1. Training\\n```\\npython ss_baselines/av_nav/run.py --exp-config ss_baselines/av_nav/config/audionav/replica/train_telephone/audiogoal_depth.yaml --model-dir data/models/replica/audiogoal_depth\\n```\\n2. Validation (evaluate each checkpoint and generate a validation curve)\\n```\\npython ss_baselines/av_nav/run.py --run-type eval --exp-config ss_baselines/av_nav/config/audionav/replica/val_telephone/audiogoal_depth.yaml --model-dir data/models/replica/audiogoal_depth\\n```\\n3. Test the best validation checkpoint based on validation curve\\n```\\npython ss_baselines/av_nav/run.py --run-type eval --exp-config ss_baselines/av_nav/config/audionav/replica/test_telephone/audiogoal_depth.yaml --model-dir data/models/replica/audiogoal_depth EVAL_CKPT_PATH_DIR data/models/replica/audiogoal_depth/data/ckpt.XXX.pth\\n```\\n4. Generate demo video with audio\\n```\\npython ss_baselines/av_nav/run.py --run-type eval --exp-config ss_baselines/av_nav/config/audionav/replica/test_telephone/audiogoal_depth.yaml --model-dir data/models/replica/audiogoal_depth EVAL_CKPT_PATH_DIR data/models/replica/audiogoal_depth/data/ckpt.220.pth VIDEO_OPTION [\\\\\"disk\\\\\"] TASK_CONFIG.SIMULATOR.USE_RENDERED_OBSERVATIONS False TASK_CONFIG.TASK.SENSORS [\\\\\"POINTGOAL_WITH_GPS_COMPASS_SENSOR\\\\\",\\\\\"SPECTROGRAM_SENSOR\\\\\",\\\\\"AUDIOGOAL_SENSOR\\\\\"] SENSORS [\\\\\"RGB_SENSOR\\\\\",\\\\\"DEPTH_SENSOR\\\\\"] EXTRA_RGB True TASK_CONFIG.SIMULATOR.CONTINUOUS_VIEW_CHANGE True DISPLAY_RESOLUTION 512 TEST_EPISODE_COUNT 1\\n```\\n5. Interactive demo\\n```\\npython scripts/interactive_demo.py\\n```\\n5. ***[New]*** Training continuous navigation agent \\n```\\npython ss_baselines/av_nav/run.py --exp-config ss_baselines/av_nav/config/audionav/mp3d/train_telephone/audiogoal_depth_ddppo.yaml --model-dir data/models/ss2/mp3d/dav_nav CONTINUOUS True\\n```\\n\\n## SoundSpaces 1.0\\nWe provide acoustically realistic audio renderings for Replica and Matterport3D datasets. \\nThe audio renderings exist in the form of pre-rendered room impulse responses (RIR), which allows \\nusers to convolve with any source sounds they wish during training. \\nSee [dataset](soundspaces/README.md) for more details.  \\nNote that we do not open source the rendering code at this time.\\n\\n## SoundSpaces 2.0\\nSoundSpaces 2.0 is a fast, continuous, configurable and generalizable audio-visual simulation platform that allows\\nusers to render sounds for arbitrary spaces and environments. \\nAs a result of rendering accuracy improvements, the rendered IRs are different from SoundSpaces 1.0.\\nCheck out example jupyter notebook for a quick tutorial.\\n\\n### Some common issues\\n* If you run into [invalid pointer issues](https://github.com/facebookresearch/habitat-sim/issues/1747), import quaternion before habitat_sim as a workaround.\\n* See this [issue](https://github.com/facebookresearch/rlr-audio-propagation/issues/9) for solutions to GLIBC version issues\\n\\n\\n## Contributing\\nSee the [CONTRIBUTING](CONTRIBUTING.md) file for how to help out.\\n\\n## License\\nSoundSpaces is CC-BY-4.0 licensed, as found in the [LICENSE](LICENSE) file.\\n\\nThe trained models and the task datasets are considered data derived from the correspondent scene datasets.\\n- Matterport3D based task datasets and trained models are distributed with [Matterport3D Terms of Use](http://kaldir.vc.in.tum.de/matterport/MP_TOS.pdf) and under [CC BY-NC-SA 3.0 US license](https://creativecommons.org/licenses/by-nc-sa/3.0/us/).\\n- Replica based task datasets, the code for generating such datasets, and trained models are under [Replica license](https://github.com/facebookresearch/Replica-Dataset/blob/master/LICENSE).\\n'},\n",
       " {'repo': 'WorldWindLabs/SpaceBirds',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': ''},\n",
       " {'repo': 'IBM/spacetech-ssa',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"# Space Situational Awareness in Low Earth Orbit\\n\\n[Low earth orbit](https://en.wikipedia.org/wiki/Low_Earth_orbit) (LEO) is a crowded place and the number of anthropogenic space objects (ASOs) entering LEO is rapidly escalating. With this population boom also comes the inevitable increase in close encounters (conjunctions) between objects. The heart of the space situational awareness (SSA) problem is to predict where ASOs are and where they are going; everything from a spec of paint to the International Space Station.\\n\\nThe current state of the art methods for orbit prediction are physics based models which require extremely accurate knowledge of the object's trajectory, the environment in which it operates, and the intent of the object to maneuver. In practice we do not have access to this data. Trajectories are measured infrequently from noisy ground based radar systems, our understanding of space weather and atmospheric density is limited, and satellite operators are not keen on sharing their plans to maneuver.\\n\\nThis project aims to be an experimental lab and playground for using ML to improve SSA and provides an end-to-end pipeline to:\\n\\n-   ETL orbit data about ASOs in LEO from the USSTRATCOM Space Track API.\\n-   Make orbit predictions based on a physical model.\\n-   Train and use machine learning models to learn the error in physics based orbit prediction models.\\n-   Quickly perform a temporal-spatial search of orbit predictions to identify conjunctions based on parameterized queries.\\n-   Visualize when and where conjunctions may occur.\\n\\n\\n## Components\\n\\nThe SSA solution is currently composed of two components.\\n\\n\\n### Orbit Prediction\\n\\nThe [orbital prediction component](orbit_prediction/README.md) combines physics and machine learning models to predict the future path of ASOs.\\n\\n\\n### Conjunction search\\n\\nThe [conjunction search component](conjunction_search/README.md) combs through future orbit predictions to determine when and where two ASOs may come close to each other based on user provided search parameters.\\n\\n## Live Demo\\n\\nThe SSA solution demo can be accessed at https://spaceorbits.net/\\n\\n## Contributing\\n\\nWe very much encourage anyone and everyone to join and contribute to this project.  Please see the [contributing file](CONTRIBUTING.md) for more details.\\n\\n## License\\n\\nThe IBM Space Situational Awareness solution is licensed under the Apache 2.0 license. Full license text is available at [LICENSE](LICENSE).\\n\"},\n",
       " {'repo': 'spacesiren/spacesiren',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"# SpaceSiren\\n\\nSpaceSiren is a [honey token](https://en.wikipedia.org/wiki/Honeypot_(computing)) \\nmanager and alert system for AWS. With this fully serverless application,\\nyou can create and manage honey tokens at scale -- up to 10,000 per SpaceSiren\\ninstance -- at close to no cost.<sup>1</sup>\\n\\n![SpaceSiren mascot](docs/images/logo/spacesiren-banner-medium.png)\\n\\n## How It Works\\n\\n* SpaceSiren provides an API to create no-permission AWS IAM users and access keys for those users.\\n* You sprinkle the access keys wherever you like, for example in proprietary code or private data stores.\\n* If one of those sources gets breached, an attacker is likely to use the stolen key to see what they can do with it.\\n* You will receive an alert that someone attempted to use the key.\\n\\n![Token API screenshot](docs/images/screenshots/token-api.png)\\n\\n## Alert Outputs\\n\\n* Email\\n* PagerDuty\\n* Slack\\n* Pushover\\n\\n> ![Email alert](docs/images/screenshots/alert-email.png)\\n\\n## Documentation Pages\\n\\n* [Getting Started](docs/getting-started.md)\\n* [Alerts](docs/alerts.md)\\n* [API Documentation](docs/api.md)\\n* [Terraform Variables](docs/tfvars.md)\\n\\n## Requirements\\n\\nAs with any open source project, this one assumes you have the required\\nfoundational tools and knowledge, mainly in AWS and Terraform.\\n\\n### Resources\\n\\n* Terraform >= 0.13\\n* AWS CLI\\n* A dedicated AWS account with admin access\\n* A registered domain\\n\\n### Knowledge\\n\\n* Basic Terraform\\n* Basic REST API\\n* Basic AWS CLI, S3, and Route 53\\n* Basic AWS Organizations and IAM Roles for cross-account access\\n* Intermediate DNS (delegating a (sub)domain with NS records)\\n\\n## Contact\\n\\nIf you notice a critical security bug (e.g., one that would grant real access to\\nan AWS account), please responsibly disclose it via email at\\n[contact@spacesiren.io](mailto:contact@spacesiren.io).\\n\\nFor standard bugs or feature requests, please open a GitHub issue.\\n\\n\\n## Attributions\\n\\nSpecial thanks to:\\n\\n* Atlassian for [Project SpaceCrab](https://bitbucket.org/asecurityteam/spacecrab), the\\n  inspiration for this project. If you want to read about why I started SpaceSiren,\\n  please see my [SpaceCrab critique](docs/spacecrab.md) page.\\n* The wonderful and talented\\n  [Alia Mancisidor](https://www.instagram.com/figmentpie/) for the artwork.\\n* Anyone who volunteered to test this application for me.\\n\\n---\\n\\n## Footnotes\\n\\n1. While SpaceSiren was designed to run as cheaply as possible, even for\\n   individuals, it will not be entirely free of operating costs. You will incur\\n   nominal costs for DynamoDB, Lambda, API Gateway, Route 53, and perhaps\\n   CloudTrail, depending on your configuration. You should expect to spend\\n   between $1 and $5 per month to run SpaceSiren. Of course, the project's\\n   maintainers are not responsible for any actual costs you incur. Please closely\\n   monitor your AWS bill while it is in use. \\n\"},\n",
       " {'repo': 'Mondego/spacetime-crawler4py',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': 'ABOUT\\n-------------------------\\nThis is the base implementation of a full crawler that uses a spacetime\\ncache server to receive requests.\\n\\nCONFIGURATION\\n-------------------------\\n\\n### Step 1: Install dependencies\\n\\nIf you do not have Python 3.6+:\\n\\nWindows: https://www.python.org/downloads/windows/\\n\\nLinux: https://docs.python-guide.org/starting/install3/linux/\\n\\nMAC: https://docs.python-guide.org/starting/install3/osx/\\n\\nCheck if pip is installed by opening up a terminal/command prompt and typing\\nthe commands `python3 -m pip`. This should show the help menu for all the \\ncommands possible with pip. If it does not, then get pip by following the\\ninstructions at https://pip.pypa.io/en/stable/installing/\\n\\nTo install the dependencies for this project run the following two commands\\nafter ensuring pip is installed for the version of python you are using.\\nAdmin privileges might be required to execute the commands. Also make sure\\nthat the terminal is at the root folder of this project.\\n```\\npython -m pip install packages/spacetime-2.1.1-py3-none-any.whl\\npython -m pip install -r packages/requirements.txt\\n```\\n\\n### Step 2: Configuring config.ini\\n\\nSet the options in the config.ini file. The following\\nconfigurations exist.\\n\\n**USERAGENT**: Set the useragent to `IR F19 uci-id1,uci-id2,uci-id3`. \\nIt is important to set the useragent appropriately to get the credit for \\nhitting our cache.\\n\\n**HOST**: This is the host name of our caching server. Please set it as per spec.\\n\\n**PORT**: This is the port number of our caching server. Please set it as per spec.\\n\\n**SEEDURL**: The starting url that a crawler first starts downloading.\\n\\n**POLITENESS**: The time delay each thread has to wait for after each download.\\n\\n**SAVE**: The file that is used to save crawler progress. If you want to restart the\\ncrawler from the seed url, you can simply delete this file.\\n\\n**THREADCOUNT**: This can be a configuration used to increase the number of concurrent\\nthreads used. Do not change it if you have not implemented multi threading in\\nthe crawler. The crawler, as it is, is deliberately not thread safe.\\n\\n\\n### Step 3: Define your scraper rules.\\n\\nDevelop the definition of the function scraper in scraper.py\\n\\n```\\ndef scraper (url: str, resp: utils.response.Response): -> list\\n    pass\\n```\\n\\nThe scraper takes in two parameters:\\n\\n**ARGS**\\n\\n*url*:\\n\\nThe URL that was added to the frontier, and downloaded from the cache.\\nIt is of type str and was an url that was previously added to the\\nfrontier.\\n\\n*resp*:\\n\\nThis is the response given by the caching server for the requested URL.\\nThe response is an object of type Response (see utils/response.py)\\n```\\nclass Response:\\n    Attributes:\\n        url:\\n            The URL identifying the response.\\n        status:\\n            An integer that identifies the status of the response. This\\n            follows the same status codes of http.\\n            (REF: https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html)\\n            In addition there are status codes provided by the caching\\n            server (600-606) that define caching specific errors.\\n        error:\\n            If the status codes are between 600 and 606, the reason for\\n            the error is provided in this attribute. Note that for status codes\\n            (400-599), the error message is not put in this error attribute; instead it\\n            must picked up from the raw_response (if any, and if useful).\\n        raw_response:\\n            If the status is between 200-599 (standard http), the raw\\n            response object is the one defined by the requests library.\\n            Useful resources in understanding this raw response object:\\n                https://realpython.com/python-requests/#the-response\\n                https://requests.kennethreitz.org/en/master/api/#requests.Response\\n            HINT: raw_response.content gives you the webpage html content.\\n```\\n**Return Value**\\n\\nThis function needs to return a list of urls that are scraped from the\\nresponse. (An empty list for responses that are empty). These urls will be\\nadded to the Frontier and retrieved from the cache. These urls have to be\\nfiltered so that urls that do not have to be downloaded are not added to the\\nfrontier.\\n\\nThe first step of filtering the urls can be by using the **is_valid** function\\nprovided in the same scraper.py file. Additional rules should be added to the is_valid function to filter the urls.\\n\\nEXECUTION\\n-------------------------\\n\\nTo execute the crawler run the launch.py command.\\n```python3 launch.py```\\n\\nYou can restart the crawler from the seed url\\n(all current progress will be deleted) using the command\\n```python3 launch.py --restart```\\n\\nYou can specify a different config file to use by using the command with the option\\n```python3 launch.py --config_file path/to/config```\\n\\nARCHITECTURE\\n-------------------------\\n\\n### FLOW\\n\\nThe crawler receives a cache host and port from the spacetime servers\\nand instantiates the config.\\n\\nIt launches a crawler (defined in crawler/\\\\_\\\\_init\\\\_\\\\_.py L5) which creates a \\nFrontier and Worker(s) using the optional parameters frontier_factory, and\\nworker_factory.\\n\\nWhen the crawler in started, workers are created that pick up an\\nundownloaded link from the frontier, download it from our cache server, and\\npass the response to your scraper function. The links that are received by\\nthe scraper is added to the list of undownloaded links in the frontier and\\nthe url that was downloaded is marked as complete. The cycle continues until\\nthere are no more urls to be downloaded in the frontier.\\n\\n### REDEFINING THE FRONTIER:\\n\\nYou can make your own frontier to use with the crawler if they meet this\\ninterface definition:\\n```\\nclass Frontier:\\n    def __init__(self, config, restart):\\n        #Initializer.\\n        # config -> Config object (defined in utils/config.py L1)\\n        #           Note that the cache server is already defined at this\\n        #           point.\\n        # restart -> A bool that is True if the crawler has to restart\\n        #           from the seed url and delete any current progress.\\n\\n    def get_tbd_url(self):\\n        # Get one url that has to be downloaded.\\n        # Can return None to signify the end of crawling.\\n\\n    def add_url(self, url):\\n        # Adds one url to the frontier to be downloaded later.\\n        # Checks can be made to prevent downloading duplicates.\\n    \\n    def mark_url_complete(self, url):\\n        # mark a url as completed so that on restart, this url is not\\n        # downloaded again.\\n```\\nA sample reference is given in utils/frontier.py L10. Note that this\\nreference is not thread safe.\\n\\n### REDEFINING THE WORKER\\n\\nYou can make your own worker to use with the crawler if they meet this\\ninterface definition:\\n```\\nfrom scraper import scraper\\nfrom utils.download import download\\nclass Worker(Thread): # Worker must inherit from Thread or Process.\\n    def __init__(self, worker_id, config, frontier):\\n        # worker_id -> a unique id for the worker to self identify.\\n        # config -> Config object (defined in utils/config.py L1)\\n        #           Note that the cache server is already defined at this\\n        #           point.\\n        # frontier -> Frontier object created by the Crawler. Base reference\\n        #           is shown in utils/frontier.py L10 but can be overloaded\\n        #           as detailed above.\\n        self.config = config\\n        super().__init__(daemon=True)\\n\\n    def run(self):\\n        In loop:\\n            > url = get one undownloaded link from frontier.\\n            > resp = download(url, self.config)\\n            > next_links = scraper(url, resp)\\n            > add next_links to frontier\\n            > sleep for self.config.time_delay\\n```\\nA sample reference is given in utils/worker.py L9.\\n\\nTHINGS TO KEEP IN MIND\\n-------------------------\\n\\n1. It is important to filter out urls that do not point to a webpage. For\\n   example, PDFs, PPTs, css, js, etc. The is_valid filters a large number of\\n   such extensions, but there may be more.\\n2. It is important to filter out urls that are not with ics.uci.edu domain.\\n3. It is important to maintain the politeness to the cache server (on a per\\n   domain basis).\\n4. It is important to set the user agent in the config.ini correctly to get\\n   credit for hitting the cache servers.\\n5. Launching multiple instances of the crawler will download the same urls in\\n   both. Mechanisms can be used to avoid that, however the politeness limits\\n   still apply and will be checked.\\n6. Do not attempt to download the links directly from ics servers.\\n'},\n",
       " {'repo': 'MICA-MNI/BrainSpace',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '====================\\nBrainSpace\\n====================\\n\\n\\n.. image:: https://github.com/MICA-MNI/brainspace/workflows/Python%20package/badge.svg\\n   :target: https://github.com/MICA-MNI/brainspace/actions\\n\\n.. image:: https://codecov.io/gh/mica-mni/brainspace/branch/master/graph/badge.svg\\n   :target: https://codecov.io/gh/mica-mni/brainspace\\n\\n.. image:: https://img.shields.io/appveyor/build/OualidBenkarim/brainspace/master?logo=appveyor\\n   :alt: AppVeyor branch\\n\\n.. image:: https://img.shields.io/pypi/v/brainspace\\n   :target:  https://pypi.python.org/pypi/brainspace\\n\\n.. image:: https://img.shields.io/pypi/l/brainspace?label=License\\n   :target: https://opensource.org/licenses/BSD-3-Clause\\n\\n.. image:: https://img.shields.io/pypi/pyversions/brainspace\\n   :alt: PyPI - Python Version\\n\\nBrainSpace is a lightweight cross-platform toolbox primarily intended \\nfor macroscale gradient mapping and analysis of \\nneuroimaging and connectome level data. The current version \\nof BrainSpace is available in Python and MATLAB, programming \\nlanguages widely used by the neuroimaging and network neuroscience \\ncommunities. The toolbox also contains several maps that allow for \\nexploratory analysis of gradient correspondence with other \\nbrain-derived features, together with tools to generate spatial null models.\\n\\nFor installation instructions, examples and documentation of BrainSpace see\\nour `documentation <https://brainspace.readthedocs.io>`_.\\n\\nHappy gradient analysis! \\n\\nLicense\\n-----------\\n\\nThe BrainSpace source code is available under the BSD (3-Clause) license.\\n\\nSupport\\n-----------\\n\\nIf you have problems installing the software or questions about usage \\nand documentation, or something else related to BrainSpace, \\nyou can post to the Issues section of our `repository <https://github.com/MICA-MNI/BrainSpace/issues>`_.\\n\\nPaper\\n-----------\\n\\nIf you consider using BrainSpace, please cite our manuscript: \\nVos de Wael R, Benkarim O, Paquola C, Lariviere S, Royer J, Tavakol S, Xu T, Hong S, Langs G, Valk S, Misic B, Milham M, Margulies D, Smallwood J, Bernhardt B (2020). BrainSpace: a toolbox for the analysis of macroscale gradients in neuroimaging and connectomics datasets. Commun Biol 3, 103.\\n\\nCore development team\\n-----------------------\\n\\n* Reinder Vos de Wael, MICA Lab - Montreal Neurological Institute\\n* Oualid Benkarim, MICA Lab - Montreal Neurological Institute\\n* Boris Bernhardt, MICA Lab - Montreal Neurological Institute\\n\\n'},\n",
       " {'repo': 'monaverse/SpaceStarter',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '![splash-min](https://user-images.githubusercontent.com/61861940/147894582-30f0b076-6647-46b2-b731-1a9535b19aa9.jpg)\\n<p align=\"center\">Start building your own custom Mona Space using our official template</p>\\n\\n## ⬇️ Unity Version\\nThis template requires the ```Unity 2020.3.18``` version. You can download it here:\\nhttps://unity3d.com/unity/whats-new/2020.3.18\\n\\n\\n## 📃 Documentation\\n\\nThe official documentation website is [docs.monaverse.com](https://docs.monaverse.com/get-started).\\n\\nMona [Video tutorials here](https://docs.monaverse.com/mona-tutorials)\\n\\n\\n## 💬 Support\\n\\nFor support, join our [Discord support channel](https://discord.gg/gcrGHzTerU)\\n\\n## ⚙️ Template version\\n1.4.1\\n'},\n",
       " {'repo': 'jmrapp1/SpaceInvaders', 'language': 'Java', 'readme_contents': ''},\n",
       " {'repo': 'ONLYOFFICE/DocSpace',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '\\ufeff# ONLYOFFICE App Server\\n\\n## Overview\\n\\nONLYOFFICE App Server is a platform based on .NET Core and React engines which comprises document management features and makes it possible to implement advanced folder management. \\n\\n## Functionality\\n\\n* Cross-platform: web and mobile environment.\\n* Integration with online editors (text documents, spreadsheets, presentations, forms).\\n* External sharing for editing, reviewing, commenting, reading.\\n* Version history.\\n* File and folder search.\\n* Connecting Dropbox, Google Drive, Box, OneDrive, Nextcloud, ownCloud, kDrive, Yandex.Disk.\\n\\n## Licensing \\n\\nApp Server is released under AGPLv3 license. See the LICENSE file for more information.\\n\\n## Project info\\n\\nOfficial website: [https://www.onlyoffice.com](https://www.onlyoffice.com/?utm_source=github&utm_medium=cpc&utm_campaign=AppServer \"https://www.onlyoffice.com/?utm_source=github&utm_medium=cpc&utm_campaign=AppServer\")\\n\\nCode repository: [https://github.com/ONLYOFFICE/AppServer](https://github.com/ONLYOFFICE/AppServer)\\n\\n## User feedback and support\\n\\nIf you have any problems or questions about App Server, use the Issues section in this repository or visit our [official forum](https://forum.onlyoffice.com/).\\n'},\n",
       " {'repo': 'Adivise/NanoSpace',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': \"## 📄 READ THIS\\n\\n**💥 THIS PROJECT WILL BE ARCHIVE NOW**\\n\\n**[NanoSpacePlus](https://github.com/Adivise/NanoSpacePlus)** (`Use this version is up to date`)\\n\\n## 📑 Short Feature\\n- [x] Music System\\n- [x] Playlists System\\n- [x] Premium System\\n- [x] Song Request System\\n- [x] Custom Prefix\\n- [x] Multi Language\\n- [x] Custom Filters\\n- [x] Easy to use\\n\\n## 🎶 Support Source\\n- [x] Youtube\\n- [x] SoundCloud\\n- [x] Spotify\\n- [x] Tidal\\n- [x] Deezer\\n- [x] Facebook \\n- [x] Twitch\\n- [x] Apple\\n- [x] Bandcamp\\n- [x] Vimeo\\n- [x] Https (Radio)\\n\\n## 🚨 Have a Problem\\n\\n✈ Join Discord:  [NanoSpace ♪♪](https://discord.gg/SNG3dh3MbR)\\n   mention me in chat #general or #javascript and ask problem okay! 👌\\n\\n<details><summary>📎 Requirements [CLICK ME]</summary>\\n<p>\\n\\n## 📎 Requirements\\n\\n- Node.js+ **[Download](https://nodejs.org/en/download/)**\\n- Discord Bot Token **[Guide](https://discordjs.guide/preparations/setting-up-a-bot-application.html#creating-your-bot)**\\n- LavaLink **[Guide](https://github.com/freyacodes/lavalink)** (*Dev Version!* **[Download](https://ci.fredboat.com/repository/downloadAll/Lavalink_Build/9311:id/artifacts.zip)** )\\n- MongoDB **[Download](https://www.mongodb.com/try/download/community)** (Download & install = Finish!)\\n\\n## 🛑 Super Requirements \\n\\n- Java 11-13 **[Download JDK13](http://www.mediafire.com/file/m6gk7aoq96db8g0/file)** (i use this version) for LAVALINK!\\n\\n</p>\\n</details>\\n\\n## 📚 Installation\\n\\n```\\ngit clone https://github.com/Adivise/NanoSpace\\ncd NanoSpace\\nnpm install\\n```\\n\\n<details><summary>📄 Configuration [CLICK ME]</summary>\\n<p>\\n\\n## 📄 Configuration\\n\\nCopy or Rename `.env.example` to `.env` and fill out the values:\\n\\n```.env\\n# Bot\\nTOKEN=REPLACE_HERE\\nPREFIX=#\\nNP_REALTIME=true\\nLEAVE_TIMEOUT=120000\\nLANGUAGE=en\\nEMBED_COLOR=#000001\\n\\n# Dev\\nOWNER_ID=REPLACE_HERE\\n\\n# Database\\nMONGO_URI=mongodb://127.0.0.1:27017/nanospace\\nLIMIT_TRACK=100\\nLIMIT_PLAYLIST=10\\n\\n# Nodes\\nNODE_HOST=localhost\\nNODE_PORT=5555\\nNODE_PASSWORD=123456\\n```\\n\\t\\nAfter installation or finishes all you can use `node .` to start the bot. or `Run Start.bat`\\n\\n</p>\\n</details>\\n\\n<details><summary>🔩 Features & Commands [CLICK ME]</summary>\\n<p>\\n\\n## 🔩 Features & Commands\\n\\n> Note: The default prefix is '#'\\n\\n🎶 **Music Commands!** \\n\\n- Play (#play, #p, #pplay [song/url])\\n- Nowplaying (#nowplaying, #np, #now)\\n- Queue (#queue <page>)\\n- Repeat (#loop (current, all), #repeat (current, all))\\n- Loopqueue (#loopall, #lq, repeatall)\\n- Shuffle (#shuffle, mix)\\n- Volume control (#vol, #v [10 - 100])\\n- Pause (#pause, #pa)\\n- Resume (#resume, #r)\\n- Skip (#skip, #s)\\n- Skipto (#skipto, #st [position])\\n- Clear (#clear)\\n- Join (#join, #summon)\\n- Leave (#leave, #dc, #lev, #stop)\\n- Forward (#forward <second>)\\n- Seek (#seek <second>)\\n- Rewind (#rewind <second>)\\n- Replay (#replay)\\n- Search (#search [songname])\\n- 247 (#247)\\n- Previous (#previous)\\n- Autoplay (#autoplay)\\n- PlaySkip (#playskip [song/url])\\n- SearchSkip (#searchskip [song/url])\\n- Move (#move [song | position])\\n- RemoveTrack (#removetrack, #rs, #rt [position])\\n\\n⏺ **Filter Commands!**\\n- Bass (#bass)\\n- Superbass (#superbass, #sb)\\n- Pop (#pop)\\n- Treblebass (#treblebass, #tb)\\n- Soft (#soft)\\n- Earrape (#earrape, #ear)\\n- Equalizer (#eq <custom>)\\n- Speed (#speed <amount>)\\n- Picth (#pitch <amount>)\\n- Vaporwave (#vaporwave)\\n- Nightcore (#nightcore)\\n- Bassboost (#bassboost, #bb [-10 - 10])\\n- Rate (#rate)\\n- Reset (#reset)\\n- 3d (#3d)\\n- China (#china)\\n- Dance (#dance)\\n- Chipmunk (#chipmunk)\\n- Darthvader (#darthvader)\\n- DoubleTime (#doubletime)\\n- SlowMotion (#slowmotion)\\n- Tremolo (#tremolo)\\n- Vibrate (#vibrate)\\n- Vibrato (#vibrato)\\n- Daycore (#daycore)\\n- Television (#Television)\\n- Jazz (#jazz)\\n\\t\\n📦 **Playlist Commands!**\\n- Create (#create [name])\\n- Add (#add [name] [link])\\n- Private (#private [name])\\n- Public (#public [name])\\n- Delete (#delete [name])\\n- Import (#import [name])\\n- Detail (#detail [name])\\n- Remove (#remove [name] [position])\\n- Savequeue (#savequeue [name])\\n- View (#view)\\n\\t\\n💎 **Premium Commands!**\\n- Profile (#profile)\\n- Generate (#generate [plan] [amount])\\n- Redeem (#redeem [code])\\n- Unpremium (#unpremium [@mention])\\n- Setup (#setup)\\n\\t\\n📑 **Utilities Commands!**\\n- Restart (#restart, #stopbot)\\n- Prefix (#prefix [new prefix])\\n- Language (#language [lang]) // Example: en, hi\\n- Help (#help, #halp [command])\\n\\n</p>\\n</details>\\n\\n\\n<details><summary>🖼 Picture [CLICK ME]</summary>\\n<p>\\n\\n## 🖼 Picture & ScreenShots\\n\\n![see](https://i.imgur.com/xUurYDJ.png)\\n![see](https://i.imgur.com/hxSCmeP.png)\\n![see](https://i.imgur.com/P3GNCbQ.png)\\n![see](https://i.imgur.com/9Plhzar.png)\\n![see](https://i.imgur.com/k2Sp8zo.png)\\n\\n</p>\\n</details>\\n\\n<details><summary>👏 Credits [CLICK ME]</summary>\\n<p>\\n\\n## 👏 THANK\\n- [lavamusic](https://github.com/brblacky/lavamusic)\\n- [EarTensifier](https://github.com/Tetracyl/EarTensifier)\\n\\n</p>\\n</details>\\n\"},\n",
       " {'repo': 'spacedrop/spacedrop',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# SpaceDrop\\nAn open-source CMS in JavaScript, built with Meteor and React.\\n\\n## Achievement unlocked, you found us.\\n\\nHands to you for Googling and finding us, the next step is up to you.\\nBecome a co-author and member of the die-hards controlling SpaceDrop\\'s future direction, by forking and creating a PR that gets committed.\\n\\n## API examples\\n\\n> Below APIs are functional, more APIs are coming soon!\\n\\n### Menu API\\n\\n#### Create route to view content of type node.\\n\\n```\\nSD.Menu.route({\\n  path: \\'node/:nid\\',\\n  component: SD.Views.Node,\\n  subscriptions: {\\n    \\'node.node\\': [\\'nid\\']\\n  }\\n})\\n```\\n\\n### Permission API\\n\\n#### Create permission to access published content.\\n\\n```\\nSD.Permission.permission({\\n  name: \\'access content\\',\\n  title: \\'View published content\\'\\n});\\n```\\n\\n#### Create a role with limited permissions.\\n\\n```\\nSD.Permission.role({\\n  name: \\'guest\\',\\n  permissions: {\\n    \\'access content\\': 1\\n  }\\n});\\n```\\n\\n### Entity API\\n\\n#### Create an entity of type \"node\".\\n\\n```\\nlet Node = class extends SD.Structure.Entity {\\n  constructor({ name, schema = {}, indexes = {} }) {\\n    super({\\n      type: \\'node\\',\\n      bundle: name,\\n      schema: _.extend({\\n        nid: {\\n          type: Number,\\n          optional: true,\\n          autoValue: () => {\\n            return this.collection.find().count() + 1;\\n          }\\n        },\\n        bundle: {\\n          type: String,\\n          optional: true,\\n          autoValue: function() {\\n            return name;\\n          }\\n        },\\n        title: {\\n          type: String\\n        }\\n      }, schema)\\n    });\\n\\n    // Ensure index for entity type node.\\n    Node._ensureIndex(\\'nid\\', {unique: 1});\\n  }\\n}\\n\\n// Export Node to packages and application.\\nSD.Structure.Node = Node;\\n```\\n\\n### Node API\\n\\n#### Create a content type (bundle) with the name \"page\".\\n\\n```\\nlet Page = new SD.Structure.Node({\\n  name: \\'page\\',\\n  schema: {\\n    body: {\\n      type: String\\n    }\\n  }\\n});\\n```\\n\\n#### List all pages (nodes of content type \"page\").\\n\\n```\\nlet nodes = Page.find();\\n```\\n\\n#### List all nodes regardless of content type (bundle).\\n\\n```\\nlet nodes = SD.Structure.Node.find();\\n```\\n\\n> Help improve these APIs! Fork and create a pull request, when it gets committed you\\'ll be added as member of the die-hards and help decide the future for this project.\\n\\n## Sub-projects\\n\\n* Core\\n  * [x] Logging system (inner logging, logs transmitted to external system)\\n  * Caching management and injection\\n    * [x] Subscription caching\\n    * [ ] Appcache\\n  * Menu\\n    * [ ] Simplify the API to create and manage routes and access management for other packages.\\n    * [ ] UI for management of menus.\\n  * User\\n    * [ ] UI for management of users.\\n  * [ ] Permission\\n    * [ ] Access management for packages.\\n    * [ ] UI for roles and user permissions.\\n* Theming\\n  * [ ] React Templates (Blaze inspired templating)\\n  * [ ] Sideburns (Blaze inspired events/helpers/callbacks)\\n  * [ ] Overriding templates\\n  * [ ] Basic theme (package)\\n* Admin UI\\n  * [ ] Administrating site settings\\n  * [ ] Editing documents, forms generated from schemas in the Structure package.\\n  * [ ] Add/remove and publish/unpublish documents.\\n  * [ ] Packages overview (on/off capabilities)\\n  * [ ] Reports package and page showing status of current installed version and more.\\n  * [ ] Admin menu for all administrative tasks.\\n* Configuration management\\n  * Structure\\n    * [ ] Creating collections\\n    * [ ] Manage schemas for collections\\n    * [ ] Manage indexes for a collection\\n  * Structure UI\\n    * [ ] UI for adding/removing collections\\n    * [ ] UI for adding/removing fields on a collections\\n  * Structure from UI to code\\n    * [ ] Structures should generate code from the UI if running locally.\\n    * [ ] Structures should be readable from production, but not  editable on a published application.\\n* SEO\\n  * [ ] Pagetitle\\n  * [ ] Metatags\\n  * [ ] Open Graph\\n  * [ ] Schema.Org\\n  * [ ] Rich Snippets v2\\n  * SEO package\\n    * [ ] UI for editing SEO defaults or per entity\\n* API\\n  * [ ] REST api to list/create collections, schemas etc.\\n  * [ ] REST api for read/write to documents in a collection.\\n  * [ ] CSV importer.\\n  * [ ] CSV exporter.\\n* Documentation\\n  * [ ] JSDoc on all packages/components/classes generated to a docs site.\\n  * [ ] Manuals for all API\\'s such as Menu, Permission, Structure, etc.\\n  * [ ] Documentation how to create a new app using a distribution (Distributions are ready sets of SpaceDrop packages one can get started using direcly).\\n* Security\\n  * [ ] Argument checks\\n  * [ ] Browser policy\\n  * [ ] Rate limiting for preventing flooding\\n* Asset management system\\n  * [ ] Media library for static files\\n  * [ ] Images (PNG, JPEG, WebP) with server side optimization (resolutions, metadata removal, conversions) and client side edition.\\n  * [ ] SVG with client side edition\\n  * [ ] MPEG-4 with server side entropy recompression and optimization (resolutions, conversions).\\n  * [ ] External asset importer for Youtube\\n* Features as packages\\n  * [ ] Browser outdated warning\\n  * [ ] Cookie information\\n  * [ ] Legal information\\n  * [ ] Favicons with manifests and screen launching for mobiles\\n  * [ ] Search engine (full text search as default, external river for Elasticsearch)\\n  * [ ] Basic recommender engine\\n  * [ ] Analytics (GA, Mixpanel, ...).\\n  * [ ] Transactional emails (SMTP, Mailjet, Mandrill, ...) with client side edition (tagging, theming)\\n  * [ ] PDF generation for catalogues, license agreement, ...\\n  * [ ] Payment capabilities (Braintree, Stripe, ...)\\n  * [ ] A/B testing (rendering depending on population, reports on funnels, ...)\\n  * [ ] Sharers for social networks\\n  * [ ] UTM campaigns\\n  * [ ] Ads / Banners\\n  * [ ] Rocket.Chat integration with bots for outside hours\\n  * [ ] Maps integration with styling\\n* System health support\\n  * [ ] Client information on connection status\\n  * [ ] Server status\\n  * [ ] Simple analytic dashboard (CPU consumption, RAM, Drive)\\n* Devops - Assistance and helpers for deployment\\n  * [ ] Docker based infrastructure for monolithic applications\\n  * [ ] Docker based infrastructure for clustered applications\\n  * [ ] Cordova project for iOS, Android and Android legacy (with Crosswalk integration)\\n* Internationalization\\n  * [ ] Simple translation support for packages.\\n  * [ ] UI for translating strings\\n  * [ ] UI to update translations from external sources, i.e. https://localize.drupal.org/\\n'},\n",
       " {'repo': 'mikadomethod/space',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': \"This repository contains messy code for practicing the Mikado Method,\\nrefactorings and restructurings, clean code and object-oriented design.\\n\\nRUNNING THE CODE\\n================\\nThe project has a `Space` class that is the main class. The `main()` method takes no parameters.\\n\\nThe Space class has two static boolean members that control behavior, `IS_BOUNCING_BALLS` and\\n`IS_BREAKOUT`.\\n\\nIf `IS_BOUNCING_BALLS` is `false`, the application is a simulation of a solar system,\\nnot that different from ours.\\nThe value of `IS_BREAKOUT` doesn't matter.\\n\\nIf `IS_BOUNCING_BALLS` is `true`, the application shows a box of bouncing balls.\\nIf `IS_BREAKOUT` is `true`, the balls can exit through the lower side of the box.\\nIf `IS_BREAKOUT` is `false`, the balls just keep bouncing in the box.\\n\\nThere are also some tests available. They don't cover all the code, just as in real life. ;-)\\n\\nEXERCISES\\n=========\\nEach of the exercises can start from a fresh pull of the code. They can also start from\\nthe previous (working) state as the exercises are completed.\\n\\nEasy level\\n----------\\n##### Background\\nThe `PhysicalObject` is to be used in another project. However, the entire `Space` class must NOT be shared.\\nYour task is to extract the `PhysicalObject` to a new project.\\n##### Goal\\nEnable reuse of `PhysicalObject`\\n\\n\\nIntermediate level\\n------------------\\n##### Background\\nThe solar system and bouncing balls applications are to be sold in separate delivery packages.\\nDue to legal issues, the solar system may not contain any bouncing balls logic and vice versa.\\n##### Goal\\nTwo separate, minimal, deliverables for bouncing balls and solar system.\\n\\n\\nDifficult level\\n---------------\\n##### Background\\nThe application is a huge success, and will be ported to a limited\\ndevice without Swing/AWT support. The exact API of the new graphics support is\\nnot ready, but to be first on the market when it arrives, you need to start\\nseparating presentation logic from domain ASAP.\\n##### Goal\\nThe domain logic is compilable without Swing/AWT dependencies. When Swing/AWT +\\nany bridging code is available, the application should be runnable.\\n\"},\n",
       " {'repo': 'lakesare/memcode',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '<div align=\"center\">\\n  <a href=\"http://memcode.com\" title=\"Website memcodec.com\">\\n    <img src=\"https://img.shields.io/website-up-down-green-red/http/shields.io.svg\"/>\\n  </a>\\n  \\n  <a href=\"https://GitHub.com/Naereen/lakesare/memcode/contributors/\" title=\"GitHub contributors\">\\n    <img src=\"https://img.shields.io/github/contributors/lakesare/memcode\"/>\\n  </a>\\n  \\n  <a href=\"https://github.com/lakesare/memcode/blob/master/LICENSE\" title=\"GitHub license\">\\n    <img src=\"https://img.shields.io/github/license/Naereen/StrapDown.js.svg\"/>\\n  </a>\\n  \\n  <a href=\"https://reactjs.org/docs/how-to-contribute.html#your-first-pull-request\" title=\"PRs welcome\">\\n    <img src=\"https://img.shields.io/badge/PRs-welcome-brightgreen.svg\"/>\\n  </a>\\n\\n  <a href=\"https://gitpod.io/#https://github.com/lakesare/memcode\" title=\"Gitpod Ready-to-Code\">\\n    <img src=\"https://img.shields.io/badge/Gitpod-Ready--to--Code-blue?logo=gitpod\"/>\\n  </a>\\n\\n  <a href=\"https://patreon.com/memcode\" title=\"Donate to Memcode project using Patreon\">\\n    <img src=\"https://img.shields.io/badge/patreon-donate-yellow.svg\"/>\\n  </a>\\n</div>\\n\\n<h1 align=\"center\">\\n  Memcode\\n</h1>\\n\\n<h2 align=\"center\">\\n  Flashcards for coders and scientists. Open-source, free for all.\\n</h2>\\n\\n<div align=\"center\">\\n  <img width=\"950px\" alt=\"Memcode Screenshot\" src=\"https://user-images.githubusercontent.com/7578559/154212696-1597a568-7a97-44d8-bda9-56cc80fcc725.png\">\\n</div>\\n\\n## Links\\n\\n**Website**: <a href=\"https://www.memcode.com\">memcode.com</a>  \\n**Patreon**: <a href=\"https://patreon.com/memcode\">patreon.com/memcode</a>   \\n**Email**:   contact@memcode.com    \\n**Twitter**: <a href=\"https://twitter.com/memcodeapp\">twitter.com/memcodeapp</a>  \\n**Alternative.to**: <a href=\"https://alternativeto.net/software/memcode/about\">alternativeto.net/software/memcode/about</a>    \\n\\n## Contributing\\n\\nFirst of all - you are very welcome to contribute, Memcode is a joint effort.   \\nFeel free to ask questions/propose features in github issues, or join our developer Slack (please write to contact@memcode.com to request access).\\n\\nNote: if you\\'d like to use online development environment, try <a href=\"https://github.com/lakesare/memcode/blob/master/Gitpod.md\">Gitpod.md</a> (might need some adjustments). The steps below are for the local setup.\\n\\n#### Create a database postgres user with a password.\\n1. Install PostgreSQL.\\n2. Go to postgres console: `psql postgres`.\\n3. Create a `postgres` user with password: `CREATE ROLE postgres WITH LOGIN PASSWORD \\'postgres\\';`.\\n4. Give the user a permission to create dbs, own all extensions, etc.: `ALTER ROLE postgres with superuser;`.\\n\\n#### Copypaste environment variables.\\n1. **Either** copy the example environment file with `cp env.example.js env.js`, and insert the required values yourself,\\n2. **Or** write to **contact@memcode.com** and I will send you a ready `env.js` file.\\nIn either case, you will need to insert your own `DB_USER` and `DB_PASSWORD` that you created in the previous step.\\n\\n#### Install the needed libraries.\\n1. Install npm.\\n2. Run `npm install`\\n\\n#### Set up the database.\\n1. Create a new development database \\'memcode\\': `make db-reset`.\\nThis will create the raw database for you - schema, a few necessary database rows, and nothing else.\\nIf you would like a bigger database to have something to work with, please write to contact@memcode.com, and I will create a development dump for you.\\n\\n#### Start code compilers and server.\\n1. Run `make all` in your terminal.\\nThis will start:\\n- `make backend-webpack` (compiles the backend code on every change)\\n- `make frontend-webpack` (compiles the frontend code on every change)\\n- `make start` (starts the node server)\\nfor you.  \\nYou can also run these separately if you wish to see the individual output.\\n2. Go to <a href=\"http://localhost:3000/\">http://localhost:3000</a>, and enjoy the development!\\n\\n\\n<br/>\\n<div align=\"center\">\\n  <img width=\"50px\" src=\"https://user-images.githubusercontent.com/7578559/154219522-280c4f96-4e3d-45e9-9beb-671b339b3f92.png\" alt=\"Memcode Logo\"/>\\n</div>\\n'},\n",
       " {'repo': 'tagtime/TagTime',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': 'To determine how you spend your time, TagTime literally randomly samples you.\\nAt random times it pops up and asks what you\\'re doing *right at that moment*.\\nYou answer with tags.\\n\\nSee\\n[messymatters.com/tagtime](http://messymatters.com/tagtime )\\nfor the whole story.\\n\\nWe\\'re currently auto-tweeting git commits: [@tagtm](http://twitter.com/tagtm ).\\n\\n# Code\\n\\nThe core Perl implementation of TagTime itself is in the following files:\\n\\n* tagtimed.pl -- the TagTime daemon\\n* launch.pl -- launches the pinger by popping up an xterm\\n* ping.pl -- prompts for the tags\\n* util.pl -- utility functions\\n* settings.pl.template -- user-specific settings\\n\\nIn addition are the following files:\\n\\n* install.py -- install script\\n* grppings.pl -- grep your TagTime log file\\n* cntpings.pl -- tally pings in your log file matching given criteria\\n\\n* tskedit.pl -- task editor / to-do list that integrates with TagTime\\n* tskproc.pl -- helper script used by tskedit.pl\\n* tasks.vim.template -- vim macros needed for the task editor\\n\\n* merge.pl -- just a stub, for fixing/merging TagTime logs\\n\\n* beeminder.pl -- sends your TagTime data to your Beeminder graph\\n* beemapi.pl -- partial Perl implementation of the Beeminder API\\n\\nThe script directory contains various scripts we\\'ve used, like for various games\\nand contests and commitment contracts and whatnot.\\nBasically, incentive schemes for getting ourselves to procrastinate less.\\nWe view TagTime as the foundation for all such lifehacks, since it\\'s a way to\\nguarantee you always have data on where your time is going.\\nIt\\'s hard to flake out on reporting to TagTime since it actively pings you.\\nYou can be perfectly passive -- just responding when prompted.\\nThat\\'s why we call it \"time-tracking for space cadets\".\\n\\nThe src directory currently contains Python code contributed by Jonathan Chang\\nfor a new back-end for TagTime. It hasn\\'t yet been integrated. Same with pyqt\\nwhich was contributed by Arthur Breitman.\\nThe src directory also contains the source for an Android app by Bethany Soule\\n(bsoule) with contributions by Michael Janssen (jamuraa).\\n\\nThanks also to Paul Fenwick, Jesse Aldridge, Kevin Lochner, and Rob Felty for\\ncontributions to the code.\\n\\n# Installation and Quick Start\\n\\n0. Clone the repository on Github\\n1. cd into your local tagtime directory\\n2. Run: python2 install.py USERNAME\\n3. Verify in settings.pl (wherever it says CHANGEME) that the install\\n   script filled in everything correctly\\n4. Make sure you have X11 (on Mac) or Cygwin (on Windows) running (not an issue\\n   on Linux)\\n5. Run: ./tagtimed.pl &\\n6. Answer the pings!\\n   (Always answer with what it caught you at right at that moment)\\n\\n# Perl Newbies\\n\\n1. Run: sudo cpan\\n2. At the cpan prompt run: upgrade (this may not actually be necessary)\\n3. For each thing that TagTime complains about, like\\n   \\'can\\'t find LWP::UserAgent\\', run: install LWP::UserAgent\\n\\n# Advanced Usage\\n\\nTagTime\\'s Task Manager is documented in the file template.tsk\\nIt\\'s for vim users only. You don\\'t need it to use TagTime.\\n\\nBasic ping-tallying:\\n\\n    ./cntpings.pl username.log  (run w/o args for options)\\n\\n    (Special tags:\\n     off = tagtime (launch.pl) didn\\'t run;\\n     afk = away from keyboard;\\n     err = you closed the window without answering the ping)\\n\\nHow to make the tagtime daemon automatically start on bootup in OSX:\\n\\n    sudo ln -s /path/to/tagtimed.pl /Library/StartupItems/tagtimed.pl\\n\\nPick a distinctive sound for your pings by setting $playsound in\\nsettings.pl.\\nSample sounds are in the sound directory.\\nNon-mac users, see README file in sound directory.\\n\\nA handy vim macro for duplicating the previous/next line\\'s tags in the tagtime log:\\n\\n    \"replace tags on this tagtime line with those from the prev/next line.\\n    \"(NB: must have timestamp in square brackets on both lines)\\n    map +    mzk0el\"vy/\\\\([\\\\\\\\|$\\\\)<cr>jd/\\\\([\\\\\\\\|$\\\\)<cr>h\"vp`zj\\n    map -    mzj0el\"vy/\\\\([\\\\\\\\|$\\\\)<cr>kd/\\\\([\\\\\\\\|$\\\\)<cr>h\"vp`zk\\n\\n\\n# Extra Features\\n\\nEditor: If you hit enter instead of answering the ping it will open up the\\neditor.\\n\\nDitto: If you enter just a double-quote character (\") it will enter whatever\\npings you entered last time. (Thanks to Paul Fenwick for implementing that.)\\n\\n# The Math\\n\\nIf your tagtime gap is g minutes then the probability of at least one ping\\nin any x minute window is 1-exp(-x/g).\\nThe window corresponding to probability p is -g\\\\*ln(1-p).\\nFor example, with g=45, there\\'s a 10% chance of getting pinged in any window\\nof duration 4 minutes 44 seconds.\\nThere\\'s a 50% chance of getting pinged within 31 minutes.\\nThere\\'s a 99% chance of a ping within 3.5 hours.\\nThe probability of waiting over 10 hours\\\\* for a ping is one in a million.\\n\\n\\\\* However, due to a [flaw in the random number generation algorithm used](http://github.com/dreeves/TagTime/issues/62#issuecomment-239705969) when this happens\\\\*\\\\*, the following ping will be more than 3 hours later.\\n\\n\\\\*\\\\* Will not occur in the 21st century.\\n\\n\\n# Beeminder Integration\\n\\n**WARNING: If you point TagTime at an existing Beeminder goal, TagTime will DELETE ALL YOUR DATA.**\\n\\nTo set up TagTime to automatically send reports to\\n[Beeminder](http://www.beeminder.com/),\\nfirst set up a goal there (either a \"Do More\" or \"Do Less\" goal).\\nCopy the url and plug it into your\\n`settings.pl` file under the Beeminder section.\\n\\nEach goal on Beeminder will track a collection of one or more tags on TagTime.\\nRegular expressions are encouraged!\\nSee `settings.pl` for more details.\\n\\nNote that TagTime sends times to Beeminder in hours.\\nHere\\'s a handy tool if you like skating the edge of a TagTime-based Beeminder goal: [tminder](http://mind.tagti.me).\\n\\n**WARNING: TagTime will replace all existing data for the goal to make it match your TagTime log. In other words, your TagTime log is the master copy and TagTime will keep Beeminder in sync with it, including deleting data that\\'s not found in your TagTime log.**\\n\\n# Android App\\n\\nThere is an Android app available [on Google\\nPlay](https://play.google.com/store/apps/details?id=bsoule.tagtime).\\nThe source and build instructions are in `src/and`.\\n\\n# Google Group\\n\\nFor discussion and questions:\\n[TagTime Google Group](https://groups.google.com/forum/?fromgroups#!forum/tagtime ).\\n'},\n",
       " {'repo': 'SpaceXLaunchBot/SpaceXLaunchBot',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '<h1 align=\"center\" style=\"font-weight: bold\">SpaceX Launch Bot</h1>\\r\\n\\r\\n<p align=\"center\">\\r\\n    <a href=\"https://top.gg/bot/411618411169447950\" >\\r\\n        <img src=\"https://top.gg/api/widget/status/411618411169447950.svg?noavatar=true\" alt=\"SpaceXLaunchBot status\" />\\r\\n    </a>\\r\\n    <a href=\"https://top.gg/bot/411618411169447950\" >\\r\\n        <img src=\"https://top.gg/api/widget/servers/411618411169447950.svg?noavatar=true\" alt=\"SpaceXLaunchBot server count\" />\\r\\n    </a>\\r\\n    <a href=\"https://top.gg/user/3204220773157502976\" >\\r\\n        <img src=\"https://top.gg/api/widget/owner/411618411169447950.svg?noavatar=true\" alt=\"SpaceXLaunchBot owner id\" />\\r\\n    </a>\\r\\n    <br/>\\r\\n    <a href=\"https://discord.com/oauth2/authorize?client_id=411618411169447950&scope=bot&permissions=2147633152\">\\r\\n        <img src=\"https://img.shields.io/badge/Discord-Bot%20Invite-blue.svg?style=flat&colorA=35383d\" alt=\"Discord Invite\"/>\\r\\n    </a>\\r\\n    <a href=\"https://discord.gg/j6vbHkYSES\">\\r\\n        <img src=\"https://img.shields.io/badge/Discord-Support%20Server%20Invite-blue.svg?style=flat&colorA=35383d\" alt=\"Discord Support Server Invite\"/>\\r\\n    </a>\\r\\n    <br/>\\r\\n    <a href=\"https://github.com/r-spacex/SpaceXLaunchBot/actions\">\\r\\n        <img src=\"https://github.com/r-spacex/SpaceXLaunchBot/workflows/CI/badge.svg\" alt=\"Github CI Build Status\"/>\\r\\n    </a>\\r\\n    <a href=\"https://github.com/psf/black\">\\r\\n        <img src=\"https://img.shields.io/badge/Code%20Style-Black-000000.svg?colorA=35383d\" alt=\"Black code formatter\"/>\\r\\n    </a>\\r\\n</p>\\r\\n\\r\\nA Discord bot for getting news, information, and notifications about upcoming SpaceX launches. Get update notifications with the latest launch information and reminders for launches that will be happening soon.\\r\\n\\r\\nSee the [official SpaceXLaunchBot website](https://spacexlaunchbot.dev/)!\\r\\n\\r\\n## Commands\\r\\n\\r\\nCommand|Description|Permissions needed\\r\\n---|---|---\\r\\n`nextlaunch`|Send the latest launch schedule message to the current channel|None\\r\\n`launch [launch number]`|Send the launch schedule message for the given launch number to the current channel|None\\r\\n`add [type] [mentions]`|Add the current channel to the notification service with the given notification type (`all`, `schedule`, or `launch`). If you chose `all` or `launch`, the second part can be a list of roles / channels / users to ping when a launch notification is sent|Admin\\r\\n`remove`|Remove the current channel from the notification service|Admin\\r\\n`info`|Send information about the bot to the current channel|None\\r\\n`help`|List these commands|None\\r\\n\\r\\n## Notifications\\r\\n\\r\\nThe `add` command allows admins to subscribe text channels to the bots notification service. This will send the subscribed channel different types of messages:\\r\\n\\r\\n- A **schedule** notification shows detailed information about the next upcoming launch. This message is sent every time the next upcoming launch has changed, e.g. if a launch date is changed or if a launch just happened so now the next upcoming launch is different. Currently changes are checked for every 60 seconds.\\r\\n\\r\\n![launch_info](images/screenshots/launch_info.png)\\r\\n\\r\\n- A **launch** notification provides useful links to things such as the livestream and press kit. This message is only sent through the notification service and will be sent 30 minutes before a launch. You can choose to have a list of mentions sent to alert users to this notification.\\r\\n\\r\\n![launch_soon](images/screenshots/launch_soon.png)\\r\\n\\r\\nIf you want to receive both types of notification you can use **all**.\\r\\n\\r\\nCurrently there is no way to update the type and/or mentions you have set for a channel. If you need to change these just call `remove` and then `add` with your new options.\\r\\n\\r\\n## New Features\\r\\n\\r\\nSee the Github [project page](https://github.com/r-spacex/SpaceXLaunchBot/projects/1) for planned updates.\\r\\n\\r\\nIf you want to request a feature, [open an issue](https://github.com/r-spacex/SpaceXLaunchBot/issues/new).\\r\\n'},\n",
       " {'repo': 'Spacelog/Spacelog',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Spacelog\\n\\nThis is the source code for [Spacelog](http://spacelog.org/), a website for experiencing space missions through radio transcripts and photography.\\n\\nWith the exception of the font and some icons (credited on the mission about page), everything outside the `missions` directory is released under the [CC-0](http://creativecommons.org/publicdomain/zero/1.0/) license. Mission images are credited in the mission\\'s `_meta` file.\\n\\nWe hope you have fun with this -- we have!\\n\\n[The Spacelog team](mail:spacelog@googlegroups.com)\\n\\n\\n\\n# Getting involved without technical knowledge\\n\\n## Correcting minor errors\\n\\nFor small errors (whether transcription errors, or something like spelling mistakes on the rest of the site), it\\'s probably easiest to just email them through to us at [spacelog@googlegroups.com](mail:spacelog@googlegroups.com).\\n\\n## Helping transcribe a new mission\\n\\nIf you [download the PDFs from NASA](http://www.jsc.nasa.gov/history/mission_trans/mission_transcripts.htm) for a mission you want to add, you\\'ll discover that you can select text in them and copy it out into a text editor, or something like OpenOffice Writer, Apple\\'s Pages or Microsoft Word. You\\'ll see lots of lines that look like this:\\n\\n    02 07 55 20 CMP I believe we\\'ve had a problem here.\\n\\nHowever some of the lines will have errors (from the small, such as `O` instead of `0`, to the large such as entire lines being completely garbled). If you go back to the PDF, you can usually quite easily figure out what was originally typed out but which the automatic OCR didn\\'t get right.\\n\\nThere are also some non-dialogue lines. These should all be indented by a single tab; the most important ones are:\\n\\n        TAPE 2/1\\n        PAGE 9\\n\\nwhich happen at the start of a new page of the PDF. In this case they mean that it\\'s the first page of the transcript of tape 2, and is page 9 of the complete transcript. We particularly need the page number so we can link back to the original typescript in the site.\\n\\nIf you clean up the text version like this, and send it through to us, we can do the rest (although it may take us a bit of time). Since some of the missions are quite long (hundreds of pages of transcript), you may want to share the load between a group. Whether you do that or decide to go it alone, it\\'d be great if you could let us know at at [spacelog@googlegroups.com](mail:spacelog@googlegroups.com) what you\\'re working on, so we can help you out, and make sure you don\\'t duplicate others\\' effort.\\n\\n### A quick note about multiple transcripts\\n\\nFor Mercury-Atlas 6, there is only one transcript available, that of the air-to-ground radio communications (John Glenn\\'s mike was hot through the entire mission). For Apollo 13, there is a second transcript from the Command Module recording, but it cuts out very early into the mission, so we didn\\'t consider it worth including.\\n\\nHowever for many other missions there are multiple transcripts. If you\\'re adding missions to Spacelog, please keep these transcripts in different files. We don\\'t yet have support to identify them distinctly, but if you move them all into one big file it\\'ll be impossible for us ever to work on that!\\n\\n# Getting involved with technical knowledge\\n\\n## Getting set up\\n\\n### Source code\\n\\nClone the repository from git:\\n\\n    $ git clone git://github.com/Spacelog/Spacelog.git\\n\\nHowever for any changes you make (fixed, new missions, or even new website features), you will want to issue a pull request to us from another [github](http://github.com/) repository). In order to do that, you\\'ll need to set up a github account, and while logged in go to [our repository there](http://github.com/Spacelog/Spacelog) and hit the \"fork\" button (top right, near the search box). This will create a copy of Spacelog under your github user; you can then grab the SSH URL (which will look like `git@github.com:<your user>/Spacelog.git`) and use for git clone, as:\\n    \\n    $ git clone <github SSH URL>\\n\\nYou can then make changes, commit them to your local copy (`git commit`), push them up to your github copy (`git push`) and finally send us a pull request (which you do via the github website). Github has some great guides to getting started with git and github linked from their homepage once you\\'re signed in, in particular [their description of forking a repository](http://help.github.com/fork-a-repo/).\\n\\n### Software to install\\n\\nNote that you may want to use Vagrant (`vagrant up`) to give yourself\\na linux VM; you shouldn\\'t then need to do anything else from this\\nsection.\\n\\n#### On macOS\\n\\nWe recommend you install [homebrew](http://brew.sh/) and then:\\n\\n```sh\\nbrew install python\\nbrew install watch\\nbrew install redis\\nbrew install xapian --with-python\\nbrew install imagemagick\\nbrew install optipng\\n```\\n\\n#### On linux\\n\\nSoftware you need, with the Debian/Ubuntu package names in parentheses.\\n\\n * python, version 2 (`python`) and pip (`python-pip`)\\n * `watch` (`procps`)\\n * redis and its python bindings (`python-redis`)\\n * Xapian and its python bindings (`python-xapian`)\\n * various python modules (run `pip install -r requirements.txt`)\\n * imagemagick and optipng, for building the stats images on the phase pages; this is optional (`imagemagick`, `optipng`)\\n\\n## Python modules\\n\\nThe easiest way to grab the python modules is to build a `virtualenv`\\nin the Spacelog checkout:\\n\\n```sh\\nvirtualenv --system-site-packages ENV\\npip install --upgrade pip\\nENV/bin/pip install -r requirements.txt\\nsource ENV/bin/activate\\n```\\n\\nFrom this point on, `python` will give you the virtual environment\\'s python.\\n\\n## Running the code\\n\\nIf you have `screen` installed (eg you will on macOS) and are using a\\nvirtualenv as above, you should be able to just run `make screen` to\\nget everything running for you. You also need to run `make reindex` to\\nload all the details of the missions. Then you can point your web\\nbrowser\\nat [http://dev.spacelog.org:8001/](http://dev.spacelog.org:8001/) and\\nthe global homepage should come up; from there you can navigate to\\nother missions, which will appear at URLs such\\nas\\n[http://apollo11.dev.spacelog.org:8000/](http://apollo11.dev.spacelog.org:8000). The\\nDNS is managed by us, and providing you\\'re online everything will just\\nwork.\\n\\n`make screen` fires up an instance of `screen`, which is an easy way\\nof running multiple programs on one terminal. Currently it leaves you\\nlooking at the development server log for the global homepage, but you\\ncan switch to a blank terminal by typing `^A 0`, ie: holding down the\\n`ctrl` key, pressing `A`, releasing `ctrl` and then pressing `0`. This\\nis a good place to run `make reindex` from.\\n\\nAll our `make` commands will take care of the virtualenv for you; if\\nyou\\'re not using one, you can use `PYTHON=python make <whatever>`\\ninstead.\\n\\n### The details\\n\\nIf you can\\'t use `make screen`, or simply if you wish to know how it\\nall fits together under the skin, then here\\'s the details. It\\'s also\\nhelpful in case you\\'re developing the code directly, since under\\ncertain circumstances the Django development server can crash, and\\nwill need restarting. Similarly if you add a new CSS file, you will\\ncurrently have to restart the appropriate devcss server.\\n\\nWe use redis for storage, so you need to have `redis-server` running\\nbefore you run `make reindex` in the checkout directory, which will\\nimport all the mission data into redis. You may also want to do `make\\nstatsporn` to build the graphs for the phases page of how much was\\nsaid at different times (and, in case we\\'ve added more graphs but\\nhaven\\'t updated this, *other things* :-).\\n\\nYou then need to have some other servers running on top of redis:\\n\\n * `make devcss` will run `sass` in watch mode, so changes to CSS files will be reflected automatically\\n * `make devcss_global` will run `sass` for the project homepage\\n * `make devserver` will run the mission-specific websites; if not using a `virtualenv`, `PYTHON=python make devserver` should do the trick\\n * `make devserver_global` will run the project homepage; if not using a `virtualenv`, `PYTHON=python make devserver_global` should do the trick\\n\\n### Hosts setup for offline use\\n\\nIf you\\'re not online, you can\\'t use our development DNS, so you\\'ll\\nneed edit `/etc/hosts` to include an alias `dev.spacelog.org`, plus\\naliases of the form `<mission>.dev.spacelog.org`, such as\\n`apollo13.dev.spacelog.org` and `mercury6.dev.spacelog.org`; these all\\nneed to point to `localhost` (or to your virtual machine, if that\\'s\\nhow you develop things). For instance, here\\'s an `/etc/hosts` entry\\nusing `localhost` (put this in addition to the `localhost` line\\nalready in there):\\n  \\n    127.0.0.1\\t\\tapollo13.dev.spacelog.org mercury6.dev.spacelog.org dev.spacelog.org\\n\\nand here\\'s one for a virtual machine (you\\'ll need to change the dotted\\nquad at the start of the line):\\n  \\n    192.168.56.101\\tapollo13.dev.spacelog.org mercury6.dev.spacelog.org dev.spacelog.org\\n\\n### Reindexing\\n\\nWhenever you edit information about a mission, or add a new one, you\\nneed to run `make reindex` again. If you get errors you may find the\\n`lognag.pl` script in `mcshred/src` useful: just give it some\\ntranscript files and it\\'ll tell you where it finds possible errors or\\nweirdnesses. (For new missions, you\\'ll probably have to add things\\ninto the valid speakers list at line 71.)\\n\\nNote that a full `make reindex` can take a while, so you can index\\njust a single mission by doing `ENV/bin/python -m backend.indexer ma6`\\nor similar (or just `python -m backend.indexer ma6` if you aren\\'t\\nusing a virtualenv.\\n\\n## External Source Images\\n\\nWe make use of external source images (which we haven\\'t created\\nourselves) in the form of:\\n\\n * .pngs of transcript PDF pages\\n * Original NASA photographs\\n\\nFor reasons of size these aren\\'t stored in git, they\\'re stored in the\\nspacelog Amazon S3 bucket (served by Cloudfront on\\nhttp://media.spacelog.org). By default, our settings point you to this\\nhost. If you want to test adding your own images, you can change the\\n`MISSIONS\\\\_IMAGE\\\\_URL` in `website/configs/settings.py` to serve them\\nlocally. File a github ticket if you need images uploaded to S3.\\n\\n## Adding a new mission\\n\\nYou\\'ll need to create a directory in `missions`. For Mercury-Redstone\\nmissions these should start `mr`, for Mercury-Atlas `ma`, for Gemini\\nthey start just `g` and for Apollo `a`. If anyone wants to do non-NASA\\nmissions, or Shuttle missions, then get in touch and we\\'ll figure out\\na naming convention.\\n\\nLook in `transcript-file-format` for a description of how we lay out\\nfiles. If you\\'re transcribing a mission we don\\'t have, you will find\\nthe example `_meta` and `TEC` files useful, since they are the main\\ntwo files you\\'ll need to create (if you\\'re going to include more than\\njust the air-to-ground transcription, you\\'ll want to put that in\\n`TEC`, the command module transcript in `CM`, and so on). If you can\\nmake them in that format (or get as close as you can), and send them\\nthrough to us along with a link to the original transcript PDFs you\\nused, we\\'ll do the rest. (If you are gifted in design, the source\\nfiles for all the artwork we\\'ve created is available, although we\\nhaven\\'t yet put it online -- yell if you need it as a basis for making\\nthings like orbital diagrams.)\\n\\n### Images\\n\\nThe mission images folder (eg `missions/a11/images`) contains a number\\nof images in standard locations and of standard sizes. This is an\\nincomplete list. There are Photoshop templates available to help with\\nconstructing some of these (talk to the mailing list to get hold of\\nthem).\\n\\n * `badge.png` (200 pixels wide, square-ish) and `badge_thumb.png` (40x40\\n   pixels): the mission patch, with a transparent background, designed\\n   for use on a dark background\\n * `avatars/` contains images for each character in the transcript, in\\n   one of the following forms:\\n     * transcript name: `F.png`, `IWO_48.png`\\n     * ground crew or similar: `capcom_generic.jpg`, `charlie_duke.png`, `nixon.png`, in black and white (ideally in shirt sleeves)\\n     * astronauts in flight: `aldrin.jpg`, `armstrong.jpg`, in black and white with a yellow filter applied (ideally in a spacesuit)\\n     * `blank_avatar_48.png` (default blank avatar)\\n\\n   See [the relevant wiki page](https://github.com/spacelog/spacelog/wiki/avatars) for some helpful pre-built avatars.\\n * `people/` contains images used on the people page, for characters in\\n   the transcript we call out, which are constructed based on their `role`.\\n   The three role groups should have consistent sizes, typically up to\\n   200 pixels wide and 150-200 pixels high (there\\'s a fair amount of\\n   flexibility here, although 190x205 for the first two and 190x155 for\\n   the others was the original design).\\n     * `astronaut`: in colour (if possible); the aim is to get them in\\n       spacesuits or flight suits, although this isn\\'t always possible\\n     * `mission-ops-title`: black and white (on the same page as\\n       astronauts; we generally include CAPCOM and Flight there as\\n       separate \"characters\" to explain these key roles, with people filling\\n       them or otherwise interesting to the mission as `mission-ops`)\\n     * `mission-ops` black and white (on another page)\\n * a number of directories for images for each act, the details of\\n   which are managed in the`_meta` file\\'s `acts` section, so can be anything,\\n   but are generally `act1.jpg` or `act1.png` etc\\n     * `banners/`: 1020x200 pixels, used as headers within the transcript\\n     * `illustration/`: 950-960 by 300-330 pixels, optional orbital\\n       diagram with spacecraft schematic (but could skip the orbital\\n       diagram eg for Apollo 9), used on the phases page\\n     * `orbital/`: 956x104 pixels, optional orbital diagram (only\\n       makes sense for moon missions), used in the expanded transcript\\n       footer\\n     * `homepage/`: 220x140 pixels, used on the mission homepage\\n       * This directory also contains a single `background.jpg`, which\\n         should be dark, and probably feathered toward the right and\\n         bottom edges. Size should be \"large\", but there are no\\n         particular dimensions or range of dimensions. Choose\\n         something that looks good.\\n\\n### Memorials\\n\\nFor missions that resulted in fatalities, we do not aim to provide a\\nregular site with a transcript. Instead, we can provide a small\\n\"memorial\" site, controlled entirely from the `_meta` file. Set the\\n`memorial` key to `true`, and the following keys will be used:\\n\\n * `name`, `subdomains`, `featured`, `incomplete` as normal\\n * `utc_launch_time` is used for ordering on the Spacelog homepage\\n * `characters` should contain only the astronauts, with:\\n   * `name`, `mission_position`, `bio`\\n   * `photo`, `photo_width` and `photo_height`\\n   * `role` of `\"astronaut\"`\\n   * optional `quote`, and optional `quote_url`\\n   * characters in memorials should not have stats\\n * `copy` with:\\n   * `title`, `upper_title`, `lower_title`, `description`, `summary`\\n   * `narrative` (main body for memorial page, doesn\\'t support HTML,\\n     should be a list of strings, each of which forms a paragraph)\\n   * `image_attributions` should contain details of the crew photo\\n     (`url`, `title`, `attrib_url`, `attrib` and `license`) unless it\\n     is public domain (which it usually will be)\\n\\nImages (eg in `missions/a1/images/`) that should be in place are:\\n\\n * `badge_thumb.png`, `badge.png` (principally for sharing)\\n * astronaut photos in `people/` (typically official NASA headshots)\\n * `homepage/crew.jpg` (ideally a photo of the entire crew in training),\\n   should be \"large\" (as `homepage/background.jpg`, but more foregroundy);\\n   on large screens it will be shown at 960px wide, and on narrower at\\n   full bleed (so on high density screens up to 1920px may be used)\\n\\n\\n### Multiple transcripts\\n\\nAs noted above in the information for non-technical folk, if you clean\\nup multiple different transcripts for a single mission (for instance\\nyou might do not only the TEC (\"technical\" ground-to-air) recording\\nbut also the CM and/or LM recordings), then please keep them in\\nseparate files rather than merging them.\\n\\n## Technical glossary\\n\\nWithin the system, there are a number of terms that describe pieces of\\nthe system but do not necessarily match what is shown on the websites.\\n\\n * TRANSCRIPT FILE -- our textual representation of the original transcript; see `transcript-file-format/TEC` for a commented example\\n * TIMESTAMP -- four colon-separated numbers that represent the GET (Ground Elapsed Time), the time since launch within the mission; the four numbers are days, hours, minutes, seconds, so ignition is 00:00:00:00; these are used in the transcript files, and also in URLs\\n * LOG LINE -- smallest linkable chunk, identified by timestamp and transcript file\\n * RANGE -- a range between two timestamps (can be the same two)\\n * LABEL -- a keyword applied to a range within a specific stream (note that labels are not currently used)\\n * META FILE -- a per-mission file called `_meta` that contains information such as glossary items, pull quotes for the homepage, and acts (see `transcript-file-format/_meta` for a commented example\\n * CHARACTER -- a speaker who appears in a transcript file; additional information about them appears in the meta file\\n * SHIFT -- a range where one \"role\" character (such as CAPCOM or the flight director) can be identified with a \"real\" character (such as Charlie Duke or Deke Slayton); ranges are defined in the meta file\\n\\nFrom this we generate a number of higher-level pieces which are used in the website.\\n\\n * ACT -- an editorially defined range that represents a segment of the mission, which may for instance reference orbital mechanics (in the websites these are referred to as phases)\\n * KEY SCENE -- an editorially defined point in the transcript where an important event or exchange starts\\n * STREAM -- a collection of related content arranged on a timeline\\n\\n## Characters\\n\\nCharacters are defined in a _meta key `characters`, which is a\\ndictionary with keys the character identifiers in the transcript and\\nvalues a further dictionary of information about that character. For\\ninstance:\\n\\n    {\\n        \"characters\": {\\n            \"P\": {\\n                \"role\": \"astronaut\",\\n                \"name\": \"Virgil Ivan Grissom\",\\n                \"short_name\": \"Gus Grissom\",\\n                \"mission_position\": \"Pilot\",\\n                \"bio\": \"A few sentence biography\",\\n                \"photo\": \"grissom.jpg\",\\n                \"photo_width\": 190,\\n                \"photo_height\": 205,\\n                \"avatar\": \"grissom.jpg\"\\n            }\\n        }\\n    }\\n\\nThis defines the character P. `bio`, `photo` (stored in the mission\\'s\\n`images/people` directory; `photo_width` / `photo_height` should be\\nset appropriately) are used on the people page.\\n\\n`role` is based on initial usage, and so can be a little confusing. It\\nshould be one of astronaut, mission-ops, mission-ops-title or other\\n(defaulting to other). Astronaut means a full-size, prominent place on\\nthe main people page (190x205 image with biography as above, and also\\nsupport for stats and a quote); mission-ops-title will get a less\\nprominent position on the main people page (190x205 with biography);\\nmission-ops go on a second page (linked as \"View Mission Control Team\"\\nfrom the main people page), where they get a 190x155 photo and brief\\nbiography.\\n\\nWhen dealing with translations, you can also set the role to any of\\nthe above with \\'-alias\\' at the end (eg astronaut-alias,\\nmission-ops-title-alias) and explicitly match the `slug` to the \"real\"\\ncharacter definition. Transcripts will show details from the alias\\n(including `short_name` and `avatar`, but will link via the explicit\\nslug to the \"real\" biography on the relevant people page, assuming\\nthere is one).\\n\\nThe people pages show the full name (the `name` key) and the mission\\nposition from the character definition. The short name is shown within\\nthe transcript, with the avatar (48x48, stored in the mission\\'s\\n`images/avatars` directory; astronauts get a yellow hue to\\ndifferentiate them from those not in space during the mission)\\nalongside.\\n\\n### Character stats and quotes\\n\\nCharacters with a role of \"astronaut\" can optionally have statistics\\nand quotes, as shown below:\\n\\n    {\\n        \"characters\": {\\n            \"CDR\": {\\n              \"role\": \"astronaut\",\\n              \"name\": \"James A. (Jim) Lovell, Jr.\",\\n              \"short_name\": \"Jim Lovell (CDR)\",\\n              \"mission_position\": \"Commander\",\\n              \"bio\": \"...\",\\n              \"photo\": \"lovell.png\",\\n              \"photo_width\": 190,\\n              \"photo_height\": 205,\\n              \"avatar\": \"jim_lovell.jpg\",\\n              \"stats\": [\\n                {\\n                    \"value\": 715,\\n                    \"text\": \"hours in space\"\\n                },\\n                {\\n                    \"value\": 4,\\n                    \"text\": \"missions\"\\n                },\\n                {\\n                    \"value\": 42,\\n                    \"text\": \"age at launch\"\\n                }\\n              ],\\n              \"quotable_log_line_id\": \"TEC:05:18:04:46\"\\n            }\\n        }\\n    }\\n\\nThe quote must be in the transcript, and is given as the transcript\\nname followed by the GET of the logline. (This means you can\\'t use\\nloglines that have multiple speakers.)\\n\\nThere should be three stats, and you will likely have to juggle things\\naround in order to make them fit the layout. We haven\\'t used stats on\\nall missions; it isn\\'t always possible to find suitable figures for\\nthe astronauts involved.\\n\\n### The shift system\\n\\nOn longer missions, generic positions such as CAPCOM or F (flight\\ndirector) are shared between several people operating in shifts. This\\nis done by having a character dictionary key of `shifts`, whose value\\nis a list of two element lists:\\n\\n    {\\n        \"characters\": {\\n            \"STONY\": {\\n              \"role\": \"other\",\\n              \"name\": \"Blockhouse Comm\",\\n              \"short_name\": \"Stony\",\\n              \"shifts\": [\\n                [ \"DEKE_SLAYTON\", \"00:00:00:00\" ]\\n              ]\\n            }\\n        }\\n    }\\n\\nThis means that the first shift is taken by the character with\\nidentifier DEKE_SLAYTON, at GET 00:00:00:00. Since identifying shifts\\nat this remove from the event isn\\'t always straightforward, there will\\noften be a third element in the list giving an annotation,\\njustification or source:\\n\\n    {\\n        \"characters\": {\\n            \"CC\": {\\n                \"role\": \"mission-ops-title\",\\n                \"name\": \"Capsule Communicator\",\\n                \"short_name\": \"CapCom\",\\n                \"bio\": \"...\",\\n                \"photo\": \"capcom.jpg\",\\n                \"photo_width\": 190,\\n                \"photo_height\": 205,\\n                \"avatar\": \"capcom_generic.png\",\\n                \"shifts\": [\\n                  [\"JOE_KERWIN\", \"-00:01:00:00\", \"strictly, only Kerwin, Brand and Lousma were taking shifts (AFAICT), however other astronauts come on as CAPCOM in the original transcript, and we use the shift mechanism to display that properly\"],\\n                  [\"JOHN_YOUNG\", \"00:04:39:01\", \"identified by PAO transcript\"],\\n                  [\"JOE_KERWIN\", \"00:04:50:45\"],\\n                  [\"VANCE_BRAND\", \"00:07:09:09\"],\\n                  [\"JACK_LOUSMA\", \"00:16:00:00\", \"uncertain (and moot) since he doesn\\'t appear in the transcript at this point\"]\\n                ]\\n            }\\n        }\\n    }\\n\\nWe also (as in the first example above, from Gus Grissom\\'s\\nMercury-Redstone 4 flight) use the shift system to \"delegate\" a\\ngeneric character (such as STONY, the callsign for an astronaut\\ncommunicator in the blockhouse during Mercury launches) to a specific\\ncharacter (in this case Deke Slayton) who served in that role for the\\nmission in question.\\n\\n## Glossary\\n\\nGlossary terms are defined in a dictionary from identifier (used in\\nthe transcripts) to an object with a number of (mostly optional)\\nattributes. Missions can bring in shared glossaries (in\\n``missions/shared/glossary/``) by having a ``_meta`` key of\\n``shared__glossaries`` containing a list of shared glossary names. They\\nalso have a per-mission glossary, in the ``_meta`` key ``glossary``.\\n\\nThe following attributes are available to a glossary entry:\\n\\n * ``type``: \"abbreviation\" or \"jargon\" (the latter is the default)\\n * ``links``: a list of objects (with ``url`` and ``caption`` attributes); currently not used\\n * ``summary``: short definition, typically the expansion for abbreviations\\n * ``description``: optional more detailed description of the term (for instance, \"Drogue\" is a glossary entry, with summary \"Drogue parachute\", and a description which explains what the drogues\\' purpose is)\\n * ``description_lang``, ``summary_lang`` and ``abbr_lang`` set the language code if not ``en-us``\\n\\nGlossary entries are referred to in transcripts (strictly in anything\\nrun through \"linkify\", which is also used for things like character\\nbiographies and quotes -- notably this includes glossary\\ndescriptions). Just do something like the following:\\n\\n```\\n[glossary:term]\\n[glossary:term|display]\\n```\\n\\nThe second form allows you to use a glossary entry while using\\ndifferent display text. (Particularly useful if dealing with\\ntranslations, since the glossary terms themselves will be in one\\nlanguage.)\\n\\nNote that unless there\\'s a description, the transcript won\\'t link to\\nthe glossary, it\\'ll just provide a title hover giving the glossary\\nitem summary.\\n\\nThe glossary page for each mission starts with the ``glossary_introduction``\\ncopy key in ``_meta``.\\n\\n## Time links\\n\\nSimilar to glossary references, parsed text can contain references to\\nother parts of the mission. This is done using a time link, of the\\nform `[time:TIME]` or `[time:TIME|display]`. The link time may be\\nabbreviated (it\\'s filled up to the four time components on the left\\nwith zeros, so if you use `[time:02:15]` the system will treat that as\\n`[time:00:00:02:15]`. The link time must use colons as separators,\\nalthough the transcript text may not: `[time:02:42:08|02 42 07]`.\\n\\n## Code layout\\n\\nThe main code is two Django projects and a python library for managing\\ntranscript files into a redis data store. There is also a directory\\nfull of per-mission information (transcript files, images and so on),\\nand some other tools directories.\\n\\n * `website/` runs the per-mission websites (Django project)\\n * `global/` runs the project global homepage (Django project)\\n * `backend/` (python library to load transcript files into redis/xappy, generate stats images, and provide an API for accessing streams and other information)\\n * `transcript-file-format/` (documentation of the transcript file format)\\n * `missions/` contains the per-mission data, particularly the transcript files and meta file, but also images and so forth\\n * `tools/` (standalone python tools)\\n * `mcshred/` (python and perl programs for dealing with OCR data from NASA PDFs)\\n * `ext/` (historical mechanism used during development because `pip` doesn\\'t work in forts)\\n'},\n",
       " {'repo': 'colbyfayock/my-final-space-characters',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Draggable Final Space Character List with React Beautiful DnD\\n\\nDemo for tutorial [How to Add Drag and Drop in React with React Beautiful DnD](https://www.youtube.com/watch?v=aYZRRyukuIw)\\n\\n🚀 See Demo: https://my-final-space-characters.netlify.app/\\n\\n📝 Article: https://www.freecodecamp.org/news/how-to-add-drag-and-drop-in-react-with-react-beautiful-dnd/\\n\\n📺 YouTube: https://www.youtube.com/watch?v=aYZRRyukuIw\\n\\n## More tutorials and walkthroughs\\n\\n🐦 [Follow me on Twitter](https://twitter.com/colbyfayock)\\n\\n📺 [Subscribe on YouTube](https://www.youtube.com/colbyfayock)\\n\\n✉️ [Sign Up for My Newsletter](https://colbyfayock.com/newsletter)\\n'},\n",
       " {'repo': 'SummitKwan/transparent_latent_gan',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# TL-GAN: transparent latent-space GAN\\n\\nThis is the repository of my three-week project: \"**Draw as you can tell: controlled image synthesis and edit using TL-GAN**\"\\n\\n## Resource lists:\\n\\n- **Blog post** expaining the motivation, architecture and results is posted on [this Medium link](https://medium.com/p/d170b1b59255)\\n- **Slides presentation** explaining the core ideas of this project are available at [this Google Drive link](https://docs.google.com/presentation/d/1OpcYLBVpUF1L-wwPHu_CyKjXqXD0oRwBoGP2peSCrSA/edit#slide=id.p1)\\n- A video presentation of this project will be available soon on YouTube\\n- An **interactive demo** can be found in this Kaggle notebook: [https://www.kaggle.com/summitkwan/tl-gan-demo](https://www.kaggle.com/summitkwan/tl-gan-demo), have fun playing with this model!\\n\\n![Alt text](./static/online_demo_run_fast_01.gif?raw=true \"Title\")\\n\\n[A high quaility video of the above GIF on YouTube](https://www.youtube.com/watch?v=O1by05eX424)\\n\\n\\n## Core ideas\\n\\n- This project provides a novel method to control the generation process of a unsupervisedly-trained generative model like GAN (generative adversarial network).  \\n- GANs can generate random photo-realistic images from random noise vectors in the latent space (see stunning examples of the Nvidia\\'s [PG-GAN](https://github.com/tkarras/progressive_growing_of_gans)), but we can no control over the features of the generated images.\\n- Knowing that the images are determined by the noise vector in the latent space, if we can understand the latent space, we can control our generation process.\\n- For a already well-trained GAN generator, I made its latent space *transparent* by discovering feature axes in it.  When a vector moves along a feature axis in the latent space, the corresponding image morphs along that feature, which enables controlled synthesis and edit.\\n- This is achieved by leveraging a coupled feature extractor network (a CNN here in this demo, but can be any other CV techniques), which enables us to find correlation between noise vectors and image features.\\n- Advantages of this method over conditional GAN and AC-GAN:\\n    - Efficiency: To add a new controller of the generator, you do not have to re-train the GAN model, thus it only takes  <1h to add 40 knobs with out methods.\\n    - Flexibility: You could use different feature extractors trained on different dataset and add knobs to the well-trained GAN\\n\\n\\n## 1. Instructions on the online demo\\n\\n#### 1.1 Why hosting the model on Kaggle\\n\\nI host the demo as a Kaggle notebook instead of a more convenient web app due to cost considerations.\\n\\nKaggle generously provides kernels with GPUs for Free! Alternatively, a web app with a backend running on an AWS GPU instance costs ~$600 per month.  Thanks to Kaggle that makes it possible for everyone to play with the model without downloading code/data to your local machine!\\n\\n#### 1.2 To use the demo\\n\\nOpen this link from your web browser: https://www.kaggle.com/summitkwan/tl-gan-demo\\n\\n1. Make sure you have a Kaggle account. If not, please register one (this can be done in seconds by linking to your Google or Facebook account). To have a Kaggle account is actually very rewarding, since allows you to participate numerous  data science challenges and join the knowledgeable and friendly community.\\n2. Fork the current notebook\\n3. run the notebook by pressing the double right arrow button at the bottom left of the web page. If something does not work right, try to restart the kernel by pressing the circular-arrow button on the bottom right and rerun the notebook\\n4. Go to the bottom of the notebook and play with the image interactively\\n5. You are all set, play with the model:\\n    - Press the “-/+“ to control every feature\\n    - Toggle the name of feature to lock one particular feature. e.g. lock “Male” when playing with “Beard\"\\n\\n## 2. Instructions on running the code on your machine\\n\\nTested on Nvidia K80 GPU with CUDA 9.0, with Anaconda Python 3.6\\n\\n### 2.1 Set up the code and environment\\n\\n1. Clone this repository\\n2. `cd` to the root directory of the project (the folder containing the `README.md`)\\n3. Install dependencies by running `pip install -r requirements.txt` in terminal.  You can use virtual environment in order not to modify your current python environment.\\n\\n### 2.2 Use the trained model on your machine\\n\\n1. Manually download the pre-trained pg-GAN model (provided by Nvidia), the trained feature extractor network, and the discovered feature axis from [my personal dropbox link](https://www.dropbox.com/sh/y1ryg8iq1erfcsr/AAB--PO5qAapwp8ILcgxE2I6a?dl=0)\\n2. Decompress the downloaded files and put it in project directory as the following format\\n\\n    ```text\\n    root(d):\\n      asset_model(d):\\n        karras2018iclr-celebahq-1024x1024.pkl   # pretrained GAN from Nvidia\\n        cnn_face_attr_celeba(d):\\n          model_20180927_032934.h5              # trained feature extractor network\\n      asset_results(d):\\n        pg_gan_celeba_feature_direction_40(d):\\n          feature_direction_20181002_044444.pkl # feature axes\\n    ```\\n\\n3. Run the interactive demo by first enter interactive python shell from terminal (make sure you are at the project root directory), and then run the commands in python\\n    ```python\\n    exec(open(\\'./src/tl_gan/script_generation_interactive.py\\').read())\\n    ```\\n    \\n    Alternatively, you can run the interactive demo from the Jupyter Notebook at `./src/notebooks/tl_gan_ipywidgets_gui.ipynb`\\n    \\n    \\n4. A interactive GUI interface will pop up and play with the model\\n\\n### 2.3 Instructions on training the model on your own\\n\\n1. Download celebA dataset `python ./src/ingestion/process_celeba.py celebA`\\n2. to be continued...\\n\\n## 3. Project structure\\n\\n- **src** : Put all source code for production within structured directory\\n- **data** : Include example a small amount of data in the Github repository so tests can be run to validate installatio\\n- **static** : Any images or content to include in the README or web framework if part of the pipeline\\n- to be continueed\\n'},\n",
       " {'repo': 'aimagelab/novelty-detection',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Latent Space Autoregression for Novelty Detection\\n\\nThis repository contains Pytorch code to replicate experiments in the CVPR19 paper \"Latent Space Autoregression for Novelty Detection\".\\n\\nPlease cite with the following BibTeX:\\n```\\n@inproceedings{abati2019latent,\\n  title={{Latent Space Autoregression for Novelty Detection}},\\n  author={Abati, Davide and Porrello, Angelo and Calderara, Simone and Cucchiara, Rita},\\n  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision and Pattern Recognition},\\n  year={2019}\\n}\\n```\\n\\n![sample results](images/model.png)\\n\\nSpecifically, performs:\\n* one class classification on MNIST.\\n* one class classification on CIFAR-10.\\n* video anomaly detection on UCSD Ped2.\\n* video anomaly detection on ShanghaiTech.\\n\\n### 0 - Clone this repo\\nFirst things first, clone this repository locally via git.\\n```\\ngit clone https://github.com/cvpr19-858/novelty-detection.git\\ncd novelty-detection\\n```\\n\\n### 1 - Environment\\nThis code runs on Python 3.6.\\nThe easiest way to set up the environment is via `pip` and the file `requirements.txt`:\\n```\\npip install -r requirements.txt\\n```\\n\\n### 2 - Datasets\\nMNIST and CIFAR-10 will be downloaded for you by torchvision. \\n\\nYou still need to download [UCSD Ped](http://www.svcl.ucsd.edu/projects/anomaly/UCSD_Anomaly_Dataset.tar.gz) and \\n[ShanghaiTech](https://onedrive.live.com/?authkey=%21AMqh2fTSemfrokE&cid=3705E349C336415F&id=3705E349C336415F%2172436&parId=3705E349C336415F%215109&o=OneUp). After download, please unpack them into the `data` folder as follows\\n\\n```\\ntar -xzvf <path-to-UCSD_Anomaly_Dataset.tar.gz> -C data\\ntar -xzvf <path-to-shanghaitech.tar.gz> -C data\\n```\\n\\n### 3 - Model checkpoints\\nCheckpoints for all trained models are available [here](https://ailb-web.ing.unimore.it/publicfiles/drive/lsa-novelty-detection/checkpoints.tar.gz).\\n\\nPlease untar them into the `checkpoints` folder as follows:\\n```\\ntar -xzvf <path-to-tar.gz> -C checkpoints\\n```\\n\\n### 4 - Run!\\nOnce your setup is complete, running tests is as simple as running `test.py`.\\n\\nUsage:\\n\\n```\\nusage: test.py [-h]\\n\\npositional arguments:\\n              The name of the dataset to perform tests on.Choose among\\n              `mnist`, `cifar10`, `ucsd-ped2`, `shanghaitech`\\n\\noptional arguments:\\n  -h, --help  show this help message and exit\\n```\\n\\nExample:\\n```\\npython test.py ucsd-ped2\\n```\\n'},\n",
       " {'repo': 'colorjs/color-space',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Color-space [![test](https://github.com/colorjs/color-space/actions/workflows/test.yml/badge.svg)](https://github.com/colorjs/color-space/actions/workflows/test.yml) [![stable](https://img.shields.io/badge/stability-stable-brightgreen.svg)](http://github.com/badges/stability-badges) [![npm](https://img.shields.io/npm/v/color-space)](https://npmjs.org/color-space) [![size](https://img.shields.io/bundlephobia/minzip/color-space/latest)](https://bundlephobia.com/package/color-space)\\n\\n<img src=\"https://raw.githubusercontent.com/colorjs/color-space/gh-pages/logo.png\" width=\"100%\" height=\"150\"/>\\n\\nCollection of color spaces conversions & data.\\n\\n[Demo](http://colorjs.github.io/color-space).\\n\\n\\n## Usage\\n\\n```js\\nimport space from \\'color-space\\';\\n\\n//convert lab to lch\\nvar result = space.lab.lch([80,50,60]);\\n```\\n\\nSpaces can be imported separately:\\n\\n```js\\nimport rgb from \\'color-space/rgb.js\\';\\nimport hsl from \\'color-space/hsl.js\\';\\n\\n//convert rgb to hsl\\nrgb.hsl([200,230,100]);\\n```\\n<!--\\nNew space can be registered as:\\n```js\\nimport space, {register} from \\'color-space\\';\\n\\nregister(spaceDefiniton)\\n``` -->\\n\\n## API\\n\\n```js\\n<fromSpace>.<toSpace>(array);\\n<space>.name //space name\\n<space>.min //channel minimums\\n<space>.max //channel maximums\\n<space>.channel //channel names\\n<space>.alias //alias space names\\n```\\n\\n## Spaces\\n\\n* [x] [RGB](https://en.wikipedia.org/wiki/CIE_1931_color_space#CIE_RGB_colour_space) — additive color model based on red, green and blue primary colors.\\n* [x] [HSL](https://en.wikipedia.org/wiki/HSL_and_HSV) — cylindrical-coordinates representation of RGB.\\n* [x] [HSV, HSB](https://en.wikipedia.org/wiki/HSL_and_HSV)\\n* [x] [HWB](http://dev.w3.org/csswg/css-color/#the-hwb-notation)\\n* [x] [HSI](https://en.wikipedia.org/wiki/HSL_and_HSV) — used for computer vision due to better separation of shapes in an image, comparing to HSL/HSB.\\n* [x] [CMYK](https://en.wikipedia.org/wiki/CMYK_color_model)\\n* [x] [CMY](https://en.wikipedia.org/wiki/CMYK_color_model)\\n* [x] [XYZ](http://en.wikipedia.org/wiki/CIE_1931_color_space)\\n* [x] [XYY (YXY)](https://en.wikipedia.org/wiki/CIE_1931_color_space#CIE_xy_chromaticity_diagram_and_the_CIE_xyY_color_space)\\n* [x] [LAB](http://en.wikipedia.org/wiki/Lab_color_space)\\n* [x] [LCH<sub>ab</sub>](https://en.wikipedia.org/wiki/Lab_color_space#Cylindrical_representation:_CIELCh_or_CIEHLC)\\n* [x] [LUV](http://en.wikipedia.org/wiki/CIELUV)\\n* [x] [LCH<sub>uv</sub>](http://en.wikipedia.org/wiki/CIELUV#Cylindrical_representation)\\n* [x] [HSL<sub>uv</sub>](http://www.hsluv.org/)\\n* [x] [HPL<sub>uv</sub>](http://www.hsluv.org/)\\n* [x] [LAB<sub>Hunter</sub>](http://en.wikipedia.org/wiki/Lab_color_space#Hunter_Lab)\\n* [x] [YUV](https://en.wikipedia.org/?title=YUV)\\n* [x] [YIQ](https://en.wikipedia.org/?title=YIQ)\\n* [x] [YC<sub>g</sub>C<sub>o</sub>](https://en.wikipedia.org/wiki/YCgCo)\\n* [x] [YD<sub>b</sub>D<sub>r</sub>](https://en.wikipedia.org/wiki/YDbDr)\\n* [x] [YP<sub>b</sub>P<sub>r</sub>](https://en.wikipedia.org/wiki/YPbPr)\\n* [x] [YC<sub>b</sub>C<sub>r</sub>](https://en.wikipedia.org/wiki/YCbCr)\\n* [x] [Y<sub>c</sub>C<sub>bc</sub>C<sub>rc</sub>](https://en.wikipedia.org/wiki/YCbCr#ITU-R_BT.2020_conversion)\\n* [x] [JPEG](https://en.wikipedia.org/wiki/YCbCr#JPEG_conversion)\\n* [x] [XvYCC](https://en.wikipedia.org/wiki/XvYCC)\\n* [x] [UCS](https://en.wikipedia.org/wiki/CIE_1960_color_space)\\n* [x] [UVW](https://en.wikipedia.org/wiki/CIE_1964_color_space)\\n* [ ] [Munsell](https://en.wikipedia.org/wiki/Munsell_color_system)\\n* [ ] [NCS](https://en.wikipedia.org/wiki/Natural_Color_System)\\n* [ ] [PMS](https://en.wikipedia.org/wiki/Pantone)\\n* [ ] [RAL](https://en.wikipedia.org/wiki/RAL_colour_standard)\\n* [ ] [TSL](https://en.wikipedia.org/wiki/TSL_color_space)\\n* [ ] [RG](https://en.wikipedia.org/wiki/RG_color_space)\\n* [ ] [RGK](https://en.wikipedia.org/wiki/RG_color_space)\\n* [x] [Coloroid](https://en.wikipedia.org/wiki/Coloroid) — color space for architects and visual constructors, Hungarian Standard MSZ 7300 since 2000.\\n* [ ] [OSA-UCS](https://en.wikipedia.org/wiki/OSA-UCS) — accurately reprsenting uniform color differences, developed by the Optical Society of America’s Committee on Uniform Color Scales.\\n* [ ] [HKS](https://en.wikipedia.org/wiki/HKS_(colour_system))\\n* [x] [LMS](http://en.wikipedia.org/wiki/LMS_color_space) — represents sensitivity of the human eye to Long, Medium and Short wavelengths.\\n* [x] [Cubehelix](https://www.mrao.cam.ac.uk/~dag/CUBEHELIX/) — colormaps for data visualization.\\n* [ ] [Gray](http://dev.w3.org/csswg/css-color/#grays)\\n* [ ] [CIECAM02](https://en.wikipedia.org/wiki/CIECAM02)\\n* [ ] [US Federal Standard 595](https://en.wikipedia.org/wiki/Federal_Standard_595)\\n* [ ] [Toyo](http://mytoyocolor.com/)\\n* [ ] [PhotoYCC](http://www5.informatik.tu-muenchen.de/lehre/vorlesungen/graphik/info/csc/COL_34.htm)\\n* [x] [HCG](https://github.com/acterhd/hcg-legacy)\\n* [ ] [HCL](http://www.chilliant.com/rgb2hsv.html)\\n* [x] [HSP](http://alienryderflex.com/hsp.html)\\n* [ ] [HCY](http://chilliant.blogspot.ca/2012/08/rgbhcy-in-hlsl.html)\\n* [x] [YES](http://www.atlantis-press.com/php/download_paper.php?id=198) — computationally effective color space for face recognition.\\n* [ ] [British Standard Colour](http://www.britishstandardcolour.com/)\\n* [ ] [RG chromacity](https://en.wikipedia.org/wiki/Rg_chromaticity)\\n* [ ] [CIE DSH](https://en.wikipedia.org/wiki/Rg_chromaticity)\\n* [ ] [HSM](http://seer.ufrgs.br/rita/article/viewFile/rita_v16_n2_p141/7428)\\n\\n## Contribute\\n\\nPlease fork, add color space with basic _conversions_ to/from XYZ or RGB and _tests_.\\nThe goal of the project is the most complete set of color spaces with minimal uniform API.\\n\\n\\n## Credits\\n\\nThanks to all scientists, who devoted their time to color research and conveyed their knowledge, for now we can use their formulas and code.\\n\\n\\n## Alternatives\\n\\n* [color-convert](https://github.com/harthur/color-convert)\\n* [chromatist](https://github.com/jrus/chromatist)\\n* [spectra](https://github.com/avp/spectra)\\n* [colorspaces.js](https://github.com/boronine/colorspaces.js)\\n\\n## See also\\n\\n* [color-api](https://github.com/LeaVerou/color-api) - color API proposal by Lea Verou\\n'},\n",
       " {'repo': '0x727/Space_view',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '\\n![logo](./doc/images/logo.png)\\n\\n# Space_view\\n\\n郑重声明：文中所涉及的技术、思路和工具仅供以安全为目的的学习交流使用，任何人不得将其用于非法用途以及盈利等目的，否则后果自行承担。\\n\\n感谢[lhc0229](https://github.com/lhc0229)、[倾旋](https://github.com/Rvn0xsy)和[alex9968](https://github.com/alex9968)的答疑解惑，以及lhc0229对这个项目前端的支持。\\n\\n\\n\\n| 类别 | 说明 |\\n| ---- | --- |\\n| 作者 | [0cat](https://github.com/0cat-r) | \\n| 团队 | [0x727](https://github.com/0x727) 未来一段时间将陆续开源工具 |\\n| 定位 | 直观了解网站资产(通过Hunter或者Fofa),自动化,交互方便，美观|\\n| 语言 | js.... |\\n| 功能 | 直观展示网站资产,资产有及时性,浮窗化交互，可自动跳转.... | \\n\\n\\n\\n\\n## 什么是 【Space_view】 ?\\n\\nSpace_view 是一款通过Hunter(鹰图平台)或者Fofa资产展示的浏览器油猴插件脚本。\\n\\n建议fofa和hunter一起装，左fofa，右hunter！\\n\\n## Why 【Space_view】 ?\\n\\n网络安全从业者需要对某一些授权的网站进行快速的资产了解。\\n\\nFofa和Hunter平台各具优势,通过这个油猴插件可以快速直观的了解网站的资产情况。\\n\\n资产的准确度：\\n\\n>采用“==”精准匹配\\n\\n资产的时效性：\\n\\n>Hunter是搜索当前站点在平台上距离当前时间前6个月的资产情况(本来是三个月 但是为了数据的隐藏价值 所以改成6个月)，以及包括对闰年，某些月份是30天的判断。\\n\\n>Fofa是默认搜索当前站点在平台上距离当前时间前一年的资产情况。\\n\\n资产的直观性：\\n\\n>title port 等等 包括去重 (Fofa由于平台本身api的问题 isp和org信息暂时无法查出 所以这块数据是空的)\\n\\n\\n交互的友好型：\\n\\n>想看的时候点击右上角就出现小模块，点击就展开显示数据详情，不看的时候就收回，Hunter固定在页面右上角，Fofa固定在页面左上角。\\n\\n>url处可以点击直接新建标签页跳转。\\n\\n查询的速度：\\n\\n>通过api查询的速度远超某些浏览器插件的速度。\\n\\n安装的快捷：\\n\\n>本项目已经公开分享到greasyfork,安装完油猴插件之后，直接搜索即可安装。\\n          可自动推送更新。\\n\\n\\n## 快速开始体验\\n\\n### 1. 安装油猴\\n\\n首先浏览器需要安装油猴插件，下面链接写的而很详细。\\nhttps://zhuanlan.zhihu.com/p/128453110\\n\\n\\n### 2. 获取脚本并安装\\n\\n方式一：可直接浏览器搜索然后点击安装即可\\n\\nHunter安装：\\n\\nhttps://greasyfork.org/zh-CN/scripts/440243-hunter-view\\n\\nFofa安装(同理)\\n\\nhttps://greasyfork.org/zh-CN/scripts/440382-fofa-view\\n\\n![](./doc/images/huoqu.png)\\n\\n方式二：你也可以在greasyfork搜索Hunter view或者FOFA view (如果搜索不到，点击显示所有语言的结果)\\n\\n然后直接安装\\n\\n安装完成之后记得在管理面板给它启用\\n![](./doc/images/dakai.png)\\n\\n## 使用方法\\n\\n1. 登录\\n\\n# !!!第一次用hunter api的同学 记得先重置（刷新）下自己的key 再去登录\\n\\n（很多朋友反映第一次使用hunter key的话查不出数据，重置下key就好了  平台的问题）\\n\\n(以下使用方法 FOFA view同理 不做赘述)\\n\\n打开任意一个网站，点击登录\\n\\n![](./doc/images/denglu.png)\\n\\n输入Hunter平台的手机号和key值\\n\\n![](./doc/images/shouji.png)\\n![](./doc/images/key.png)\\n\\nkey值在hunter平台的个人中心有展示。\\n\\n登录一次之后即可 其他网站不需要重复登录。\\n\\n2. 使用演示\\n\\n第一次使用需要给权限，这里选总是允许。\\n\\n![](./doc/images/yunxu.png)\\n\\n然后刷新网站，加载脚本\\n\\n之后点击 Hunter或者Fofa 识别结果\\n\\n![](./doc/images/shibie.png)\\n\\n这个时候网站右上角会出现小按钮\\n\\n![](./doc/images/anniu.png)\\n\\n点开按钮 展示数据\\n\\nHunter展示：\\n\\n![](./doc/images/zhanshi.png)\\n\\nFofa展示\\n\\n![](./doc/images/fofa.png)\\n\\n展示界面的url 可以直接点击会新建标签页跳转到指定链接。\\n\\n并且返回来的时候 标签不会收缩，方便继续阅读，想收缩的时候，点击收缩即可。\\n\\n\\n3. 更新！！\\n\\n# 更新得强调下\\n\\n手动更新：\\n>Hunter view点击这个链接 会提示 https://greasyfork.org/zh-CN/scripts/440243-hunter-view 是否需要更新。\\n\\n>Fofa view点击这个链接 会提示 https://greasyfork.org/zh-CN/scripts/440382-fofa-view 是否需要更新。\\n\\n\\n自动更新（推荐）：打开油猴找管理面板 找到hunter_view(Fofa同理)\\n\\n![](./doc/images/gengxin1.png)\\n![](./doc/images/gengxin.png)\\n\\n勾选自动更新！\\n\\n\\n\\n## 注意事项\\n1. Hunter view默认收集的是前六个月的50条资产，（扣除的积分情况是根据查询的数据来定的，一般情况扣不了多少积分）\\n\\n2. Hunter view 每次使用消耗个位数的积分，当然有黑名单机制，你可以点击这里 这样就不会去查询这个网站，建议添加*.baidu.com 等等\\n\\n![](./doc/images/paichu.png)\\n\\n3. Fofa view 搜集的是一年内的50条数据，fofa原本的浏览器插件时搜集全部资产，为了数据的时效性，这里油猴脚本查的是一年内。（所以数据不同不必奇怪）\\n\\n4. ！！！记得做好更新措施！！！很多新功能得更新！！！！\\n\\n\\n\\n\\n\\n## 为 【Space_view】 做贡献\\n\\n【Space_view】 是一个免费且开源的项目，我们欢迎任何人为其开发和进步贡献力量。\\n\\n- 在使用过程中出现任何问题，可以通过 issues 来反馈。\\n- Bug 的修复可以直接提交 Pull Request 到 dev 分支。\\n- 如果是增加新的功能特性，请先创建一个 issue 并做简单描述以及大致的实现方法，提议被采纳后，就可以创建一个实现新特性的 Pull Request。\\n- 欢迎对说明文档做出改善，帮助更多的人使用 【Space_view】\\n- 贡献代码请提交 PR 至 dev 分支，master 分支仅用于发布稳定可用版本。\\n- 如果你有任何其他方面的问题或合作，欢迎发送邮件至 0x727Team@gmail.com或者2034009618@qq.com本人邮箱。\\n\\n> 提醒：和项目相关的问题最好在 issues 中反馈，这样方便其他有类似问题的人可以快速查找解决方法，并且也避免了我们重复回答一些问题。\\n\\n## FOFA共创者计划\\nSpace_view已加入FOFA[共创者计划](https://fofa.info/development)，感谢FOFA提供的账号支持。\\n\\n![](./doc/images/fofa.PNG)\\n'},\n",
       " {'repo': 'oculus-samples/Unity-SharedSpaces',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '![Showcase Banner](./Media/banner.png \"SharedSpaces\")\\n\\n# SharedSpaces\\n\\nSharedSpaces was built by the VR Developer Tools team to demonstrate how you can quickly get people together in VR using the Oculus Social Platform APIs.  This version was built for the Unity engine using [Photon Realtime](https://github.com/Unity-Technologies/multiplayer-community-contributions/tree/main/Transports/com.community.netcode.transport.photon-realtime) as the transport layer and [Unity Netcode for GameObjects](https://github.com/Unity-Technologies/com.unity.netcode.gameobjects). Other versions are available, in particular one built for the [Unreal Engine](https://github.com/oculus-samples/Unreal-SharedSpaces).\\n\\nThis codebase is available both as a reference and as a template for multiplayer VR games. The [Oculus License](LICENSE) applies to the SDK and supporting material. The MIT License applies to only certain, clearly marked documents. If an individual file does not indicate which license it is subject to, then the Oculus License applies.\\n\\nSee the [CONTRIBUTING](CONTRIBUTING.md) file for how to help out.\\n\\n## Getting started\\n\\nFirst, ensure you have Git LFS installed by running this command:\\n```sh\\ngit lfs install\\n```\\n\\nThen, clone this repo using the \"Code\" button above, or this command:\\n```sh\\ngit clone https://github.com/oculus-samples/Unity-SharedSpaces.git\\n```\\n\\nTo run the showcase, open the project folder in *Unity 2020.3.15f1* or newer. Load the [Assets/SharedSpaces/Scenes/Startup](Assets/SharedSpaces/Scenes/Startup.unity) scene.\\n\\nAfter loading the scene, you may encounter this pop-up:\\n\\n<div style=\"text-align: center; padding: 10pt;\"><img src=\"./Media/tmp_essentials.png\" width=\"650\"></div>\\n\\nClick \"Import TMP Essentials\" to import the necessary TextMesh Pro assets.\\n\\n## Setting up Photon\\n\\nTo get the sample working, you will need to configure the NetDriver with your own Photon account. Their base plan is free.\\n- Visit [photonengine.com](https://www.photonengine.com) and [create an account](https://doc.photonengine.com/en-us/realtime/current/getting-started/obtain-your-app-id)\\n- From your Photon dashboard, click “Create A New App”\\n- Fill out the form making sure to set type to “Photon Realtime”. Then click Create.\\n\\nYour new app will now show on your Photon dashboard. Click the App ID to reveal the full string and copy the value.\\n\\nPaste your App ID in [Assets/Photon/Resources/PhotonAppSettings](Assets/Photon/Resources/PhotonAppSettings.asset).\\n\\nThe Photon Realtime transport should now work. You can check the dashboard in your Photon account to verify there is network traffic.\\n\\n## Where are the Oculus and Photon packages?\\n\\nIn order to keep the project organized, the [Oculus Integration](https://assetstore.unity.com/packages/tools/integration/oculus-integration-82022) and [Photon Voice 2](https://assetstore.unity.com/packages/tools/audio/photon-voice-2-130518) packages are stored in the [Packages](./Packages) folder. To update them, import their updated Asset Store packages, then copy them into their respective `Packages` folders.\\n\\nThe *Oculus Integration* package is released under the *[Oculus SDK License Agreement](./Packages/Oculus/LICENSE.txt)*.\\n\\nThe *Photon Voice 2* package is released under the *[License Agreement for Exit Games Photon](./Packages/Photon/Photon/license.txt)*.\\n\\nAlso, the Photon Realtime package is referenced in [Packages/manifest.json](./Packages/manifest.json) as `com.mlapi.contrib.transport.photon-realtime`.\\n\\n***\\n\\n<div style=\"width: 60%; padding: 10pt;\">\\n<table>\\n<tr style=\"background-color:#EEEEEE;\">\\n<td>\\nA. <a href=\"#A\">Overview of SharedSpaces</a><br/>\\nB. <a href=\"#B\">SharedSpaces in Action</a><br/>\\nC. <a href=\"#C\">Oculus Application Configuration</a><br/>\\n&nbsp;&nbsp;&nbsp;1. <a href=\"#C1\">Application Identifier</a><br/>\\n&nbsp;&nbsp;&nbsp;2. <a href=\"#C2\">Destinations</a><br/>\\n</td>\\n</tr>\\n</table>\\n</div>\\n\\n# A. <a id=\"A\">Overview of SharedSpaces</a>\\n\\n<div style=\"margin: auto; width: 60%; padding: 10pt;\">\\n<table>\\n<tr style=\"background-color:#FFEEEE;\">\\n\\t<td style=\"border:0px;\"><b>Oculus</b></td>\\n\\t<td style=\"border:0px;\">Group presence with <i>destination</i>, <i>lobby</i> and <i>match</i> ids.</td>\\n</tr>\\n<tr style=\"background-color:#EEFFEE;\">\\n\\t<td style=\"border:0px;\"><b>Photon Realtime</b></td>\\n\\t<td style=\"border:0px;\">Transport via a <i>room</i> named after the <i>lobby</i> or <i>match</i> id.</td>\\n</tr>\\n<tr style=\"background-color:#EEEEFF;\">\\n\\t<td style=\"border:0px;\"><b>Netcode for GameObjects</b></td>\\n\\t<td style=\"border:0px;\">Replication between <i>room members</i> with <i>the master client</i> as host.</td>\\n</tr>\\n</table>\\n</div>\\n\\nSharedSpaces networking is divided into three layers.  The Oculus layer provides presence information needed to find and connect with friends.  The Photon layer provides the transport layer for sending messages to other players.  And the Netcode for GameObjects layer handles the replication of game objects.\\n\\nIn this overview we will explore each of these layers and show how we connected them together to make a simple multiplayer application which allows people to connect and play together, without the need for a dedicated server.\\n\\n## *A Private Lobby Connected to Rooms*\\n<div style=\"text-align: center; padding: 10pt;\"><img src=\"./Media/layout.png\" align=\"middle\" width=\"600\"></div>\\n\\nSharedSpaces is made of a few connected levels, known as destinations.  In the center is your personal lobby with doors leading to the surrounding matches.  The matches on the left are private and are reachable from your own lobby only.  The match on the right is public, reachable from any lobby.\\n\\n## *Social Layer - Destination, Lobby & Match Session IDs*\\n\\n<div style=\"text-align: center; padding: 10pt;\"><img src=\"./Media/presence.png\" align=\"middle\" width=\"750\"></div>\\n\\nWe use this layout as a direct representation of the new group presence apis.  To get you to a SharedSpaces destination, we first set your destination and a pair of session identifiers in your group presence: one for your lobby session id, which should not change very often, and one for your match session id, only set when you join a match.\\n\\nThe destinations are specific areas of your application that are defined on the [Oculus dashboard](https://developer.oculus.com/manage) under **Platform Services > Destinations**. The lobby session id represents a tight group of people that want to stay together between games and possibly play as part of the same team during matches.  The match session id is shared by people currently playing a match together, whether they are on the same team or not.\\n\\n<div style=\"text-align: center; padding: 10pt;\"><img src=\"./Media/invitation_to_lobby.png\" align=\"middle\" width=\"600\"></div>\\n\\nWhen you first launch SharedSpaces, you start in your own private lobby for which we create a unique id. To form a group to be with before and after matches, you invite people to share your lobby.  If they accept the invitation, their lobby session id will be updated to be the same as yours, and whenever you will be in  the lobby at the same time, you will be together in the same space.\\n\\nYou can think of the lobby as the base camp for your group.  Different groups always go back to their respective lobbies after matches.\\n\\n<div style=\"text-align: center; padding: 10pt;\"><img src=\"./Media/private_room.png\" align=\"middle\" width=\"700\"></div>\\n\\nMembers of your group are free to travel at any time between your lobby and their private matches. This only affects the match session ids of their group presence.\\n\\n<div style=\"text-align: center; padding: 10pt;\"><img src=\"./Media/invitation_to_match.png\" align=\"middle\" width=\"700\"></div>\\n\\nYou can also grant access to your private match to anyone.  You invite them from that match, and they join you when they accept the invitation.  In SharedSpaces, accepting an invitation to a match only affects your match session id, not your lobby session id.  \\n\\n<div style=\"text-align: center; padding: 10pt;\"><img src=\"./Media/respective_lobbies.png\" align=\"middle\" width=\"650\"></div>\\n\\nAs a consequence, when they leave the match through the lobby door, users effectively go back to their separate lobbies if they are not members of the same group.\\n\\n<div style=\"text-align: center; padding: 10pt;\"><img src=\"./Media/public_room.png\" align=\"middle\" width=\"650\"></div>\\n\\nSharedSpaces also has the purple room to represent a public match that is reachable from all lobbies. Again, anybody is free to go from their lobby to the purple room at any time, and it only affects their match session id.  It is a space where you can meet people from outside your group without a prior invitation.\\n\\n## *Transport Layer - Photon Rooms*\\n\\nTo connect users, Photon has the concept of room.  People in the same match or lobby instance will be in the same Photon room in order for data to flow between them.  The transport layer is responsible for routing packets between your users who are most likely behind network firewalls.\\n\\n<div style=\"text-align: center; padding: 10pt;\"><img src=\"./Media/session_to_room.png\" align=\"middle\" width=\"650\"></div>\\n\\nPhoton rooms have *unique names*.  The name of the room that we will use comes directly from the social layer: we either use your match session id, if you have one, or your lobby session id, otherwise.\\n\\nA key feature of the Photon room system is that it keeps track of the oldest member in the room, called the “master client”, here identified with stars.\\n\\n<div style=\"text-align: center; padding: 10pt;\"><img src=\"./Media/photon_join_or_create.png\" align=\"middle\" width=\"650\"></div>\\n\\nLet’s look at Alice, Bob and Charlie entering the Purple room.  Charlie is first to join, so the room is created for him and he is marked as its **master client**.  Alice and Bob join shortly after and they are added as **normal clients**.\\n\\n<div style=\"text-align: center; padding: 10pt;\"><img src=\"./Media/photon_notification.png\" align=\"middle\" width=\"650\"></div>\\n\\nIf Charlie, as the master client, leaves the room, a new master client is selected and all remaining clients are notified of that change.  This is a key feature for the next networking layer.\\n\\n## *Game Replication Layer - Netcode for GameObjects*\\n\\nYou can find extensive documentation on Netcode for GameObjects on the [Unity Documentation site](https://docs-multiplayer.unity3d.com/docs/tutorials/helloworld/helloworldintro).\\n\\nFor some applications, like SharedSpaces, we can host the server on one of the headsets as a listen-server.  In that mode, the game acts both as a server and as its first connected client.  It will accept connections from the other players.\\n\\nSo for each room, we need to select one of the users to be the listen-server.  This decision comes from the transport layer: the **master client** of the corresponding Photon room will be our host.\\n\\n<div style=\"text-align: center; padding: 10pt;\"><img src=\"./Media/photon_to_ue4_1.png\" align=\"middle\" width=\"650\"></div>\\n\\nWhen the player hosting leaves, we perform a host migration.  Here we can see that Alice is leaving the purple room.  Photon picks Bob as the new master client. The remaining members of the room are notified and they reestablish their Unity connections.\\n\\n<div style=\"text-align: center; padding: 10pt;\"><img src=\"./Media/photon_to_ue4_2.png\" align=\"middle\" width=\"650\"></div>\\n\\nWe end up with two Photon rooms, the Purple room is now hosted by Bob, with Charlie and Donna connected to him. Alice just left the room through the door to her lobby, but since she is the only one there, she becomes both the master client and host of her group lobby.\\n\\n\\n# B. <a id=\"B\">SharedSpaces in Action</a>\\n\\nLet’s have a look at SharedSpaces in action.\\n\\n<div style=\"text-align: center; padding: 10pt;\">\\n\\t<img src=\"./Media/screenshots/1a.jpg\" width=\"250\">\\n\\t<img src=\"./Media/screenshots/1b.jpg\" width=\"250\">\\n</div>\\n\\nWhen Alice starts SharedSpaces, she starts alone in her private lobby.  She is the master client and host of the lobby, as indicated by the star next to her name.\\n\\n<div style=\"text-align: center; padding: 10pt;\">\\n\\t<img src=\"./Media/screenshots/2a.jpg\" width=\"250\">\\n\\t<img src=\"./Media/screenshots/2b_id.jpg\" width=\"250\">\\n\\t<img src=\"./Media/screenshots/2c.jpg\" width=\"250\">\\n\\t<img src=\"./Media/screenshots/2d.jpg\" width=\"250\">\\n</div>\\n\\nAlice wants Bob to form a group with her so that they can be together between matches. To do that, she steps on the invite panel switch and she sends him an invitation from her lobby. By accepting, SharedSpaces starts on Bob’s headset with a deeplink message that will let him join Alice in game.  From now on, Bob will have the same lobby session id as Alice and they will share the same lobby.\\n\\n<div style=\"text-align: center; padding: 10pt;\">\\n\\t<img src=\"./Media/screenshots/3a.jpg\" width=\"250\">\\n\\t<img src=\"./Media/screenshots/3b.jpg\" width=\"250\">\\n\\t<img src=\"./Media/screenshots/3c.jpg\" width=\"250\">\\n\\t<img src=\"./Media/screenshots/3d.jpg\" width=\"250\">\\n</div>\\n\\nBob goes through the blue door to start a private match, followed by Alice. They end up in the same Blue Room and they now have the same match session id that corresponds to their private room.  Since Bob was there first, he is the one hosting the room and Alice is connected to him.\\n\\n<div style=\"text-align: center; padding: 10pt;\">\\n\\t<img src=\"./Media/screenshots/4a_id.jpg\" width=\"250\">\\n\\t<img src=\"./Media/screenshots/4b_id.jpg\" width=\"250\">\\n\\t<img src=\"./Media/screenshots/4c.jpg\" width=\"250\">\\n\\t<img src=\"./Media/screenshots/4d.jpg\" width=\"250\">\\n</div>\\n\\nAlice decides to invite her friend Charlie to join their match, and he happens to be in his own lobby when he accepts the invitation.  Charlie has his match session id updated with the private match id, but on the other hand he still retains his own lobby session id.  He is still part of a different group.\\n\\n<div style=\"text-align: center; padding: 10pt;\">\\n\\t<img src=\"./Media/screenshots/5a.jpg\" width=\"250\">\\n\\t<img src=\"./Media/screenshots/5b.jpg\" width=\"250\">\\n\\t<img src=\"./Media/screenshots/5c.jpg\" width=\"250\">\\n</div>\\n\\nWhen Bob leaves the blue room, Photon notifies Alice and Charlie that the master client has changed. A host migration is needed: Alice opens a new listen-server, since she is the new master client of the blue room, and Charlie connects to her.\\n\\nAs for Bob, he started hosting his group lobby.\\n\\n<div style=\"text-align: center; padding: 10pt;\">\\n\\t<img src=\"./Media/screenshots/6a.jpg\" width=\"250\">\\n\\t<img src=\"./Media/screenshots/6b.jpg\" width=\"250\">\\n</div>\\n<div style=\"text-align: center; padding: 10pt;\">\\n\\t<img src=\"./Media/screenshots/6c_id.jpg\" width=\"250\">\\n\\t<img src=\"./Media/screenshots/6d_id.jpg\" width=\"250\">\\n\\t<img src=\"./Media/screenshots/6e_id.jpg\" width=\"250\">\\n</div>\\n\\nNow when Charlie leaves the blue room, he does not join Bob. They are not part of the same group since they do have different lobby session ids. Instead, he goes back to his own separate lobby.  This can be checked by stepping on the roster panel switch and you will see your different groups explicitly listed.\\n\\n<div style=\"text-align: center; padding: 10pt;\">\\n\\t<img src=\"./Media/screenshots/7a.jpg\" width=\"250\">\\n\\t<img src=\"./Media/screenshots/7b.jpg\" width=\"250\">\\n</div>\\n\\nIn the case of Alice, therefore, going back to lobby means that she will rejoin Bob who is waiting for her.\\n\\n<div style=\"text-align: center; padding: 10pt;\">\\n\\t<img src=\"./Media/screenshots/8a_id.jpg\" width=\"250\">\\n\\t<img src=\"./Media/screenshots/8b_id.jpg\" width=\"250\">\\n\\t<img src=\"./Media/screenshots/8c.jpg\" width=\"250\">\\n\\t<img src=\"./Media/screenshots/8d.jpg\" width=\"250\">\\n</div>\\n\\nTo have Charlie join their group, Alice or Bob simply need to send him an invitation from their lobby. Again, by accepting an invitation to lobby, you also accept to join a group. Charlie’s lobby session id is updated and the three of them will now share the same lobby between matches.\\n\\n\\n# C. <a id=\"C\">Oculus Application Configuration</a>\\n\\nTo build and run your own copy of SharedSpaces, you will need to create an application for it on the [Oculus developer dashboard](https://developer.oculus.com/).\\n\\n## 1. <a id=\"C1\">Application Identifier</a>\\n\\n<div style=\"text-align: center; padding: 10pt;\">\\n\\t<img src=\"./Media/dashboard/dashboard_app.png\"  width=\"800\">\\n</div>\\n\\nYou Oculus application identfier must be placed in [Assets/Resources/OculusPlatformSettings.asset](Assets/Resources/OculusPlatformSettings.asset).\\n\\nThe identifier (__App ID__) can be found in the _API_ section.\\n\\n<div style=\"text-align: center; padding: 10pt;\">\\n\\t<img src=\"./Media/dashboard/dashboard_api.png\"  width=\"800\">\\n</div>\\n\\n## 2. <a id=\"C2\">Destinations</a>\\n\\nYou need to recreate the SharedSpaces destinations in your own application.  Destinations can be found under __Platform Services__.\\n\\n<div style=\"text-align: center; padding: 10pt;\">\\n\\t<img src=\"./Media/dashboard/dashboard_platform_services.png\"  width=\"800\">\\n</div>\\n\\nYou need to recreate the SharedSpaces destinations in your own application.  Destinations can be found under __Platform Services__.\\n\\n<div style=\"text-align: center; padding: 10pt;\">\\n\\t<img src=\"./Media/dashboard/dashboard_destinations.png\"  width=\"800\">\\n</div>\\n\\nSharedSpaces has five destinations: a Lobby, three private rooms (the red, green and blue rooms), and one public room (the purple room).  Here are the settings for each of them.\\n\\n| API Name | Deeplink Message | Display Name | Description |\\n| :--- | :--- | :--- | :--- |\\n| [Lobby](./Media/dashboard/dashboard_destination_lobby.png) | {\"is_lobby\":\"true\",\"map\":\"Lobby\"} | Lobby | The Lobby |\\n| [RedRoom](./Media/dashboard/dashboard_destination_redroom.png) | {\"map\":\"RedRoom\"} | Red Room | The Red Room |\\n| [GreenRoom](./Media/dashboard/dashboard_destination_greenroom.png) | {\"map\":\"GreenRoom\"} | Green Room | The Green Room |\\n| [BlueRoom](./Media/dashboard/dashboard_destination_blueroom.png) | {\"map\":\"BlueRoom\"} | Blue Room | The Blue Room |\\n| [PurpleRoom](./Media/dashboard/dashboard_destination_purpleroom.png) | {\"map\":\"PurpleRoom\",\"public_room_name\":\"ThePurpleRoom\"} | Purple Room | The Purple room |\\n\\nIn addition to these settings, you need to set __Deeplink Type__ to __Enabled__ and add an image for your destination.  In the case of SharedSpaces, the destination is __Audience__ is set to __Everyone__. Also make sure to set the max group launch capacity for each destination so that the group launch feature can be used.\\n\\n## 3. <a id=\"D3\">Data Use Checkup</a>\\n\\nYou will need to request access to platform data needed by SharedSpaces. Under __Data Use Checkup__, add the following items and submit for certification.\\n\\n+  User ID\\n+  User Profile\\n+  Deep Linking\\n+  Friends\\n+  Invites\\n'},\n",
       " {'repo': 'nasa-jpl-memex/image_space',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': 'ImageSpace\\n==========\\nImageSpace is an application built on top of [ImageCat](http://github.com/chrismattmann/imagecat/)\\nthat allows a user to browse a rich catalog of EXIF-metadata extracted and OCR extracted information\\nfrom images. It allows histogram and D3-based visual search, free text search and retrieval and \\nperforms image similarity metrics using computer vision techniques and metadata-techniques (e.g., \\n[Jaccard Similarity](http://github.com/chrismattmann/tika-img-similarity)).\\n\\nQuickstart\\n==========\\nImageSpace can be spun up quickly by means of Docker containers, see the [ImageSpace Quickstart](https://github.com/nasa-jpl-memex/image_space/wiki/Quick-Start-Guide-with-ImageCat).\\n\\nImage Similarity through [SMQTK](https://github.com/nasa-jpl-memex/SMQTK) can be made available by following the [ImageSpace SMQTK Quickstart](https://github.com/nasa-jpl-memex/image_space/blob/master/imagespace_smqtk/Docker.md).\\n\\nInstallation\\n============\\nSee the [installation README](https://github.com/nasa-jpl-memex/image_space/tree/master/imagespace) for\\ninstructions.\\n\\nCredits\\n=======\\nImageSpace is developed by the [JPL-Kitware](http://memex.jpl.nasa.gov/) team funded\\nthrough the [DARPA Memex](http://www.darpa.mil/newsevents/releases/2014/02/09.aspx) program.\\n\\nQuestions, comments?\\n===================\\nSend them to [Chris A. Mattmann](mailto:chris.a.mattmann@jpl.nasa.gov).\\n\\nAuthors\\n=======\\n* Jeff Baumes, Kitware  \\n* Chris Mattmann, JPL\\n* Dan LaManna, Kitware\\n* Harshavardhan Manjunatha, USC\\n* Lewis John McGibbney, JPL\\n* Madhav Sharan, JPL & USC\\n\\nLicense\\n===\\nThis project is licensed under the [Apache License, version 2.0](http://www.apache.org/licenses/LICENSE-2.0).\\n\\n'},\n",
       " {'repo': 'HoloArchivists/twspace-dl',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '<!-- markdownlint-disable MD033 MD041 -->\\n\\n<div align=\"center\">\\n  <h1 id=\"twspace-dl\">Twspace-dl</h1>\\n  <p>\\n    <a href=\"https://pypi.org/project/twspace-dl/\">\\n      <img src=\"https://img.shields.io/pypi/v/twspace-dl?style=for-the-badge\" alt=\"PyPI\">\\n    </a>\\n    <a href=\"https://pypi.org/project/twspace-dl/\">\\n      <img src=\"https://img.shields.io/pypi/dm/twspace-dl?label=DOWNLOADS%20%28PYPI%29&amp;style=for-the-badge\" alt=\"PyPI DLs\">\\n    </a>\\n    <a href=\"https://github.com/HoloArchivists/twspace-dl/releases\">\\n      <img src=\"https://img.shields.io/github/downloads/HoloArchivists/twspace-dl/total?label=DOWNLOADS%20%28GITHUB%29&amp;style=for-the-badge\" alt=\"Github Releases DLs\">\\n    </a>\\n  </p>\\n  <p>A python module to download twitter spaces.</p>\\n</div>\\n\\n## Screensots\\n\\n<details>\\n<summary>GUI</summary>\\n\\n![general tab](https://user-images.githubusercontent.com/77058942/172580094-3663f86d-3ee2-48d0-9313-f4ed71f048aa.png)\\n![input tab](https://user-images.githubusercontent.com/77058942/172580476-bb34dce0-08b0-41f6-852b-b68d32532add.png)\\n![running tab](https://user-images.githubusercontent.com/77058942/172580589-fd6b05bd-f081-4c7a-ab05-0640abda00ce.png)\\n![success pop up](https://user-images.githubusercontent.com/77058942/172580861-18b3ac9f-88d2-44cf-8b5d-135990a78f77.png)\\n\\n</details>\\n\\n<details>\\n<summary>CLI</summary>\\n\\n![help](https://user-images.githubusercontent.com/77058942/172581224-9b465f78-4894-456f-9b85-5b76ee9bbfca.png)\\n![running](https://user-images.githubusercontent.com/77058942/172581500-174834c5-6883-44f9-a0a7-610dbb2103e5.png)\\n\\n</details>\\n\\n\\n## Requirements\\n\\nffmpeg if not using portable binaries\\n\\n## Install\\n\\n### GUI\\n\\nUse this if you\\'re not sure.\\n\\n### From portable binaries\\n\\n[Windows](https://github.com/HoloArchivists/twspace-dl/releases/latest/download/twspace-dl-GUI.exe)\\n\\n### From source\\n\\n```bash\\npip install git+https://github.com/HoloArchivists/twspace-dl@gooey\\n```\\n\\n### CLI\\n\\n### From portable binaries\\n\\n[Windows](https://github.com/HoloArchivists/twspace-dl/releases/latest/download/twspace-dl-CLI.exe)\\n\\n### From PyPI\\n\\n```bash\\npip install twspace-dl\\n```\\n\\n### From source\\n\\n```bash\\npip install git+https://github.com/HoloArchivists/twspace-dl\\n```\\n\\n## Usage\\n\\n```bash\\ntwspace_dl -i space_url\\n```\\n\\n<details>\\n<summary>With binaries</summary>\\n\\n### Windows\\n\\n```bash\\n.\\\\twspace_dl.exe -i space_url\\n```\\n\\n</details>\\n\\n## Features\\n\\nHere\\'s the output of the help option\\n\\n```txt\\nusage: twspace_dl [-h] [-v] [-s] [-k] [-l] [--input-cookie-file COOKIE_FILE]\\n                  [--username USERNAME] [--password PASSWORD]\\n                  [--output-cookie-file OUTPUT_COOKIE_FILE]\\n                  [-i SPACE_URL | -U USER_URL] [-d DYN_URL] [-f URL] [-M PATH]\\n                  [-o FORMAT_STR] [-m] [-p] [-u] [--write-url URL_OUTPUT]\\n\\nScript designed to help download twitter spaces\\n\\noptional arguments:\\n  -h, --help            show this help message and exit\\n  -v, --verbose\\n  -s, --skip-download\\n  -k, --keep-files\\n  -l, --log             create logfile\\n  --input-cookie-file COOKIE_FILE\\n\\ninput:\\n  -i SPACE_URL, --input-url SPACE_URL\\n  -U USER_URL, --user-url USER_URL\\n  -d DYN_URL, --from-dynamic-url DYN_URL\\n                        use the dynamic url for the processes(useful for ended\\n                        spaces) example: https://prod-fastly-ap-northeast-1.vi\\n                        deo.pscp.tv/Transcoding/v1/hls/zUUpEgiM0M18jCGxo2eSZs9\\n                        9p49hfyFQr1l4cdze-Sp4T-DQOMMoZpkbdyetgfwscfvvUkAdeF-I5\\n                        hPI4bGoYg/non_transcode/ap-northeast-1/periscope-\\n                        replay-direct-prod-ap-northeast-1-public/audio-\\n                        space/dynamic_playlist.m3u8?type=live\\n  -f URL, --from-master-url URL\\n                        use the master url for the processes(useful for ended\\n                        spaces) example: https://prod-fastly-ap-northeast-1.vi\\n                        deo.pscp.tv/Transcoding/v1/hls/YRSsw6_P5xUZHMualK5-ihv\\n                        ePR6o4QmoZVOBGicKvmkL_KB9IQYtxVqm3P_vpZ2HnFkoRfar4_uJO\\n                        jqC8OCo5A/non_transcode/ap-northeast-1/periscope-\\n                        replay-direct-prod-ap-northeast-1-public/audio-\\n                        space/master_playlist.m3u8\\n  -M PATH, --input-metadata PATH\\n                        use a metadata json file instead of input url (useful\\n                        for very old ended spaces)\\n\\noutput:\\n  -o FORMAT_STR, --output FORMAT_STR\\n  -m, --write-metadata  write the full metadata json to a file\\n  -p, --write-playlist  write the m3u8 used to download the stream(e.g. if you\\n                        want to use another downloader)\\n  -u, --url             display the master url\\n  --write-url URL_OUTPUT\\n                        write master url to file\\n\\nlogin:\\n  --username USERNAME\\n  --password PASSWORD\\n  --output-cookie-file OUTPUT_COOKIE_FILE\\n```\\n\\n## Format\\n\\nYou can use the following identifiers for the formatting\\n\\n```python\\n%(title)s\\n%(id)s\\n%(start_date)s\\n%(creator_name)s\\n%(creator_screen_name)s\\n%(url)s\\n%(creator_id)s\\n```\\n\\nExample: `[%(creator_screen_name)s]-%(title)s|%(start_date)s`\\n\\n## Known Errors\\n\\n`Changing ID3 metadata in HLS audio elementary stream is not implemented....`\\n\\nThis is an error in ffmpeg that does not affect twspace_dl at all as far as I\\xa0know.\\n\\n## Service \\n\\nTo run as a systemd service please refer to https://github.com/HoloArchivists/twspace-dl/blob/main/SERVICE.md\\n\\n## Docker\\n\\n### Run once\\n\\n> Use ${pwd} in powershell, or $(pwd) in bash\\n\\n```bash\\ndocker run --rm -v ${pwd}:/output ryu1845/twspace-dl -i space_url\\n```\\n\\n### Run as monitoring service\\n\\nUsing a cookie can help solve some problem with the twitter api. However, using one is not necessary.\\n\\n#### Without cookie\\n\\n1. Download the `docker-compose.yml`, `.env`, `monitor.sh` files and put them in a folder named `twspace-dl`.\\n2. Edit `.env` and fill in the Twitter username you want to monitor.\\n3. \\\\[Optional] If you want to used a cookies file, put it into the folder and named it `cookies.txt`.\\n4. `docker-compose up -d`\\n'},\n",
       " {'repo': 'XereoNet/SpaceBukkit',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': \"# SpaceBukkit - the awesome way!\\nA powerful yet simple web panel for administering your Bukkit Minecraft servers with ease.\\n\\n## Description\\nSpaceBukkit is a project developed over the last 6 months aiming to bring an advanced and powerful web administration framework to Bukkit. What makes it unique is, on one hand, it's graphical user interface, and on the other hand some nifty features enlisted just below.\\n\\n## Features\\n- General\\n    * Attractive Interface\\n    * Non lethal doses of awesomeness\\n    * Multiple Servers\\n    * Multi-user access with role setting\\n    * Theme Support for extra commiseration\\n    * Per server - per user role settings\\n    * Console access\\n- Dashboard\\n    * Pretty and quick statistics about your server\\n    * Activity feed - who did what, and when\\n    * Chat - talk with your players\\n- Players\\n    * Player management - kick, kill, feed, heal, ban, op and more!\\n    * Whitelist\\n    * Bans\\n- Plugins\\n    * Plugin management\\n    * Config editor\\n    * Installing and Deinstalling\\n    * Updating and disabling\\n    * Bukget integration\\n- Worlds\\n    * World management\\n    * Multiworld support\\n    * Backups\\n    * Chunkster\\n    * MapAutoTrim\\n- Servers\\n    * Craftbukkit one-click installing and updating\\n    * Server Properties saving\\n    * Schedules\\n    \\n## Credits\\n * [Antariano](https://github.com/Antariano/) - SpaceCP.\\n * [JamyDev](https://github.com/JamyDev/) - SpaceCP.\\n * [NeatMonster](https://github.com/NeatMonster/) - SpaceModule, SpaceRTK and SpaceBukkit.\\n * [Drdanick](https://github.com/Drdanick/) - RemoteToolkit, SpaceModule, SpaceRTK and Spacebukkit.\\n\\n## Links\\n- Website: [http://spacebukkit.xereo.net/](http://spacebukkit.xereo.net/).\\n- Forums: [http://forums.xereo.net/](http://forums.xereo.net/).\\n- Wiki: [http://spacebukkit.xereo.net/wiki](http://spacebukkit.xereo.net/wiki).\\n \\n## License\\nSpaceBukkit is free software: you can redistribute it and/or modify it under\\nthe terms of the Attribution-NonCommercial-ShareAlike Unported (CC BY-NC-SA)\\nlicense as published by the Creative Common organization, either version 3.0 of\\nthe license, or (at your option) any later version.\\n\\nSpaceBukkit is distributed in the hope that it will be useful, but WITHOUT ANY\\nWARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A\\nPARTICULAR PURPOSE. See the Attribution-NonCommercial-ShareAlike Unported (CC \\nBY-NC-SA) license for more details.\\n\\nYou should have received a copy of the Attribution-NonCommercial-ShareAlike \\nUnported (CC BY-NC-SA) license along with this program. If not, see \\n[http://creativecommons.org/licenses/by-nc-sa/3.0/](http://creativecommons.org/licenses/by-nc-sa/3.0/).\"},\n",
       " {'repo': 'Yeah-Kun/python',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '## python学习历程\\r\\n### 日志 \\r\\n\\r\\n- 创建Crawler，上传爬虫相关代码（2020-6-2 11:52:10）\\r\\n\\r\\n- 创建DeepLearning/ModelOptimization，学习模型优化相关内容（2019年11月27日19:54:21）\\r\\n- 创建Blender，写Blender的脚本（2019年9月9日15:28）\\r\\n- 创建ComputerGraphics，开始尝试图形学（2019年8月28日13:50）\\r\\n- 创建GUI，开始设计图形界面（2017年10月1日16:05:52）\\r\\n- 创建Mathematical modeling，上传数学建模代码（2017年9月14日20:00:00）\\r\\n- 创建NAO，上传了NAO机器人代码（2017-9-2 22:23:12）\\r\\n- 创建Django，上线Django写的成绩查询系统（2017-8-22）\\r\\n- 创建game，写pygame游戏（2017-6-25）\\r\\n- 创建new，开始注意注释和规范地写代码（2017-6-1）\\r\\n- 开始接触机器学习，并写情感分析的代码（2017-04-04）\\r\\n- 创建Tipdm Cup，开始为泰迪杯竞赛写代码（2017-03-27）\\r\\n- 刚学会用git，上传之前写过的代码，并开始写日志（2017-3-10）\\r\\n- 写old文件夹里的代码（2017-1-19）'},\n",
       " {'repo': 'Xerxes1138/StochasticScreenSpaceReflection',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# StochasticScreenSpaceReflection\\n\\n![SSSR](https://dl.dropboxusercontent.com/u/1812933/Unity/SSSR.png)\\n\\n# Features\\n\\n* Contact hardening\\n* Specular elongation\\n* Sharp and blurry reflection\\n* Per-pixel roughness and normal\\n* Previous frame reprojection giving one free bounce reflection.\\n* Convolved scene buffer mip chain.\\n\\n# Requirements\\n\\nUnity 2017.1 and a shader model 5.0 ( dx11 ) graphic card.\\n\\n# How to use\\n\\nSet project to Linear color space and deferred shading.\\n\\nThen select your main camera and go to \"cCharkes/Image Effects/Rendering/Stochastic Screen Space Reflection\" or drag and drop the StochasticSSR.cs to your main camera inspector.\\n\\n# References\\n- [Jimenez 2014] \"Next Generation Post Processing In Call Of Duty Advanced Warfare\"  \\n- [Michal Valient, Siggraph14] \"Reflections and Volumetrics of Killzone Shadow Fall\"\\n- [Tomasz Stachowiak and Yasin Uludag, Siggraph15] \"Stochastic Screen-Space Reflections\"\\n- [Michele Giacolone, 2016] \"Screen Space Reflections in The Surge\"\\n- Lasse Jon Fuglsang Pedersen <lasse@playdead.com>  https://github.com/playdeadgames/temporal\\n- Bart Wronski <@BartWronsk> https://github.com/bartwronski/BlueNoiseGenerator/\\n\\nThanks to Tomasz Stachowiak (@h3r2tic) for his help.\\n'},\n",
       " {'repo': 'despoisj/LatentSpaceVisualization',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# LatentSpaceVisualization\\nVisualization techniques for the latent space of a convolutional autoencoder in Keras/Tensorflow \\n\\n[Check out the post on Medium](https://medium.com/@juliendespois/latent-space-visualization-deep-learning-bits-2-bd09a46920df#.8teem7fm4) \\n'},\n",
       " {'repo': 'godarklight/DarkMultiPlayer',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': \"# DarkMultiPlayer\\n\\nDarkMultiPlayer is a multiplayer mod for Kerbal Space Program. It supports subspace style (and master controlled warp) warping and career mode, with an easy to edit server database.\\n\\nThe DarkMultiPlayer client and server are cross platform, see [Install](#install).\\n\\n## Install\\n### Client\\n* Download the [DMPClient zip](https://spacedock.info/mod/10) and extract to `[KSP root folder]/GameData`\\n* Download [DMPUpdater](http://godarklight.privatedns.org/dmp/downloads/dmpupdater/), place the program on your KSP folder and run it.\\n\\n### Server\\nThe DarkMultiPlayer server is cross platform, meaning you can run it on any platform that supports .NET.\\nIn Linux or macOS, you must have [Mono](http://mono-project.com) installed to be able to run the server.\\n* Download the [DMPServer zip](https://spacedock.info/mod/11/DarkMultiPlayer%20Server)\\n* Download [DMPUpdater](http://godarklight.privatedns.org/dmp/downloads/dmpupdater/), place the program on your server folder and run it.\\n  - NOTE: you must have a previous server version in the folder for DMPUpdater to work.\\n\\nYou can configure your server by editing `Config/Settings.txt`.  \\nIf your server's game difficulty is set to `CUSTOM`, you can alter gameplay settings by editing `Config/GameplaySettings.txt`.\\n\\n## Compiling\\n- Copy the assemblies from `[KSP root folder]/KSP_Data/Managed` to `External/KSPManaged`:\\nRun msbuild /p:Configuration=Release (or build with your preferred IDE) to build the plugin and the .NET Framework version of the server. This single build is completely cross platform.\\nNavigate to DotNet/Server/ and either run `dotnet publish -c release -f netcoreapp3.1` to build for only your OS, or `./compile.sh` to build for every OS\\n\\n## Mod Control\\nRead `DMPModControl.txt`, it's commented. The file can be copied from a development KMPServer (The one where you can use SHA sums, not the one with the !md5 section) as the file format is the same.\\n\\nIf you are running a private server, it's safe enough to just add the missing parts.\\n\\nYou can get the DMP client to make a `DMPModControl.txt` file specific for your GameData directory by pressing `Options -> Advanced -> Mod Control -> Generate`. The whitelist option will only allow you to connect with the mods in your GameData directory. The blacklist option will allow you to connect with any mods.\\n\"},\n",
       " {'repo': 'Whiplash141/SpaceEngineersScripts',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# Description \\nThis is a repository of many niche scripts as well as all of my actively supported workshop scripts.\\n\\nYou are more than welcome to use these scripts in your builds and upload blueprints or worlds that use them; however, I do **not** under any circumstance allow anyone to reupload these scripts themselves to the steam workshop without my explicit permission.\\n\\nAll of my workshop released scripts live in the [Released](Released) folders. These scripts are actively supported and should be generally more efficient than the unpolished scripts that live over in [Unpolished](Unpolished).\\n'},\n",
       " {'repo': 'YuanBoot/Intrinsic_Garment_Space',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '### Learning an Intrinsic Garment Space for Interactive Authoring of Garment Animation\\r\\n\\r\\n***\\r\\n\\r\\n\\r\\n\\r\\n### Overview\\r\\n\\r\\n***\\r\\n\\r\\nThis is the demo code for training a motion invariant encoding network. The following diagram provides an overview of the network structure.\\r\\n\\r\\nFor more information, please visit http://geometry.cs.ucl.ac.uk/projects/2019/garment_authoring/ \\r\\n\\r\\n<img src=\".\\\\resource\\\\network.png\" alt=\"network\" width=600 />\\r\\n\\r\\n\\r\\n\\r\\n### Structure\\r\\n\\r\\n***\\r\\n\\r\\nThe project\\'s directory is shown as follows. The data set is in the `data_set` folder, including cloth mesh(generated by Maya Qualoth), garment template, character animation and skeletons. Some supporting files can be found in `support`. The shape feature descriptor and motion invariant encoding network are saved in `nnet`.\\r\\n\\r\\n\\r\\n\\r\\n```\\r\\n├─data_set\\r\\n│  ├─anim\\r\\n│  ├─case\\r\\n│  ├─garment\\r\\n│  ├─skeleton\\r\\n│  └─Maya\\r\\n├─nnet\\r\\n│  ├─basis\\r\\n│  └─mie\\r\\n├─support\\r\\n│  ├─eval_basis\\r\\n│  ├─eval_mie\\r\\n│  ├─info_basis\\r\\n│  └─info_mie\\r\\n└─scripts\\r\\n```\\r\\n\\r\\n\\r\\n\\r\\nIn the `scripts` folder, there are several python scripts which implement the training process. We also provide a data set for testing, generated from a sequence of dancing animation and a skirt.\\r\\n\\r\\n\\r\\n\\r\\n### Data Set\\r\\n\\r\\n***\\r\\n\\r\\nThe data set includes not only the meshes and garment template, but also some supporting information. You can check the animation in the `Maya` folder. The animation information is saved in the `anim` folder. In the `case` folder, there are many meshes generated by Qualoth in different simulation parameters. The garment template is in the `garment` folder.  \\r\\n\\r\\n<img src=\".\\\\resource\\\\1.png\" alt=\"network\" width=800 />   \\r\\n\\r\\n\\r\\n\\r\\n### Installation\\r\\n\\r\\n***\\r\\n\\r\\n- Clone the repo:\\r\\n\\r\\n```\\r\\ngit clone https://github.com/YuanBoot/Intrinsic_Garment_Space.git\\r\\n```\\r\\n\\r\\n- Install PyTorch\\r\\n- Install  tqdm (https://tqdm.github.io/)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n### Model Training\\r\\n\\r\\n***\\r\\n\\r\\n\\r\\n\\r\\n#### Shape Descriptor\\r\\n\\r\\nAfter all preparing works done, you can start to train the network. In `scripts` folder, some scripts named `basis_*` are used for training shape descriptor. \\r\\n\\r\\nRun them as follows:\\r\\n\\r\\n`01.basis_prepare.py` (data preparing)\\r\\n\\r\\n`02.basis_train.py` (training)\\r\\n\\r\\n`03.basis_eval.py` (evaluation)\\r\\n\\r\\nAfter running 01 and 02 scripts, there will be a `*.net` file in the `nnet/basis` folder. It is the shape feature descriptor.\\r\\n\\r\\nThe result of a specific frame after running `03.basis_eval.py` script. The yellow skirt is our output and the blue one is the ground truth. If the loss of the descriptor is low enough, these two skirt are almost overlap.\\r\\n\\r\\n<img src=\".\\\\resource\\\\f2.png\" alt=\"f2\" width=300 />\\r\\n\\r\\n\\r\\n\\r\\n#### Motion Invariant Encoding\\r\\n\\r\\nThen, you can run `mie_*.py` scripts to get the motion invariant encoding network. \\r\\n\\r\\n`04.mie_prepare.py` (data preparing)\\r\\n\\r\\n`05.mie_train.py` (training)\\r\\n\\r\\n`06.mie_eval.py` (evaluation)\\r\\n\\r\\nIf everything goes well, the exported mesh would be like the following figures. For the output from`06.mie_eval.py` is painted by red and the green one is the ground truth.\\r\\n\\r\\n<img src=\".\\\\resource\\\\f3.png\" alt=\"f3\" width=300 />'},\n",
       " {'repo': 'sunshinev/go-space-chat',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '##  孤独 Lonely\\n\\n![d2139b33a9868d1f17a471201d1272371588868902.jpg](https://cdn.jsdelivr.net/gh/sunshinev/remote_pics/d2139b33a9868d1f17a471201d1272371588868902.jpg)\\n\\n## Demo\\n\\nhttp://chat.osinger.com/\\n\\n## 特色\\n1. 支持性别修改、并且有颜色替换\\n2. 支持敏感词过滤\\n3. 支持姓名修改\\n\\n## 介绍\\n\\n通过canvas 2d来模拟了3D的视觉效果。\\n\\n并且在该项目中使用了protobuf来进行前端和后端的通讯协议，这一点非常方便！\\n\\n## 操作\\n\\n1. 项目使用传统`WASD`按键来控制上下左右\\n2. 眼睛可以跟随鼠标的位置进行转动\\n3. 按下`space` 空格可以输入消息，按下回车发送消息\\n4. 左上角按钮可以输入名称，点击空白处名称生效\\n\\n\\n## 运行\\n\\n```$xslt\\ngo run main.go\\n```\\n\\n该命令会启动web-server作为静态服务，默认80端口，如果需要修改端口，用下面的命令\\n```\\ngo run main.go -web_server 8081\\n```\\n\\n项目启动默认websocket服务端口为9000端口，如果需要修改\\n```\\ngo run main.go -socket_server 9001\\n```\\n注意：如果修改websocket端口，同时需要修改js里面的socket端口\\n\\n\\n## 技术工具\\n\\n前端 Vue+canvas+websocket+protobuf\\n\\n后端 Golang+websocket+protobuf+goroutine\\n\\n## 有意思的难点\\n> 这里列举几个在实现过程中，遇到的很有意思的问题\\n\\n1. 如何实现无限画布？\\n2. 如何实现游戏状态同步？\\n\\n\\n## proto 文件生成指令\\n```\\nprotoc -I ./ *.proto --go_out=.\\n```\\n\\n```\\nprotoc --js_out=import_style=commonjs,binary:. *.proto\\n\\n```\\n\\n\\n## 相关链接\\n\\n[Canvas 基本用法](https://developer.mozilla.org/zh-CN/docs/Web/API/Canvas_API/Tutorial/Basic_usage)\\n\\n[Protobuf Guide](https://developers.google.com/protocol-buffers/docs/proto3)\\n\\n[Vue.js](https://cn.vuejs.org/index.html)'},\n",
       " {'repo': 'breck7/space',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': 'Update 3/3/2017\\n===============\\n\\nUse Tree Notation! Space has evolved into Tree Notation: https://github.com/breck7/jtree\\n\\nThe significant changes of this encoding in a nutshell:\\n\\nNote (2012): Only supported hash tables.\\nSpace (2013): Support for other complex structures but Input !== Output for all documents.\\nTree (2017): Input === Output for all documents.\\n\\nThis project will no longer be updated.\\n\\nSpace\\n=====\\n\\nSpace is a lightweight language for objects.\\n\\nSpace is like XML or JSON, with less punctuation and more power.\\n\\nExample\\n-------\\n\\nAn object like this:\\n\\n    {\"name\" : \"John\", \"age\" : 29}\\n    \\n    <person><name>John</name><age>29</age></person>\\n\\nCan be written in Space like this:\\n\\n    name John\\n    age 29\\n\\n\\nTry It Now\\n----------\\n\\nhttp://breckyunits.com/spaceconsole/\\n\\nInstalling\\n----------\\n\\nNode.js:\\n\\n    npm install space\\n\\nUsing\\n-----\\n\\n    // Creating a Space Object\\n    var person = new Space(\\'name John\\')\\n    // Accessing a property    \\n    console.log(person.get(\\'name\\'))\\n    // Setting a property\\n    person.set(\\'age\\', 29)\\n    // Printing the object\\n    console.log(person.toString())\\n    \\n\\nExamples\\n--------\\n\\nHere\\'s how I could write a tax return in Space:\\n\\n    socialSecurityNumber 555-55-5555\\n    name John Smith\\n    taxYear 2012\\n    income 10,000\\n    dependents 1\\n    exemptions 2\\n    address 123 Main Street\\n    city San Francisco\\n    state California\\n   \\nSpace supports recursion. Here\\'s an example of web page stats:\\n\\n    homepage\\n     pageviews 2312\\n     uniques 231\\n     referers\\n      about 23\\n      contact 41\\n    about\\n     pageviews 314\\n     uniques 201\\n     referers\\n      home 100\\n      contact 21\\n    contact\\n     pageviews 214\\n     uniques 124\\n     referers\\n      home 110\\n      about 10\\n    \\nWorking with the above stats example:\\n\\n    var stats = new Space(exampleStringFromAbove)\\n    // Get a nested property using an xpath like query\\n    stats.get(\\'contact referers home\\')\\n    // Returns 110\\n    stats.set(\\'about uniques\\', 500)\\n\\nLibraries for Other Languages\\n-----------------------------\\n\\n- Python: <a href=\"https://github.com/jyxt/spacepython\">https://github.com/jyxt/spacepython</a>\\n- Ruby: <a href=\"https://github.com/fruchtose/space_object\">https://github.com/fruchtose/space_object</a>\\n\\nCommunity\\n---------\\n\\n- Email List: <a href=\"https://groups.google.com/forum/#!forum/spacedev\">spacedev@googlegroups.com</a>\\n- Freenode: #space-dev\\n\\n\\nBuild Status\\n------------\\n\\n[![Build Status](https://travis-ci.org/breck7/space.png?branch=master)](https://travis-ci.org/breck7/space)\\n\\nCopyright & License\\n-------------------\\n\\nCopyright (C) 2015 Breck Yunits - Released under the MIT License.\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n\\n'},\n",
       " {'repo': 'SEModCommunity/SE-Community-Mod-API',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': 'SE-Community-Mod-API\\n====================\\n\\nSpace Engineers Community Modding API\\n'},\n",
       " {'repo': 'SEModCommunity/SE-Community-Mod-API',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': 'SE-Community-Mod-API\\n====================\\n\\nSpace Engineers Community Modding API\\n'},\n",
       " {'repo': 'JonAbrams/SpaceAce',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# SpaceAce\\n\\nA fancy immutable storage library for JavaScript\\n\\n[![Build Status](https://travis-ci.org/JonAbrams/SpaceAce.svg?branch=master)](https://travis-ci.org/JonAbrams/SpaceAce)\\n\\n## Intro\\n\\nSpaceAce is a JS library for storing and updating the _state_ of your front-end application.\\n\\nLike Redux, it has unidirectional data flow, uses an immutable state, allows for clearly defined actions, but makes it much easier to generate new states.\\n\\n* [Introductory blog post](https://medium.com/@jonathanabrams/introducing-spaceace-a-new-kind-of-front-end-state-library-5215b18adc11)\\n* [SpaceAce Todo App on CodeSandbox](https://codesandbox.io/s/k3opnmolo5)\\n* [Original Redux Todo App on CodeSandbox](https://codesandbox.io/s/ql8k7wr079) (for comparison)\\n\\n## Benefits\\n\\n* **Immutable** – Centralized state with easy to track changes.\\n* **Modular** – View components can manage their own part of the state, update logic doesn\\'t need to be centralized. Also known as [loose coupling](https://en.wikipedia.org/wiki/Loose_coupling).\\n* **Convenient** – Ridiculously easy to update the state in the most common cases, such as a user editing a text field.\\n* **Framework Agnostic** – Designed with React in mind, but works with any stateless view library. Has no external dependencies. Requires only ES5, which can be [polyfilled](https://github.com/es-shims/es5-shim)/[transpiled](https://babeljs.io).\\n* **Small** – ~7.9K uncompressed, ~2.2K gzipped. No dependencies.\\n* Works with Redux DevTools\\n\\n## Install\\n\\n```bash\\nnpm install spaceace\\n# or\\nyarn add spaceace\\n```\\n\\n## Documentation\\n\\n* [What is a space?](#what-is-a-space)\\n* [React App Example](#react-app-example)\\n* [Updating Spaces](#updating-spaces)\\n  * [Immediate Update](#immediate-update-object)\\n  * [Quick Actions](#quick-actions-string)\\n  * [Custom Actions](#custom-actions-function)\\n  * [Action Named Params](#action-named-params)\\n  * [Promises and Async](#promises-and-async)\\n  * [Arrays](#arrays)\\n* [Utility Functions](#utility-functions)\\n  * [subscribe](#subscribe)\\n  * [isSpace](#isspace)\\n  * [rootOf](#rootof)\\n* [toJSON](#tojson)\\n* [Redux DevTools](#redux-devtools)\\n* [FAQ](#faq)\\n\\n### What is a Space?\\n\\n```js\\nimport { createSpace } from \\'spaceace\\';\\nconst space = createSpace({ name: \\'Bilbo\\', todos: [] });\\nconsole.log(space.name); // Bilbo\\n```\\n\\nA lot of developers don\\'t know that JavaScript functions can have attributes, just like objects. SpaceAce takes advantage of that feature.\\n\\nEach `Space` is a function, but with a twist. You can access attributes, like you would any object literal, but you can also call it like a function to change its’ contents. But since each space is immutable, the contents are not directly changed, instead a new space is returned:\\n\\n```js\\nconst newSpace = space({ name: \\'Frodo\\' });\\nconsole.log(newSpace.name); // Frodo\\nconsole.log(newSpace.todos); // []\\n```\\n\\nAlthough, you\\'ll want to instead subscribe to a space to easily update your app:\\n\\n```js\\nsubscribe(space, ({ newSpace }) => {\\n  // called whenever the passed in space/newSpace is updated\\n  renderApp(newSpace);\\n});\\nrenderApp(space); // initial render\\n```\\n\\nSince most changes to your application\\'s state is caused by user interactions, it’s very easy to bind _actions_ to events:\\n\\n```jsx\\nconst clearName = ({ merge }, event) => {\\n  // The first parameter is an object with the space and methods for changing it\\n  // Treat these as \"named param\"\\n  // The rest of the parameters are whatever was eventually passed to the\\n  // wrapped function.\\n  event.preventDefault();\\n  merge({ name: \\'\\' });\\n};\\n\\nconst ClearButton = space => (\\n  <button onClick={space(clearName)}>Clear name</button>\\n);\\n```\\n\\nYou\\'ll find that most actions just take in a value (or a value from an event) and apply it to a particular attribute, SpaceAce provides a shortcut, just give the name of the attribute that needs updating:\\n\\n```jsx\\n// \\'name\\' is a string, so `space` knows to apply the event\\'s value onto\\n// the corresponding attribute in `space`\\nconst NameField = space => <input value={space.name} onChange={space(\\'name\\')} />;\\n)\\n```\\n\\nIf a space has any objects or arrays as children, they\\'re automatically turned into spaces. This allows them to easily be passed into components. If a child space gets updated, all parent spaces are notified. This allows components to manage their own spaces, but for the application\\'s root space to still see and access everything, refreshing views when necessary.\\n\\n```js\\nconst addTodo = ({ push }, content) => {\\n  // In addition to `push`, array spaces also have these functions available:\\n  // `remove`, `unshift`, `replace`\\n  push({ done: false, content });\\n};\\nspace.todos(addTodo)(\\'Destroy ring\\');\\n```\\n\\n## React App Example\\n\\nSpaceAce can be used with any front-end view library (such as Vue and Angular), but the example below is with React.\\n\\n**index.js**\\n\\n```jsx\\nimport react from \\'react\\';\\nimport ReactDOM from \\'react-dom\\';\\nimport { createSpace, subscribe } from \\'spaceace\\';\\nimport Container from \\'./Container\\';\\n\\n// Create the initial root \"space\" along with its initial state\\nconst rootSpace = createSpace({ name: \\'Jon\\', todos: [] });\\n\\n// Subscribe to any changes that occur within the space\\n// so the app can be re-rendered\\nsubscribe(rootSpace, ({ newSpace, oldSpace, causedBy }) => {\\n  // Example `causedBy`s:\\n  // \\'#addTodo\\', \\'todos[1]#toggleDone\\'\\n  console.log(`Re-render of <Container /> caused by ${causedBy}`);\\n  renderApp(newSpace); //re-render on space change\\n});\\nrenderApp(rootSpace); // initial render\\n\\nfunction renderApp(space) {\\n  ReactDOM.render(\\n    <App space={space} />,\\n    document.getElementById(\\'react-container\\')\\n  );\\n}\\n```\\n\\n**App.js**\\n\\n```jsx\\nimport uuid from \\'uuid/v4\\';\\nimport Todo from \\'./Todo\\';\\n\\nexport default function App({ space }) {\\n  return (\\n    <div>\\n      <h2>{name}\\'s Todos:</h2>\\n      <button onClick={space(addTodo)}>Add Todo</button>\\n      <ul className=\"todos\">\\n        {space.map(todo => <Todo space={todo} key={todo.id} />)}\\n      </ul>\\n    </div>\\n  );\\n}\\n\\nconst addTodo = ({ merge, space }, event) => {\\n  event.preventDefault();\\n\\n  merge({\\n    todos: space.todos.concat({\\n      content: \\'A new TODO\\',\\n      done: false,\\n      id: uuid(),\\n    }),\\n  });\\n};\\n```\\n\\n**Todo.js**\\n\\n```jsx\\n// You can rename the space to something more meaningful\\n// it’s recommended that the prop still be called `space`, so when you’re\\n// working on the parent component, you’ll know it expects a space to be passed in\\nexport default const Todo = ({ space: todo }) => {\\n  const doneClassName = todo.done ? \\'done\\' : \\'\\';\\n\\n  return (\\n    <li>\\n      <form onSubmit={todo(saveTodo)}>\\n        {/*\\n            todo(\\'done\\') sets `todo.done` to true/false when the checkbox’s\\n            onChange is triggered\\n        */}\\n        <input type=\"checkbox\" checked={todo.done} onChange={todo(\\'done\\')} />\\n        {/*\\n          todo(\\'content\\') sets `todo.content` to whatever is typed into\\n          the text input.\\n        */}\\n        <input value={todo.content} onChange={todo(\\'content\\')} />\\n        <button disabled={todo.saving}>Save</button>\\n      </form>\\n    </li>\\n  );\\n};\\n\\nconst saveTodo = async ({ space: todo, merge }, event) => {\\n  event.preventDefault();\\n\\n  merge({ saving: true });\\n\\n  // `todo.toJSON()` returns the space as a simple JS object\\n  // JSON.stringify(space) auto-calls `space.toJSON()` before stringifying\\n  await fetch(\\'/api/todos\\', { method: \\'POST\\', body: todo.toJSON() });\\n\\n  merge({ saving: false });\\n};\\n```\\n\\n## Updating Spaces\\n\\nThere are three ways to update spaces. Each method involves calling the space as a function, the type of value you pass in determines which of the three update methods is used: Immediate updates (object), quick actions (string), and custom actions (function).\\n\\n### Immediate Update (object)\\n\\nImmediately \"changes\" the space by shallowly merging the given object onto the space. A new space is returned with the changes applied and subscribers are immediately invoked.\\n\\n```js\\nconst space = createSpace({ name: \\'Frodo\\', race: \\'Hobbit\\' });\\nconst newSpace = space({ name: \\'Bilbo\\' }); // { name: \\'Bilbo\\', race: \\'Hobbit’ }\\n```\\n\\n### Quick Actions (string)\\n\\nReturns a callback function that will change the specified attribute when called. If the parameter is an _event_, the event’s target’s value will be used, and `event.preventDefault()` will automatically be called. Very useful for `input`, `select`, and `button` elements.\\n\\nIf the event\\'s target is of `input[type=\"number\"]`, the value will be cast to type `number`.\\n\\nIf the event\\'s target is of `input[type=\"checkbox\"]`, the value is taken from the _checked_ property instead, which is a boolean.\\n\\n```jsx\\nexport default const Todo = ({ space: todo }) => {\\n  return (\\n    <li>\\n      <input type=\"checkbox\" checked={todo.done} onChange={todo(\\'done\\')} />\\n      <input value={todo.content} onChange={todo(\\'content\\')} />\\n    </li>\\n  );\\n};\\n```\\n\\n### Custom Actions (function)\\n\\nA custom action is a function that is passed to a space. It returns a wrapped function. When the wrapped function is called, the action is called with a few named parameters (as the first actual parameter). The rest of the parameters are whatever is passed to the wrapped function when it’s eventually called.\\n\\nEvery time you change a space within a custom action, subscribers will be notified. Every change you apply will apply on the latest version of the space.\\n\\n### Action named params\\n\\n[Named params](http://exploringjs.com/es6/ch_parameter-handling.html#sec_named-parameters) passed to custom actions.\\n\\n* **space** – space – The space that the action belongs to. Very useful for applying new values based on existing values.\\n* **rootSpace** – space – The root space that `space` belongs to. Should be used sparingly, as reading or writing to the rootSpace from a child space makes your code more tightly coupled (aka less modular).\\n* **getSpace** — function() -> space – Returns the latest version of the space that this action was called on. Use inside of promises or after any async call to make sure you have this space’s latest version!\\n* **merge** – function(object) -> space – Copies each property from the passed in object onto the space. This is a non-recursive (aka shallow) merge. Returns the new space.\\n* **replace** – function(object) -> space – Replaces the contents of the current space with the object or array you pass to it. Returns the new space.\\n\\n### Promises and Async\\n\\nCustom actions can optionally be async functions. If they\\'re async (i.e. return a promise), the wrapped action will also return a promise, which is resolved when the custom action is resolved. Return values like this are typically only needed when writing tests.\\n\\n**Note**: After an `await`, the `space` that\\'s passed in at the top of the action may be out of date, it’s _highly_ recommended to always use `getSpace()` to get the latest version of the space after an `await` (or inside a callback).\\n\\n### Arrays\\n\\nArrays in a space are proper JS arrays, but are frozen. You cannot update them like you can normal spaces. These arrays do have some similarities to spaces though. You can call `toJSON()` on arrays taken from spaces to get a version of the array containing only plain object literals. If you call `isSpace(…)` on an array taken from a space, it will return true.\\n\\nTo update an array in a space, modify its parent by assigning a fresh new array:\\n\\n```jsx\\nconst changeTodos = ({ merge, space }, itemToRemove) => {\\n  // Add an item to the beginning\\n  space = merge({\\n    todos: [\\n      {\\n        content: \\'A new TODO\\',\\n        done: false,\\n        id: uuid(),\\n      },\\n      ...space.todos\\n    ]\\n  });\\n\\n  // Add an item to the end\\n  space = merge({\\n    todos: [\\n      ...space.todos,\\n      {\\n        content: \\'A new TODO\\',\\n        done: false,\\n        id: uuid(),\\n      }],\\n  });\\n\\n  // Insert an item\\n  space = merge({\\n    todos: [\\n      ...space.todos.slice(0, 2),\\n      {\\n        content: \\'A new TODO\\',\\n        done: false,\\n        id: uuid(),\\n      },\\n      ...space.todos.slice(2)\\n    ],\\n  });\\n\\n  // Remove an item\\n  merge({\\n    todos: space.todos.filter(item => item !== itemToRemove);\\n  })\\n};\\n```\\n\\n## Utility Functions\\n\\n### subscribe\\n\\nAct on, or cancel, state changes.\\n\\nParameters:\\n\\n1. A space that you want to subscribe to.\\n2. A subsriber function that is called whenever the space is updated.\\n\\nThe subscriber function is called with the following named params:\\n\\n* **newSpace**: The new space that was just created.\\n* **oldSpace**: The old version of the space that existing before it was changed.\\n* **causedBy**: A string specifying which part of the space triggered the change, and the name of the action responsible.\\n* **cancel**: A function, when called, prevents future subsribers from being called.\\n\\n```js\\nimport { createSpace, subscribe } from \\'spaceace\\';\\n\\nconst space = createSpace({});\\n\\nsubscribe(space, ({ newSpace, causedBy }) => {\\n  console.log(newSpace.toJSON(), \\'Space updated by \\', causedBy);\\n  renderApp(newSpace);\\n});\\n```\\n\\n* Subscribers can optionally **act as middleware**. If a subscriber returns an object, it will be turned into a space, and future subscribers will receive it as their `newSpace`.\\n* Subscribers are called in the order that they\\'re subscribed, synchronously.\\n* Parent spaces receive the latest returned version after all child space subscribers are called.\\n* All future subscribers can be skipped by calling the passed in named param `cancel`.\\n\\nIf you want your \"middleware\" to not apply changes immediately, but instead do something async and then apply the changes, consider using [`cancel()`](#subscribe), and then applying changes using an [immediate update](#immediate-update-object) when you’re ready.\\n\\n```js\\nsubscribe(space.address, ({ oldSpace, newSpace, cancel }) => {\\n  const stateCode = newSpace.stateCode.toUpperCase();\\n  if (stateCode !== oldSpace.stateCode) {\\n    if (!listOfUSStates.includes(stateCode)) {\\n      // Cancel the update if the provided code is not allowed\\n      cancel();\\n      return;\\n    }\\n\\n    // Alter the specified stateCode to uppercase\\n    return {\\n      ...newSpace,\\n      stateCode,\\n    };\\n  }\\n});\\n\\nsubscribe(space, ({ newSpace }) => {\\n  // Receives `newSpace.address` with `stateCode` uppercase\\n  renderApp(newSpace);\\n});\\n```\\n\\n### isSpace\\n\\nReturns: `true` if the given value is a space, `false` otherwise.\\n\\n```js\\nimport { createSpace, isSpace } from \\'spaceace\\';\\n\\nisSpace(createSpace({})); // returns true\\nisSpace({}); // returns false\\n```\\n\\n### rootOf\\n\\nParameter: A space\\n\\nReturns: The root space associated with the given space.\\n\\n**Note**: This function goes all the way up in the chain of parent spaces, returning the latest root space of the parameter. This means you can safely do `rootOf(space)` anywhere in a custom action, and always get the latest version.\\n\\n```js\\nimport { createSpace, rootOf } from \\'spaceace\\';\\n\\nconst space = createSpace({ user: { name: \\'Frodo\\' } });\\n\\nrootOf(space.user) === space; // true\\n\\nspace.user(({ merge, space }, name) => {\\n  merge({ name });\\n  rootOf(space).user.name === \\'Sam\\'; // true\\n})(\\'Sam\\');\\n```\\n\\n### newestSpace\\n\\nParameter: A space\\n\\nReturns: The newest copy of the space, at the same level.\\n\\n```js\\nimport { createSpace, newestSpace } from \\'spaceace\\';\\n\\nconst space = createSpace({ user: { name: \\'Frodo\\' } });\\nconst latestSnapshot = space.user({ name: \\'Sam\\' });\\n\\nlatestSnapshot.name === \\'Sam\\'; // true\\nnewestSpace(space.user).name === \\'Sam\\'; // true\\n```\\n\\n## toJSON\\n\\nReturns: The contents of a space as an object literal.\\n\\nThis is called automatically by `JSON.stringify`, that’s a built-in feature of JS.\\n\\n```js\\nconst space = createSpace({ user: \\'Frodo\\', todos: [] });\\nspace.toJSON(); // { user: \\'Frodo\\', todos: [] }\\nJSON.stringify(space); // \\'{ \"user\": \"Frodo\", \"todos\": [] }\\'\\n```\\n\\n## Redux DevTools\\n\\nEven though the Redux DevTools broswer extension was originally made for Redux, it works with any immutable store, including SpaceAce!\\n\\nit’s as easy as:\\n\\n1. Install [Redux DevTools](http://extension.remotedev.io/) in your browser.\\n2. Hook up you root space to the extension, if it’s detected:\\n\\n```js\\nconst rootSpace = createSpace({ [pageName]: {}, ...initialState, user });\\n\\nlet sendToDevtools;\\nif (typeof window !== \\'undefined\\' && window.__REDUX_DEVTOOLS_EXTENSION__) {\\n  const devtools = window.__REDUX_DEVTOOLS_EXTENSION__.connect({\\n    name: \\'rootSpace\\',\\n  });\\n  sendToDevtools = devtools.send;\\n  devtools.init(rootSpace);\\n  let devtoolsUpdater = ({ replace }, newState) => replace(newState);\\n  devtoolsUpdater = rootSpace(devtoolsUpdater);\\n  devtools.subscribe(message => {\\n    if (\\n      message.type === \\'DISPATCH\\' &&\\n      [\\'JUMP_TO_ACTION\\', \\'JUMP_TO_STATE\\'].includes(message.payload.type)\\n    ) {\\n      devtoolsUpdater(JSON.parse(message.state));\\n    }\\n  });\\n}\\n\\nsubscribe(rootSpace, ({ newSpace, oldSpace, causedBy }) => {\\n  if (sendToDevtools && causedBy !== \\'#devtoolsUpdater\\') {\\n    sendToDevtools(causedBy, newSpace);\\n  }\\n});\\n```\\n\\n## FAQ\\n\\n**How do I add middleware like in Redux?**\\n\\nI haven\\'t encountered a need for it yet, please add an issue if you’re interested!\\n\\n**Are spaces really immutable?**\\n\\nYes? Ok… you got me.\\n\\nEach space is indeed frozen, all of its child spaces, arrays, and values are also frozen. If a space has changed, you can tell by doing an equality check between both versions, if they\\'re equal to each other, then all their properties are guaranteed to be the identical. it’s safe to pass a space into a [Pure Component](https://reactjs.org/docs/react-api.html#reactpurecomponent), for example.\\n\\nBut! There are a few hidden, non-enumerable, properties on every space (and array) that are mutable. They\\'re not meant to be changed by you, they\\'re used by SpaceAce to track subscribers and newer versions of the same space.\\n\\n**Has this been used in production?**\\n\\nPreviously Yes! It **was** being used at https://www.trustedhealth.com/\\n\\n**Is there an example app?**\\n\\nYup-a-roni! I modified [Redux\\'s sample todo list](https://codesandbox.io/s/github/reduxjs/redux/tree/master/examples/todos) app to use [SpaceAce instead](https://codesandbox.io/s/zl1n53mwwl)\\n\\n**Won’t action names be garbled in product? Can I use constants for action names?**\\n\\nYes, to both. Since most productions code is minified, and the names of custom actions are inferred from their function\\'s name, the action name might end up looking like `a` or `b1` in production.\\n\\nI\\'ve found this to not be a problem since I\\'ve never needed to troubleshoot state issues in production, but if you need this it can fixed by using string constants for action names.\\n\\nFeel free to experiment as to where you store these actions and/or constants.\\n\\n```jsx\\n// actions.js\\n\\nexport const ADD_TODO = \\'ADD_TODO\\';\\n\\nexport const actions = {\\n  [ADD_TODO]({ merge, space }) {\\n    merge({\\n      todos: space.todos.concat({\\n        content: \\'A new TODO\\',\\n        done: false,\\n        id: uuid(),\\n      }),\\n    });\\n  },\\n};\\n\\n// someComponent.js\\n\\nimport { actions, ADD_TODO } from \\'../actions\\';\\n//…\\n<button onClick={space(actions[ADD_TODO])}>…</button>;\\n```\\n\\n## License\\n\\nMIT\\n\\n## Author\\n\\nCreated with the best of intentions by [Jon Abrams](https://twitter.com/JonathanAbrams)\\n'},\n",
       " {'repo': 'ItsCalebJones/SpaceLaunchNow-Android',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '[![discord](https://discordapp.com/api/guilds/380226438584074242/embed.png?style=shield)](https://discord.gg/WVfzEDW) [![Open Source Love](https://badges.frapsoft.com/os/v1/open-source.svg?v=102)](https://github.com/ellerbrock/open-source-badge/) [![Semver](http://img.shields.io/SemVer/2.6.0.png)](http://semver.org/spec/v2.0.0.html)\\n# SpaceLaunchNow\\nA space launch tracker for Android using data from the Launch Library API, the current roadmap can be viewed on [Trello](https://trello.com/b/DwLCfv7g/space-launchCategory-now).\\n\\n## Data Sources\\n\\nA majority of the data is currently sourced from [Launch Libary](https://launchlibrary.net/) a wonderful set of API\\'s and Librarians that are constantly tracking and updating launches around the world. Additionally I utilize [Space Launch Now - Server](https://github.com/ItsCalebJones/SpaceLaunchNow-Server) to provide additional vehicle data and push notifications for launch times.\\n\\n## Translations\\nSpace Launch Now is now translated into five languages, huge thanks to those that have contributed. If you are interested in helping improve the translations feel free to take a look [here](https://spacelaunchnow.oneskyapp.com).\\n\\nThanks to the following translators for their work:\\n\\nFosco85, Francescog91, Ndre85f, Ajtudela, Pedroleon, SwGustav, Ogoidmatos, Ludi.vogt, Bullinger.mathis, Lukas Affolter, Castelle.arnaud, Nem.meric, Arnaud.muller1308, Jaros.jan.j, Jirkatp, Peter.handless\\n\\n## Screenshot\\n\\n![alt tag](https://raw.github.com/caman9119/SpaceLaunchNow/master/screenshot.png)\\n\\n## Setup\\n\\nTo properly build this project you will need to create a few files and add a few extra string keys.\\n\\nNOTE: I will not be able to provide support beyond what is inside of this setup guide. It is on you as a developer to get the correct keys and create the correct accounts to build the project.\\n\\n### Keystore File\\nFrom the root of the project create a keystore.properties file used by both mobile and wear modules.\\n\\nkeystore.properties\\n```\\nstorePassword=yourStorePassword\\nkeyPassword=yourKeyPassword\\nkeyAlias=yourAlias\\nstoreFile=/full/path/to/keystore\\n```\\n\\n### API Keys\\nAdd a api_keys.xml to res/values.\\n```\\n<resources>\\n    <string name=\"wunderground_key\">yourWeatherUndergroundKey</string>\\n    <string name=\"forecast_io_key\">yourForecastIOKey</string>\\n    <string name=\"GoogleMapsKey\">yourGoogleMapsKey</string>\\n    <string name=\"banner_ad_unit_id\">yourAdUnitID</string>\\n    <string name=\"rsa_key\">yourGoogleBillingRSAKey</string>\\n    <string name=\"sln_token\">noNeedToChangeThis</string>\\n</resources>\\n```\\n\\n### version.properties\\nIn the mobile and wear modules add a version.properties or reset AI_VERSION to 0 if it exists.\\n\\n```\\n#Thu Feb 15 11:52:58 EST 2018\\nAI_VERSION_CODE=3\\n```\\n\\n### Replace google-services.json \\nReplace the google-services.json with your own.\\n\\n### AndroidX\\nUpdate `gradle.properties` to include the following lines if you have issues pertaining to AndroidX,\\ne.g. an Adapter or Activity doesn\\'t match the required type:\\n\\n```groovy\\nandroid.useAndroidX=true\\nandroid.enableJetifier=true\\n```\\n\\n### Memory issues when building\\nIf you\\'re seeing stalled command-line builds, or warnings/errors pertaining to running out of memory\\nwhile building, you can increase the amount of memory allocated to Gradle using the following line\\nin `gradle.properties`\\n\\n```groovy\\norg.gradle.jvmargs=-Xmx3g -XX:MaxPermSize=2048m -XX:+HeapDumpOnOutOfMemoryError -Dfile.encoding=UTF-8\\n```\\n\\n## License\\n\\nThis project utilizes the [MIT License](https://raw.github.com/caman9119/SpaceLaunchNow/master/LICENSE.md).\\n'},\n",
       " {'repo': 'spacecraft-repl/SpaceCraft',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': \"[![spacecraft-logo](https://i.imgur.com/f9RQ9GC.png)](https://spacecraft-repl.com)\\n## Overview\\n[SpaceCraft](https://spacecraft-repl.com) is an open-source, real-time, collaborative REPL (Read-Eval-Print Loop) that runs in the browser.\\n\\nCurrently, there are several existing solutions in this problem domain that attempt to allow developers to easily try out unfamiliar languages and provide a REPL-like experience. However, each of these solutions has made decisions that limit their ability to provide a comprehensive experience for developers. Coderpad.io, for example, requires sign-up and only provides a 30-minute demo environment. Repl.it recently announced the launch of its collaborative feature, however it is closed source and requires sign-up. Our team wants to create a free open-source alternative that developers can deploy on their own and use it to explore different languages through a collaborative REPL and code editor.\\n\\nWe currently limit our support to three main languages: Ruby, JavaScript and Python. More languages may be added in the future.\\n\\n## Case Study\\n[Learn more about our project here](https://spacecraft-repl.com/whitepaper), including the challenges we solved by implementing containers, pseudo-terminals, input synchronization, and a reverse proxy.\\n\\n## The Team\\n![gooi](https://i.imgur.com/lBvHH9j.jpg?2)\\n\\n**[Ying Chyi Gooi](https://gooi.tech) - New York City, NY**\\n\\n![nick](https://i.imgur.com/2atacXb.jpg?2)\\n\\n**[Nick Johnson](https://njohnson7.github.io) - San Francisco, CA**\\n\\n![julius](https://i.imgur.com/FUQCN67.jpg?2)\\n\\n**[Julius Zerwick](https://rouxcaesar.github.io/) - New York City, NY**\\n\\n\\n## Deploying with Heroku\\nMake sure that you are signed-in to Heroku in your browser, then deploy using the one-click button below:\\n\\n[![Deploy](https://www.herokucdn.com/deploy/button.svg)](https://heroku.com/deploy)\\n\\n*Note: this will deploy the latest `master` branch from this repository.*\\n\\n## Local Setup\\nMake sure you have Node.js installed. Then, run:\\n\\n```\\nnpm install\\n```\\n\\nOnce the dependencies are installed, run:\\n\\n```\\nnpm start\\n```\\n\\nNavigate to `http://localhost:3000/` to start the app.\\n\\n*Note: in order for Ruby and Python REPLs to run properly, you need to have those runtimes installed in your system.*\\n\\n## Deploying with Docker\\nMake sure you have Docker installed in your host system. Then, clone this repository, navigate to the root path of the project folder, and then run:\\n\\n```\\ndocker build -t spacecraft-app .\\n```\\nNote: `spacecraft-app` can be replaced by a name of your choice.\\n\\nOnce Docker is finished with building the image, run the following command to verify that `spacecraft-app` exists and has been built successfully.\\n\\n```\\ndocker images\\n```\\nIf the build succeeds, execute the following command to launch the application:\\n\\n```\\ndocker run -p 80:3000 -d spacecraft-app\\n```\\nOnce that is done, navigate to `localhost` to view the app. If running on a remote host however, you'll need to request the remote host IP in order to connect with the app. \\n\\nFor remote host only: if there's issues with connecting to the app, make sure to check your remote host's firewall settings to allow port 80 to be accessible. Example: `ufw allow 80`\\n\\n\\n\"},\n",
       " {'repo': 'rebassjs/space',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': \"\\n# @rebass/space\\n\\n*Formerly styled-space*\\n\\nReact component for applying responsive margin and padding to child elements without a wrapping HTML container.\\nBuilt with [styled-components][sc] and [styled-system][]\\n\\n```sh\\nnpm i @rebass/space\\n```\\n\\n```js\\nimport React from 'react'\\nimport Space from '@rebass/space'\\n\\n// Apply margin to child components without a wrapping <div>\\nconst App = props => (\\n  <Space mx={3} my={[ 2, 3 ]}>\\n    <h1>Hello</h1>\\n    <h2>Hi</h2>\\n    <button>Beep</button>\\n  </Space>\\n)\\n```\\n\\n## Props\\n\\nThe Space component uses [styled-system's][sys] `space` utility to add margin and padding props.\\n\\nProp | Description | Type\\n---|---|---\\n`m` | margin | number, string, or array\\n`mt` | margin-top | number, string, or array\\n`mr` | margin-right | number, string, or array\\n`mb` | margin-bottom | number, string, or array\\n`ml` | margin-left | number, string, or array\\n`mx` | margin x-axis (left and right) | number, string, or array\\n`my` | margin y-axis (top and bottom) | number, string, or array\\n`p` | padding | number, string, or array\\n`pt` | padding-top | number, string, or array\\n`pr` | padding-right | number, string, or array\\n`pb` | padding-bottom | number, string, or array\\n`pl` | padding-left | number, string, or array\\n`px` | padding x-axis (left and right) | number, string, or array\\n`py` | padding y-axis (top and bottom) | number, string, or array\\n\\nMIT License\\n\\n[sc]: https://github.com/styled-components/styled-components\\n[styled-system]: https://styled-system.com\\n\"},\n",
       " {'repo': 'd3/d3-color',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# d3-color\\n\\nEven though your browser understands a lot about colors, it doesn’t offer much help in manipulating colors through JavaScript. The d3-color module therefore provides representations for various color spaces, allowing specification, conversion and manipulation. (Also see [d3-interpolate](https://github.com/d3/d3-interpolate) for color interpolation.)\\n\\nFor example, take the color named “steelblue”:\\n\\n```js\\nconst c = d3.color(\"steelblue\"); // {r: 70, g: 130, b: 180, opacity: 1}\\n```\\n\\nLet’s try converting it to HSL:\\n\\n```js\\nconst c = d3.hsl(\"steelblue\"); // {h: 207.27…, s: 0.44, l: 0.4902…, opacity: 1}\\n```\\n\\nNow rotate the hue by 90°, bump up the saturation, and format as a string for CSS:\\n\\n```js\\nc.h += 90;\\nc.s += 0.2;\\nc + \"\"; // rgb(198, 45, 205)\\n```\\n\\nTo fade the color slightly:\\n\\n```js\\nc.opacity = 0.8;\\nc + \"\"; // rgba(198, 45, 205, 0.8)\\n```\\n\\nIn addition to the ubiquitous and machine-friendly [RGB](#rgb) and [HSL](#hsl) color space, d3-color supports color spaces that are designed for humans:\\n\\n* [CIELAB](#lab) (*a.k.a.* “Lab”)\\n* [CIELCh<sub>ab</sub>](#lch) (*a.k.a.* “LCh” or “HCL”)\\n* Dave Green’s [Cubehelix](#cubehelix)\\n\\nCubehelix features monotonic lightness, while CIELAB and its polar form CIELCh<sub>ab</sub> are perceptually uniform.\\n\\n## Extensions\\n\\nFor additional color spaces, see:\\n\\n* [d3-cam16](https://github.com/d3/d3-cam16)\\n* [d3-cam02](https://github.com/connorgr/d3-cam02)\\n* [d3-hsv](https://github.com/d3/d3-hsv)\\n* [d3-hcg](https://github.com/d3/d3-hcg)\\n* [d3-hsluv](https://github.com/petulla/d3-hsluv)\\n\\nTo measure color differences, see:\\n\\n* [d3-color-difference](https://github.com/Evercoder/d3-color-difference)\\n\\n## Installing\\n\\nIf you use npm, `npm install d3-color`. You can also download the [latest release on GitHub](https://github.com/d3/d3-color/releases/latest). For vanilla HTML in modern browsers, import d3-color from Skypack:\\n\\n```html\\n<script type=\"module\">\\n\\nimport {rgb} from \"https://cdn.skypack.dev/d3-color@3\";\\n\\nconst steelblue = d3.rgb(\"steelblue\");\\n\\n</script>\\n```\\n\\nFor legacy environments, you can load d3-color’s UMD bundle from an npm-based CDN such as jsDelivr; a `d3` global is exported:\\n\\n```html\\n<script src=\"https://cdn.jsdelivr.net/npm/d3-color@3\"></script>\\n<script>\\n\\nconst steelblue = d3.rgb(\"steelblue\");\\n\\n</script>\\n```\\n\\n[Try d3-color in your browser.](https://observablehq.com/collection/@d3/d3-color)\\n\\n## API Reference\\n\\n<a name=\"color\" href=\"#color\">#</a> d3.<b>color</b>(<i>specifier</i>) [<>](https://github.com/d3/d3-color/blob/main/src/color.js \"Source\")\\n\\nParses the specified [CSS Color Module Level 3](http://www.w3.org/TR/css3-color/#colorunits) *specifier* string, returning an [RGB](#rgb) or [HSL](#hsl) color, along with [CSS Color Module Level 4 hex](https://www.w3.org/TR/css-color-4/#hex-notation) *specifier* strings. If the specifier was not valid, null is returned. Some examples:\\n\\n* `rgb(255, 255, 255)`\\n* `rgb(10%, 20%, 30%)`\\n* `rgba(255, 255, 255, 0.4)`\\n* `rgba(10%, 20%, 30%, 0.4)`\\n* `hsl(120, 50%, 20%)`\\n* `hsla(120, 50%, 20%, 0.4)`\\n* `#ffeeaa`\\n* `#fea`\\n* `#ffeeaa22`\\n* `#fea2`\\n* `steelblue`\\n\\nThe list of supported [named colors](http://www.w3.org/TR/SVG/types.html#ColorKeywords) is specified by CSS.\\n\\nNote: this function may also be used with `instanceof` to test if an object is a color instance. The same is true of color subclasses, allowing you to test whether a color is in a particular color space.\\n\\n<a name=\"color_opacity\" href=\"#color_opacity\">#</a> *color*.<b>opacity</b>\\n\\nThis color’s opacity, typically in the range [0, 1].\\n\\n<a name=\"color_rgb\" href=\"#color_rgb\">#</a> *color*.<b>rgb</b>() [<>](https://github.com/d3/d3-color/blob/main/src/color.js \"Source\")\\n\\nReturns the [RGB equivalent](#rgb) of this color. For RGB colors, that’s `this`.\\n\\n<a name=\"color_copy\" href=\"#color_copy\">#</a> <i>color</i>.<b>copy</b>([<i>values</i>]) [<>](https://github.com/d3/d3-color/blob/main/src/color.js \"Source\")\\n\\nReturns a copy of this color. If *values* is specified, any enumerable own properties of *values* are assigned to the new returned color. For example, to derive a copy of a *color* with opacity 0.5, say\\n\\n```js\\ncolor.copy({opacity: 0.5})\\n```\\n\\n<a name=\"color_brighter\" href=\"#color_brighter\">#</a> *color*.<b>brighter</b>([<i>k</i>]) [<>](https://github.com/d3/d3-color/blob/main/src/color.js \"Source\")\\n\\nReturns a brighter copy of this color. If *k* is specified, it controls how much brighter the returned color should be. If *k* is not specified, it defaults to 1. The behavior of this method is dependent on the implementing color space.\\n\\n<a name=\"color_darker\" href=\"#color_darker\">#</a> *color*.<b>darker</b>([<i>k</i>]) [<>](https://github.com/d3/d3-color/blob/main/src/color.js \"Source\")\\n\\nReturns a darker copy of this color. If *k* is specified, it controls how much darker the returned color should be. If *k* is not specified, it defaults to 1. The behavior of this method is dependent on the implementing color space.\\n\\n<a name=\"color_displayable\" href=\"#color_displayable\">#</a> *color*.<b>displayable</b>() [<>](https://github.com/d3/d3-color/blob/main/src/color.js \"Source\")\\n\\nReturns true if and only if the color is displayable on standard hardware. For example, this returns false for an RGB color if any channel value is less than zero or greater than 255 when rounded, or if the opacity is not in the range [0, 1].\\n\\n<a name=\"color_formatHex\" href=\"#color_formatHex\">#</a> *color*.<b>formatHex</b>() [<>](https://github.com/d3/d3-color/blob/main/src/color.js \"Source\")\\n\\nReturns a hexadecimal string representing this color in RGB space, such as `#f7eaba`. If this color is not displayable, a suitable displayable color is returned instead. For example, RGB channel values greater than 255 are clamped to 255.\\n\\n<a name=\"color_formatHex8\" href=\"#color_formatHex8\">#</a> *color*.<b>formatHex8</b>() [<>](https://github.com/d3/d3-color/blob/main/src/color.js \"Source\")\\n\\nReturns a hexadecimal string representing this color in RGBA space, such as `#f7eaba90`. If this color is not displayable, a suitable displayable color is returned instead. For example, RGB channel values greater than 255 are clamped to 255.\\n\\n<a name=\"color_formatHsl\" href=\"#color_formatHsl\">#</a> *color*.<b>formatHsl</b>() [<>](https://github.com/d3/d3-color/blob/main/src/color.js \"Source\")\\n\\nReturns a string representing this color according to the [CSS Color Module Level 3 specification](https://www.w3.org/TR/css-color-3/#hsl-color), such as `hsl(257, 50%, 80%)` or `hsla(257, 50%, 80%, 0.2)`. If this color is not displayable, a suitable displayable color is returned instead by clamping S and L channel values to the interval [0, 100].\\n\\n<a name=\"color_formatRgb\" href=\"#color_formatRgb\">#</a> *color*.<b>formatRgb</b>() [<>](https://github.com/d3/d3-color/blob/main/src/color.js \"Source\")\\n\\nReturns a string representing this color according to the [CSS Object Model specification](https://drafts.csswg.org/cssom/#serialize-a-css-component-value), such as `rgb(247, 234, 186)` or `rgba(247, 234, 186, 0.2)`. If this color is not displayable, a suitable displayable color is returned instead by clamping RGB channel values to the interval [0, 255].\\n\\n<a name=\"color_toString\" href=\"#color_toString\">#</a> *color*.<b>toString</b>() [<>](https://github.com/d3/d3-color/blob/main/src/color.js \"Source\")\\n\\nAn alias for [*color*.formatRgb](#color_formatRgb).\\n\\n<a name=\"rgb\" href=\"#rgb\">#</a> d3.<b>rgb</b>(<i>r</i>, <i>g</i>, <i>b</i>[, <i>opacity</i>]) [<>](https://github.com/d3/d3-color/blob/main/src/color.js \"Source\")<br>\\n<a href=\"#rgb\">#</a> d3.<b>rgb</b>(<i>specifier</i>)<br>\\n<a href=\"#rgb\">#</a> d3.<b>rgb</b>(<i>color</i>)<br>\\n\\nConstructs a new [RGB](https://en.wikipedia.org/wiki/RGB_color_model) color. The channel values are exposed as `r`, `g` and `b` properties on the returned instance. Use the [RGB color picker](http://bl.ocks.org/mbostock/78d64ca7ef013b4dcf8f) to explore this color space.\\n\\nIf *r*, *g* and *b* are specified, these represent the channel values of the returned color; an *opacity* may also be specified. If a CSS Color Module Level 3 *specifier* string is specified, it is parsed and then converted to the RGB color space. See [color](#color) for examples. If a [*color*](#color) instance is specified, it is converted to the RGB color space using [*color*.rgb](#color_rgb). Note that unlike [*color*.rgb](#color_rgb) this method *always* returns a new instance, even if *color* is already an RGB color.\\n\\n<a name=\"rgb_clamp\" href=\"#rgb_clamp\">#</a> *rgb*.<b>clamp</b>() [<>](https://github.com/d3/d3-color/blob/main/src/color.js \"Source\")\\n\\nReturns a new RGB color where the `r`, `g`, and `b` channels are clamped to the range [0, 255] and rounded to the nearest integer value, and the `opacity` is clamped to the range [0, 1].\\n\\n<a name=\"hsl\" href=\"#hsl\">#</a> d3.<b>hsl</b>(<i>h</i>, <i>s</i>, <i>l</i>[, <i>opacity</i>]) [<>](https://github.com/d3/d3-color/blob/main/src/color.js \"Source\")<br>\\n<a href=\"#hsl\">#</a> d3.<b>hsl</b>(<i>specifier</i>)<br>\\n<a href=\"#hsl\">#</a> d3.<b>hsl</b>(<i>color</i>)<br>\\n\\nConstructs a new [HSL](https://en.wikipedia.org/wiki/HSL_and_HSV) color. The channel values are exposed as `h`, `s` and `l` properties on the returned instance. Use the [HSL color picker](http://bl.ocks.org/mbostock/debaad4fcce9bcee14cf) to explore this color space.\\n\\nIf *h*, *s* and *l* are specified, these represent the channel values of the returned color; an *opacity* may also be specified. If a CSS Color Module Level 3 *specifier* string is specified, it is parsed and then converted to the HSL color space. See [color](#color) for examples. If a [*color*](#color) instance is specified, it is converted to the RGB color space using [*color*.rgb](#color_rgb) and then converted to HSL. (Colors already in the HSL color space skip the conversion to RGB.)\\n\\n<a name=\"hsl_clamp\" href=\"#hsl_clamp\">#</a> *hsl*.<b>clamp</b>() [<>](https://github.com/d3/d3-color/blob/main/src/color.js \"Source\")\\n\\nReturns a new HSL color where the `h` channel is clamped to the range [0, 360), and the `s`, `l`, and `opacity` channels are clamped to the range [0, 1].\\n\\n<a name=\"lab\" href=\"#lab\">#</a> d3.<b>lab</b>(<i>l</i>, <i>a</i>, <i>b</i>[, <i>opacity</i>]) [<>](https://github.com/d3/d3-color/blob/main/src/lab.js \"Source\")<br>\\n<a href=\"#lab\">#</a> d3.<b>lab</b>(<i>specifier</i>)<br>\\n<a href=\"#lab\">#</a> d3.<b>lab</b>(<i>color</i>)<br>\\n\\nConstructs a new [CIELAB](https://en.wikipedia.org/wiki/Lab_color_space#CIELAB) color. The channel values are exposed as `l`, `a` and `b` properties on the returned instance. Use the [CIELAB color picker](http://bl.ocks.org/mbostock/9f37cc207c0cb166921b) to explore this color space. The value of *l* is typically in the range [0, 100], while *a* and *b* are typically in [-160, +160].\\n\\nIf *l*, *a* and *b* are specified, these represent the channel values of the returned color; an *opacity* may also be specified. If a CSS Color Module Level 3 *specifier* string is specified, it is parsed and then converted to the CIELAB color space. See [color](#color) for examples. If a [*color*](#color) instance is specified, it is converted to the RGB color space using [*color*.rgb](#color_rgb) and then converted to CIELAB. (Colors already in the CIELAB color space skip the conversion to RGB, and colors in the HCL color space are converted directly to CIELAB.)\\n\\n<a name=\"gray\" href=\"#gray\">#</a> d3.<b>gray</b>(<i>l</i>[, <i>opacity</i>]) [<>](https://github.com/d3/d3-color/blob/main/src/lab.js \"Source\")<br>\\n\\nConstructs a new [CIELAB](#lab) color with the specified *l* value and *a* = *b* = 0.\\n\\n<a name=\"hcl\" href=\"#hcl\">#</a> d3.<b>hcl</b>(<i>h</i>, <i>c</i>, <i>l</i>[, <i>opacity</i>]) [<>](https://github.com/d3/d3-color/blob/main/src/lab.js \"Source\")<br>\\n<a href=\"#hcl\">#</a> d3.<b>hcl</b>(<i>specifier</i>)<br>\\n<a href=\"#hcl\">#</a> d3.<b>hcl</b>(<i>color</i>)<br>\\n\\nEquivalent to [d3.lch](#lch), but with reversed argument order.\\n\\n<a name=\"lch\" href=\"#lch\">#</a> d3.<b>lch</b>(<i>l</i>, <i>c</i>, <i>h</i>[, <i>opacity</i>]) [<>](https://github.com/d3/d3-color/blob/main/src/lab.js \"Source\")<br>\\n<a href=\"#lch\">#</a> d3.<b>lch</b>(<i>specifier</i>)<br>\\n<a href=\"#lch\">#</a> d3.<b>lch</b>(<i>color</i>)<br>\\n\\nConstructs a new [CIELCh<sub>ab</sub>](https://en.wikipedia.org/wiki/CIELAB_color_space#Cylindrical_representation:_CIELCh_or_CIEHLC) color. The channel values are exposed as `l`, `c` and `h` properties on the returned instance. Use the [CIELCh<sub>ab</sub> color picker](http://bl.ocks.org/mbostock/3e115519a1b495e0bd95) to explore this color space. The value of *l* is typically in the range [0, 100], *c* is typically in [0, 230], and *h* is typically in [0, 360).\\n\\nIf *l*, *c*, and *h* are specified, these represent the channel values of the returned color; an *opacity* may also be specified. If a CSS Color Module Level 3 *specifier* string is specified, it is parsed and then converted to CIELCh<sub>ab</sub> color space. See [color](#color) for examples. If a [*color*](#color) instance is specified, it is converted to the RGB color space using [*color*.rgb](#color_rgb) and then converted to CIELCh<sub>ab</sub>. (Colors already in CIELCh<sub>ab</sub> color space skip the conversion to RGB, and colors in CIELAB color space are converted directly to CIELCh<sub>ab</sub>.)\\n\\n<a name=\"cubehelix\" href=\"#cubehelix\">#</a> d3.<b>cubehelix</b>(<i>h</i>, <i>s</i>, <i>l</i>[, <i>opacity</i>]) [<>](https://github.com/d3/d3-color/blob/main/src/cubehelix.js \"Source\")<br>\\n<a href=\"#cubehelix\">#</a> d3.<b>cubehelix</b>(<i>specifier</i>)<br>\\n<a href=\"#cubehelix\">#</a> d3.<b>cubehelix</b>(<i>color</i>)<br>\\n\\nConstructs a new [Cubehelix](http://www.mrao.cam.ac.uk/~dag/CUBEHELIX/) color. The channel values are exposed as `h`, `s` and `l` properties on the returned instance. Use the [Cubehelix color picker](http://bl.ocks.org/mbostock/ba8d75e45794c27168b5) to explore this color space.\\n\\nIf *h*, *s* and *l* are specified, these represent the channel values of the returned color; an *opacity* may also be specified. If a CSS Color Module Level 3 *specifier* string is specified, it is parsed and then converted to the Cubehelix color space. See [color](#color) for examples. If a [*color*](#color) instance is specified, it is converted to the RGB color space using [*color*.rgb](#color_rgb) and then converted to Cubehelix. (Colors already in the Cubehelix color space skip the conversion to RGB.)\\n'},\n",
       " {'repo': 'JohnEdChristensen/WebbCompare',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '![](img/WebbCompareLogo.png)\\n\\nHow much more powerful is the James Webb Space Telescope when compared to Hubble?\\n\\n[Find out!](https://johnedchristensen.github.io/WebbCompare)\\n![](img/Example.png)\\n### More info\\n\\nWant to help out? [CONTRIBUTING.md](CONTRIBUTING.md)\\n\\nBlog post with more info about this tool: https://johnedchristensen.github.io/notes/WebbCompare/\\n\\n\\n\\n### Resources\\n- Zoom and pan using OpenSeadragon https://openseadragon.github.io/\\n- Special thanks to Illya Moskvin for the OpenSeadragon slider implementation https://codepen.io/imoskvin/pen/yOXqvO\\n- Images from NASA https://www.nasa.gov/webbfirstimages'},\n",
       " {'repo': 'lelouchB/final-space-api',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': \"# Final Space API\\n\\n[![CodeFactor](https://www.codefactor.io/repository/github/lelouchb/final-space-api/badge)](https://www.codefactor.io/repository/github/lelouchb/final-space-api)\\n[![DeepScan grade](https://deepscan.io/api/teams/11524/projects/14429/branches/268739/badge/grade.svg)](https://deepscan.io/dashboard#view=project&tid=11524&pid=14429&bid=268739)\\n[![Language grade: JavaScript](https://img.shields.io/lgtm/grade/javascript/g/lelouchB/final-space-api.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/lelouchB/final-space-api/context:javascript)\\n[![Total alerts](https://img.shields.io/lgtm/alerts/g/lelouchB/final-space-api.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/lelouchB/final-space-api/alerts/)\\n![CodeQL](https://github.com/lelouchB/final-space-api/workflows/CodeQL/badge.svg)\\n\\n[![Final Space API](https://raw.githubusercontent.com/lelouchB/lelouchB/master/wallpaper.jpg)](https://finalspaceapi.com)\\n\\n![](https://img.shields.io/badge/Maintained-Yes-orange)\\n![](https://img.shields.io/badge/PRs-Accepting-brightgreen)\\n![](https://img.shields.io/github/issues/lelouchB/final-space-api)\\n![](https://img.shields.io/github/contributors/lelouchB/final-space-api)\\n![](https://img.shields.io/github/issues-pr/lelouchB/final-space-api)\\n![](https://img.shields.io/github/license/lelouchB/final-space-api)\\n\\nThe Final Space API is a RESTful API based on the television show [Final Space](https://en.wikipedia.org/wiki/Final_Space). All the data is taken from [Final Space wiki](https://final-space.fandom.com/wiki/Final_Space_Wiki).\\n\\n## Project Structure\\n```\\nfinal-space-api\\n├───.github\\n│   ├───ISSUE_TEMPLATE\\n│   └───workflows\\n├───backend\\n│   ├───config\\n│   ├───controllers\\n│   ├───helpers\\n│   ├───images\\n│   │   ├───character\\n│   │   ├───episode\\n│   │   └───location\\n│   ├───models\\n│   └───routes\\n└───frontend\\n    ├───blog\\n    ├───docs\\n    ├───src\\n    │   ├───css\\n    │   └───pages\\n    │       └───Components\\n    │           ├───Body\\n    │           └───Head\\n    └───static\\n        └───img\\n            ├───assets\\n            └───pwa\\n                ├───android\\n                ├───chrome\\n                ├───firefox\\n                └───msteams\\n```\\n\\n## Installation ⭐\\n\\nFirst, Clone this repo. Both the frontend and the backend need to be installed seperately.\\n```bash\\ngit clone https://github.com/lelouchB/final-space-api.git\\n```\\n\\nFor **frontend** run the following commands.\\n \\n ```bash\\n cd frontend\\n npm install\\n npm run build\\n ```\\n It is important that frontend is installed first and a `build` directory is created since the backend serves this `build` folder. \\n \\n For **backend** run the following commands.\\n \\n ```bash\\n cd backend\\n npm install\\n ```\\n ## Starting the Development Server\\n \\n Run the following command in the backend directory to start the development server at `PORT=8000` by default.\\n \\n ```bash\\n cd backend\\n npm run dev\\n ```\\n \\n This will start the development server at [http://localhost:8000](http://localhost:8000).\\n **Note :** If `build` folder doen't exist you will see an error at the baseURL i.e. `/`. Create the `build` folder to fix this issue.\\n \\n ## Get Contributing 🤩\\n First things first, In order to contribute you have to create a Pull Request from your forked repo which is a remote clone of this upstream repository.\\n \\n 1. Click this button at the top of screen to fork this repo, **don't forget to star the              repository!** ⭐⭐\\n    ![form-button](https://github-images.s3.amazonaws.com/help/bootcamp/Bootcamp-Fork.png)\\n\\n2. Next, clone this repository using\\n    ```bash\\n    git clone https://github.com/lelouchB/final-space-api.git\\n    ```\\n\\n3. It is critical to keep your forked repository in sync with the upstream repository so merge           conflicts can be avoided:\\n    ```bash\\n    git remote add upstream https://github.com/lelouchB/final-space-api.git\\n    git fetch upstream\\n    git pull upstream main\\n    git push\\n    ```\\n\\n4. Create a new branch to work upon\\n    The branch name must be selected according to the issue\\n    ```bash\\n    git checkout -b <branch-name>\\n    ```\\n\\n5. After the contribution work is ready, go ahead and add it to the staging area:\\n    ```bash\\n    git add -A\\n    ```\\n\\n6. Now it is time to commit your changes and sync these changes to the forked repo:\\n    ```bash\\n    git commit -m <your_message>\\n    git push origin <branch-name>\\n    ```\\n    **Note :** Branch Name is the branch you created earlier\\n\\n7. Issue a pull request from forked repo to this repo by clicking on ``New Pull Request``:\\n    ![](https://guides.github.com/activities/hello-world/create-pr.png)\\n\\n8. Fill in the title and provide a concise description.\\n9. Wait for respose on the PR. Congratulations you just contributed to open source! 👏👏\\n\\n### Contributing to Frontend\\n \\n The frontend of this project is made with [Docusaurus](https://v2.docusaurus.io/docs/) and [React](https://reactjs.org/).\\n To start the dev server, run the following commands in `frontend` directory:\\n \\n ```bash\\n cd frontend\\n npm start\\n ```\\n \\n This will start the frontend dev server on [http://localhost:3000](http://localhost:3000).\\n \\nAfter you have done with your contribution, create the production build by running the following commands:\\n\\n```bash\\ncd frontend\\nnpm run build\\nnpm run serve\\n```\\nThe last command will server the build folder on [http://localhost:3000](http://localhost:3000).\\n\\nMake sure your contributions are reflected properly in this build.\\n\\n**For more details related to installation and requirements please check the README in Frontend directory.** \\n\\n### Contributing to Backend\\n\\nThe backend is made with **NodeJS**, **Express** and **MongoDB**.\\nRun the following commands to open the dev server.\\n\\n```bash\\ncd backend\\nnpm run dev\\n```\\n\\nHead to [http://localhost:8000](http://localhost:8000), if a `build` folder is present in `frontend` directory then you will see the landing page of the project.\\n\\n**For more details related to installation and requirements please check the README in Backend directory.** \\n\\n## License\\nFinal Space is created by Olan Rogers for [TBS](https://www.tbs.com/) and later picked by [Adult Swim](https://www.adultswim.com). The data and images are used without claim of ownership and belong to their respective owners.\\n\\nThis API is open source and uses a [BSD license](/LICENSE).\\n \\n\"},\n",
       " {'repo': 'chaoscollective/Space_Editor',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': \"# Space is a real-time collaborative code editor!\\n\\nSpace is built on NodeJS and uses NowJS under the hood to support websockets for realtime collaboration. The editor is built on ACE (the same front-end used in Cloud9 IDE) and uses Google's diff-match-patch to send edits information to contributors as changes are made to the code.\\n\\n## Demo\\n\\nTry out Space for yourself on the demo site here:\\nhttp://spacedemo.chaoscollective.org/?project=SandboxApp\\n\\n## More Details\\n\\nCheck out the Space overview page here for more details and a video:\\nhttp://chaoscollective.org/projects/builtinspace.html\\n\\n## Libraries/Platforms used\\n - NodeJS\\n - NowJS\\n - ACE\\n - diff-match-patch\\n \\n## What's Next?\\n\\nWe took a vote, and the results are in. The community wants Space to be open source, so now it is!\\n\\nBut making Space open source is only half the battle, now we need to take it to a more robust and scalable level. Feel free to fork Space, make suggestion, and spread the word. :)\\n\\nHappy coding!\\n\\nxoxo,\\nthe Chaos Collective\\n\"},\n",
       " {'repo': 'lordofduct/spacepuppy-unity-framework-3.0',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# TODO OBSOLETE - See Spacepuppy Unity Framework 4.0\\nhttps://github.com/lordofduct/spacepuppy-unity-framework-4.0\\n\\nThis version of spacepuppy is now obsolete. We\\'ve moved to a newer version of Unity that supports the newer C# versions in the compiler.\\n\\nThis framework will remain for posterity sake.\\n\\n# spacepuppy-unity-framwork-3.0\\nA modular framework of tools for use with the Unity game engine version 2017.3.\\n\\nThis framework starts with the base portion of SpaceuppyUnityFramework which contains all the general tools needed by each module. All subsequent modules require this dll to be included.\\n\\nThe base framework includes a number of types. Several of them are extensions or replacements of similar Unity types. Often I designed these tools years before Unity added their versions (a Coroutine object token, custom yield instructions, UnityEvent). Even though Unity has added/improved their versions of these objects, I still prefer my versions as they often have more features than the built-in Unity ones.\\n\\n- SPComponent: a more robust version of MonoBehaviour\\n- RadicalCoroutine: an extension of the built in Unity Coroutine. It adds events/pausing/scheduling/custom yield instructions.\\n- RadicalTask: multi-threaded coroutines (async/await is planned to replace this feature)\\n- SPEvent: Similar to UnityEvent. This was designed prior to UnityEvent existing, and as a result has a different interface to it, and as a result can perform many tasks that UnityEvent can not.\\n- SPEntity: an entity structure to relate multiple GameObjects together as a single entity.\\n- MultiTag: add more than 1 tag to any given GameObject.\\n- VariantReference: a dynamic/variant data type that allows selecting the type & value through the inspector for a given serialized property\\n- Collections: several collection types like BinaryHeap, MultitonPool, ObjectCachePool, etc.\\n\\nAfter adding the SpaceuppyUnityFramework you can pick and choose the modules to include with it for those tools sets.\\n\\n- SPAnim - an extension of the Unity Legacy Animation system. If you don\\'t like mecanim (like me), but find the legacy animation system a bit lacking. This perks it up a bit.\\n\\n- SPCamera - a uniform interface and manager for cameras and their effects. One feature of this interface is that it facilitates treating a group of cameras as one. For example if you wanted to make a racing game and have multiple view modes for it. Your ICamera script can handle the swapping of actual cameras, but itself be accessed by all other scripts as if it were the camera regardless of which is active.\\n\\n- SPInput - An input library that defines input devices as a IPlayerInputDevice interface. Can be integrated with built in UnityInput, as well with 3rd party.\\n\\n- SPMotor - motor scripts to facilitate movement of player and enemies. This includes a generalized interface IMotor so you can handle Rigidbody and CharacterController as a uniform type. As well as a state machine for movement styles.\\n\\n- SPPathfinding - currently is in early development, but is planned to house a robust pathfinding (A*) system for use in games. Currently only houses basic implementations of the algorithms as well as fundamental interface/contracts for use with AI agents.\\n\\n- SPProject - A collection of classes intended to make project/asset management easier.\\n\\n- SPScenes - an extension of SceneManager adopting the IService model.\\n\\n- SPSensors - attach an \\'aspect\\' to an object, and then your AI/Player can use a \\'sensor\\' of various shapes/types to determine if said aspect can be seen.\\n\\n- SPSerialization - a serialization library built on top of the .Net serialization interface. It supports various formats (json/binary included), and attempts to allow serializing GameObjects as asset id\\'s that can be pulled from Resources/AssetBundles.\\n\\n- SPSpawn - A Spawn Pool Library\\n\\n- SPTriggers - (requires SPTween) A feature of SPEvent that UnityEvent does not contain is an interface for simple visual programming by using GameObjects as trigger nodes in a chain of commands. SPTriggers is a collection of reusable commands such as \\'T_OnStart\\', \\'I_SetValue\\', \\'I_Destroy\\', and many more. We have found these tools to be very useful for creating scenarios in game. Note, various other modules contain commands specific to their module. SPAnim has a \\'I_PlayAnimation\\', SPTween has \\'I_Tween\\', and SPWaypoint has \\'I_MoveOnPath\\'.\\n\\n- SPTween - a tween engine built on Spacepuppy\\n\\n- SPUtils - (requires SPTween) some useful utility classes. These used to be in SpacepuppyUnityFramework base dll, but was moved here to reduce the size of that dll since they aren\\'t necessary for any module to work.\\n\\n- SPWaypoint - (requires SPTween) a waypoint library with algorithms for bezier, catmull-rom, linear, as well as a UI to set up paths in your game. This is very useful for setting up camera paths and other animated events. (warning - this is not a pathfinding system, that is in development in another module)\\n\\n# Quick Import\\n\\nDownload the latest build from the github project and unzip contents into your project\\'s Asset folder.\\n\\n# Quick Build\\n\\nFirst open \\'dobuild.release.bat\\' and make sure the path to MSBuild.exe matches where you have it installed on your computer.\\n\\nRun \\'dobuild.release.bat\\'.\\n\\nA \\'Builds\\' folder will be created with a \\'SpaceuppyUnityFramework\\' inside of it.\\n\\nDelete any module\\'s dll\\'s you don\\'t want from inside the \\'SpaceuppyUnityFramework\\' folder. You could also have commented out the xcopy lines for any modules you don\\'t want in the bat file.\\n\\nCopy the \\'SpaceuppyUnityFramework\\' folder in there into your project wherever you\\'d like (this is the same thing you would find in the downloaded builds from the github page).\\n\\n# Manual Build\\n\\nOpen the SpacepuppyUnityFramework.sln in Visual Studio (or other suitable IDE) and select Build like you would any project.\\n\\nTraverse through each module\\'s output directories (%module%/bin/Release) and copy the appropriate *.dll for that module into an appropriate folder in Assets. I prefer to name mine \\'Assets/SpaceuppyUnityFramework\\'.\\n\\nTraverse through each module\\'s editor output directories (%module%editor/bin/Release) and copy the appropriate *.dll for that module editor into an appropriate Editor folder in Assets. I prefer to name mine \\'Assets/SpacepuppyUnityFramework/Editor\\'.\\n\\nGo into the \\'Resources\\' folder.\\n\\nIf you use the SPSensors module, copy \\'Shaders\\' folder into the previously created editor folder for the editor script dll\\'s.\\n\\nMake sure you have \\'Visible Meta Files\\' enabled in \\'Edit->Project Settings->Editor Settings\\'.\\n\\nCopy \\'SpacepuppyUnityFramework.dll.meta\\' to the same folder as \\'SpacepuppyUnityFramework.dll\\', overwrite if Unity already created the meta file.\\n\\nIf you don\\'t show meta files in your project, and would like to keep it that way, you may have to manually configure the execution order. In this case open the SpacepuppyUnityFramework.dll.meta and locate the \\'executionOrder\\' line, in your project go to \\'Edit->Project Settings->Script Execution Order\\' and in the appropriate screen drag in the matching scripts and set them to the corresponding values. It is advised to use the included meta file though as it\\'s easier.\\n\\n# License\\nCopyright (c) 2015, Dylan Engelman, Jupiter Lighthouse Studio\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.'},\n",
       " {'repo': 'RemoteTechnologiesGroup/RemoteTech',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '<!---\\n**master** [![Build Status](https://travis-ci.org/RemoteTechnologiesGroup/RemoteTech.svg?branch=master)](https://travis-ci.org/RemoteTechnologiesGroup/RemoteTech)\\n**develop** [![Build Status](https://travis-ci.org/RemoteTechnologiesGroup/RemoteTech.svg?branch=develop)](https://travis-ci.org/RemoteTechnologiesGroup/RemoteTech)\\n-->\\n**master** [![Build status](https://ci.appveyor.com/api/projects/status/18ksahrxar3ghaoh/branch/master?svg=true)](https://ci.appveyor.com/project/KSP-TaxiService/remotetech-hx082/branch/master)\\n**develop** [![Build status](https://ci.appveyor.com/api/projects/status/4kvchix4253kmc58/branch/develop?svg=true)](https://ci.appveyor.com/project/KSP-TaxiService/remotetech/branch/develop)\\n\\n\\nRemoteTech\\n==========\\n\\nA community-developed continuation of the original RemoteTech mod for Kerbal Space Program.\\n\\nRemoteTech allows you to construct vast relay networks of communication satellites and remotely-controlled unmanned vehicles.\\nYour unmanned vessels require an uplink to a command station to be controlled.\\nThis adds a new layer of difficulty that compensates for the lack of live crew members.\\n\\n\\nInstallation\\n------------\\n\\n[Download Here](https://github.com/RemoteTechnologiesGroup/RemoteTech/releases/latest) on Github or on [SpaceDock](http://spacedock.info/mod/520/RemoteTech).\\nReleases deemed stable are numbered (e.g. `1.8.0`).\\nDevelopment releases are prefixed with `build-develop`.\\n\\n\\nDocumentation\\n-------------\\n\\nA [detailed manual](http://remotetechnologiesgroup.github.io/RemoteTech) is available online.\\n\\nQuestions? Try the [KSP forum thread](http://forum.kerbalspaceprogram.com/index.php?/topic/139167-113-remotetech-v171-2016-07-02/).\\n\\n\\nContributing\\n------------\\n\\nSee the [guidelines](CONTRIBUTING.md) for both bug reports and pull requests.\\n'},\n",
       " {'repo': 'pankajladhar/GFontsSpace',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': \"# GFontsSpace \\r\\n\\r\\nIt is a simple tool which allows user to play with Google Fonts. This tool allows user to change some properties and generates the preview. \\r\\n### When to use \\r\\n\\r\\nIt user wants to use Google Font then that font should be imported/embedded in code.\\r\\nBut it gets difficult in case user wants to try different font before start consuming.\\r\\nIn above can case this helps a lot. User can test all available fonts without any extra effort. \\r\\n### Features \\r\\n\\r\\n1. Fonts are segregated into multiple categories, which help user to choose fonts easily.\\r\\n1. Allows user to change Font Family, Font Variant, Font Size, Text Color, Background Color. \\r\\n1. Clicking on **How to Use** button gives info about how to use in HTML/CSS and also contains link to Google Fonts official website.\\r\\n### Features in Pipeline\\r\\n\\r\\n- [x] Allow user to compare multiple fonts.\\r\\n- [x] Add **Hide Control** button to hide all with options (in right side) to get better preview\\r\\n- [x] Add category for layout i.e. Image with text, blog template. \\r\\n\\r\\n## like it?\\r\\n⭐️ this repo\\r\\n\\r\\n## Contributing\\r\\nFeel free to give feedback or raise issue. I'd love to have your helping hand on GFontsSpace! See [CONTRIBUTING.md](https://github.com/pankajladhar/GFontsSpace/blob/master/CONTRIBUTING.md) for more information on how to get started.\\r\\n\\r\\n## License\\r\\n[MIT](https://github.com/pankajladhar/GFontsSpace/blob/master/LICENSE) Licensed. Copyright (c) [Pankaj Ladhar](mailto:ladharpankaj@gmail.com) 2017.\\r\\n\"},\n",
       " {'repo': 'trailheadapps/easy-spaces',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': \"# Easy Spaces Aura Sample Application\\n\\n> IMPORTANT: This is the Aura version of the Easy Spaces sample application. If you are looking for the new Lightning Web Components version, click [here](https://github.com/trailheadapps/easy-spaces-lwc).\\n\\n![easy-spaces-logo](easy-spaces-logo.png)\\n\\n[![CircleCI](https://circleci.com/gh/trailheadapps/easy-spaces/tree/master.svg?style=svg)](https://circleci.com/gh/trailheadapps/easy-spaces/tree/master)\\n\\nEasy Spaces is a fictional event management company that creates and manages custom pop-up spaces for companies and individuals. Easy Spaces helps customers create temporary spaces like cafés, game rooms or themed rooms for special occasions in their offices and homes.\\n\\n[![Thumbnail](./docs/thumbnail.png)](https://youtu.be/ZwvegTLx9kk)\\n\\n## Table of Contents\\n\\n\\n*   Installation\\n    *   [Installing Easy Spaces using Salesforce DX](#installing-easy-spaces-using-salesforce-dx)\\n    *   [Installing Easy Spaces by URL](#installing-easy-spaces-by-url)\\n*   [Features](#features)\\n*   [Code Highlights](#code-highlights)\\n\\n## Installation\\n\\n\\n### Installing Easy Spaces using Salesforce DX\\n\\nThis is the recommended installation option for developers who want to experience the app and code.\\n\\n1.  Authenticate with your hub org (if not already done):\\n\\n    ```zsh\\n    sfdx force:auth:web:login -d -a myhuborg\\n    ```\\n\\n1.  Clone this repository:\\n\\n    ```zsh\\n    git clone https://github.com/trailheadapps/easy-spaces\\n    cd easy-spaces\\n    ```\\n\\n1.  Create a scratch org and provide it with an alias (**easyspaces** in the command below):\\n\\n    ```zsh\\n    sfdx force:org:create -s -f config/project-scratch-def.json -a easyspaces\\n    ```\\n\\n1.  Push source to your scratch org:\\n\\n    ```zsh\\n    sfdx force:source:push\\n    ```\\n\\n1.  Assign two EasySpaces permission sets to the default user:\\n\\n    ```zsh\\n    sfdx force:user:permset:assign -n EasySpacesObjects\\n    sfdx force:user:permset:assign -n SpaceManagementApp\\n    ```\\n\\n1.  Load sample data:\\n\\n    ```zsh\\n    sfdx force:data:tree:import -p ./data/Plan1.json\\n    sfdx force:data:tree:import -p ./data/Plan2.json\\n    ```\\n\\n1.  Open the scratch org:\\n\\n    ```zsh\\n    sfdx force:org:open\\n    ```\\n\\n1. Follow the steps in the 'Completing the Installation' section below to activate the app theme.\\n\\n### Installing Easy Spaces by URL\\nUse this option if you don't have Salesforce DX configured and want to experience the sample app. You'll be installing a series of unlocked packages.\\n\\n1. [Sign up](https://developer.salesforce.com/signup) for a Developer Edition (DE) org.\\n\\n1. Enable MyDomain in your DE org. Instructions to do this are [here](https://trailhead.salesforce.com/modules/identity_login/units/identity_login_my_domain).\\n\\n1. Click [this link](https://login.salesforce.com/packaging/installPackage.apexp?p0=04t1I0000036tXgQAI) to install the **es-base-objects** package and choose **Install for All Users**.\\n\\n1. Click [this link](https://login.salesforce.com/packaging/installPackage.apexp?p0=04t1I0000036tXlQAI) to install the **es-base-code** package and choose **Install for All Users**.\\n\\n1. Click [this link](https://login.salesforce.com/packaging/installPackage.apexp?p0=04t1I0000036tXqQAI) to install the **es-base-styles** package and choose **Install for All Users**.\\n\\n1. Click [this link](https://login.salesforce.com/packaging/installPackage.apexp?p0=04t1I0000036tYUQAY) to install the **es-space-mgmt** package and choose **Install for All Users**.\\n\\n1.  From the command line, enter to following commands to clone this repository. You'll want to do this to get the files with sample data on your computer:\\n\\n    ```zsh\\n    git clone https://github.com/trailheadapps/easy-spaces\\n    cd easy-spaces\\n    ```\\n\\n1. Import Lead data:\\n    - In **Setup**, type **Data Import** in the Quick Find box and click **Data Import Wizard**.\\n    - Click **Launch Wizard**.\\n    - Click the **Standard objects** tab, click **Leads**, and click **Add New Records**.\\n    - Drag **Lead_Data.csv** from the data folder of this project to the upload area.\\n    - Click **Next**, **Next**, and **Start Import**.\\n\\n1. Import Contact data:\\n    - In **Setup**, type **Data Import** in the Quick Find box and click **Data Import Wizard**.\\n    - Click **Launch Wizard**.\\n    - Click the **Standard objects** tab, click **Accounts and Contacts**, and click **Add New Records**.\\n    - Drag **Contact_Data.csv** from the data folder of this project to the upload area.\\n    - Click **Next**, **Next**, and **Start Import**.\\n\\n1. Import Market data:\\n    - In **Setup**, type **Data Import** in the Quick Find box and click **Data Import Wizard**.\\n    - Click **Launch Wizard**.\\n    - Click the **Custom objects** tab, click **Market**, and click **Add New Records**.\\n    - Drag **Market_Data.csv** from the data folder of this project to the upload area.\\n    - Click **Next**, **Next**, and **Start Import**.\\n\\n1. Import Spaces data:\\n    - Open the **Space_Data.csv** from the data folder of this project.\\n    - In the **Market__c** column, add the record Id for the correct Market imported in the previous step. Use the **Market City Name** column to help match spaces to the correct Market.\\n    - Save the changes to your file. *Note: You __must__ choose UTF-8 encoding when you save the file.*\\n    - In **Setup**, type **Data Import** in the Quick Find box and click **Data Import Wizard**.\\n    - Click **Launch Wizard**.\\n    - Click the **Custom objects** tab, click **Space**, and click **Add New Records**.\\n    - Drag **Space_Data.csv** from the data folder of this project to the upload area.\\n    - Click **Next**, **Next**, and **Start Import**.\\n    - If you see any issues with restricted picklist values blocking import, double-check that you saved your .csv with UTF-8 encoding and try again.\\n\\n\\n1. Follow the instructions in the **Completing the Installation** section below to enable the Easy Spaces custom theme.\\n\\n1. In **App Launcher**, select the **Space Management** app.\\n\\n1. Note: Before trying to work with the Spaces Designer, use the **Reservation Manager** to create some draft reservations.\\n\\n1. Have fun exploring!\\n\\n## Completing the Installation\\n\\n### Activate the Easy Spaces theme\\n1. In **Setup**, navigate to **Themes and Branding**\\n1. Activate the **Easy Spaces** theme\\n\\n## Features\\n\\nA quick overview of the features you can explore in Easy Spaces:\\n\\n*  Modular app design and Unlocked Packages\\n*  Lightning Console APIs & Navigation Methods\\n*\\tLightning Flow\\n\\t*  Dynamic flow interview components\\n\\t*  Custom flow screen components\\n\\t*  Local Action components\\n* \\tCustom Lightning Page Templates\\n*\\tLightning Themes and Design Token Bundles\\n* \\tCustom Metadata Types\\n\\n\\n## Code Highlights\\n\\n### Dynamic Flows and Local Action Components\\n\\nThe **spaceDesigner** and **reservationHelper** components render flow interviews dynamically, by using the **lightning:flow** base component. You can see the **customerDetails** and **smartGallery** components at work as screens in these dynamic flows. Both of these components use the functionality of the **lightning:availableForFlowScreens** interface to control flow navigation actions.\\n\\nSee this [blog post](https://developer.salesforce.com/blogs/2018/06/announcing-the-easy-spaces-app.html) for more detail about custom flow navigation and dynamic flow interviews.\\n\\nComponents used as Lightning Flow screens also enforce a convention in the markup of their design files, to help developers better track how attributes are being used by flow interviews. See the [customerDetails](./es-space-mgmt/main/default/aura/customerDetails/customerDetails.design) and [smartGallery](./es-space-mgmt/main/default/aura/smartGallery/smartGallery.design) component design files for examples.\\n\\n### Object-Agnostic Design\\n\\nThe **customerList** and **customerTile** components can display information from Contact objects or Lead objects.\\n\\n![Two instances of customerList component on canvas in Lightning App Builder](./docs/customerList_view.png)\\n\\nThe customerList component uses a design attribute to allow for users working in Lightning App Builder to control which object an instance of the component should display:\\n\\n![customerList component design attribute as picklist in Lightning App Builder](./docs/customerList_design.png)\\n\\nThis is just one example of object-agnostic design at work in Easy Spaces. See this [blog post](https://developer.salesforce.com/blogs/2018/06/announcing-the-easy-spaces-app.html) for more detail about this pattern.\\n\\n### Control Navigation and Behavior in Console Apps\\n\\nEasy Spaces uses the Lightning Console JavaScript API to control navigation and behaviors of tabs and subtabs in the Space Management console app. You can see the lightning:workspaceAPI component at work in the **openRecordAction** component, which enables flow interviews to navigate users to a new subtab. The **reservationHelper** and **spaceDesigner** components use the lightning:navigationItemAPI component to refresh custom Lightning Page tabs in the background as a user works. You can get more detail about using the Workspace API in your components in this [blog post](https://developer.salesforce.com/blogs/2018/06/announcing-the-easy-spaces-app.html).\\n\\n### Modular Design and Unlocked Packaging\\n\\nEasy Spaces illustrates how to organize application metadata into granular units or modules. This approach is reflected in the design patterns at work throughout the application, like the use of design tokens and object-agnostic components. But you'll also see this at work in the structure of the Easy Spaces repo itself.\\n\\nThe Easy Spaces application is made of several, interdependent unlocked packages. The dependecies between the Easy Spaces packages are listed in the [sfdx-project.json](./sfdx-project.json) file for this repo.\\n\\nYou can also explore the contents of each package by looking at the related package folder within this repo. The `path` attribute entries in the sfdx-project.json show which folder contains the metadata for a particular package.\\n\\nFor more about how the Easy Spaces metadata is organized into package modules, check out [this post](https://developer.salesforce.com/blogs/2018/06/working-with-modular-development-and-unlocked-packages-part-2.html).\\n\"},\n",
       " {'repo': 'SpaceGroupUCL/qgisSpaceSyntaxToolkit',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"# Space Syntax Toolkit for QGIS\\n\\n## News\\n03.07.2017 - SST workshop at the 11th International Space Syntax Symposium, in Lisbon, Portugal\\n\\n30.06.2017 - SST 0.2.0 has been released, including several new modules.\\n\\n06.11.2016 - For the latest information on the Space Syntax Toolkit you should now consult the [Wiki](https://github.com/SpaceGroupUCL/qgisSpaceSyntaxToolkit/wiki) and its [FAQ](https://github.com/SpaceGroupUCL/qgisSpaceSyntaxToolkit/wiki).\\n\\n03.06.2016 – Subscribe to the new [Space Syntax Toolkit mailing list on JISCMAIL](https://www.jiscmail.ac.uk/cgi-bin/webadmin?A0=SPACESYNTAX-TOOLKIT) for discussions, suggestions, and the latest news. You can send an e-mail to the list (spacesyntax-toolkit at jiscmail.ac.uk) if you need support using the toolkit.\\n\\n## About\\nThe “Space Syntax Toolkit” is a [QGIS](http://www.qgis.org/en/site/) plugin offering user friendly space syntax analysis workflows in a GIS environment. It provides a front-end for the [depthmapX](https://varoudis.github.io/depthmapX/) software within QGIS, for seamless spatial network analysis. Furthermore, it includes tools for urban data management and analysis, namely land use, entrances, frontages, pedestrian movement, road centre lines, and service areas.\\n\\nOriginally developed by Jorge Gil at the Space Syntax Laboratory, The Bartlett, UCL, the plugin includes contributions from:\\n* [Space Syntax Limited](https://github.com/spacesyntax) - Ioanna Kovolou, Abhimanyu Acharya, Stephen Law, Laurens Versluis\\n\\n## Installation\\nThe plug-in can be installed from the QGIS Plugins Manager, and updates become automatically available once submitted to the QGIS plugins repository.\\n\\n## Software Requirements\\n* QGIS (2.14 or above) - [http://www.qgis.org/en/site/](http://www.qgis.org/en/site/)\\n* depthmapXnet - [http://archtech.gr/varoudis/depthmapX/?dir=depthmapXnet](http://archtech.gr/varoudis/depthmapX/?dir=depthmapXnet)\\n\\n## Support\\nIf you need help using the toolkit in your space syntax research, you can send an e-mail to the mailing list (spacesyntax-toolkit@jiscmail.ac.uk) for support from the user community.\\nIf you encounter problems when using the software, please check the [Wiki](https://github.com/SpaceGroupUCL/qgisSpaceSyntaxToolkit/wiki) and the current [issues list](https://github.com/SpaceGroupUCL/qgisSpaceSyntaxToolkit/issues) for solutions. If it's a new problem, you can add the issue to the [issues list](https://github.com/SpaceGroupUCL/qgisSpaceSyntaxToolkit/issues).\\n\\n## Where to find...\\n* The toolkit source code can be downloaded from the 'esstoolkit' folder.\\n* Documentation can be obtained from the 'documents' folder.\\n* A sample dataset is in the 'data' folder, for experimenting with the plugin and following the documentation.\\n\\n## Development notes:\\n* Development of this module has been done primarily using PyCharm, with the top folder (qgisSpaceSyntaxToolkit) selected and the QGIS python selected as an interpreter. This allows for having a similar module loading process as QGIS itself.\\n* Unit tests reside in the esstoolkit/tests directory, but have to be carrie out from the top directory of the repository (qgisSpaceSyntaxToolkit).\\n\"},\n",
       " {'repo': 'SpaceGroupUCL/qgisSpaceSyntaxToolkit',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"# Space Syntax Toolkit for QGIS\\n\\n## News\\n03.07.2017 - SST workshop at the 11th International Space Syntax Symposium, in Lisbon, Portugal\\n\\n30.06.2017 - SST 0.2.0 has been released, including several new modules.\\n\\n06.11.2016 - For the latest information on the Space Syntax Toolkit you should now consult the [Wiki](https://github.com/SpaceGroupUCL/qgisSpaceSyntaxToolkit/wiki) and its [FAQ](https://github.com/SpaceGroupUCL/qgisSpaceSyntaxToolkit/wiki).\\n\\n03.06.2016 – Subscribe to the new [Space Syntax Toolkit mailing list on JISCMAIL](https://www.jiscmail.ac.uk/cgi-bin/webadmin?A0=SPACESYNTAX-TOOLKIT) for discussions, suggestions, and the latest news. You can send an e-mail to the list (spacesyntax-toolkit at jiscmail.ac.uk) if you need support using the toolkit.\\n\\n## About\\nThe “Space Syntax Toolkit” is a [QGIS](http://www.qgis.org/en/site/) plugin offering user friendly space syntax analysis workflows in a GIS environment. It provides a front-end for the [depthmapX](https://varoudis.github.io/depthmapX/) software within QGIS, for seamless spatial network analysis. Furthermore, it includes tools for urban data management and analysis, namely land use, entrances, frontages, pedestrian movement, road centre lines, and service areas.\\n\\nOriginally developed by Jorge Gil at the Space Syntax Laboratory, The Bartlett, UCL, the plugin includes contributions from:\\n* [Space Syntax Limited](https://github.com/spacesyntax) - Ioanna Kovolou, Abhimanyu Acharya, Stephen Law, Laurens Versluis\\n\\n## Installation\\nThe plug-in can be installed from the QGIS Plugins Manager, and updates become automatically available once submitted to the QGIS plugins repository.\\n\\n## Software Requirements\\n* QGIS (2.14 or above) - [http://www.qgis.org/en/site/](http://www.qgis.org/en/site/)\\n* depthmapXnet - [http://archtech.gr/varoudis/depthmapX/?dir=depthmapXnet](http://archtech.gr/varoudis/depthmapX/?dir=depthmapXnet)\\n\\n## Support\\nIf you need help using the toolkit in your space syntax research, you can send an e-mail to the mailing list (spacesyntax-toolkit@jiscmail.ac.uk) for support from the user community.\\nIf you encounter problems when using the software, please check the [Wiki](https://github.com/SpaceGroupUCL/qgisSpaceSyntaxToolkit/wiki) and the current [issues list](https://github.com/SpaceGroupUCL/qgisSpaceSyntaxToolkit/issues) for solutions. If it's a new problem, you can add the issue to the [issues list](https://github.com/SpaceGroupUCL/qgisSpaceSyntaxToolkit/issues).\\n\\n## Where to find...\\n* The toolkit source code can be downloaded from the 'esstoolkit' folder.\\n* Documentation can be obtained from the 'documents' folder.\\n* A sample dataset is in the 'data' folder, for experimenting with the plugin and following the documentation.\\n\\n## Development notes:\\n* Development of this module has been done primarily using PyCharm, with the top folder (qgisSpaceSyntaxToolkit) selected and the QGIS python selected as an interpreter. This allows for having a similar module loading process as QGIS itself.\\n* Unit tests reside in the esstoolkit/tests directory, but have to be carrie out from the top directory of the repository (qgisSpaceSyntaxToolkit).\\n\"},\n",
       " {'repo': 'BlesseNtumble/GalaxySpace',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '# GalaxySpace\\nAddon for GalactiCraft\\n\\nAbout :\\nThis mod adds a lot of planets and moons for modification GalactiCraft. The addon will not work without a main mod - GalactiCraft.\\nWe do not pursue other creators of addons. We try to make high quality.\\nThe addon is made for version Minecraft 1.7.10 and 1.12.2, and earlier versions will not be transferred.\\n\\n'},\n",
       " {'repo': 'Linux74656/SpaceEngineersLinuxPatches',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '# This is currently the easiest solution to getting Space Engineers to run smoothly using DotNet:\\n### This guide assumes you have already have Steamplay/Proton installed with all of the prerequisites and such, as well as Space Engineers, and winetricks. If not, you can check around the internet for guides that can explain it much better than I can!\\n\\n#### Here is a list of Known Issues, and potential solutions:\\n\\n0. [Crash After splashscreen](#issue-0)\\n\\n1. [Freeze at starup](#issue-1)\\n\\n2. [rundl32.exe error popup](#issue-2)\\n\\n3. [Looping Sound](#issue-3)\\n\\n4. [Popping/Crackling Sound](#issue-4)\\n\\n5. [Mouse/Keyboard issues when changing window focus(alt+tab)](#issue-5)\\n\\n6. [Game crashes shortly after start with `System.OutOfMemoryException: Array dimensions exceeded supported range.` error in log.](#issue-6)\\n\\n7. [Game hardlocks(sometimes including entire OS) when using 5700/5700xt](#issue-7)\\n\\n8. [SpaceEngineers.exe becomes zombie on exit](#issue-8)\\n\\n9. [SpaceEngineers does not work when installed on NFS](#issue-9)\\n\\n10. [Crash at or before main menu](#issue-10)\\n\\n11. [Dotnet4.0 failed in proton 6.3-2](#issue-11)\\n\\n## NOTE: Space Engineers and presumable many other proton games will fail with a \\'too many files open\\' error as seen here: https://github.com/Linux74656/SpaceEngineersLinuxPatches/issues/35 while using OpenRC instead of systemd. The solution is to increase your file limits(the specific solution is in the original post of the issue linked previously).\\n\\n# New Automated Setup Guide:\\n\\n#### This can patch the original SE DOTNET issue and several others.\\n\\n### Step 0) Prerequisites\\n\\n#### Note: Ensure you have proton setup and configured prorperly. You can find more information here: https://github.com/ValveSoftware/Proton/wiki and https://github.com/ValveSoftware/Proton/wiki/Requirements\\n\\nThis java program simply requires Java 7 or later. This should be standard on most modern Linux systems. If it is not you can search the internet for a specific guide on how to install it for your particular Linux distribution.\\n\\n### Step 1) Get it\\n\\nDownload this pre-compiled jar file: ![PatcherGUI.jar](PatcherGUI.jar)\\nIf you would like to look at the source code or compile it yourself it is in this repository as Source-PatchGUI.java\\n\\n### Step 2) Run it\\n\\nSimply double click it and it should open. The program should be fairly intuitive and has a built-in help feature.\\n\\nIF YOU CAN NOT START THE PatcherGUI.jar, because computers… THEN CONTINUE. \\n\\nSome distros special circumstances do not allow you to simply double click the jar and have it run.\\n\\n### Step 3) Make it runnable\\n\\nOn some Distros and Desktop Environments you can simply right click on the PatcherGUI.jar file, then click properties > Permissions and ensure the check box: Is executable is marked.\\n\\n## OR\\n\\nYou can open your terminal/console of choice and cd into the directory where the PatcherGUI.jar file is located, and run \\'sudo chmod a+x PatcherGUI.jar\\'\\n\\n### Step 4) Open the GUI through command line\\n\\nUsing your favorite terminal/console program, cd into the directory and run \\'java -jar PatcherGUI.jar\\'\\n\\n# The Old Automated Setup Guide:\\n### This will create the wineprefix and modify the config file for you. If you want you can scroll down and find the manual instruction on what this patcher automates.\\n\\n## Step 0:\\n  Backup any save data, blueprints, ETC...\\n  Then verify the integrity of your game files in Steam.\\n\\n## Step 1:\\n  Ensure prerequisites are installed and up to date.\\n  \\n  Wine(5.0 or higher... although the newest version is recommended). Find out more at winehq: https://wiki.winehq.org/Download\\n  \\n  Winetricks: from here https://wiki.winehq.org/Winetricks then:\\n  \\n  `sudo winetricks --self-update`\\n  \\n\\n## Step 2:\\n  Download(Save to downloads) this python script: ![autoprefix-patcher.py](autoprefix-patcher.py)\\n  \\n  Run this script with \\n  \\n  `python3 autoprefix-patcher.py`\\n  \\n  Note: The script will ask you if it needs help, so make sure you read what it says! It can apply another fix for one of the issues below(freeze at startup) and will ask you if you want to fix it.\\n\\n Congratulations Space Engineers should now work properly on Linux. Have fun and enjoy!\\n\\n# The Manual Setup Guide(Skip if you used the autopatchers, and they seemed to work):\\n\\n\\n## Step 0:\\n  Backup any save data, blueprints, ETC...  \\n  Then verify the integrity of your game files in Steam.\\n\\n\\n## Step 1:\\nEnsure prerequisites are installed and up to date.\\n  \\n  Wine(5.0 or higher... although the newest version is recommended). Find out more at winehq: https://wiki.winehq.org/Download\\n  \\n### With winetricks\\n  Ensure winetricks is up to date.\\n\\n  `sudo winetricks --self-update`\\n___________________\\n### OR with protontricks (You can use protontricks is you already have it installed.)\\n\\n  Ensure protontricks is up to date.\\n  \\n  follow the instructions ![here](https://github.com/Matoking/protontricks) based on how you installed it.\\n\\n\\n## Step 2:\\n### With winetricks\\n\\n  Create your wine prefix with winetricks in the compatdata folder in steamapps/common/\\n\\n  `WINEPREFIX=\"INSERT/DIRECTORY/TO/SPACEENGINEERS/pfx\" winetricks --force -q dotnet48 vcrun2015 faudio d3dcompiler_47`\\n___________________\\n### OR with protontricks\\n\\n  Create your wine prefix using protontricks if it is already installed, or if you want to install it from the instructions ![here](https://github.com/Matoking/protontricks).\\n  \\n  protontricks 244850 --force -q dotnet472 vcrun2015 faudio d3dcompiler_47\\n  \\n\\n## Step 3:\\n  Open your Space Engineers bin64 directory: usually at $HOME/.local/share/Steam/steamapps/common/SpaceEngineers/Bin64\\n\\n  In this folder find the file **SpaceEngineers.exe.config** and open it in a text editor(Gedit, Kwrite, ETC...). \\n\\n  You should see something like this:\\n  ![before](ignorethis/Before.png)\\n    \\n  \\n\\n## Step 4:\\n  Now add `<gcServer enabled = \"true\"/>` to a new line after the line that says `<runtime>`\\n  It should now look like this:\\n  ![before](ignorethis/After.png)\\n\\n  Save the file and close it.\\n\\n  Congratulations Space Engineers should now work properly on Linux. Have fun and enjoy!\\n\\n  Note you may have to reapply these fixes if the game updates. It depends on if the file is changed durring the update.\\n  If you encounter issues try following these steps again.\\n___________________  \\n\\n  # Known issues:\\n\\n\\n  ### Issue 0:  \\n  When using the autopatcher (even sometimes when using the manual method) the game will show a splash screen and then crash.\\n\\n  ### Known Fix:\\n  Manually install dotnet48 into your prefix. More information can be found in step 2 of manual guide above.\\n___________________  \\n  ### Issue 1:  \\n  Broken startup and missing videos, causing startup freeze that requires user input to get past the splash screen.\\n\\n  ### Known Fix:\\n  Rename the file here: \"LOCATION_OF_SPACE_ENGINEERS_INSTALL/SpaceEngineers/Content/Videos/KSH.wmv to KSH.wmv.old\\n  \\n  OR\\n  \\n  Set Steam launch options (right click Space Engineers in Steam, click Properties, then in the General tab) to `%command% -skipintro`\\n___________________  \\n\\n  ### Issue 2:  \\n  Upon startup the game will show this error one or more times.\\n\\n  ![rundllerror](ignorethis/rundll32.png)\\n\\n  ### Known Fixes:\\n  1) Just hit no each time it appears. It should not impact the ability of the game to run.\\n\\n  #### OR\\n\\n  2) Run `WINEPREFIX=\"INSERT/DIRECTORY/TO/SPACEENGINEERS/pfx\" winetricks`\\n  In the library tab of the configuration window add rundll32.exe to the New override for library box and click add. Then find it in the list below and click edit and ensure disabled is selected. (See image below:) Click apply, then close the window.\\n\\n  ![rundlldisable](ignorethis/disablerundll32.png)\\n___________________  \\n\\n  ### Issue 3:  \\n  Some users report that faudio has looping sound issues.\\n\\n  ### Known Fix:  \\n  None is known at this time. More testing is required.\\n___________________  \\n\\n  ### Issue 4:\\n  While using faudio some users report crackling or popping audio.\\n\\n  ### Known Fix:\\n  Try adding the following to your steam launch options(right click Space Engineers in steam, click Properties, then in the General tab, click SET LAUNCH OPTIONS...) add the following in the box that appears.  \\n\\n  A) If something is already in the box add PULSE_LATENCY_MSEC=60 after those items but before %command% \\n\\n   e.g. `DXVK_HUD=full PULSE_LATENCY_MSEC=60 %command%` Make sure you have spaces between each item.\\n\\n B) If the box is empty add:\\n\\n   `PULSE_LATENCY_MSEC=60 %command%`\\n    \\n   Then hit ok on the launch options window. If you are still experiencing issues, try modifying the value of PULSE_LATENCY_MSEC try 30, or 90 instead of 60.\\n___________________  \\n\\n  ### Issue 5:  \\n  When the user alt+tab from the game, the game will keep the mouse from moving to another screen or selecting other windows. It also prevents keyboard input to other windows, even if they are on the same screen, or there is only one monitor.\\n\\n  ### Known Fix:  \\n  The known workaround causes more issues than it fixes. If you are so inclinded to try it you can find it in issue ![#14 (Mouse Capture)](https://github.com/Linux74656/SpaceEngineersLinuxPatches/issues/14) #Be warned this fix breaks other things. Try at your own risk!\\n___________________  \\n\\n  ### Issue 6:  \\n  Game crashes shortly after start with `System.OutOfMemoryException: Array dimensions exceeded supported range.` error in log.\\n\\n  ### Known Fix:  \\n  Game sends analytics to 81.0.234.196 and 88.146.207.227 (Keen SWH analytics servers) which apparently sends back some garbage that causes the issue (unintentionally). To resolve the issue block this service via:\\n  \\n  `sudo iptables -A INPUT -s 88.146.207.227 -j DROP`\\n  \\n___________________  \\n\\n  ### Issue 7:  \\n  The game will freeze and accept no input just after loading a world. This will also sometimes lockup the OS. This seems to only happen on the newest line of AMD graphics cards(currently tested(5700/5700XT)).\\n\\n  ### Known Fix:  \\n  Set all ingame graphics settings to minimum. Especially Voxel Quaity, and grass density. You may wish to increase some settings, such as texure quality, but do so at your own peril, the results may be mixed.\\n___________________\\n\\n  ### Issue 8:  \\n  From this issue https://github.com/Linux74656/SpaceEngineersLinuxPatches/issues/26\\n  \\n  SpaceEngineers.exe refuses to die after exiting the game.\\n\\n  ### Known Fix:\\n  You can take a look at this script by inetknght:\\n  https://github.com/inetknght/linux-profile/blob/master/.bash/kill_space_engineers.bash\\n  \\n  It will help eleminate those pesky zombie processes.\\n___________________\\n\\n  ### Issue 9:  \\n  From this issue https://github.com/Linux74656/SpaceEngineersLinuxPatches/issues/33\\n  \\n  It appears space engineers does not work when run from an NFS(Network File System.)\\n\\n  ### Known Fix:\\n  Do not install or run SE from an NFS.\\n___________________  \\n\\n  ### Issue 10:  \\n  From this issue https://github.com/Linux74656/SpaceEngineersLinuxPatches/issues/46\\n  \\n  Running Space Engineers will sometimes crash at or before the main menu. This sometimes is caused by mono being used instead of DotNet, however if dotnet is used then and this issue persist, try the following fix.\\n\\n  ### Known Fix:\\n  In winecfg, enable \\'Emulate a virtual desktop\\' and set your desired resolution.\\n  ___________________  \\n\\n  ### Issue 11:  \\n  From this issue https://github.com/Linux74656/SpaceEngineersLinuxPatches/issues/52\\n  Dotnet4.0, a requirement for dotnet4.7.2 and dotnet4.8 fails to install on some systems in newer proton versions.\\n  \\n\\n  ### Known Fix:\\n  Downgrade to an older version of proton such as 4.11-13, or possibly 5.0 and install dotnet, before swithing to a newer version of proton.\\n___________________  \\n\\n  ## Special thanks to InflexCZE for taking the time to help. Without his help it could have taken many more months to figure this out.\\n  ## and Huge thanks to everyone else on https://github.com/ValveSoftware/Proton/issues/1792 for helping solve these issues.\\n'},\n",
       " {'repo': 'golsun/SpaceFusion',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"# SpaceFusion\\ncode/data for the NAACL'19 paper [Jointly Optimizing Diversity and Relevance in Neural Response Generation](https://arxiv.org/abs/1902.11205)\\n\\nSpaceFusion is a regularized multi-task learning paradigm proposed to align and structure the unstructured latent spaces learned by different models trained over different datasets. Of particular interest is its application to neural conversation modelling, where SpaceFusion is used to jointly optimize the relevance and diversity of generated responses.\\n\\nMore documents:\\n* our [paper](https://arxiv.org/abs/1902.11205) at NAACL'19 (long, oral). \\n* The [slides](https://github.com/golsun/SpaceFusion/blob/master/slides.pdf) presented at NAACL'19.\\n* We published a [MSR blog](https://www.microsoft.com/en-us/research/blog/spacefusion-structuring-the-unstructured-latent-space-for-conversational-ai/) to discuss the intuition and implication\\n* our follow-up work, [StyleFusion](https://github.com/golsun/StyleFusion) at EMNLP'19\\n* **our latest Dialogue Evaluation/Ranking models, [DialogRPT](https://github.com/golsun/DialogRPT), at EMNLP'20**\\n\\n## Requirement\\nthe code is tested using Python 3.6 and Keras 2.2.4\\n\\n## Dataset\\nWe provided scripts to generate [Reddit](https://github.com/golsun/SpaceFusion/blob/master/data/reddit.py) and process [Switchboard](https://github.com/golsun/SpaceFusion/blob/master/data/switchboard.py) datasets as well as a [toy dataset](https://github.com/golsun/SpaceFusion/blob/master/data/toy) in this repo for debugging.\\n\\nPlease check [here](https://github.com/golsun/SpaceFusion/blob/master/data/README.md) for more details.\\n\\n## Usage\\n* To train a SpaceFusion model: `python src/main.py mtask train --data_name=toy`\\n* To visualize the learned latent space: `python src/vis.py --data_name=toy`\\n* To interact with the trained model: `python src/main.py mtask interact --data_name=toy --method=?`, where method can be `greedy`, `rand`, `sampling` or `beam`. We used `rand` in the paper\\n* To generate hypotheses for testing with the trained model: `python src/main.py mtask test --data_name=toy`\\n* To evaluate the generated hypotheses `python src/eval.py --path_hyp=? --path_ref=? --wt_len=?`, which outputs the precision, recall, and F1 as defined in the paper. You may want to first run this command with `-len_only` to find a proper `wt_len` that minimize the difference between the average length (number of tokens) of hypothesis and reference.\\n\\n## Discription\\n* `main.py` is the main file\\n* `model.py` defines the SpaceFusion model (see `class MTask`) and some baselines\\n* `vis.py` defines the function we used to visulize and analysis the latent space\\n* `dataset.py` defines the data feeder\\n* `shared.py` defines the default hyperparameters\\n\\n## Citation\\nPlease cite our NAACL paper if this repo inspired your work :)\\n```\\n@article{gao2019spacefusion,\\n  title={Jointly Optimizing Diversity and Relevance in Neural Response Generation},\\n  author={Gao, Xiang and Lee, Sungjin and Zhang, Yizhe and Brockett, Chris and Galley, Michel and Gao, Jianfeng and Dolan, Bill},\\n  journal={NAACL-HLT 2019},\\n  year={2019}\\n}\\n```\\n\\n![](https://github.com/golsun/SpaceFusion/blob/master/fig/intro_fig.PNG)\\n\\n\\n\"},\n",
       " {'repo': 'alyssaxuu/carden',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Carden\\n![Demo](https://media.giphy.com/media/ixBAW98ih0qOo2kf4u/giphy.gif)\\n<br>\\n\\nFlashcards with spaced repetition and gamification 🌱\\n\\nCarden is a Chrome Extension that helps you turn the content you consume into long-term knowledge. Create flashcards in context, view in-depth stats, collect points and level up, and much more.\\n\\n[Get it now](https://chrome.google.com/webstore/detail/carden-flashcards-with-sp/effdlfnfholapaddppkjmkhmfgdbeomj?hl=en&authuser=0)\\n\\nMade by [Anne-Laure Le Cunff](https://twitter.com/anthilemoon) & [Alyssa X](https://alyssax.com)\\n\\n<a href=\"https://www.producthunt.com/posts/carden?utm_source=badge-featured&utm_medium=badge&utm_souce=badge-carden\" target=\"_blank\"><img src=\"https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=300325&theme=light\" alt=\"Carden - Science-based flashcard app with spaced repetition | Product Hunt\" style=\"width: 250px; height: 54px;\" width=\"250\" height=\"54\" /></a>\\n<a href=\"https://news.ycombinator.com/item?id=27512769\" target=\"_blank\"><img height=53 src=\"https://hackerbadge.now.sh/api?id=27512769&type=orange\" alt=\"Featured on HackerNews\"></a>\\n\\n## Table of contents\\n- [Features](#features)\\n- [Self-hosting Carden](#self-hosting-carden)\\n- [Libraries used](#libraries-used)\\n\\n## Features\\n🔬 Practice using spaced repetition based on the evidence-based [SM2 algorithm](https://en.wikipedia.org/wiki/SuperMemo)<br>\\n✏️ Create flashcards in context whenever you read something interesting<br>\\n🗄️ Organize your flashcards into decks<br>\\n🏆 Collect points and level up as you practice and grow your knowledge<br>\\n🧩 Cross-compatible with apps like Anki, Quizlet, Brainscape, and more<br>\\n📦 Import and export flashcards as .txt and .csv files<br>\\n📊 View and download in-depth stats<br>\\n...and much more!\\n\\n## Self-hosting Carden\\nYou can run Carden locally without having to install it from the Chrome Store. Here\\'s how:\\n\\n1. Download the code. In the web version of GitHub, you can do that by clicking the green \"Code\" button, and then \"Download ZIP\".\\n2. Note that there\\'s a [server](https://github.com/alyssaxuu/carden/tree/master/server) folder for the back-end. You will need to setup a server with a MySQL database.\\n3. Add the appropriate credentials in [config.php](https://github.com/alyssaxuu/carden/tree/master/server/config.php) to access your database.\\n4. Modify the access-control-allow-origin in all PHP files to allow requests from the extension (with its unique ID).\\n5. Replace the POST URLs throughout the javascript files in the [chrome-extension](https://github.com/alyssaxuu/carden/tree/master/chrome-extension) folder.\\n6. Go to chrome://extensions/ in your browser, and [enable developer mode](https://developer.chrome.com/docs/extensions/mv2/faq/#:~:text=You%20can%20start%20by%20turning,a%20packaged%20extension%2C%20and%20more.).\\n7. Drag the [chrome-extension](https://github.com/alyssaxuu/carden/tree/master/chrome-extension) folder (make sure it\\'s a folder and not a ZIP file, so unzip first), or click on the \"Load unpacked\" button and locate the folder.\\n4. That\\'s it, you will now be able to use Carden locally. Make sure you pin it on the toolbar by clicking the \"puzzle\" icon in the toolbar and pinning Carden.\\n\\n## Libraries used\\n- [jQuery](https://jquery.com/) - for better event handling and DOM manipulation\\n- [jQuery Nice Select](https://hernansartorio.com/jquery-nice-select/) - for better, more stylish dropdowns\\n- [Chart.js](https://www.chartjs.org/) - for rendering the chart for the stats\\n- [ExtensionPay](https://extensionpay.com/home) - for handling the yearly subscription\\n- [Twemoji](https://twemoji.twitter.com/) - for the cool emojis throughout the extension ✨\\n\\n#\\n Feel free to reach out to me through email at hi@alyssax.com or [on Twitter](https://twitter.com/alyssaxuu) if you have any questions or feedback! Hope you find this useful 💜\\n'},\n",
       " {'repo': 'scalaspace/scalaspace.github.io',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': 'To add your Scala group to [Scala Space](http://scala.space), add an item to the collection in ```data/groups.json``` with the following five elements: group name, Meetup group ID or full URL, latitude and longitude of the place where you usually meet, and `true` if the group is a purely Scala group, or `false` if it\\'s a more general FP group.\\n\\nThe easiest way to find these is to do a Google search for \"coordinates of [place name]\", and to reformat the response as real numbers, remembering to negate longitudes in the Western Hemisphere, and latitudes in the Southern Hemisphere.\\n\\nFor example, searching for \"coordinates of Brighton\" returns the response \"50.8429° N, 0.1313° W\". This should be translated to the following entry:\\n\\n    {\\n      name: \"Brighton Scala Users Group\",\\n      url: \"http://example.com/\",\\n      latitude: 50.8429, \\n      longitude: -0.1313, \\n      justScala: true\\n    }\\n\\nTry to keep the locations in alphabetical order of name.\\n\\n# Contributing\\n\\n## Building locally\\n\\n```\\n# clone the repository\\ngit clone git@github.com:scalaspace/scalaspace.github.io.git\\ncd scalaspace.github.io\\n# make your changes to the code\\n\\n# build the javascript file\\nsbt fullOptJS\\n\\n# open browser\\nopen index.html\\n```\\n\\n\\n[![Join the chat at https://gitter.im/scalaspace/scalaspace.github.io](https://badges.gitter.im/scalaspace/scalaspace.github.io.svg)](https://gitter.im/scalaspace/scalaspace.github.io?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)'},\n",
       " {'repo': 'seoungwugoh/STM',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '## Video Object Segmentation using Space-Time Memory Networks\\n### Seoung Wug Oh, Joon-Young Lee, Ning Xu, Seon Joo Kim\\n### ICCV 2019\\n[[paper]](http://openaccess.thecvf.com/content_ICCV_2019/html/Oh_Video_Object_Segmentation_Using_Space-Time_Memory_Networks_ICCV_2019_paper.html)\\n\\n[![Video Object Segmentation using Space-Time Memory Networks (ICCV 2019)](https://img.youtube.com/vi/vVZiBEDmgIU/0.jpg)](https://www.youtube.com/watch?v=vVZiBEDmgIU \"Video Object Segmentation using Space-Time Memory Networks (ICCV 2019)\")\\n\\n\\n\\n### - Requirements\\n- python 3.6\\n- pytorch 1.0.1.post2\\n- numpy, opencv, pillow\\n\\n### - How to Use\\n#### Download weights\\n##### Place it the same folder with demo scripts\\n```\\nwget -O STM_weights.pth \"https://www.dropbox.com/s/mtfxdr93xc3q55i/STM_weights.pth?dl=1\"\\n```\\n\\n#### Run\\n##### DAVIS-2016 validation set (Single-object)\\n``` \\npython eval_DAVIS.py -g \\'1\\' -s val -y 16 -D [path/to/DAVIS]\\n```\\n##### DAVIS-2017 validation set (Multi-object)\\n``` \\npython eval_DAVIS.py -g \\'1\\' -s val -y 17 -D [path/to/DAVIS]\\n```\\n\\n### - Pre-computed Results\\nIf you are not able to run our code but interested in our results. The pre-computed results will be helpful.\\n- [[DAVIS-16-val]](https://www.dropbox.com/sh/anz83wzw5ki8g2r/AACytyhGdAMMuJiOQ0pmx2spa?dl=1)\\n- [[DAVIS-17-val]](https://www.dropbox.com/sh/9ijjmm1gajkxx5b/AAA3a_55Z6eD54B3hPB7Zi3oa?dl=1)\\n- [[DAVIS-17-test-dev]](https://www.dropbox.com/sh/orztasfa4uaym2o/AADy3LgNByWUWxAQHk5j-3cfa?dl=1)\\n- [[YouTube-VOS-18-valid]](https://www.dropbox.com/s/5mxy0715sdtnprm/YoutubeVOS-valid.zip?dl=1)\\n- [[YouTube-VOS-19-valid]](https://www.dropbox.com/s/l1lvnylcoo288y5/YoutubeVOS19-valid.zip?dl=1)\\n\\n\\n### - Reference \\nIf you find our paper and repo useful, please cite our paper. Thanks!\\n``` \\nVideo Object Segmentation using Space-Time Memory Networks\\nSeoung Wug Oh, Joon-Young Lee, Ning Xu, Seon Joo Kim\\nICCV 2019\\n```\\n\\n### - Related Project\\n``` \\nFast Video Object Segmentation by Reference-Guided Mask Propagation\\nSeoung Wug Oh, Joon-Young Lee, Kalyan Sunkavalli, Seon Joo Kim\\nCVPR 2018\\n```\\n[[paper]](http://openaccess.thecvf.com/content_cvpr_2018/papers/Oh_Fast_Video_Object_CVPR_2018_paper.pdf)\\n[[github]](https://github.com/seoungwugoh/RGMP)\\n\\n\\n### - Interactive VOS (Quantitative Evaluation)\\nA modified STM model is used for DAVIS Interactive VOS Challenge 2019 (https://davischallenge.org/challenge2019/interactive.html). \\nIf you are intersted in comparison with our interactive STM model, please use evaluation summary obtained from the [DAVIS framework](https://interactive.davischallenge.org/). \\n[[Download link (DAVIS-17-val)]](https://www.dropbox.com/s/owoms3rtalg52wn/STM_Interactive_summary_DAVIS17_val.json?dl=1).\\nThe timing is measured using a single 2080Ti GPU. We will make a demo software available soon. \\nFor the further questions, please contact me by E-mail.\\n\\n\\n### - Terms of Use\\nThis software is for non-commercial use only.\\nThe source code is released under the Attribution-NonCommercial-ShareAlike (CC BY-NC-SA) Licence\\n(see [this](https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode) for details)\\n'},\n",
       " {'repo': 'pgriess/node-msgpack',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '`node-msgpack` is an addon for [NodeJS](http://nodejs.org) that provides an\\nAPI for serializing and de-serializing JavaScript objects using the\\n[MessagePack](http://msgpack.sourceforge.net) library. The performance of this\\naddon compared to the native `JSON` object isn\\'t too bad, and the space\\nrequired for serialized data is far less than JSON.\\n\\n### Performance\\n\\n`node-msgpack` is currently slower than the built-in `JSON.stringify()` and\\n`JSON.parse()` methods.  In recent versions of node.js, the JSON functions\\nhave been heavily optimized.  node-msgpack is still more compact, and we are\\ncurrently working performance improvements.  Testing shows that, over 500k\\niterations, `msgpack.pack()` is about 5x slower than `JSON.stringify()`, and\\n`msgpack.unpack()` is about 3.5x slower than `JSON.parse()`.\\n\\nOld performance numbers are below.\\n\\nThe following tests were performed with 500,000 instances of\\nthe JavaScript object `{\\'abcdef\\' : 1, \\'qqq\\' : 13, \\'19\\' : [1, 2, 3, 4]}`:\\n\\n   * `JSON.stringify()` 7.17 seconds\\n   * `JSON.parse(JSON.stringify())` 22.18 seconds\\n   * `msgpack.pack()` 5.80 seconds\\n   * `msgpack.unpack(msgpack.pack())` 8.62 seconds\\n\\nNote that `node-msgpack` produces and consumes Buffer objects, and a such does\\nnot incur encoding/decoding overhead when performing I/O with native strings.\\n\\n### Usage\\n\\nThis module provides two methods: `pack()`, which consumes a JavaScript object\\nand produces a node Buffer object; and `unpack()`, which consumes a node Buffer\\nobject and produces a JavaScript object. Packing of all native JavaScript types\\n(undefined, boolean, numbers, strings, arrays and objects) is supported, as\\nis the node Buffer type.\\n\\nThe below code snippet packs and then unpacks a JavaScript object, verifying\\nthe resulting object at the end using `assert.deepEqual()`.\\n\\n```javascript\\n    var assert = require(\\'assert\\');\\n    var msgpack = require(\\'msgpack\\');\\n\\n    var o = {\"a\" : 1, \"b\" : 2, \"c\" : [1, 2, 3]};\\n    var b = msgpack.pack(o);\\n    var oo = msgpack.unpack(b);\\n\\n    assert.deepEqual(oo, o);\\n```\\n\\nAs a convenience, a higher level streaming API is provided in the\\n`msgpack.Stream` class, which can be constructed around a `net.Stream`\\ninstance. This object emits `msg` events when an object has been received.\\n\\n```javascript\\n    var msgpack = require(\\'msgpack\\');\\n\\n    // ... get a net.Stream instance, s, from somewhere\\n    \\n    var ms = new msgpack.Stream(s);\\n    ms.addListener(\\'msg\\', function(m) {\\n        sys.debug(\\'received message: \\' + sys.inspect(m));\\n    });\\n```\\n\\n### Type Mapping\\n\\nThe JavaScript type system does not map cleanly on to the MsgPack type system,\\nthough it\\'s pretty close.\\n\\nWhen packing, JavaScript values are mapped to MsgPack types as follows\\n\\n   * `undefined` and `null` values map to `MSGPACK_OBJECT_NIL`\\n   * `boolean` values map to `MSGPACK_OBJECT_BOOLEAN`\\n   * `number` values map differently depending on their value\\n      * Floating point values map to `MSGPACK_OBJECT_DOUBLE`\\n      * Positive values map to `MSGPACK_OBJECT_POSITIVE_INTEGER`\\n      * Negative values map to `MSGPACK_OBJECT_NEGATIVE_INTEGER`\\n   * `string` values map to `MSGPACK_OBJECT_RAW`; all strings are serialized\\n     with UTF-8 encoding\\n   * Array values (as defined by `Array.isArray()`) map to\\n     `MSGPACK_OBJECT_ARRAY`; each element in the array is packed individually\\n     the rules in this list\\n   * NodeJS Buffer values map to `MSGPACK_OBJECT_RAW`\\n   * Everything else maps to `MSGPACK_OBJECT_MAP`, where we iterate over\\n     the object\\'s properties and pack them and their values as per the\\n     mappings in this list\\n\\nWhen unpacking, MsgPack types are mapped to JavaScript values as follows\\n\\n   * `MSGPACK_OBJECT_NIL` values map to the `null` value\\n   * `MSGPACK_OBJECT_BOOLEAN` values map to `boolean` values\\n   * `MSGPACK_OBJECT_POSITIVE_INTEGER`, `MSGPACK_OBJECT_NEGATIVE_INTEGER` and\\n     `MSGPACK_OBJECT_DOUBLE` values map to `number` values\\n   * `MSGPACK_OBJECT_ARRAY` values map to arrays; each object in the array is\\n      packed individually using the rules in this list\\n   * `MSGPACK_OBJECT_RAW` values are mapped to `string` values; these values are\\n      unpacked using either UTF-8 or ASCII encoding, depending on the contents\\n      of the raw buffer\\n   * `MSGPACK_OBJECT_MAP` values are mapped to JavaScript objects; keys and\\n      values are unpacked individually using the rules in this list\\n\\nStrings are particularly problematic here, as it\\'s difficult to get hints down\\ninto the packing and unpacking codepaths about how to interpret a particular\\nstring or `MSGPACK_OBJECT_RAW`. If you have strict requirements about the\\nencoding of your strings, it\\'s recommended that you populate a Buffer object\\nyourself (e.g. using `Buffer.write()`) and pack that buffer rather than the\\nstring. This will ensure that you can control what gets packed.\\n\\nWhen unpacking, things are trickier as there is no way to know the encoding\\nused when a string was packed. There is an [an open\\nticket](http://github.com/msgpack/msgpack/issues/issue/13) for the MsgPack\\nformat to address this.\\n\\n### Command Line Utilities\\n\\nAs a convenience and for debugging, `bin/json2msgpack` and `bin/msgpack2json`\\nare provided to convert JSON data to and from MessagePack data, reading from\\nstdin and writing to stdout.\\n\\n    % echo \\'[1, 2, 3]\\' | ./bin/json2msgpack | xxd -\\n    0000000: 9301 0203                                ....\\n    % echo \\'[1, 2, 3]\\' | ./bin/json2msgpack | ./bin/msgpack2json \\n    [1,2,3]\\n\\n### Building, Installation, Testing\\n\\nThere are two ways to install msgpack.\\n\\n## NPM\\n\\n\\t\\tnpm install msgpack\\n\\nThis should build and install msgpack for you. Then just `require(\\'msgpack\\')`.\\n\\n## Manually\\n\\nYou will need node-gyp:\\n\\n    npm install -g node-gyp\\n\\nThen from the root of the msgpack repo, you can run:\\n\\n    node-gyp rebuild\\n\\n<dl>\\n  <dt>NOTE:</dt>\\n  <dd>\\n    node-gyp attempts to contact the Internet and download the target version\\n    of node.js source and store it locally.  This will only happen once for\\n    each time it sees a new node.js version.  If you\\'re on a host with no\\n    direct Internet access, you may need to shuffle this source over from\\n    another box or sneaker net.  Good luck!\\n  </dd>\\n</dl>\\n\\n## Testing\\n\\nTo run all tests use:\\n\\n    ./run_tests\\n\\nTo run a specific test:\\n\\n    ./run_tests test/lib/msgpack.js\\n\\n<dl>\\n  <dt>NOTE:</dt>\\n  <dd>\\n    Tests are based on a modified version of\\n    nodeunit (https://github.com/godsflaw/nodeunit).\\n    Follow ./run_tests instructions if you run into problems.\\n  </dd>\\n</dl>\\n\\n## Benchmarks\\n\\nTo run benchmarks:\\n\\n    ./run_tests test/benchmark/benchmark.js\\n'},\n",
       " {'repo': 'genforce/interfacegan',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# InterFaceGAN - Interpreting the Latent Space of GANs for Semantic Face Editing\\n\\n![Python 3.7](https://img.shields.io/badge/python-3.7-green.svg?style=plastic)\\n![pytorch 1.1.0](https://img.shields.io/badge/pytorch-1.1.0-green.svg?style=plastic)\\n![TensorFlow 1.12.2](https://img.shields.io/badge/tensorflow-1.12.2-green.svg?style=plastic)\\n![sklearn 0.21.2](https://img.shields.io/badge/sklearn-0.21.2-green.svg?style=plastic)\\n\\n![image](./docs/assets/teaser.jpg)\\n**Figure:** *High-quality facial attributes editing results with InterFaceGAN.*\\n\\nIn this repository, we propose an approach, termed as InterFaceGAN, for semantic face editing. Specifically, InterFaceGAN is capable of turning an unconditionally trained face synthesis model to controllable GAN by interpreting the very first latent space and finding the hidden semantic subspaces.\\n\\n[[Paper (CVPR)](https://arxiv.org/pdf/1907.10786.pdf)]\\n[[Paper (TPAMI)](https://arxiv.org/pdf/2005.09635.pdf)]\\n[[Project Page](https://genforce.github.io/interfacegan/)]\\n[[Demo](https://www.youtube.com/watch?v=uoftpl3Bj6w)]\\n[[Colab](https://colab.research.google.com/github/genforce/interfacegan/blob/master/docs/InterFaceGAN.ipynb)]\\n\\n## How to Use\\n\\nPick up a model, pick up a boundary, pick up a latent code, and then EDIT!\\n\\n```bash\\n# Before running the following code, please first download\\n# the pre-trained ProgressiveGAN model on CelebA-HQ dataset,\\n# and then place it under the folder \".models/pretrain/\".\\nLATENT_CODE_NUM=10\\npython edit.py \\\\\\n    -m pggan_celebahq \\\\\\n    -b boundaries/pggan_celebahq_smile_boundary.npy \\\\\\n    -n \"$LATENT_CODE_NUM\" \\\\\\n    -o results/pggan_celebahq_smile_editing\\n```\\n\\n## GAN Models Used (Prior Work)\\n\\nBefore going into details, we would like to first introduce the two state-of-the-art GAN models used in this work, which are ProgressiveGAN (Karras *el al.*, ICLR 2018) and StyleGAN (Karras *et al.*, CVPR 2019). These two models achieve high-quality face synthesis by learning unconditional GANs. For more details about these two models, please refer to the original papers, as well as the official implementations.\\n\\nProgressiveGAN:\\n  [[Paper](https://arxiv.org/pdf/1710.10196.pdf)]\\n  [[Code](https://github.com/tkarras/progressive_growing_of_gans)]\\n\\nStyleGAN:\\n  [[Paper](https://arxiv.org/pdf/1812.04948.pdf)]\\n  [[Code](https://github.com/NVlabs/stylegan)]\\n\\n## Code Instruction\\n\\n### Generative Models\\n\\nA GAN-based generative model basically maps the latent codes (commonly sampled from high-dimensional latent space, such as standart normal distribution) to photo-realistic images. Accordingly, a base class for generator, called `BaseGenerator`, is defined in `models/base_generator.py`. Basically, it should contains following member functions:\\n\\n- `build()`: Build a pytorch module.\\n- `load()`: Load pre-trained weights.\\n- `convert_tf_model()` (Optional): Convert pre-trained weights from tensorflow model.\\n- `sample()`: Randomly sample latent codes. This function should specify what kind of distribution the latent code is subject to.\\n- `preprocess()`: Function to preprocess the latent codes before feeding it into the generator.\\n- `synthesize()`: Run the model to get synthesized results (or any other intermediate outputs).\\n- `postprocess()`: Function to postprocess the outputs from generator to convert them to images.\\n\\nWe have already provided following models in this repository:\\n\\n- ProgressiveGAN:\\n  - A clone of official tensorflow implementation: `models/pggan_tf_official/`. This clone is only used for converting tensorflow pre-trained weights to pytorch ones. This conversion will be done automitally when the model is used for the first time. After that, tensorflow version is not used anymore.\\n  - Pytorch implementation of official model (just for inference): `models/pggan_generator_model.py`.\\n  - Generator class derived from `BaseGenerator`: `models/pggan_generator.py`.\\n  - Please download the official released model trained on CelebA-HQ dataset and place it in folder `models/pretrain/`.\\n- StyleGAN:\\n  - A clone of official tensorflow implementation: `models/stylegan_tf_official/`. This clone is only used for converting tensorflow pre-trained weights to pytorch ones. This conversion will be done automitally when the model is used for the first time. After that, tensorflow version is not used anymore.\\n  - Pytorch implementation of official model (just for inference): `models/stylegan_generator_model.py`.\\n  - Generator class derived from `BaseGenerator`: `models/stylegan_generator.py`.\\n  - Please download the official released models trained on CelebA-HQ dataset and FF-HQ dataset and place them in folder `models/pretrain/`.\\n  - Support synthesizing images from $\\\\mathcal{Z}$ space, $\\\\mathcal{W}$ space, and extended $\\\\mathcal{W}$ space (18x512).\\n  - Set truncation trick and noise randomization trick in `models/model_settings.py`. Among them, `STYLEGAN_RANDOMIZE_NOISE` is highly recommended to set as `False`. `STYLEGAN_TRUNCATION_PSI = 0.7` and `STYLEGAN_TRUNCATION_LAYERS = 8` are inherited from official implementation. Users can customize their own models. NOTE: These three settings will NOT affect the pre-trained weights.\\n- Customized model:\\n  - Users can do experiments with their own models by easily deriving new class from `BaseGenerator`.\\n  - Before used, new model should be first registered in `MODEL_POOL` in file `models/model_settings.py`.\\n\\n### Utility Functions\\n\\nWe provide following utility functions in `utils/manipulator.py` to make InterFaceGAN much easier to use.\\n\\n- `train_boundary()`: This function can be used for boundary searching. It takes pre-prepared latent codes and the corresponding attributes scores as inputs, and then outputs the normal direction of the separation boundary. Basically, this goal is achieved by training a linear SVM. The returned vector can be further used for semantic face editing.\\n- `project_boundary()`: This function can be used for conditional manipulation. It takes a primal direction and other conditional directions as inputs, and then outputs a new normalized direction. Moving latent code along this new direction will manipulate the primal attribute yet barely affect the conditioned attributes. NOTE: For now, at most two conditions are supported.\\n- `linear_interpolate()`: This function can be used for semantic face editing. It takes a latent code and the normal direction of a particular semantic boundary as inputs, and then outputs a collection of manipulated latent codes with linear interpolation. These interpolation can be used to see how the synthesis will vary if moving the latent code along the given direction.\\n\\n### Tools\\n\\n- `generate_data.py`: This script can be used for data preparation. It will generate a collection of syntheses (images are saved for further attribute prediction) as well as save the input latent codes.\\n\\n- `train_boundary.py`: This script can be used for boundary searching.\\n\\n- `edit.py`: This script can be usd for semantic face editing.\\n\\n## Usage\\n\\nWe take ProgressiveGAN model trained on CelebA-HQ dataset as an instance.\\n\\n### Prepare data\\n\\n```bash\\nNUM=10000\\npython generate_data.py -m pggan_celebahq -o data/pggan_celebahq -n \"$NUM\"\\n```\\n\\n### Predict Attribute Score\\n\\nGet your own predictor for attribute `$ATTRIBUTE_NAME`, evaluate on all generated images, and save the inference results as `data/pggan_celebahq/\"$ATTRIBUTE_NAME\"_scores.npy`. NOTE: The save results should be with shape `($NUM, 1)`.\\n\\n### Search Semantic Boundary\\n\\n```bash\\npython train_boundary.py \\\\\\n    -o boundaries/pggan_celebahq_\"$ATTRIBUTE_NAME\" \\\\\\n    -c data/pggan_celebahq/z.npy \\\\\\n    -s data/pggan_celebahq/\"$ATTRIBUTE_NAME\"_scores.npy\\n```\\n\\n### Compute Conditional Boundary (Optional)\\n\\nThis step is optional. It depends on whether conditional manipulation is needed. Users can use function `project_boundary()` in file `utils/manipulator.py` to compute the projected direction.\\n\\n## Boundaries Description\\n\\nWe provided following boundaries in folder `boundaries/`. The boundaries can be more accurate if stronger attribute predictor is used.\\n\\n- ProgressiveGAN model trained on CelebA-HQ dataset:\\n  - Single boundary:\\n    - `pggan_celebahq_pose_boundary.npy`: Pose.\\n    - `pggan_celebahq_smile_boundary.npy`: Smile (expression).\\n    - `pggan_celebahq_age_boundary.npy`: Age.\\n    - `pggan_celebahq_gender_boundary.npy`: Gender.\\n    - `pggan_celebahq_eyeglasses_boundary.npy`: Eyeglasses.\\n    - `pggan_celebahq_quality_boundary.npy`: Image quality.\\n  - Conditional boundary:\\n    - `pggan_celebahq_age_c_gender_boundary.npy`: Age (conditioned on gender).\\n    - `pggan_celebahq_age_c_eyeglasses_boundary.npy`: Age (conditioned on eyeglasses).\\n    - `pggan_celebahq_age_c_gender_eyeglasses_boundary.npy`: Age (conditioned on gender and eyeglasses).\\n    - `pggan_celebahq_gender_c_age_boundary.npy`: Gender (conditioned on age).\\n    - `pggan_celebahq_gender_c_eyeglasses_boundary.npy`: Gender (conditioned on eyeglasses).\\n    - `pggan_celebahq_gender_c_age_eyeglasses_boundary.npy`: Gender (conditioned on age and eyeglasses).\\n    - `pggan_celebahq_eyeglasses_c_age_boundary.npy`: Eyeglasses (conditioned on age).\\n    - `pggan_celebahq_eyeglasses_c_gender_boundary.npy`: Eyeglasses (conditioned on gender).\\n    - `pggan_celebahq_eyeglasses_c_age_gender_boundary.npy`: Eyeglasses (conditioned on age and gender).\\n- StyleGAN model trained on CelebA-HQ dataset:\\n  - Single boundary in $\\\\mathcal{Z}$ space:\\n    - `stylegan_celebahq_pose_boundary.npy`: Pose.\\n    - `stylegan_celebahq_smile_boundary.npy`: Smile (expression).\\n    - `stylegan_celebahq_age_boundary.npy`: Age.\\n    - `stylegan_celebahq_gender_boundary.npy`: Gender.\\n    - `stylegan_celebahq_eyeglasses_boundary.npy`: Eyeglasses.\\n  - Single boundary in $\\\\mathcal{W}$ space:\\n    - `stylegan_celebahq_pose_w_boundary.npy`: Pose.\\n    - `stylegan_celebahq_smile_w_boundary.npy`: Smile (expression).\\n    - `stylegan_celebahq_age_w_boundary.npy`: Age.\\n    - `stylegan_celebahq_gender_w_boundary.npy`: Gender.\\n    - `stylegan_celebahq_eyeglasses_w_boundary.npy`: Eyeglasses.\\n\\n- StyleGAN model trained on FF-HQ dataset:\\n  - Single boundary in $\\\\mathcal{Z}$ space:\\n    - `stylegan_ffhq_pose_boundary.npy`: Pose.\\n    - `stylegan_ffhq_smile_boundary.npy`: Smile (expression).\\n    - `stylegan_ffhq_age_boundary.npy`: Age.\\n    - `stylegan_ffhq_gender_boundary.npy`: Gender.\\n    - `stylegan_ffhq_eyeglasses_boundary.npy`: Eyeglasses.\\n  - Conditional boundary in $\\\\mathcal{Z}$ space:\\n    - `stylegan_ffhq_age_c_gender_boundary.npy`: Age (conditioned on gender).\\n    - `stylegan_ffhq_age_c_eyeglasses_boundary.npy`: Age (conditioned on eyeglasses).\\n    - `stylegan_ffhq_eyeglasses_c_age_boundary.npy`: Eyeglasses (conditioned on age).\\n    - `stylegan_ffhq_eyeglasses_c_gender_boundary.npy`: Eyeglasses (conditioned on gender).\\n  - Single boundary in $\\\\mathcal{W}$ space:\\n    - `stylegan_ffhq_pose_w_boundary.npy`: Pose.\\n    - `stylegan_ffhq_smile_w_boundary.npy`: Smile (expression).\\n    - `stylegan_ffhq_age_w_boundary.npy`: Age.\\n    - `stylegan_ffhq_gender_w_boundary.npy`: Gender.\\n    - `stylegan_ffhq_eyeglasses_w_boundary.npy`: Eyeglasses.\\n\\n## BibTeX\\n\\n```bibtex\\n@inproceedings{shen2020interpreting,\\n  title     = {Interpreting the Latent Space of GANs for Semantic Face Editing},\\n  author    = {Shen, Yujun and Gu, Jinjin and Tang, Xiaoou and Zhou, Bolei},\\n  booktitle = {CVPR},\\n  year      = {2020}\\n}\\n```\\n\\n```bibtex\\n@article{shen2020interfacegan,\\n  title   = {InterFaceGAN: Interpreting the Disentangled Face Representation Learned by GANs},\\n  author  = {Shen, Yujun and Yang, Ceyuan and Tang, Xiaoou and Zhou, Bolei},\\n  journal = {TPAMI},\\n  year    = {2020}\\n}\\n```\\n'},\n",
       " {'repo': 'OrderOfThePorcupine/ProjectPorcupine',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# ProjectPorcupine [![Build Status](https://travis-ci.org/OrderOfThePorcupine/ProjectPorcupine.svg?branch=master)](https://travis-ci.org/OrderOfThePorcupine/ProjectPorcupine)\\n\\n### Project Porcupine: A Base-Building Game...in Space!\\n\\n![Preview Thumbnail](https://cloud.githubusercontent.com/assets/22880786/19826387/7ad0f0d2-9dd4-11e6-92f3-eb47b395ac63.png)\\n\\n[About](#about)  \\n[Copyright & Licensing](#copyright--licensing)  \\n[Contributing](#contributing)  \\n[Weeklies](#weeklies)  \\n[Community](#community)  \\n[Contact](#contact) \\n\\n## About\\n\\nProject Porcupine was created to serve two purposes:\\n\\n1. To act as a tutorial to teach people how to make a full,\\n  complex, and multi-featured game inside of Unity (as opposed\\n  to the more typical one-off, single-feature tutorials that\\n  are more common).\\n\\n2. To provide a basic skeleton for any game that requires a\\n  tile-based map with self-governing agents (i.e. characters)\\n  as well as highly customizable objects (i.e. XML/Lua defined\\n  furniture.)  To this end, we would be making a program themed as\\n  a starbase-construction game -- though there\\'s no reason that \\n  someone couldn\\'t produce something with a wildly different theme\\n  or purpose (including not being a base-building game at all).\\n\\n**However, our true ultimate goal is to produce a community-developed\\nbase building game in a similar style to Dwarf Fortress or RimWorld!**\\n\\nIf you want to watch the tutorials that cover the entirety of\\ncreating this project from scratch, please visit:\\n\\n * <https://www.youtube.com/user/quill18creates/playlists>\\n\\nProject Porcupine was created by Martin \"quill18\" Glaude, whose work\\nwas supported via Patreon:\\n\\n * <https://www.patreon.com/quill18creates>\\n\\n## Copyright & Licensing\\n\\nThe base project code is copyrighted by Martin \"quill18\" Glaude and\\nis covered by multiple licenses.\\n\\nAll program code (i.e. C#, Lua, XML) is licensed under GPL v3.0 unless otherwise\\nspecified.  Please see the \"LICENSE\" file for more information.\\n\\nAll non-code assets (e.g. art, sound) is licensed under CC BY-NC-SA 3.0\\n(Attribution-NonCommercial-ShareAlike 3.0 Unported) unless otherwise specified.\\n\\nThe original tutorial project files, which feature no community-contributed code,\\nare licensed under the MIT License and can be found here:\\n * <http://quill18.com/porcupine/project_files/>\\n \\nAudio engine : [FMOD](http://www.fmod.com/) by Firelight Technologies\\n\\nLogger: Based on UnityDebugger by Valian (at <https://github.com/Valian/UnityDebugger>), modified source code available at <https://github.com/koosemose/UnityDebugger>\\n\\n## Contributing\\n\\nPlease check the [CONTRIBUTING.md](CONTRIBUTING.md) file for contribution instructions and guidelines.\\n\\nFor further information, such as Roadmaps, explanations of systems and features, Standards and Conventions, and all your Git needs and troubleshooting see the [Wiki](https://github.com/TeamPorcupine/ProjectPorcupine/wiki)\\n\\nMake sure that you are using Unity 2017.4.22 [Windows](https://unity3d.com/get-unity/download?thank-you=update&download_nid=61174&os=Win) | [Mac](https://unity3d.com/get-unity/download?thank-you=update&download_nid=61174&os=Mac)\\n\\n## Weeklies\\n\\nYou can download and play already built \\'weekly\\' versions for Windows, Mac, and Linux [here](https://bintray.com/orderoftheporcupine/ProjectPorcupine/Prebuilt-Binaries).\\n\\nHowever, these are merely \\'snapshots\\', a preview of the current development build, and are not representative of any type of release. They may contain errors, bugs, partially completed features, and other potential issues.  If you open a bug report please include what week you are using (either the week number or the date is fine) as this will help us find the bug easier and determine if your error has already been fixed.\\n\\n## Community\\n\\n* [Offical Discord Channel ](https://discord.gg/68hkpSA)\\n* [Unoffical Subreddit](https://reddit.com/r/ProjectPorcupine)\\n\\n## Contact\\n\\nYou can contact Quill18 by Twitter (@quill18) or email:\\n    quill18@quill18.com\\n\\nHowever, please note that Quill receives a lot of email and may\\nnot be able to respond to everyone in a timely manner.\\n\\n'},\n",
       " {'repo': 'clear-code-projects/Space-invaders',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': ''},\n",
       " {'repo': 'gengchenmai/space2vec',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Multi-Scale Representation Learning for Spatial Feature Distributions using Grid Cells\\nCode for recreating the results in [our ICLR 2020 paper](https://openreview.net/forum?id=rJljdh4KDH).\\n\\n## Related Link\\n1. [OpenReview Paper](https://openreview.net/forum?id=rJljdh4KDH)\\n2. [Arxiv Paper](https://arxiv.org/abs/2003.00824)\\n3. [ICLR 2020 Presentation](https://www.youtube.com/watch?v=7hTtBXqea1E)\\n4. [ICLR 2020 Presentation Slides](http://www.geog.ucsb.edu/~gengchen_mai/presentations/2020-ICLR2020-Space2Vec.pdf)\\n\\nPlease visit [my Homepage](http://www.geog.ucsb.edu/~gengchen_mai/) for more information.\\n\\n## Motivation\\n<p align=\"center\">\\n  <img src=\"res_fig/intro.png\" alt=\"intro\" width=\"1000\" />\\n</p>\\n\\nThis code contains two parts which are corresponding to two tasks in our paper:\\n## Point of Interest (POI) Type Classification Task\\n`spacegraph/` folder contains codes for recreating the evaluation results of POI type classification task: both location modeling and context modeling.\\n\\n### Dependencies\\n- Python 2.7+\\n- Torch 1.0.1+\\n- Other required packages are summarized in `spacegraph/requirements.txt`.\\n\\nTo set up the code for space2vec for POI Type classfication, run `python spacegraph/setup.py`.\\n\\n### Data\\nYou can find the POI type classification dataset in `spacegraph/data_collection/Place2Vec_center/`.\\n\\n1. `spacegraph/data_collection/Place2Vec_center/poi_type.json`: the POI type ID to POI type name;\\n2. `spacegraph/data_collection/Place2Vec_center/pointset.pkl`: the total list of POI beng considered in the form: `(id, (X, Y), (Type1, Type2,...TypeM), training/validation/test)` where (X,Y) are projected coordinates of this POI;\\n3. `spacegraph/data_collection/Place2Vec_center/neighborgraphs_train.pkl` (neighborgraphs_validation.pkl or neighborgraphs_test.pkl): the neighboring POI graph structure for training/validation/test in the form: `[CenterPT, [ContextPT1, ContextPT2, ..., ContextPTN], training/validation/test]`. CenterPT is the center POI id while ContextPTs are its nearby POI id. Note that for validation and test dataset, the center point corresponds to the POI with `validation/test` in `pointset.pkl`.\\n\\n\\n\\n### Code Usage\\nThis code is implemented in Python 2.7\\nAll codes about the POI type classification task are in `spacegraph/spacegraph_codebase/`.\\n\\n#### Location Modeling (See Section 5.1.1 and 5.1.2 in [our ICLR 2020 paper](https://openreview.net/forum?id=rJljdh4KDH))\\nYou can train different models from different bash files in `spacegraph/`(See Appendix A.1 in [our ICLR 2020 paper](https://openreview.net/forum?id=rJljdh4KDH)):\\n1. `direct`: run `bash Place2Vec_2_enc_dec_global_naive.sh`.\\n2. `tile`: run `bash Place2Vec_2_enc_dec_global_gridlookup.sh`.\\n3. `wrap`: run `bash Place2Vec_2_enc_dec_global_aodha.sh`.\\n4. `rbf`: run `bash Place2Vec_2_enc_dec_global_rbf.sh`.\\n5. `gird`: run `bash Place2Vec_2_enc_dec_global_grid.sh`.\\n6. `theory`: run `bash Place2Vec_2_enc_dec_global_theory.sh`.\\n\\nResults:\\n1. The trained model will be available in the directory you specified for `--model_dir`.\\n2. The log including the evaluation results will be available in the directory you specified for `--log_dir`.\\n\\n###### The Comparison Among the Response Maps of Different Models\\n<p align=\"center\">\\n  <img src=\"res_fig/location_modeling.png\" alt=\"location_modeling\" width=\"1000\" />\\n</p>\\n\\n#### Spatial Context Modeling (See Section 5.1.3 in [our ICLR 2020 paper](https://openreview.net/forum?id=rJljdh4KDH))\\nYou can train different models from different bash files in `spacegraph/`(See Appendix A.1 in [our ICLR 2020 paper](https://openreview.net/forum?id=rJljdh4KDH)):\\n1. `none`: run `bash Place2Vec_2_enc_dec_none.sh`.\\n2. `direct`: run `bash Place2Vec_2_enc_dec_naive.sh`.\\n3. `polar`: run `bash Place2Vec_2_enc_dec_polar.sh`.\\n4. `tile`: run `bash Place2Vec_2_enc_dec_gridlookup.sh`.\\n5. `polar_tile`: run `bash Place2Vec_2_enc_dec_polargridlookup.sh`.\\n6. `wrap`: run `bash Place2Vec_2_enc_dec_aodha.sh`.\\n7. `rbf`: run `bash Place2Vec_2_enc_dec_rbf.sh`.\\n8. `scaled_rbf`: run `bash Place2Vec_2_enc_dec_scaledrbf.sh`.\\n9. `gird`: run `bash Place2Vec_2_enc_dec_grid.sh`.\\n10. `theory`: run `bash Place2Vec_2_enc_dec_theory.sh`.\\n\\nResults:\\n1. The trained model will be available in the directory you specified for `--model_dir`.\\n2. The log including the evaluation results will be available in the directory you specified for `--log_dir`.\\n\\n###### The Comparison Among the Response Maps of Different Models\\n<p align=\"center\">\\n  <img src=\"res_fig/context_modeling.png\" alt=\"context_modeling\" width=\"1000\" />\\n</p>\\n\\n## Geo-Aware Fine-Grained Image Classification Task\\n\\n`geo_prior/` folder contains codes for recreating the evaluation results of the geo-aware fine-grained image classification task in [our ICLR 2020 paper](https://openreview.net/forum?id=rJljdh4KDH).\\n\\nThese codes are modified from [Mac Aodha et al.\\'s GitHub codebase](https://github.com/macaodha/geo_prior) in which we add multiple Space2Vec location encoder modules to capture the geographic priors information about images. \\n\\n### Dependencies\\n- Python 3.6+\\n- Torch 1.3.0+\\n- Other required packages are summarized in `geo_prior/requirements.txt`.\\n\\n\\n### Data\\nIn order to obtain the data, please go to [Mac Aodha et al.\\'s project website](http://www.vision.caltech.edu/~macaodha/projects/geopriors/index.html).\\n\\n### Code Usage\\nThis code is implemented in Python 3.\\n\\n`geo_prior/geo_prior/` contains the main code for training and evaluating models (We use BirdSnap† dataset as an example):\\n1. `geo_prior/geo_prior/train_geo_net.py` is used to train the location encoder model. Run `python3 train_geo_net.py`. \\n2. `geo_prior/geo_prior/run_evaluation.py` is used to evaluate the location encoder model by combining the pre-trained CNN features.  Run `python3 run_evaluation.py`. \\n\\n\\n\\n\\n\\n### Reference\\nIf you find our work useful in your research please consider citing our paper.  \\n```\\n@inproceedings{space2vec_iclr2020,\\n\\ttitle={Multi-Scale Representation Learning for Spatial Feature Distributions using Grid Cells},\\n\\tauthor={Mai, Gengchen and Janowicz, Krzysztof and Yan, Bo and Zhu, Rui and  Cai, Ling and Lao, Ni},\\n\\tbooktitle={The Eighth International Conference on Learning Representations},\\n\\tyear={2020},\\n\\torganization={openreview}\\n}\\n```\\nIf you want to use our code for image classification, please cite both [our ICLR 2020 paper](https://openreview.net/forum?id=rJljdh4KDH) and [Mac Aodha et al\\'s ICCV paper](https://arxiv.org/abs/1906.05272):\\n```\\n@inproceedings{geo_priors_iccv19,\\n  title     = {{Presence-Only Geographical Priors for Fine-Grained Image Classification}},\\n  author    = {Mac Aodha, Oisin and Cole, Elijah and Perona, Pietro},\\n  booktitle = {ICCV},\\n  year = {2019}\\n}\\n```'},\n",
       " {'repo': 'BlenderCN/add_mesh_SpaceshipGenerator',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# add_mesh_SpaceshipGenerator_2.8\\nSpaceshipGenerator for blender 2.8\\n\\n[b站](https://www.bilibili.com/video/BV1m4411L7cu)\\n\\nfrom [SpaceshipGenerator](https://github.com/a1studmuffin/SpaceshipGenerator)\\n\\n![](https://github.com/BlenderCN/add_mesh_SpaceshipGenerator_2.8/blob/master/spaceship.png)\\n'},\n",
       " {'repo': 'dotnet/csharpstandard',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# C# language standard\\n\\nWorking space for [ECMA-TC49-TG2](https://www.ecma-international.org/task-groups/tc49-tg2/), the C# standard committee.\\n\\n- The text is licensed under the [Creative Commons license](LICENSE).\\n- The code for our tools is licensed under the [MIT license](LICENSE-CODE).\\n\\nThis project has adopted the code of conduct defined by the Contributor Covenant to clarify expected behavior in our community. For more information, see the [.NET Foundation Code of Conduct](https://dotnetfoundation.org/code-of-conduct).\\n\\n## C# Language Specification\\n\\n### C# 7.0 draft\\n\\nThe branch `draft-v7` has the draft text for C# 7.0. It has not been submitted as a formal standard to ECMA. This version is a working draft that contains the features for C# 7.0.\\n\\n### C# 6.0 standard\\n\\nThe branch `standard-v6` has the ECMA C# C# 6.0 standard text, in Markdown format. For the official standard, see the [ECMA site](https://www.ecma-international.org/publications-and-standards/standards/ecma-334/).\\n\\n### C# 5.0 standard\\n\\nThe branch `standard-v5` has the ECMA C# 5.0 standard text, converted to Markdown. For the official standard, see the [ECMA site](https://www.ecma-international.org/publications-and-standards/standards/ecma-334/).\\n\\nThis version is stored in this branch as a base markdown version to compare with future updated standard texts.\\n\\n<!--\\n(This document is also available for download: [csharp.pdf](CSharp%20Language%20Specification.pdf?raw=true) and [csharp.docx](CSharp%20Language%20Specification.docx?raw=true))\\n-->\\n\\n### Comments within the standard\\n\\nThere are HTML comments (`<!-- comment -->`) within the standard for the sake of tooling. Some help in the process of converting the standard to Word, and others are for automated testing purposes.\\n\\nSome automated test comments refer to error codes that are specific to the Microsoft C# compiler (e.g. \"CS0509\") to test that compilation fails as expected, where an example presents deliberately-invalid code. These error codes are not part of the standard, and should not be viewed as any kind of compliance check for other compilers.\\n\\nMore broadly, *no* comments should be regarded as being part of the standard itself.\\n\\n## Admin folder\\n\\nA home for adminstrative files (such as [eventually] meeting agendas and minutes).\\n\\nFor now, it contains separate logs for the work going on to add V6 (and then V7) features.\\n\\n## Tools folder\\n\\nThis folder contains tools related to maintaining and converting the ECMA C# spec (ECMA-354).\\n\\n### GetGrammar\\n\\nThis folder contains an ANTLR grammar-extraction tool and support files.\\n\\n- ExtractGrammar.exe - the simple-minded grammar-extraction program. It processes only *one* md file.\\n\\n- GetGrammar.bat - the Windows batch file that invokes ExtractGrammar on each md file of the C# specification that contains ANTLR grammar blocks, in clause order, inserting some md headers and such along the way. The result is a file called grammar.md, **which is a direct replacement for that file in the specification repo**.\\n\\n> A minor wart: There is an extraneous blank line at the beginning of each of the lexical, syntactic, and unsafe grammars. At a glance, the amount of programming effort probably needed to stop this from happening seems to be *huge* compared with simply deleting those three lines manually.\\n\\n### MarkdownConverter\\n\\nThis tool is used by the committee to produce a Word format of the standard for submission to ECMA or ISO. This is run on each PR to ensure we can always produce the correct format when needed.\\n\\n### StandardAnchorTags\\n\\nThis tool creates the outline using section numbers, and updates all links to the correct section number. Its purpose is to ensure that all references continue to point to the correct section, and that the table of contents shows the correct section numbers for all sections.\\n\\nContributors that add sections should follow the guidance in our [contributor guide](CONTRIBUTING.md#how-to-add-or-remove-clauses) to ensure that links to new sections are incorporated correctly. This tool is run on each PR in a `dry-run` mode to ensure that the changes will parse correctly. When a PR is merged, the tool runs to update all section links.\\n\\n### ExampleExtractor and ExampleTester\\n\\nThese two tools work in tandem to test that the examples presented work (or fail, where invalid code is presented) as expected.\\n\\nExampleExtractor populates a temporary directory with code and metadata extracted from the standard. ExampleTester then compiles and runs (where applicable) that code. The test-examples.sh script provides an easy way of running both tools together.\\n\\n## .NET Foundation\\n\\nThis project is supported by the [.NET Foundation](https://dotnetfoundation.org).\\n\\n## Table of contents - C# standard\\n\\nThe [README.md](standard/README.md) file in the `standard` folder contains a detailed table of contents for the C# standard.'},\n",
       " {'repo': 'appsecco/spaces-finder',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"# Spaces finder\\n\\n #### Spaces finder is a tool to quickly enumerate [DigitalOcean Spaces](https://www.digitalocean.com/community/tutorials/an-introduction-to-digitalocean-spaces) to look for loot. It's similar to a subdomain bruteforcer but is made specifically for DigitalOcean Spaces and also has some extra features that allow you to grep for delicious files as well as download interesting files if you're not afraid to quickly fill up your hard drive.\\n #### By [Bharath](https://twitter.com/0xbharath)\\n #### Built on top of AWSBucketDump by [@ok_bye_now](https://twitter.com/ok_bye_now)\\n\\n## Pre-Requisites\\nNon-Standard Python Libraries:\\n\\n- [xmltodict](https://pypi.python.org/pypi/xmltodict)\\n- [requests](docs.python-requests.org/)\\n- Written in Python 3.6\\n\\n## Overview\\n\\n- This is a tool that enumerates DigitalOcean Spaces and looks for interesting files \\n- I have example wordlists but I haven't put much time into refining them\\n- `https://github.com/danielmiessler/SecLists` will have all the word lists you need\\n- If you are targeting a specific company, you will likely want to use jhaddix's [enumall](https://github.com/jhaddix/domain) tool which leverages [recon-ng](https://bitbucket.org/LaNMaSteR53/recon-ng) and [Alt-DNS](https://github.com/infosec-au/altdns) \\n- As far as word lists for grepping interesting files, that is completely up to you. The one I provided has some basics and yes, those word lists are based on files that I personally have found with this tool.\\n- Using the download feature might fill your hard drive up, you can provide a max file size for each download at the command line when you run the tool. Keep in mind that it is in bytes.\\n\\n\\n## Usage:\\n\\n```\\nusage: python3 spaces_finder.py [-h] [-D] [-t THREADS] -l HOSTLIST [-g GREPWORDS] [-m MAXSIZE]\\n\\noptional arguments:\\n  -h, --help    show this help message and exit`\\n  -D            Download files. This requires significant diskspace`\\n  -d            If set to 1 or True, create directories for each host w/ results`\\n  -t THREADS    number of threads`\\n  -l HOSTLIST`\\n  -g GREPWORDS  Provide a wordlist to grep for`\\n  -m MAXSIZE    Maximum file size to download.`\\n```\\n\\n`python3 spaces_finder.py -l SpacesNames.txt -g interesting_keywords.txt -D -m 500000 -d 1 -t 5`\\n\"},\n",
       " {'repo': 'open-notify/Open-Notify-API',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"# Open Nofity APIs\\n\\n![](https://img.shields.io/badge/language-python%202-green.svg)\\n[![Build Status](https://travis-ci.org/open-notify/Open-Notify-API.svg)](https://travis-ci.org/open-notify/Open-Notify-API)\\n[![Requirements Status](https://requires.io/github/open-notify/Open-Notify-API/requirements.svg?branch=master)](https://requires.io/github/open-notify/Open-Notify-API/requirements/?branch=master)\\n[![Docs](https://readthedocs.org/projects/open-notify-api/badge/?version=latest)](http://open-notify-api.readthedocs.org/)\\n\\n\\nAPIs for [api.open-notify.org](http://api.open-notify.org)\\n\\n\\n## Install for the first time:\\n\\nMake sure you have some packages:\\n\\n    # apt-get install python python-dev python-pip virtualenvwrapper redis-server\\n\\nNote: if you're installing `virtualenvwrapper` for the first time, be sure to log out and back in before continuing.\\n\\nCreate a virtual environment\\n\\n    $ mkvirtualenv opennotify\\n    (opennotify)$ pip install -r requirements.txt\\n\\nGet data\\n\\n    (opennotify)$ python update.py\\n\\n\\n## Run locally:\\n\\nStart virtual environment\\n\\n    $ workon opennotify\\n\\nRun with [foreman](https://github.com/ddollar/foreman) using dev procfile:\\n\\n    (opennotify)$ foreman start -f Procfile.dev\\n\\nOpen a browser to [localhost:5000](http://localhost:5000).\\n\\n\\n## Run Testsuite\\n\\n    (opennotify)$ cd testsuite\\n    (opennotify)$ pip install -r requirements.txt\\n    (opennotify)$ cd ..\\n    (opennotify)$ make test\\n\\n\\n## API Documentation\\n\\nDocs are in the gh-pages branch, or on the web here:\\n\\n - [Open Notify API Documentation](http://open-notify.org/Open-Notify-API/)\\n\\n\\n## License\\n\\nCopyright (C) 2016 Nathan Bergey\\n\\nThis program is free software: you can redistribute it and/or modify\\nit under the terms of the GNU General Public License as published by\\nthe Free Software Foundation, either version 3 of the License, or\\n(at your option) any later version.\\n\\nThis program is distributed in the hope that it will be useful,\\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\nGNU General Public License for more details.\\n\\nYou should have received a copy of the GNU General Public License\\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\\n\"},\n",
       " {'repo': 'datascopeanalytics/traces',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"# traces\\n\\n[![Version](https://img.shields.io/pypi/v/traces.svg?)](https://pypi.python.org/pypi/traces) [![PyVersions](https://img.shields.io/pypi/pyversions/traces.svg)](https://pypi.python.org/pypi/traces) [![CircleCI](https://circleci.com/gh/datascopeanalytics/traces/tree/master.svg?style=shield)](https://circleci.com/gh/datascopeanalytics/traces/tree/master) [![Documentation Status](https://readthedocs.org/projects/traces/badge/?version=master)](https://traces.readthedocs.io/en/master/?badge=master) [![Coverage Status](https://coveralls.io/repos/github/datascopeanalytics/traces/badge.svg?branch=master)](https://coveralls.io/github/datascopeanalytics/traces?branch=master)\\n\\nA Python library for unevenly-spaced time series analysis.\\n\\n## Why?\\n\\nTaking measurements at irregular intervals is common, but most tools are\\nprimarily designed for evenly-spaced measurements. Also, in the real\\nworld, time series have missing observations or you may have multiple\\nseries with different frequencies: it can be useful to model these as\\nunevenly-spaced.\\n\\nTraces was designed by the team at\\n[Datascope](https://datascopeanalytics.com/) based on several practical\\napplications in different domains, because it turns out [unevenly-spaced\\ndata is actually pretty great, particularly for sensor data\\nanalysis](https://datascopeanalytics.com/blog/unevenly-spaced-time-series/).\\n\\n## Installation\\n\\nTo install traces, run this command in your terminal:\\n\\n```bash\\n$ pip install traces\\n```\\n\\n## Quickstart: using traces\\n\\nTo see a basic use of traces, let's look at these data from a light\\nswitch, also known as _Big Data from the Internet of Things_.\\n\\n![](docs/_static/img/trace.svg)\\n\\nThe main object in traces is a [TimeSeries](https://traces.readthedocs.io/en/master/api_reference.html#timeseries), which you\\ncreate just like a dictionary, adding the five measurements at 6:00am,\\n7:45:56am, etc.\\n\\n```python\\n>>> time_series = traces.TimeSeries()\\n>>> time_series[datetime(2042, 2, 1,  6,  0,  0)] = 0 #  6:00:00am\\n>>> time_series[datetime(2042, 2, 1,  7, 45, 56)] = 1 #  7:45:56am\\n>>> time_series[datetime(2042, 2, 1,  8, 51, 42)] = 0 #  8:51:42am\\n>>> time_series[datetime(2042, 2, 1, 12,  3, 56)] = 1 # 12:03:56am\\n>>> time_series[datetime(2042, 2, 1, 12,  7, 13)] = 0 # 12:07:13am\\n```\\n\\nWhat if you want to know if the light was on at 11am? Unlike a python\\ndictionary, you can look up the value at any time even if it's not one\\nof the measurement times.\\n\\n```python\\n>>> time_series[datetime(2042, 2, 1, 11,  0, 0)] # 11:00am\\n0\\n```\\n\\nThe `distribution` function gives you the fraction of time that the\\n`TimeSeries` is in each state.\\n\\n```python\\n>>> time_series.distribution(\\n>>>   start=datetime(2042, 2, 1,  6,  0,  0), # 6:00am\\n>>>   end=datetime(2042, 2, 1,  13,  0,  0)   # 1:00pm\\n>>> )\\nHistogram({0: 0.8355952380952381, 1: 0.16440476190476191})\\n```\\n\\nThe light was on about 16% of the time between 6am and 1pm.\\n\\n### Adding more data...\\n\\nNow let's get a little more complicated and look at the sensor readings\\nfrom forty lights in a house.\\n\\n![](docs/_static/img/traces.svg)\\n\\nHow many lights are on throughout the day? The merge function takes the\\nforty individual `TimeSeries` and efficiently merges them into one\\n`TimeSeries` where the each value is a list of all lights.\\n\\n```python\\n>>> trace_list = [... list of forty traces.TimeSeries ...]\\n>>> count = traces.TimeSeries.merge(trace_list, operation=sum)\\n```\\n\\nWe also applied a `sum` operation to the list of states to get the\\n`TimeSeries` of the number of lights that are on.\\n\\n![](docs/_static/img/count.svg)\\n\\nHow many lights are on in the building on average during business hours,\\nfrom 8am to 6pm?\\n\\n```python\\n>>> histogram = count.distribution(\\n>>>   start=datetime(2042, 2, 1,  8,  0,  0),   # 8:00am\\n>>>   end=datetime(2042, 2, 1,  12 + 6,  0,  0) # 6:00pm\\n>>> )\\n>>> histogram.median()\\n17\\n```\\n\\nThe `distribution` function returns a [Histogram](https://traces.readthedocs.io/en/master/api_reference.html#histogram) that\\ncan be used to get summary metrics such as the mean or quantiles.\\n\\n### It's flexible\\n\\nThe measurements points (keys) in a `TimeSeries` can be in any units as\\nlong as they can be ordered. The values can be anything.\\n\\nFor example, you can use a `TimeSeries` to keep track the contents of a\\ngrocery basket by the number of minutes within a shopping trip.\\n\\n```python\\n>>> time_series = traces.TimeSeries()\\n>>> time_series[1.2] = {'broccoli'}\\n>>> time_series[1.7] = {'broccoli', 'apple'}\\n>>> time_series[2.2] = {'apple'}          # puts broccoli back\\n>>> time_series[3.5] = {'apple', 'beets'} # mmm, beets\\n```\\n\\nTo learn more, check the [examples](https://traces.readthedocs.io/en/master/examples.html) and the detailed [reference](https://traces.readthedocs.io/en/master/api_reference.html#).\\n\\n## More info\\n\\n## Contributing\\n\\nContributions are welcome and greatly appreciated! Please visit our [guidelines](https://github.com/datascopeanalytics/traces/blob/master/CONTRIBUTING.md)\\nfor more info.\\n\"},\n",
       " {'repo': 'fat/space-tweet',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': \"SPACE TWEET\\n=============\\n\\nSpace Tweet is a visualization of the twitter stream - it uses node.js and the twitter firehose to stage a match of good vs evil: every evil tweet creates a new space invader, while each positive tweet fires a bullet of good from the space defender... Watch the youtube demo @ http://www.youtube.com/watch?v=xvDzLODyDBo\\n\\n![spacetweet](http://f.cl.ly/items/1A2u2f052C0s1h2r1F15/screenshot.png)\\n\\nINSTALLATION\\n============\\n\\nYou will need both node.js and npm installed to get Space Tweet running locally.\\n\\nIf you have those, just cd into the root of this repo and run:\\n\\n    $ npm install .\\n\\nboom! all server dependencies installed...\\n\\nRUNNING THIS LOCALLY\\n====================\\n\\nOnce you've got everything installed, make sure to update the config.json with your twitter username and password! Then:\\n\\n    sudo node server.js\\n\\nthis will start up space tweet on port 3000... so go to your browser:\\n\\n    localhost:3000\\n\\nhopefully you should see space tweet!! hooray!\\n\"},\n",
       " {'repo': 'autonomousvision/occupancy_networks',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Occupancy Networks\\n![Example 1](img/00.gif)\\n![Example 2](img/01.gif)\\n![Example 3](img/02.gif)\\n\\nThis repository contains the code to reproduce the results from the paper\\n[Occupancy Networks - Learning 3D Reconstruction in Function Space](https://avg.is.tuebingen.mpg.de/publications/occupancy-networks).\\n\\nYou can find detailed usage instructions for training your own models and using pretrained models below.\\n\\nIf you find our code or paper useful, please consider citing\\n\\n    @inproceedings{Occupancy Networks,\\n        title = {Occupancy Networks: Learning 3D Reconstruction in Function Space},\\n        author = {Mescheder, Lars and Oechsle, Michael and Niemeyer, Michael and Nowozin, Sebastian and Geiger, Andreas},\\n        booktitle = {Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)},\\n        year = {2019}\\n    }\\n\\n## Installation\\nFirst you have to make sure that you have all dependencies in place.\\nThe simplest way to do so, is to use [anaconda](https://www.anaconda.com/). \\n\\nYou can create an anaconda environment called `mesh_funcspace` using\\n```\\nconda env create -f environment.yaml\\nconda activate mesh_funcspace\\n```\\n\\nNext, compile the extension modules.\\nYou can do this via\\n```\\npython setup.py build_ext --inplace\\n```\\n\\nTo compile the dmc extension, you have to have a cuda enabled device set up.\\nIf you experience any errors, you can simply comment out the `dmc_*` dependencies in `setup.py`.\\nYou should then also comment out the `dmc` imports in `im2mesh/config.py`.\\n\\n## Demo\\n![Example Input](img/example_input.png)\\n![Example Output](img/example_output.gif)\\n\\nYou can now test our code on the provided input images in the `demo` folder.\\nTo this end, simply run\\n```\\npython generate.py configs/demo.yaml\\n```\\nThis script should create a folder `demo/generation` where the output meshes are stored.\\nThe script will copy the inputs into the `demo/generation/inputs` folder and creates the meshes in the `demo/generation/meshes` folder.\\nMoreover, the script creates a `demo/generation/vis` folder where both inputs and outputs are copied together.\\n\\n## Dataset\\n\\nTo evaluate a pretrained model or train a new model from scratch, you have to obtain the dataset.\\nTo this end, there are two options:\\n\\n1. you can download our preprocessed data\\n2. you can download the ShapeNet dataset and run the preprocessing pipeline yourself\\n\\nTake in mind that running the preprocessing pipeline yourself requires a substantial amount time and space on your hard drive.\\nUnless you want to apply our method to a new dataset, we therefore recommmend to use the first option.\\n\\n### Preprocessed data\\nYou can download our preprocessed data (73.4 GB) using\\n\\n```\\nbash scripts/download_data.sh\\n```\\n\\nThis script should download and unpack the data automatically into the `data/ShapeNet` folder.\\n\\n### Building the dataset\\nAlternatively, you can also preprocess the dataset yourself.\\nTo this end, you have to follow the following steps:\\n* download the [ShapeNet dataset v1](https://www.shapenet.org/) and put into `data/external/ShapeNet`. \\n* download the [renderings and voxelizations](http://3d-r2n2.stanford.edu/) from Choy et al. 2016 and unpack them in `data/external/Choy2016` \\n* build our modified version of [mesh-fusion](https://github.com/davidstutz/mesh-fusion) by following the instructions in the `external/mesh-fusion` folder\\n\\nYou are now ready to build the dataset:\\n```\\ncd scripts\\nbash dataset_shapenet/build.sh\\n``` \\n\\nThis command will build the dataset in `data/ShapeNet.build`.\\nTo install the dataset, run\\n```\\nbash dataset_shapenet/install.sh\\n```\\n\\nIf everything worked out, this will copy the dataset into `data/ShapeNet`.\\n\\n## Usage\\nWhen you have installed all binary dependencies and obtained the preprocessed data, you are ready to run our pretrained models and train new models from scratch.\\n\\n### Generation\\nTo generate meshes using a trained model, use\\n```\\npython generate.py CONFIG.yaml\\n```\\nwhere you replace `CONFIG.yaml` with the correct config file.\\n\\nThe easiest way is to use a pretrained model.\\nYou can do this by using one of the config files\\n```\\nconfigs/img/onet_pretrained.yaml\\nconfigs/pointcloud/onet_pretrained.yaml\\nconfigs/voxels/onet_pretrained.yaml\\nconfigs/unconditional/onet_cars_pretrained.yaml\\nconfigs/unconditional/onet_airplanes_pretrained.yaml\\nconfigs/unconditional/onet_sofas_pretrained.yaml\\nconfigs/unconditional/onet_chairs_pretrained.yaml\\n```\\nwhich correspond to the experiments presented in the paper.\\nOur script will automatically download the model checkpoints and run the generation.\\nYou can find the outputs in the `out/*/*/pretrained` folders.\\n\\nPlease note that the config files  `*_pretrained.yaml` are only for generation, not for training new models: when these configs are used for training, the model will be trained from scratch, but during inference our code will still use the pretrained model.\\n\\n### Evaluation\\nFor evaluation of the models, we provide two scripts: `eval.py` and `eval_meshes.py`.\\n\\nThe main evaluation script is `eval_meshes.py`.\\nYou can run it using\\n```\\npython eval_meshes.py CONFIG.yaml\\n```\\nThe script takes the meshes generated in the previous step and evaluates them using a standardized protocol.\\nThe output will be written to `.pkl`/`.csv` files in the corresponding generation folder which can be processed using [pandas](https://pandas.pydata.org/).\\n\\nFor a quick evaluation, you can also run\\n```\\npython eval.py CONFIG.yaml\\n```\\nThis script will run a fast method specific evaluation to obtain some basic quantities that can be easily computed without extracting the meshes.\\nThis evaluation will also be conducted automatically on the validation set during training.\\n\\nAll results reported in the paper were obtained using the `eval_meshes.py` script.\\n\\n### Training\\nFinally, to train a new network from scratch, run\\n```\\npython train.py CONFIG.yaml\\n```\\nwhere you replace `CONFIG.yaml` with the name of the configuration file you want to use.\\n\\nYou can monitor on <http://localhost:6006> the training process using [tensorboard](https://www.tensorflow.org/guide/summaries_and_tensorboard):\\n```\\ncd OUTPUT_DIR\\ntensorboard --logdir ./logs --port 6006\\n```\\nwhere you replace `OUTPUT_DIR` with the respective output directory.\\n\\nFor available training options, please take a look at `configs/default.yaml`.\\n\\n# Notes\\n* In our paper we used random crops and scaling to augment the input images. \\n  However, we later found that this image augmentation decreases performance on the ShapeNet test set.\\n  The pretrained model that is loaded in `configs/img/onet_pretrained.yaml` was hence trained without data augmentation and has slightly better performance than the model from the paper. The updated table looks a follows:\\n  ![Updated table for single view 3D reconstruction experiment](img/table_img2mesh.png)\\n  For completeness, we also provide the trained weights for the model which was used in the paper in  `configs/img/onet_legacy_pretrained.yaml`.\\n* Note that training and evaluation of both our model and the baselines is performed with respect to the *watertight models*, but that normalization into the unit cube is performed with respect to the *non-watertight meshes* (to be consistent with the voxelizations from Choy et al.). As a result, the bounding box of the sampled point cloud is usually slightly bigger than the unit cube and may differ a little bit from a point cloud that was sampled from the original ShapeNet mesh.\\n\\n# Futher Information\\nPlease also check out the following concurrent papers that have proposed similar ideas:\\n* [Park et al. - DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation (2019)](https://arxiv.org/abs/1901.05103)\\n* [Chen et al. - Learning Implicit Fields for Generative Shape Modeling (2019)](https://arxiv.org/abs/1812.02822)\\n* [Michalkiewicz et al. - Deep Level Sets: Implicit Surface Representations for 3D Shape Inference (2019)](https://arxiv.org/abs/1901.06802)\\n'},\n",
       " {'repo': 'Doctoror/ParticlesDrawable',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '# ParticlesDrawable\\nDraws random flying particles in space forming constellations.\\n\\nMin API level 9.\\n\\nContains:\\n- `ParticlesDrawable`, which is an `Animatable` `Drawable`.\\n- `ParticlesView`, which is a `View`.\\n\\nBoth have the same public methods for customization and may be inflated using the same customization xml attributes.\\n\\n# Add to your project\\n\\n[![Maven Central](https://maven-badges.herokuapp.com/maven-central/com.github.doctoror.particlesdrawable/library/badge.png?style=flat)](https://maven-badges.herokuapp.com/maven-central/com.github.doctoror.particlesdrawable/library)\\n\\n```groovy\\ndependencies {\\n    implementation \\'com.github.doctoror.particlesdrawable:library:[version]\\'\\n}\\n```\\n\\n## Screenshots\\n![screenshot](/screenshots/demo.gif?raw=true)\\n\\n# Usage\\n`ParticlesDrawable` usage example\\n```java\\nprivate final ParticlesDrawable mDrawable = new ParticlesDrawable();\\n\\n@Override\\nprotected void onCreate(Bundle savedInstanceState) {\\n    super.onCreate(savedInstanceState);\\n    setContentView(R.layout.activity_demo);\\n    findViewById(R.id.view).setBackground(mDrawable);\\n}\\n\\n@Override\\nprotected void onStart() {\\n    super.onStart();\\n    mDrawable.start();\\n}\\n\\n@Override\\nprotected void onStop() {\\n    super.onStop();\\n    mDrawable.stop();\\n}\\n```\\n\\n## Customization\\nHere is a list of all attributes, set with default values\\n\\n```xml\\n<com.doctoror.particlesdrawable.ParticlesView\\n    app:density=\"60\"\\n    app:frameDelayMillis=\"10\"\\n    app:lineColor=\"@android:color/white\"\\n    app:lineLength=\"86dp\"\\n    app:lineThickness=\"1dp\"\\n    app:particleColor=\"@android:color/white\"\\n    app:particleRadiusMax=\"3dp\"\\n    app:particleRadiusMin=\"1dp\"\\n    app:speedFactor=\"1\" />\\n```\\nThe conventional getters and setters are also available.\\n\\nSince API 24, you may also customize the Drawable in `xml`. For example, create\\n`drawable-v24/particles_density_120.xml`\\n```xml\\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<!-- Some versions of Android Studio may show false warning,\\n\"Element drawable must be declared\", but it works fine when\\ncompiling and running -->\\n<drawable\\n    class=\"com.doctoror.particlesdrawable.ParticlesDrawable\"\\n    xmlns:app=\"http://schemas.android.com/apk/res-auto\"\\n    app:density=\"120\"/>\\n```\\nAnd inflate, like\\n```java\\nmDrawable = (ParticlesDrawable) ContextCompat\\n        .getDrawable(this, R.drawable.particles_density_120);\\n```\\n\\n## Configuration Demo\\n[Particle Constellations Live Wallpaper](https://github.com/Doctoror/ParticleConstellationsLiveWallpaper) has a great configuration screen which allows you to quickly see through what can you do with the library:\\n\\n[![Video](https://github.com/Doctoror/ParticlesWallpaper/raw/master/screenshots/video.png)](https://www.youtube.com/watch?v=Q7qvmCMUN20)\\n\\n\\n## License\\n```\\nCopyright 2017 Yaroslav Mytkalyk\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n\\n```\\n'},\n",
       " {'repo': 'ehecker/spaceflix',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Spaceflix\\n<img src=\"https://i.imgur.com/sneEo43.jpg\">\\n<a href=\"https://spaceflix.herokuapp.com\" target=\"_blank\" rel=\"noreferrer\">Visit the Live Site here</a>\\n\\nSpaceflix is a space-themed, pixel-perfect clone of Netflix. Like the real thing, Spaceflix allows users to stream movies (trailers) on demand. Users can create and delete profiles, each of which has its own independent Watch List where movies can be saved for later viewing. Movies can be previewed by hovering over their thumbnails, and additional details can be accessed by opening their respective show-sections. In the future, Spaceflix will feature search functionality as well as recommendations based on movies a profile has already watched.\\n\\n## Features\\n<ul>\\n    <li>Secure user authentication with component, model, and database level validations.</li>\\n    <li>User passwords are securely stored in hashed and salted format.</li>\\n    <li>Ability to create and delete up to 5 custom profiles per user, each of which has its own dedicated Watch List.</li>\\n    <li>Ability to add or remove videos from Watch lists from the video preview or show page.</li>\\n    <li>Ability to switch between profiles seamlessly using the Navigation bar or dedicated Profiles page.</li>\\n    <li>Ability to watch videos with custom video control panel designed specifically for Spaceflix.</li>\\n    <li>User authentication process includes error handling, rendering descriptive errors to the user for an intuitive signup/login experience.</li>\\n</ul>\\n\\n## Technologies\\n\\nSpaceflix is built with <strong>Ruby on Rails</strong> on the backend and <strong>React/Redux</strong> on the frontend. User and movie information is stored in a <strong>PostgreSQL</strong> database and <strong>AWS S3</strong> is used for cloud storage of image and video files. Secure user authentication is implemented without any dedicated authentication libraries, but user passwords are stored securely in hashed + salted format with assistance from BCrypt. The application is additionally supported by Webpack, jQuery, and Jbuilder.\\n\\n## Technical Challenges\\n### Authorizing AWS S3 Requests with proper expiration guidelines\\nIn the final stages of this project, I noticed that attempts to play videos on the Browse page would return a 403 Forbidden error from AWS S3 a few minutes after the page was initially loaded. To debug this, I began researching pre-signed signatures in AWS and eventually learned that the media request URLs being sent to AWS by way of ActiveStorage were setting a default query parameter of expires_in: 300 - meaning that each request would only remain valid for five minutes. Using the Ruby on Rails and ActiveStorage documentation, I researched and built a custom Rails initializer, <em>active_storage.rb</em>, to override that default parameter with the following line of code:\\n\\n    ActiveStorage::Service.url_expires_in = 1.hour\\n\\n### Fading video controls based on user inactivity\\nThe problem here is pretty straightforward: there is no event listener for the <em>absence</em> of user activity. To implement such behavior, I defined a series of functions which increment a timer while the mouse is not actively moving, and trigger a fade animation when that timer reaches three seconds. The timer starts/ends by hovering over the parent element, and is reset onMouseMove, ensuring that the controls do not disappear too quickly and that they can reappear instantaneously. This functionality can be observed in the application when hovering over a movie preview, or on a movie\\'s dedicated Watch page.\\n\\n\\n    // components/movies.jsx\\n    startFadeTimer() {\\n        this.fadeInterval = window.setInterval(this.incrementFadeTimer, 1000)\\n        this.containerElement.classList.remove(\"fade-trigger\")\\n    }\\n\\n    resetFadeTimer() {\\n        this.containerElement.classList.remove(\"fade-trigger\")\\n\\n        this.fadeTime = 0;\\n        clearInterval(this.fadeInterval);\\n        this.fadeInterval = window.setInterval(this.incrementFadeTimer, 1000)\\n    }\\n\\n    endFadeTimer() {\\n        this.fadeTime = 0;\\n        clearInterval(this.fadeInterval);\\n\\n        this.containerElement.classList.remove(\"fade-trigger\");\\n    }\\n\\n    incrementFadeTimer() {\\n        this.fadeTime++;\\n        \\n        if (this.fadeTime >= 3) {\\n            this.fadeControls();\\n            this.fadeTime = 0;\\n        }\\n    }\\n\\n    fadeControls() {\\n        this.containerElement.classList.add(\"fade-trigger\")\\n    }\\n    \\n## Upcoming Features\\n<ul>\\n  <li>Searchable Movie Database</li>\\n  <li>Dedicated Genre View pages</li>\\n  <li>Recommended movies based on user activity</li>\\n  <li>Continue Watching feature which keeps track of movies that have been started but not finished</li>\\n</ul>\\n'},\n",
       " {'repo': 'f-prime/SpaceInvaders',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': ''},\n",
       " {'repo': 'Nils277/KerbalPlanetaryBaseSystems',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': 'KERBAL PLANETARY BASE INC.\\n---------------\\n\\tThis is an expansion for the stock ksp parts. It adds components for bases on other planets or even on Kerbin.\\n\\tNo other mods are required for this mod to work.\\n\\nINSTALLATION\\n---------------\\n\\tTo install, simply copy the gamedata folder into your Kerbal Space Program folder.\\n\\tWhen you already have another version of this mod installed, delete this folder first.\\n\\nCHANGE FILTER\\n---------------\\n\\tYou can change if and where filter for this mods are added. This can be done in the \"KPBS_settings.cfg\" file.\\n\\nDEPRECATED PARTS\\n---------------\\n\\tPrior to version 1.3.2 this mod shipped a few deprecated parts that should not have been used since a long time. They were removed. If you still have crafts that use these parts,\\n    you can download the parts from here: https://www.dropbox.com/s/ti2vqbqcn5vsxys/PlanetaryBaseInc_Deprecated_Parts.zip?dl=0 \\n    Copy the \"PlanetaryBaseInc\" folder from that download into the \"GameData\" folder of KSP.\\n\\nLICENSING\\n---------------\\n\\tThe art assets are licensed under: \\n    CC-BY-NC (http://creativecommons.org/licenses/by-nc/4.0/)\\n\\n    The plugin code is licensed under:\\n    APACHE LICENSE, VERSION 2.0 (http://www.apache.org/licenses/LICENSE-2.0)\\n\\nAUTHOR\\n---------------\\n\\tNils277'},\n",
       " {'repo': 'learn-co-students/space-invaders-v-000',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': \"# Space Invaders Lab\\n\\n## Instructions\\n\\nSpace flight is no joke, and neither is this lab. You're going to need to create\\ntwo ES6 JS classes, `Spaceship` and `CrewMember`. These classes will pump out\\ninstances that are by default unable to interact with one another. A space ship\\nwill be inactive if it is created without a crew. And a crew member by default\\nwill be hanging out in the Cantina looking for a ride if they were not put\\ninside a ship when it is initialized.\\n\\nCrew members will also be unable to use their special abilities if they're not\\nassigned to a ship. For example, a pilot cannot `engageWarpDrive` if s/he is not\\nin a ship and, therefore, can't make the Kessel Run in less than twelve parsecs.\\nCrew members also should be unable to perform certain actions that their\\npositions are incapable of doing.\\n\\nTo have these two objects be aware of one another, we need to have the ship\\nbecome aware of its crew members on instantiation. This will also require you to\\nmake sure a crew member becomes aware of their ship when they are added to it.\\n\\nTLDR;\\n\\nA ship cannot exist without a crew and an individual crew member can't use their\\nspecial ability if not assigned to a ship.\\n\\n![alt text](https://media.giphy.com/media/26uf9QPzzlKPvQG5O/giphy.gif 'space ship gif')\\n\\n<p data-visibility='hidden'>View <a href='https://learn.co/lessons/space-invaders'>Space Invaders Lab</a> on Learn.co and start learning to code for free.</p>\\n\"},\n",
       " {'repo': 'NITJSR-OSS/My-SpaceX-Console',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# My SpaceX Console - Hacktoberfest 2020\\n\\n  \\n\\n![spacex wallpaper](https://raw.githubusercontent.com/AdityaPratap006/My-SpaceX-Console/master/readme-image.jpg)\\n\\nHi! Welcome to this beginner-friendly **Frontend Engineering** challenge for [Hactoberfest](https://hacktoberfest.digitalocean.com/). A project focused on people getting started with frontend development with [React](https://reactjs.org/) where one can build a very cool personal [SpaceX](https://www.spacex.com/) launch tracker.\\n\\n  \\n\\nEveryone is encouraged to participate, regardless of your skill level (If you don\\'t have the required skills, use this project as a motivation to learn those!). This is a practice project and should be considered a playground.\\n\\n  \\n\\n# Project Summay\\n\\n  \\n\\nYou have to build a **React** based web application which will act as your personal console for all things SpaceX!\\n\\nUsing the apis and tools given below, try to implement the following features:\\n\\n  \\n\\n-  ### Minimum Required Features:\\n\\n- [ ] A home screen, where the users can view the company info. :computer:\\n\\n- [ ] A launches screen, where th user can view the list of all the launches. :rocket:\\n\\n  - Divide this into 3 sections.\\n\\n  - One section for upcoming launches, one for latest launches and one for past launches.\\n\\n- [ ] A launch detail screen, where the user can view all the details of a particular launch. :chart:\\n\\n- [ ] A launchpad map screen, where the user can view all the SpaceX launchpad locations on a map. :earth_americas:\\n\\n- [ ] A navbar or menubar to navigate between different screens\\n\\n- [ ] UI should be completely responsive! (i.e should be able to adapt accordingly for various device sizes)\\n\\n-  ### Nice-to-have Features\\n\\n- [ ] Add a \"mark for tracking\" feature to mark a particular launch for tracking, save the launch details in the local storage only (no need to use a 3rd party service like firebase or mongoDB).\\n\\n- [ ] When the launch date approaches, the app should show some kind of alert that this launch is scheduled today. (You can build another screen, for e.g: \"Notifications Screen\" where all the alerts will appear).\\n\\n  \\n\\n## APIs and Services to be used\\n\\n  \\n\\n- [ ] [**SpaceX api**](https://github.com/r-spacex/SpaceX-API/blob/master/docs/v4/README.md) : For fetching all the company info, launch details, launchpad data etc.\\n\\n- [ ] [**React-Leaflet maps api**](https://react-leaflet.js.org/) : A free alternative for Google maps, use this to mark the launchpads on a map.\\n\\n  \\n\\n## Guidelines\\n\\n  \\n\\nSystem requirements:\\n\\n  \\n\\n- [ ] Make sure node version 12 or higher is installed on your system.\\n\\n- [ ] Make sure yarn version 1.19.1 or higher is installed on your system.\\n\\n  \\n\\nPlease try to follow these guidelines:\\n\\n  \\n\\n- [ ] Use only CSS/SCSS or Styled Components for styling the app, DON\\'T use bootstrap, material UI, semantic UI or any other styling library. (Haha! :wink: No shortcuts here :rofl:)\\n\\n- [ ] Try to make the UI as elegant as possible, use Cards, box-shadows etc. Choose a subtle color-theme.\\n\\n- [ ] Prefer functional components over class components, use react hooks and react context apis as far as possible.\\n\\n- [ ] Unless absolutely necessary, don\\'t use Redux for state management, try keeping things simple!\\n\\n- [ ] Only use images/content that are available for free and don\\'t need any special license.\\n\\n- [ ] Last but not the least, use your imagination to build the coolest app you can think of ! :heart_eyes_cat: :fire: :fire: :fire:\\n\\n  \\n\\n## How to Contribute\\n\\n  \\n\\n>  **Note 1:** If you\\'ve never made a pull request before, or participated in an open-source project, we recommend taking a look at this [wonderful video tutorial](https://youtu.be/ZI2D0CI4TXs). And if you want a more complete tutorial on using github, creating branches etc. , [here\\'s a detailed video series](https://www.youtube.com/watch?v=3RjQznt-8kE&list=PL4cUxeGkcC9goXbgTDQ0n_4TBzOO0ocPR).Once you\\'ve got your feet wet, you\\'re ready to come back and dive into Hacktoberfest fun!\\n\\n  \\n\\n>  **Note 2:**  **Super Important** Only the pull requests created between October 1st, 2020 and October 31st, 2020 will be counted!\\n\\n  \\n1. Star this repository. :stuck_out_tongue:\\n\\n2. And then you have to fork (make a copy) of this repo to your Github account.\\n\\n3. Clone (download) your fork to your computer.\\n\\n4. Set your streams so you can sync your clone with the original repo (get the latest updates)\\n\\n\\t- [ ] <code>git remote add upstream https://github.com/NITJSR-OSS/My-SpaceX-Console.git</code>\\n\\n\\t- [ ] <code>git pull upstream master</code>\\n\\n\\t- [ ] The above 2 commands will synchronize your forked version of the project with the actual repository.\\n\\n5. Create a branch with your name (for e.g: if your name is John Wick, create a branch named `John_Wick`).\\n\\n6. Make the changes in your branch.\\n\\n7. Commit and push the code to YOUR fork.\\n\\n8. Create a pull request to have the changes merged into the origin.\\n\\n  \\n\\n## Running the project locally\\n\\n  \\n\\n### After downloading this repo, run the command `yarn` (or if using npm - `npm install`) in the project directory to install the dependencies.\\n\\n  \\n\\nThis project was bootstrapped with [Create React App](https://github.com/facebook/create-react-app).\\n\\n  \\n\\n## Available Scripts\\n\\n  \\n\\nIn the project directory, you can run:\\n\\n  \\n\\n### `yarn start`\\n\\n  \\n\\nRuns the app in the development mode.<br  />\\n\\n  \\n\\nOpen [http://localhost:3000](http://localhost:3000) to view it in the browser.\\n\\n  \\n\\nThe page will reload if you make edits.<br  />\\n\\n  \\n\\nYou will also see any lint errors in the console.\\n\\n  \\n\\n### `yarn test`\\n\\n  \\n\\nLaunches the test runner in the interactive watch mode.<br  />\\n\\n  \\n\\nSee the section about [running tests](https://facebook.github.io/create-react-app/docs/running-tests) for more information.\\n\\n  \\n\\n### `yarn build`\\n\\n  \\n\\nBuilds the app for production to the `build` folder.<br  />\\n\\n  \\n\\nIt correctly bundles React in production mode and optimizes the build for the best performance.\\n\\n  \\n\\nThe build is minified and the filenames include the hashes.<br  />\\n\\n  \\n\\nYour app is ready to be deployed!\\n\\n  \\n\\nSee the section about [deployment](https://facebook.github.io/create-react-app/docs/deployment) for more information.\\n\\n  \\n\\n### `yarn eject`\\n\\n  \\n\\n**Note: this is a one-way operation. Once you `eject`, you can’t go back!**\\n\\n  \\n\\nIf you aren’t satisfied with the build tool and configuration choices, you can `eject` at any time. This command will remove the single build dependency from your project.\\n\\n  \\n\\nInstead, it will copy all the configuration files and the transitive dependencies (webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except `eject` will still work, but they will point to the copied scripts so you can tweak them. At this point you’re on your own.\\n\\n  \\n\\nYou don’t have to ever use `eject`. The curated feature set is suitable for small and middle deployments, and you shouldn’t feel obligated to use this feature. However we understand that this tool wouldn’t be useful if you couldn’t customize it when you are ready for it.\\n\\n  \\n\\n## Learn More\\n\\n  \\n\\nYou can learn more in the [Create React App documentation](https://facebook.github.io/create-react-app/docs/getting-started).\\n\\n  \\n\\nTo learn React, check out the [React documentation](https://reactjs.org/).\\n\\n  \\n\\n### Code Splitting\\n\\n  \\n\\nThis section has moved here: https://facebook.github.io/create-react-app/docs/code-splitting\\n\\n  \\n\\n### Analyzing the Bundle Size\\n\\n  \\n\\nThis section has moved here: https://facebook.github.io/create-react-app/docs/analyzing-the-bundle-size\\n\\n  \\n\\n### Making a Progressive Web App\\n\\n  \\n\\nThis section has moved here: https://facebook.github.io/create-react-app/docs/making-a-progressive-web-app\\n\\n  \\n\\n### Advanced Configuration\\n\\n  \\n\\nThis section has moved here: https://facebook.github.io/create-react-app/docs/advanced-configuration\\n\\n  \\n\\n### Deployment\\n\\n  \\n\\nThis section has moved here: https://facebook.github.io/create-react-app/docs/deployment\\n\\n  \\n\\n### `yarn build` fails to minify\\n\\n  \\n\\nThis section has moved here: https://facebook.github.io/create-react-app/docs/troubleshooting#npm-run-build-fails-to-minify'},\n",
       " {'repo': 'mattatz/unity-volume-rendering',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': 'unity-volume-rendering\\n=====================\\n\\nVolume rendering by object space raymarching for Unity.\\n\\n<img src=\"https://raw.githubusercontent.com/mattatz/unity-volume-rendering/master/Captures/Demo.gif\">\\n\\nVolumeRendering.shader cut a volume each axes by _SliceMin, _SliceMax properties.\\n\\n## Object space raymarching\\n\\n<img src=\"https://raw.githubusercontent.com/mattatz/unity-volume-rendering/master/Captures/Geometry.png\">\\n\\nVolumeRendering component generates a Cube geometry which has 1.0 length edges.\\nBy object space raymarching techniques, rendering a volume with a MeshRenderer. (See references)\\n\\n## Slice axes rotation\\n\\n<img src=\"https://raw.githubusercontent.com/mattatz/unity-volume-rendering/master/Captures/Axis.gif\">\\n\\nBy setting an axis quaternion in VolumeRendering component, \\nyou can cut a volume from arbitrary angles.\\n\\n## VolumeAssetBuilder\\n\\nMenu : Window -> VolumeAssetBuilder\\n\\n<img src=\"https://raw.githubusercontent.com/mattatz/unity-volume-rendering/master/Captures/VolumeAssetBuilder.png\">\\n\\nVolumeAssetBuilder builds a 3D texture asset from a pvm raw file. (volume raw data)\\n\\n## Compatibility\\n\\ntested on Unity 2017.2.8f1, windows10 (GTX 1060).\\n\\n## Sources\\n\\n- primitive: blog | object space raymarching - http://i-saint.hatenablog.com/entry/2015/08/24/225254\\n- The Volume Library - http://lgdv.cs.fau.de/External/vollib/\\n- Graphics Runner : Volume Rendering 101 - http://graphicsrunner.blogspot.jp/2009/01/volume-rendering-101.html\\n'},\n",
       " {'repo': 'michaelbromley/css-space-shooter',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# CSS Space Shooter\\n\\n![Screen shot](https://raw.githubusercontent.com/michaelbromley/css-space-shooter/master/assets/images/screenshot-02.jpg \"Screen shot\")\\n\\n## [Play The Game](https://www.michaelbromley.co.uk/experiments/css-space-shooter/)\\n\\nThis is an experiment I made to investigate the capabilities of CSS 3D transforms.\\nHaving played about with this technology a little (see [this](https://www.michaelbromley.co.uk/experiments/css-3d-butterfly/) or [this](http://www.michaelbromley.co.uk/horizonal/demo/),\\nand having seen some very impressive demos ([CSS FPS](http://www.keithclark.co.uk/labs/css-fps/), [CSS X-Wing](http://codepen.io/juliangarnier/details/hzDAF),\\nI wanted to explore the idea of making a simple 3D game with only DOM and CSS.\\n\\n## Everything in CSS? Cool!\\n\\n[CSS transforms](https://developer.mozilla.org/en-US/docs/Web/Guide/CSS/Using_CSS_transforms) allow us to position and rotate DOM elements in 3D space. The big advantage of this over, say, using canvas or webGL is that we do not need to\\nworry about any of the complex maths involved in projecting a 3D object onto the screen. The browser\\'s rendering engine (with the help of your GPU) will take care of all\\nthat. You just need to specify the x, y, z coordinates as well as the rotation along any axis. This makes it really simple to map your JavaScript objects onto the\\nscreen, by just keeping track of these simple coordinate and rotation values.\\n\\nHaving [previously played with pseudo-3D in canvas](https://www.michaelbromley.co.uk/experiments/soundcloud-vis/#muse/undisclosed-desires), I have some idea\\nof the massive amount of calculation involved in plotting all the lines and vertices of each\\nobject manually. In this regard, the simple, declarative nature of CSS allows some really powerful 3D effects with astonishingly little code.\\n\\n## ...or not so cool.\\n\\nThat convenience comes at a cost, however. For one, in CSS it is really really hard to create any shape other than a rectangle or an ellipse. Triangles, for example, are\\nonly possible through [dirty hacks with the border property](http://davidwalsh.name/css-triangles).\\n\\nSecondly, performance. Despite hardware acceleration for these 3D transforms, I quickly ran into performance issues when scaling up the number of objects\\n interacting on screen simultaneously. Certain CSS operations are also *very* expensive, such as transitioning box-shadow values or gradient backgrounds.\\n\\nI\\'m sure my code can be optimized and this performance ceiling can be raised considerably. However, I wouldn\\'t recommend using CSS and DOM for a serious 3D game.\\n\\n## Browser Compatibility\\n\\n* Right now this works properly in the latest version of Chrome.\\n* In my tests with Firefox it is very jerky and then usually grinds to a complete halt after a minute or so.\\n* Internet Explorer has a couple of fatal issues - it does not yet support a key CSS property - [`transform-style: preserve3d`](https://developer.mozilla.org/en-US/docs/Web/CSS/transform-style#Browser_compatibility) -\\nwhich is essential to this method of building up 3D objects and 3D scenes which all share the same perspective. Additionally, IE does not currently support the\\nWeb Audio API, which I use for the sound effects and music. The game currently won\\'t even load for this latter reason.\\n* I\\'ve not tested in any other browsers, but feedback is welcome.\\n\\n## Credits\\n\\n### Inspiration and implementation details:\\n\\n* [Keith Clark](http://www.keithclark.co.uk/) - seriously, check out his stuff. It\\'s amazing. Used his advice on positioning the DOM elements in the center of the viewport and moving them only\\nwith transforms, which works well.\\n* html5Rocks - Some really helpful tutorials [here](http://www.html5rocks.com/en/tutorials/webaudio/games/) and [here](http://www.html5rocks.com/en/tutorials/webaudio/intro/)\\n on how to use the Web Audio API.\\n* Dive Into HTML5 [article on the localStorage API](http://diveintohtml5.info/storage.html), which I use to store high scores.\\n\\n### Sound effects\\n\\nI got all my sounds effects from https://www.freesound.org.\\n\\n* gun: https://www.freesound.org/people/afirlam/sounds/236939/\\n* explosion: https://www.freesound.org/people/plamdi1/sounds/95058/\\n* alien noise: https://www.freesound.org/people/mensageirocs/sounds/234442/\\n* alien drone: https://www.freesound.org/people/klankbeeld/sounds/243702/\\n* 1-down: https://www.freesound.org/people/leviclaassen/sounds/107789/\\n\\n### Music\\n\\nLudwig van Beethoven - Symphony No.7 in A major op.92 - II, Allegretto\\n\\n\\n## Developing\\n\\n```\\nnpm install\\nnpm run watch // dev mode \\nnpm run compile // production build\\n```\\n\\n## License\\n\\nMIT'},\n",
       " {'repo': 'nateraw/stable-diffusion-videos',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# stable-diffusion-videos\\n\\nTry it yourself in Colab: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nateraw/stable-diffusion-videos/blob/main/stable_diffusion_videos.ipynb)\\n\\n**Example** - morphing between \"blueberry spaghetti\" and \"strawberry spaghetti\"\\n\\nhttps://user-images.githubusercontent.com/32437151/188721341-6f28abf9-699b-46b0-a72e-fa2a624ba0bb.mp4\\n\\n# How it Works\\n\\n## The Notebook/App\\n\\nThe [in-browser Colab demo](https://colab.research.google.com/github/nateraw/stable-diffusion-videos/blob/main/stable_diffusion_videos.ipynb) allows you to generate videos by interpolating the latent space of [Stable Diffusion](https://github.com/CompVis/stable-diffusion).\\n\\nYou can either dream up different versions of the same prompt, or morph between different text prompts (with seeds set for each for reproducibility).\\n\\nThe app is built with [Gradio](https://gradio.app/), which allows you to interact with the model in a web app. Here\\'s how I suggest you use it:\\n\\n1. Use the \"Images\" tab to generate images you like.\\n    - Find two images you want to morph between\\n    - These images should use the same settings (guidance scale, scheduler, height, width)\\n    - Keep track of the seeds/settings you used so you can reproduce them\\n\\n2. Generate videos using the \"Videos\" tab\\n    - Using the images you found from the step above, provide the prompts/seeds you recorded\\n    - Set the `num_interpolation_steps` - for testing you can use a small number like 3 or 5, but to get great results you\\'ll want to use something larger (60-200 steps). \\n    - You can set the `output_dir` to the directory you wish to save to\\n\\n## Python Package\\n\\n### Setup\\n\\nInstall the package\\n\\n```bash\\npip install -U stable_diffusion_videos\\n```\\n\\nAuthenticate with Hugging Face\\n\\n```bash\\nhuggingface-cli login\\n```\\n\\n### Making Videos\\n\\n```python\\nfrom stable_diffusion_videos import StableDiffusionWalkPipeline\\nimport torch\\n\\npipeline = StableDiffusionWalkPipeline.from_pretrained(\\n    \"CompVis/stable-diffusion-v1-4\",\\n    torch_dtype=torch.float16,\\n    revision=\"fp16\",\\n).to(\"cuda\")\\n\\nvideo_path = pipeline.walk(\\n    prompts=[\\'a cat\\', \\'a dog\\'],\\n    seeds=[42, 1337],\\n    num_interpolation_steps=3,\\n    height=512,  # use multiples of 64 if > 512. Multiples of 8 if < 512.\\n    width=512,   # use multiples of 64 if > 512. Multiples of 8 if < 512.\\n    output_dir=\\'dreams\\',        # Where images/videos will be saved\\n    name=\\'animals_test\\',        # Subdirectory of output_dir where images/videos will be saved\\n    guidance_scale=8.5,         # Higher adheres to prompt more, lower lets model take the wheel\\n    num_inference_steps=50,     # Number of diffusion steps per image generated. 50 is good default\\n)\\n```\\n\\n### Making Music Videos\\n\\n*New!* Music can be added to the video by providing a path to an audio file. The audio will inform the rate of interpolation so the videos move to the beat 🎶\\n\\n```python\\nfrom stable_diffusion_videos import StableDiffusionWalkPipeline\\nimport torch\\n\\npipeline = StableDiffusionWalkPipeline.from_pretrained(\\n    \"CompVis/stable-diffusion-v1-4\",\\n    torch_dtype=torch.float16,\\n    revision=\"fp16\",\\n).to(\"cuda\")\\n\\n\\n# Seconds in the song.\\naudio_offsets = [146, 148]  # [Start, end]\\nfps = 30  # Use lower values for testing (5 or 10), higher values for better quality (30 or 60)\\n\\n# Convert seconds to frames\\nnum_interpolation_steps = [(b-a) * fps for a, b in zip(audio_offsets, audio_offsets[1:])]\\n\\nvideo_path = pipeline.walk(\\n    prompts=[\\'a cat\\', \\'a dog\\'],\\n    seeds=[42, 1337],\\n    num_interpolation_steps=num_interpolation_steps,\\n    audio_filepath=\\'audio.mp3\\',\\n    audio_start_sec=audio_offsets[0],\\n    fps=fps,\\n    height=512,  # use multiples of 64 if > 512. Multiples of 8 if < 512.\\n    width=512,   # use multiples of 64 if > 512. Multiples of 8 if < 512.\\n    output_dir=\\'dreams\\',        # Where images/videos will be saved\\n    guidance_scale=7.5,         # Higher adheres to prompt more, lower lets model take the wheel\\n    num_inference_steps=50,     # Number of diffusion steps per image generated. 50 is good default\\n)\\n```\\n\\n#### Run the App Locally\\n\\n```python\\nfrom stable_diffusion_videos import StableDiffusionWalkPipeline, Interface\\nimport torch\\n\\npipeline = StableDiffusionWalkPipeline.from_pretrained(\\n    \"CompVis/stable-diffusion-v1-4\",\\n    torch_dtype=torch.float16,\\n    revision=\"fp16\",\\n).to(\"cuda\")\\n\\ninterface = Interface(pipeline)\\ninterface.launch()\\n```\\n\\n## Credits\\n\\nThis work built off of [a script](https://gist.github.com/karpathy/00103b0037c5aaea32fe1da1af553355\\n) shared by [@karpathy](https://github.com/karpathy). The script was modified to [this gist](https://gist.github.com/nateraw/c989468b74c616ebbc6474aa8cdd9e53), which was then updated/modified to this repo. \\n\\n## Contributing\\n\\nYou can file any issues/feature requests [here](https://github.com/nateraw/stable-diffusion-videos/issues)\\n\\nEnjoy 🤗\\n\\n## Extras\\n\\n### Upsample with Real-ESRGAN\\n\\nYou can also 4x upsample your images with [Real-ESRGAN](https://github.com/xinntao/Real-ESRGAN)!\\n\\nIt\\'s included when you pip install the latest version of `stable-diffusion-videos`! \\n\\nYou\\'ll be able to use `upsample=True` in the `walk` function, like this:\\n\\n```python\\npipeline.walk([\\'a cat\\', \\'a dog\\'], [234, 345], upsample=True)\\n```\\n\\nThe above may cause you to run out of VRAM. No problem, you can do upsampling separately.\\n\\nTo upsample an individual image:\\n\\n```python\\nfrom stable_diffusion_videos import RealESRGANModel\\n\\nmodel = RealESRGANModel.from_pretrained(\\'nateraw/real-esrgan\\')\\nenhanced_image = model(\\'your_file.jpg\\')\\n```\\n\\nOr, to do a whole folder:\\n\\n```python\\nfrom stable_diffusion_videos import RealESRGANModel\\n\\nmodel = RealESRGANModel.from_pretrained(\\'nateraw/real-esrgan\\')\\nmodel.upsample_imagefolder(\\'path/to/images/\\', \\'path/to/output_dir\\')\\n```\\n\\n\\n'},\n",
       " {'repo': 'space-syndicate/space-station-14',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '<p align=\"center\"> <img alt=\"Space Station 14\" width=\"880\" height=\"300\" src=\"https://raw.githubusercontent.com/space-wizards/asset-dump/de329a7898bb716b9d5ba9a0cd07f38e61f1ed05/github-logo.svg\" /></p>\\n\\nSpace Station 14 это ремейк SS13, который работает на собственном движке [Robust Toolbox](https://github.com/space-wizards/RobustToolbox), написанном на C#.\\n\\nЭто репозиторий первого русскоязычного сервера по Space Station 14, целью которого является полный перевод игры на русский язык, поддержка актуальных изменений из основного репозитория, а так же добавление собственных изменений по необходимости.\\n\\n## Ссылки\\n\\n[Наш Discord](https://discord.station14.ru) | [Наша Вики](https://wiki.station14.ru) | [Steam](https://store.steampowered.com/app/1255460/Space_Station_14/) | [Клиент без Steam](https://spacestation14.io/about/nightlies/) | [Основной репозиторий](https://github.com/space-wizards/space-station-14)\\n\\n## Документация\\n\\nНа официальном сайте с [документацией](https://docs.spacestation14.io/) имеется вся необходимая информация о контенте SS14, движке, дизайне игры и многом другом. Также имеется много информации для начинающих разработчиков.\\n\\n## Контрибьют\\n\\nВ случае если вы хотите добавить новый контент будет лучше, если сначала вы предложите его в [основной репозиторий](https://github.com/space-wizards/space-station-14) или обсудите его необходимость на нашем сервере [Discord](https://discord.station14.ru).\\n\\n## Сборка\\n\\n1. Склонируйте этот репозиторий локально\\n2. Запустите `RUN_THIS.py` для инициализации подмодулей и скачивания движка.\\n3. Скомпилируйте проект.\\n\\n[Более подробная инструкция по запуску проекта.](https://docs.spacestation14.io/getting-started/dev-setup)\\n\\n## Лицензия\\n\\nВесь код репозитория лицензирован под [MIT](https://github.com/space-syndicate/space-station-14/blob/master/LICENSE.TXT).\\n\\nБольшинство ассетов лицензированы под [CC-BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/), если не указано иное. Ассеты имеют свою лицензию и авторские права в файле метаданных. [Пример](https://github.com/space-syndicate/space-station-14/blob/master/Resources/Textures/Objects/Tools/crowbar.rsi/meta.json). \\n\\nОбратите внимание, что некоторые ассеты лицензированы на некоммерческой основе [CC-BY-NC-SA 3.0](https://creativecommons.org/licenses/by-nc-sa/3.0/) или аналогичной некоммерческой лицензией, и их необходимо удалить, если вы хотите использовать этот проект в коммерческих целях.\\n'},\n",
       " {'repo': 'victorqribeiro/invaderz',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': \"# InvaderZ\\n\\nInvaderZ is like the game Space Invaders, except InvaderZ uses the [genetic algorithm](https://en.wikipedia.org/wiki/Genetic_algorithm) to mutate the invaders as you play.\\n\\n![InvaderZ](screenshot.png)\\n\\nIf you want to see a live version of it, it's hosted [here](https://victorribeiro.com/invaderz).\\n\\n[Alternative link](https://victorqribeiro.github.io/invaderz).\\n\\nYou can also add it as an app to your phone (menu / add to home screen), if you wish so.\\n\\n## How to Play\\n\\nLeft arrow or A keys - move the cannon to the left\\n\\nRight arrow or D keys - move the cannon to the right\\n\\nSpace bar  - Shoots; \\n\\nIF you are on your phone or tablet, use the buttons\\n\\n## About\\n\\nYou are the last hope of the human kind as the defense against the InvaderZ. They are deployed by their mothership with the sole purpose of entering earth's atmosphere. Although they are not a danger for you as an individual, if 5 of them gets through, everything is doomed. You can only shoot one projectile at the time, so be mindful, as you are not getting another shot until the last fired projectile disperse into the space or hits a target. Each InvaderZ has a shape that influence how they move. When a InvaderZ dies, they upload their progress to the mothership, so it can generate more InvaderZ like the ones who did well before. After each 7 generations of InvaderZ, the mothership generates a complete new wave of InvaderZ keeping only the very best of the last past 7 generations.\\n\\n## Genetic Algorithm\\n\\nA random population of InvaderZ are generated at the beginning of the game. \\n\\nThe fit score of the InvaderZ is how far it made through the earth's atmosphere. \\n\\nThe way they move are directly related to their body shape. \\n\\nAfter they die, a new wave of InvaderZ are created by crossing over the InvaderZ with the higher fit score. \\n\\nThe cross over could happen of 6 different forms:\\n\\n* First horizontal half of the first parent with the second horizontal half of the second parent.\\n* First horizontal half of the second parent with the second horizontal half of the first parent.\\n* First vertical half of the first parent with the second vertical half of the second parent.\\n* First vertical half of the second parent with the second vertical half of the first parent.\\n* Odd genes from the first parent, even genes from the second parent.\\n* Even genes from the first parent, odd genes from the second parent.\\n\\nThere's a 10% chance of mutation after each cross over, altering their body shape and so the way they move. \\n\\nTo keep things interesting, after 7 generations, a new wave is created from scratch and only the best of the 7 past generations is keep.\\n\\n[![Donations](https://www.paypalobjects.com/en_US/i/btn/btn_donateCC_LG.gif)](https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=victorqribeiro%40gmail%2ecom&lc=BR&item_name=Victor%20Ribeiro&item_number=donation&currency_code=USD&bn=PP%2dDonationsBF%3abtn_donateCC_LG%2egif%3aNonHosted)\\n\"},\n",
       " {'repo': 'manankohlii/spacex-launch-data',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# 🚀🚀🚀SpaceX Launch Data🚀🚀🚀\\nUses the freely available [SpaceX API](https://github.com/r-spacex/SpaceX-API) and HTML CSS and JS to give details of missions. The main aim of this project is to make [SpaceX](https://www.spacex.com/) related data easily available. <br>\\n\\n[Launch SpaceX](https://launchspacex.netlify.app)❤️\\n\\n[![Open Source Love](https://badges.frapsoft.com/os/v1/open-source.svg?v=102)](https://github.com/manankohlii/spacex-launch-data)&nbsp;\\n[![PRs Welcome](https://img.shields.io/badge/PRs-Welcome-brightgreen.svg?style=flat&logo=github)](https://github.com/manankohlii/spacex-launch-data)&nbsp;\\n![contributions welcome](https://img.shields.io/static/v1.svg?label=Contributions&message=Welcome&color=brightgreen&style=flat&logo=github)&nbsp;\\n[![first-timers-only](https://img.shields.io/badge/first--timers--only-friendly-blue.svg?style=flat)](https://github.com/manankohlii/spacex-launch-data)&nbsp;\\n[![spacex-launch-data](https://img.shields.io/website-up-down-green-red/http/shields.io.svg?color=blue)](https://launchspacex.netlify.app/)&nbsp;\\n<a href=\"https://app.slack.com/client/T022A4RL16V/C024JU7UNFK\">\\n <img src=\"https://img.shields.io/badge/Join community%20-Slack-4A154B.svg?&logo=slack\" alt=\"Join Slack Community\" />\\n</a>\\n\\n[![GitHub stars](https://img.shields.io/github/stars/manankohlii/spacex-launch-data)](https://github.com/manankohlii/spacex-launch-data/stargazers)\\n[![GitHub forks](https://img.shields.io/github/forks/manankohlii/spacex-launch-data)](https://github.com/manankohlii/spacex-launch-data/network/members)\\n[![GitHub Contributers](https://img.shields.io/github/contributors/manankohlii/spacex-launch-data)](https://github.com/manankohlii/spacex-launch-data/graphs/contributors)\\n[![GitHub issues](https://img.shields.io/github/issues/manankohlii/spacex-launch-data)](https://github.com/manankohlii/spacex-launch-data/issues)\\n[![GitHub pull-requests](https://img.shields.io/github/issues-pr/manankohlii/spacex-launch-data)](https://github.com/manankohlii/spacex-launch-data/pulls)\\n[![GitHub closed-issues](https://img.shields.io/github/issues-closed-raw/manankohlii/spacex-launch-data)](https://github.com/manankohlii/spacex-launch-data/pulls)\\n[![GitHub closed-prs](https://img.shields.io/github/issues-pr-closed-raw/manankohlii/spacex-launch-data)](https://github.com/manankohlii/spacex-launch-data/pulls)\\n\\n\\n\\n\\n\\n# Overview\\n\\nThe main aim of the project is to make data related to SpaceX easily available.\\n\\n# Tech Stack\\n[![HTML](https://img.shields.io/badge/html5%20-%23E34F26.svg?&style=for-the-badge&logo=html5&logoColor=white)](https://github.com/manankohlii/spacex-launch-data/search?l=html)&nbsp;\\n[![CSS](https://img.shields.io/badge/css3%20-%231572B6.svg?&style=for-the-badge&logo=css3&logoColor=white)](https://github.com/manankohlii/spacex-launch-data/search?l=css)&nbsp;\\n[![JS](https://img.shields.io/badge/javascript%20-%23323330.svg?&style=for-the-badge&logo=javascript&logoColor=%23F7DF1E)](https://github.com/manankohlii/spacex-launch-data/search?l=javascript)\\n\\n## Contributing\\n\\n**1.**  Fork [this](https://github.com/manankohlii/spacex-launch-data) repository.\\n\\n**2.**  Clone your forked copy of the project.\\n\\n```\\ngit clone --depth 1 https://github.com/<your_name>/spacex-launch-data.git\\n```\\n\\n**3.** Navigate to the project directory :file_folder: .\\n\\n```\\ncd spacex-launch-data\\n```\\n\\n**4.** Add a reference(remote) to the original repository.\\n\\n```\\ngit remote add upstream https://github.com/manankohlii/spacex-launch-data.git\\n```\\n\\n**5.** Check the remotes for this repository.\\n```\\ngit remote -v\\n```\\n\\n**6.** Always take a pull from the upstream repository to your master branch to keep it at par with the main project(updated repository).\\n\\n```\\ngit pull upstream master\\n```\\n\\n**7.** Create a new branch.\\n\\n```\\ngit checkout -b <your_branch_name>\\n```\\n\\n**8.** Perfom your desired changes to the code base.\\n<p align=\"center\"><img width=35% src=\"https://media2.giphy.com/media/L1R1tvI9svkIWwpVYr/giphy.gif?cid=ecf05e47pzi2rpig0vc8pjusra8hiai1b91zgiywvbubu9vu&rid=giphy.gif\"></p>\\n\\n**9.** Track your changes:heavy_check_mark: .\\n\\n```\\ngit add . \\n```\\n\\n**10.** Commit your changes .\\n\\n```\\ngit commit -m \"Relevant message\"\\n```\\n\\n**11.** Push the committed changes in your feature branch to your remote repo.\\n```\\ngit push -u origin <your_branch_name>\\n```\\n\\n**12.** To create a pull request, click on `compare and pull requests`. Please ensure you compare your feature branch to the desired branch of the repo you are suppose to make a PR to.\\n\\n\\n**13.** Add appropriate title and description to your pull request explaining your changes and efforts done. Always make sure you have pulled the latest code from the master branch before making a PR.\\n\\n**14.** Click on `Create Pull Request`.\\n## 📌 Opensource Programs\\n\\n### This project is a part of the following Open Source Program(s)\\n<br>\\n\\n<table style=\"width:80%;background-color:white;border-radius:30px;\">\\n    <tr>\\n  <td>\\n<center>\\n  <a href=\"https://letsgrowmore.in/soc/\"><img src=\"https://letsgrowmore.in/wp-content/uploads/2021/05/cropped-growmore-removebg-preview.png\"></img></a>\\n  </center>\\n  </td>\\n  </tr>\\n</table>\\n    <hr>\\n\\n## ✨ Our valuable Contributors👩\\u200d💻👨\\u200d💻 :\\n<a href=\"https://github.com/manankohlii/spacex-launch-data/graphs/contributors\">\\n  <img src=\"https://contrib.rocks/image?repo=manankohlii/spacex-launch-data\" />\\n</a>\\n\\n## Project Admin👨\\u200d\\n<table>\\n  <tr>\\n    <td align=\"center\"><a href=\"https://github.com/manankohlii\"><img src=\"https://avatars.githubusercontent.com/u/43683368?v=4\" height=\"120px\" width=\"120px\"/><br/><sub><b>Manan Kohli👨\\u200d</b></sub></a></td>\\n  </tr>\\n</table>\\n\\n[![forthebadge](https://forthebadge.com/images/badges/made-with-javascript.svg)](https://forthebadge.com)\\n[![forthebadge](https://forthebadge.com/images/badges/built-with-love.svg)](https://forthebadge.com) \\n[![forthebadge](https://forthebadge.com/images/badges/built-by-developers.svg)](https://forthebadge.com) \\n'},\n",
       " {'repo': 'deanoemcke/spaces',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# <img src=\"/img/icon48.png\" align=\"absmiddle\"> Spaces\\n\\n### A Chrome extension for Intuitive tab management\\n\\nSpaces is a workspace manager for chrome.\\nIt treats each chrome window like a different workspace and lets you name and save each space.\\nYou can close a window full of tabs at any time then reopen it later and continue exactly\\nwhere you left off.\\n\\nSpaces keeps track of new tabs opened in each workspace and also tabs that you close.\\nIt also allows you to quickly move a tab that you are currently viewing into any\\nother space- whether it\\'s open or closed.\\nGreat for when you find yourself opening a tab out of context with what you are currently\\nworking on and want to come back to it later.\\n\\nSpaces was developed to help users that tend to have way too many tabs open in a chrome window.\\nIt encourages you to move tabs that are not immediately relevant into a different,\\nmore appropriate space - thus removing it from your current window.\\nThis keeps your chrome session manageable - both visually and from a memory perspective.\\n\\nIsn\\'t this essentially just bookmarks with folders? Yeah, pretty much - but who uses bookmarks?\\n\\n### Chrome Web Store\\n\\nSpaces is also [available via the official Chrome Web Store](https://chrome.google.com/webstore/detail/spaces/cenkmofngpohdnkbjdpilgpmbiiljjim).\\n\\nPlease note that the webstore version may be behind the latest version here.\\n\\n### Install as an extension from source\\n\\n1. Download the **[latest available version](https://github.com/deanoemcke/spaces/archive/v1.1.1.zip)** \\n2. Unarchive to your preferred location (e.g., `Downloads`).\\n2. In **Google Chrome**, navigate to [chrome://extensions/](chrome://extensions/) and enable <kbd>Developer mode</kbd> in the upper right corner.\\n3. Click on the <kbd>LOAD UNPACKED</kbd> button.\\n4. Browse to the _root directory_ of the unarchived download, and click <kbd>OPEN</kbd>.\\n\\n> **TODO** &mdash; add more sections\\n> - [ ] Build from github\\n> - [ ] License (currently unspecified)\\n'},\n",
       " {'repo': 'RobertoIA/Invaders',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': \"Invaders\\n=\\n[Space Invaders](http://en.wikipedia.org/wiki/Space_Invaders) clone, written in Java.\\n\\n### Screenshots\\n \\n\\nTitle Screen               |  Game Screen              | Score Screen\\n:-------------------------:|:-------------------------:|:---------\\n![image](https://user-images.githubusercontent.com/69495129/136980139-7ad6adab-3f11-4711-b0a6-341080aa3361.png)   |  ![image](https://user-images.githubusercontent.com/69495129/136980236-c5d9ef85-f09a-47a7-b9d9-948f7b624002.png)|![image](https://user-images.githubusercontent.com/69495129/136980681-93dcadaf-08cb-48d8-90c9-68c651a115c9.png)\\n\\n\\n### Download\\n[Dropbox](https://dl.dropboxusercontent.com/u/23829102/invaders.zip)\\n\\nMirror: [Mediafire](http://www.mediafire.com/download/kwv9s90j9i1o4kc/invaders.zip)\\n\\n### Installation\\nAfter downloading the file above, unpack it and save it to an appropiate location.\\n\\nOpen the directory and double-click invaders.jar. In some systems (i.e. Ubuntu) you may need to select 'Open with OpenJDK 7 Runtime' or similar from the context menu.\\n\\nIf the Java path is not set correctly, you can also navigate to the directory from the command line and execute\\n\\n>java -jar invaders.jar\\n\\n### System Requirements\\nRequires Java 7 or better.\\n\\n### Resources\\n[Space Invaders Regular (font)](http://www.fonts2u.com/space-invaders-regular.font) - &copy; kylemaoin 2010\"},\n",
       " {'repo': 'Zaargh/ocr.space_code_example',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': ''},\n",
       " {'repo': 'joedel/spaced-repetition',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': 'Command Line Spaced Repetition\\n=================\\nThis is a command line spaced repetition program written in Javascript using Node.js.\\n\\nSpaced repetition is a learning technique that incorporates increasing intervals of time between subsequent review of \\npreviously learned material in order to exploit the psychological spacing effect (<a href=\"http://en.wikipedia.org/wiki/Spaced_repetition\">from Wikipedia</a>). \\nIt is very useful for learning a new language or specific sets of information.\\n\\n<strong>Basically information you know well is shown less often, information you have trouble is shown more often.</strong>\\n\\nThe algorithm implemented in this app is the SM-2 algorithm used in the SuperMemo-2 software as well as the popular \\nopen source Anki software. The algorithm is described in detail here: http://www.supermemo.com/english/ol/sm2.htm\\n\\nTo run\\n------\\n- Make sure you have <a href=\"http://nodejs.org\">Node.js</a> installed.\\n- Clone the repo and create your word list (see baseCards.json)\\n- On the command line run: node spaced.js\\n\\nWhy?\\n------\\nThis will likely turn into a web app, this was an opportunity to start small and learn a little more about Node.js.\\nOn the plus side, it\\'s very distraction free :)\\n\\nTODO: \\n- record the history and all data to a real db, do something interesting with it\\n- refactor\\n- choose when to start or skip to answer without waiting 4 seconds?\\n- choose input file from command line instead of hard coded\\n\\nLicense\\n-----\\nMIT\\n'},\n",
       " {'repo': 'Chung-I/Variational-Recurrent-Autoencoder-Tensorflow',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Gerating Sentences from a Continuous Space\\n\\nTensorflow implementation of [Generating Sentences from a Continuous Space](https://arxiv.org/abs/1511.06349).\\n\\n## Prerequisites\\n1. Python packages:\\n    - Python 3.4 or higher\\n    - Tensorflow r0.12\\n    - Numpy\\n\\n## Setting up the environment:\\n1. Clone this repository:\\n```shell=\\ngit clone https://github.com/Chung-I/Variational-Recurrent-Autoencoder-Tensorflow.git\\n```\\n2. Set up conda environment:\\n```=bash\\nconda create -n vrae python=3.6\\nconda activate vrae\\n```\\n3. Install python package requirements:\\n```=bash\\npip install -r requirements.txt\\n```\\n## Usage\\n\\n\\nTraining:\\n```shell=\\npython vrae.py  --model_dir models --do train --new True\\n```\\n\\nReconstruct:\\n```shell=\\npython vrae.py --model_dir models --do reconstruct --new False --input input.txt --output output.txt\\n```\\n\\nSample (this script read only the first line of `input.txt`, generate `num_pts` samples, and write them into `output.txt`):\\n```shell=\\npython vrae.py --model_dir models --do sample --new False --input input.txt --output output.txt\\n```\\n\\nInterpolate (this script requires that `input.txt` consists of only two sentences; it generate `num_pts` interpolations between them, and write those interpolated sentences into `output.txt`):: \\n```shell=\\npython vrae.py --model_dir models --do interpolate --new False --input input.txt --output output.txt\\n```\\n\\n`model_dir`: The location of the config file `config.json` and the checkpoint file.\\n\\n`do`: Accept 4 values: `train`, `encode_decode`, `sample`, or `interpolate`.\\n\\n`new`: create models with fresh parameters if set to `True`; else read model parameters from checkpoints in `model_dir`.\\n\\n## config.json\\n\\nHyperparameters are not passed from command prompt like that in [tensorflow/models/rnn/translate/translate.py](https://github.com/tensorflow/tensorflow/blob/r0.12/tensorflow/models/rnn/translate/translate.py). Instead, [vrae.py](https://github.com/Chung-I/Variational-Recurrent-Autoencoder-Tensorflow/blob/master/vrae.py) reads hyperparameters from [config.json](https://github.com/Chung-I/Variational-Recurrent-Autoencoder-Tensorflow/blob/master/models/config.json) in `model_dir`.\\n\\nBelow are hyperparameters in [config.json](https://github.com/Chung-I/Variational-Recurrent-Autoencoder-Tensorflow/blob/master/models/config.json):\\n\\n- `model`:\\n    - `size`: embedding size, and encoder/decoder state size.\\n    - `latent_dim`: latent space size.\\n    - `in_vocab_size`: source vocabulary size.\\n    - `out_vocab_size`: target vocabulary size.\\n    - `data_dir`: path to the corpus.\\n    - `num_layers`: number of layers for encoder and decoder.\\n    - `use_lstm`: use lstm for encoder and decoder or not. Use `BasicLSTMCell` if set to `True`; else `GRUCell` is used.\\n    - `buckets`: A list of pairs of [input size, output size] for each bucket.\\n    - `bidirectional`: `bidirectional_rnn` is used if set to `True`.\\n    - `probablistic`: variance is set to zero if set to `False`.\\n    - `orthogonal_initializer`: `orthogonal_initializer` is used if set to `True`; else `uniform_unit_scaling_initializer` is used.\\n    - `iaf`: [inverse autoregressive flow](https://github.com/openai/iaf) is used if set to `True`.\\n    - `activation`: activation for encoder-to-latent layer and latent-to-decoder layer.\\n        - `elu`: exponential linear unit.\\n        - `prelu`: parametric linear unit. (default)\\n        - `None`: linear.\\n- `train`:\\n    - `batch_size`\\n    - `beam_size`: beam size for decoding. __Warning__: beam search is still under implementation. `NotImplementedError` would be raised if `beam_size` is set to be greater than 1.\\n    - `learning_rate`: learning rate parameter passed into `AdamOptimizer`.\\n    - `steps_per_checkpoint`: save checkpoint every `steps_per_checkpoint` steps.\\n    - `anneal`: do [KL cost annealing](https://aclweb.org/anthology/K/K16/K16-1002.pdf#page=4) if set to `True`.\\n    - `kl_rate_rise_factor`: KL term weight is increasd by this much every `steps_per_checkpoint` steps.\\n    - `max_train_data_size`: Limit on the size of training data (0: no limit).\\n    - `feed_previous`: If `True`, only the first of decoder_inputs will be\\n      used (the \"GO\" symbol), and all other decoder inputs will be generated by: `next = embedding_lookup(embedding, argmax(previous_output))`. In effect, this implements a greedy decoder. It can also be used during training to emulate http://arxiv.org/abs/1506.03099. If `False`, `decoder_inputs` are used as given (the standard decoder case).\\n    - `kl_min`: the [minimum information constraint](https://arxiv.org/pdf/1606.04934v1.pdf#page=7). Should be a non-negative float (where 0 is no constraint).\\n    - `max_gradient_norm`: gradients will be clipped to maximally this norm.\\n    - `word_dropout_keep_prob`: probability of  randomly replacing some fraction of the conditioned-on word tokens with the generic unknown word token `UNK`. when equal to 0, the decoder sees no input.\\n\\n- reconstruct:\\n    - `feed_previous`\\n    - `word_dropout_keep_prob`\\n- sample:\\n    - `feed_previous`\\n    - `word_dropout_keep_prob`\\n    - `num_pts`: sample `num_pts` points.\\n- interpolate:\\n    - `feed_previous`\\n    - `word_dropout_keep_prob`\\n    - `num_pts`: sample `num_pts` points.\\n\\n## Data\\n\\nPenn TreeBank corpus is included in the repo. We also provide a [Chinese poem corpus](https://drive.google.com/file/d/178u6rYoupyT9crrIxXwHBMQ9v-7VhyqL/view?usp=sharing), [its preprocessed version](https://drive.google.com/file/d/1jfUuuVDf0dg9KZtof7Q-gotVmo-Pd3cF/view?usp=sharing) (set `{\"model\":{\"data_dir\": \"<corpus_dir>\"}}` in `<model_dir>/config.json` to it), and [its pretrained model](https://drive.google.com/file/d/1jfUuuVDf0dg9KZtof7Q-gotVmo-Pd3cF/view?usp=sharing) (set `model_dir` to it), all of which can be found [here](https://drive.google.com/drive/folders/1d7185c4qL6laphyEf5GZRV0I-2aSZcRK?usp=sharing).\\n'},\n",
       " {'repo': 'JoshuaHernandezMartinez/SpaceShipGame',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '# SpaceShipGame\\nSpaceShipGame\\n'},\n",
       " {'repo': 'charliegerard/safe-space',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Safe space - Github action\\n\\nGithub action that uses machine learning to detect potential toxic comments added to PRs and issues so authors can have a chance to edit them and keep repos a safe space.\\n\\nIt uses the [Tensorflow.js toxicity classification model](https://github.com/tensorflow/tfjs-models/tree/master/toxicity).\\n\\nIt currently works when comments are posted on issues and PRs, as well as when pull request reviews are submitted.\\n\\n## Demo\\n\\n![](demo.gif)\\n\\nIf you want some details about how it works, feel free to check the [blog post](https://charliegerard.dev/blog/github-action-toxic-comments).\\n\\n## How to use\\n\\n_If you do not have any Github actions already set up in your repo, start by creating a .github/workflows folder._\\n\\nInside your workflows folder, create a new .yml file, for example `main.yml` and copy the following lines:\\n\\n```yml\\non: [issue_comment, pull_request_review]\\n\\njobs:\\n  toxic_check:\\n    runs-on: ubuntu-latest\\n    name: Safe space\\n    steps:\\n      - uses: actions/checkout@v2\\n      - name: Safe space - action step\\n        uses: charliegerard/safe-space@master\\n        with:\\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\\n```\\n\\n`GITHUB_TOKEN` is **required** (note that Github [automatically creates this token](https://docs.github.com/en/free-pro-team@latest/actions/reference/authentication-in-a-workflow#:~:text=and%20use%20secrets.-,About%20the%20GITHUB_TOKEN%20secret,authenticate%20in%20a%20workflow%20run.&text=The%20token\\'s%20permissions%20are%20limited,%22Permissions%20for%20the%20GITHUB_TOKEN%20.%22)) but two other parameters are optional:\\n\\n\\n- `message` - a custom message you\\'d like to display in the automatic comment\\n- `toxicity_threshold` - a float number between 0 and 1. It will be used when loading the machine learning model. Its default value is 0.9.\\n\\n```yml\\non: [issue_comment, pull_request_review]\\n\\njobs:\\n  toxic_check:\\n    runs-on: ubuntu-latest\\n    name: Toxicity check\\n    steps:\\n      - uses: actions/checkout@v2\\n      - name: Safe space - action step\\n        uses: charliegerard/safe-space@master\\n        with:\\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\\n          message: \"this is my custom message\"\\n          toxicity_threshold: 0.7\\n```\\n\\nThe action can take up to 40s to run so, if you are testing it out in your repository, keep in mind that the bot will not display right after a toxic comment is posted.\\n'},\n",
       " {'repo': 'bloominstituteoftechnology/Space-X-Web-GuidedProject',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Space X Guided Project\\n\\n## Setup\\nSteps to be ready for the GP today:\\n1. clone the repo to your machine (no need to fork, unless you want to)\\n1. cd to project\\n1. run npm install (all dependencies we need to start are there, so no need to add new dependencies)\\n1. run npm start to fire up project in the browser\\n1. Open the project in your editor\\n'},\n",
       " {'repo': 'nvjob/Infinity-Square-Space',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# Infinity Square/Space 1.0.1\\n#### [nvjob.github.io/unity/infinity-square-space](https://nvjob.github.io/unity/infinity-square-space)\\n\\n![GitHub Logo](https://raw.githubusercontent.com/nvjob/nvjob.github.io/master/repo/unity%20assets/infinity%20square%20space/101/pic/10.jpg)\\n\\n**The prototype of the game is open source. Unity Asset.**<br/>\\nFeatures: infinite procedurally generated world, almost complete destructible of everything, a very large number of NPCs (up to 1000 in one star system), battles involving hundreds of NPC, gravity is an important game element.\\n\\nThis prototype of the game is completely playable, but nevertheless, this is not a complete game.<br/>\\nThe source contains all the tools for the development of the game, but you need the above-average programming skill. There are no comments in the code, but the code itself is well structured, all the scripts and shaders, functions and variables are named so that it is clear what they are responsible for.\\n\\n**This game prototype consists of five main parts:**\\n- Pools of static game objects.\\n- Procedural generation based on the coordinate system.\\n- The system of destruction, based on the substitution of objects, objects from the pool of static objects.\\n- Artificial intelligence, with a primitive simulation of life and interaction with each other, the world and the player.\\n- Game controller, with a set of weapons and skills.\\n\\n**Download standalone version on Itch.io** - https://nvjob.itch.io/infinity-squarespace-standalone <br/>\\n**WEB version on Itch.io** - https://nvjob.itch.io/infinity-square-space-web\\n\\n-------------------------------------------------------------------\\n\\n### Prerequisites\\nTo work on the project, you will need a Unity version of at least 2019.1.8 or higher (64-bit).<br/>\\nScripting Runtime Version - .net 4.x Equivalent \\n\\n### Installation:\\nhttps://www.youtube.com/watch?v=1DalkV98lyI<br/>\\n- Create a new project in Unity.\\n- Download Assets and ProjectSettings and place them in the folder of your new project.\\n- Open the desired scene in the Scenes directory.\\n\\n![GitHub Logo](https://raw.githubusercontent.com/nvjob/nvjob.github.io/master/repo/unity%20assets/infinity%20square%20space/101/pic/3.png)\\n\\n### Basic information\\nThe “Main” directory contains all the files and scripts associated with the procedural generation of the planetary system. The “AI“ directory contains all the files and scripts associated with artificial intelligence. The “Player” directory contains all the files and scripts associated with the game controller, inventory and interface. The “Menu” directory contains all the files and scripts associated with the initial menu for selecting a planet.\\n\\nThe “Universe” script in the “Menu” scene is responsible for generating the star field.\\n\\n![GitHub Logo](https://raw.githubusercontent.com/nvjob/nvjob.github.io/master/repo/unity%20assets/infinity%20square%20space/101/pic/1.png)\\n![GitHub Logo](https://raw.githubusercontent.com/nvjob/nvjob.github.io/master/repo/unity%20assets/infinity%20square%20space/101/pic/22.jpg)\\n\\nThe “StarSystem” script in the “Main” scene is responsible for generating the star system.\\n\\n![GitHub Logo](https://raw.githubusercontent.com/nvjob/nvjob.github.io/master/repo/unity%20assets/infinity%20square%20space/101/pic/2.png)\\n![GitHub Logo](https://raw.githubusercontent.com/nvjob/nvjob.github.io/master/repo/unity%20assets/infinity%20square%20space/101/pic/7.jpg)\\n\\nWhen you first start in the editor, first start the “Menu” scene to apply the settings of the “Main” scene, which are stored in “PlayerPrefs”. To test the main “Main” scene, you can generate the star systems you need using test seed.**\\n\\n![GitHub Logo](https://raw.githubusercontent.com/nvjob/nvjob.github.io/master/repo/unity%20assets/infinity%20square%20space/101/pic/2a.png)\\n\\n-------------------------------------------------------------------\\n\\n**Authors:** [#NVJOB. Developer Nicholas Veselov. Разработчик Николай Веселов. Санкт-Петербург.](https://nvjob.github.io)\\n\\n**License:** MIT License. [Clarification of licenses](https://nvjob.github.io/mit-license).\\n\\n**Sorry:** This project is currently frozen and cannot be supported or updated due to its complete non-profitability.\\n'},\n",
       " {'repo': 'TeamGalacticraft/Galacticraft',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': \"[![](https://img.shields.io/github/workflow/status/TeamGalacticraft/Galacticraft/build/main?style=flat-square&logo=github)](https://github.com/TeamGalacticraft/Galacticraft/actions/workflows/build.yml?query=branch%3Amain)\\n[![](https://img.shields.io/github/issues/TeamGalacticraft/Galacticraft?style=flat-square&logo=github)](https://github.com/TeamGalacticraft/Galacticraft/issues)\\n[![](https://img.shields.io/github/issues-pr/TeamGalacticraft/Galacticraft?logo=github&style=flat-square)](https://github.com/TeamGalacticraft/Galacticraft/pulls)\\n[![](https://img.shields.io/discord/775251052517523467.svg?colorB=5865F2&label=discord&style=flat-square&logo=discord&logoColor=azure)](https://discord.gg/n3QqhMYyFK)\\n[![](https://img.shields.io/twitch/status/galacticraftdev.svg?style=flat-square&logo=twitch&logoColor=azure)](https://twitch.tv/galacticraftdev)\\n\\n# Galacticraft 5\\nThe classic Minecraft space mod, rewritten from the ground up for modern versions of the game.\\n\\n## Addon Development\\nFor information regarding addon development please see the [GalacticraftAPI](https://github.com/TeamGalacticraft/GalacticraftAPI) repository.\\n\\n## Common Questions\\nHere we answer a few common questions we get regarding the release of the mod.\\\\\\nMore information can be found in the `#faq` channel in [our Discord](https://discord.gg/n3QqhMYyFK).\\n\\n**Q:** When will Galacticraft 5 for 1.16+ be released?\\\\\\n**A:** There is currently no ETA for a release, keep an eye out in [our Discord server](https://discord.gg/n3QqhMYyFK) for an alpha release announcement.\\n\\n**Q:** Will Galacticraft 5 be on Forge?\\\\\\n**A:** Yes, Forge development will start *after* we have a survival playable alpha build out for Fabric.\\nWe will be using [architectury](https://github.com/architectury) to compile the mod for both loaders.\\n\\n**Q:** Will Galacticraft 5 work with `*` Galacticraft 4 addon?\\\\\\n**A:** No, Galacticraft 5 is a complete rewrite, and does not have backwards compatibility with the old addon api.\\n\\n## Pre-Alpha Builds\\nPre-alpha builds are available for each commit to the `main` branch of this repository in the [Actions tab](https://github.com/TeamGalacticraft/Galacticraft/actions/workflows/build.yml?query=branch%3Amain) on GitHub.\\nThese builds are not production ready or survival playable, they exist solely for testing.\\\\\\nPlease note that ***NO SUPPORT*** will be provided for the use of these builds and asking for such may result in a ban or mute,\\nconsider this your one and only warning.\\n\\n## Contributing\\nWe welcome and encourage community contributions but before you start please read our [contributing guidelines](https://github.com/TeamGalacticraft/Galacticraft/blob/main/.github/CONTRIBUTING.md). \\\\\\nPull requests require at least one team member's approving review, and a successful build before they will be merged.\"},\n",
       " {'repo': 'uw-it-aca/spacescout-android',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': 'This app has been superseded by https://github.com/uw-it-aca/scout-android\\n\\n\\nspacescout-android\\n==================\\nThis is the Android client for SpaceScout.  It connects to services provided by https://github.com/uw-it-aca/spotseeker_server\\n\\nTest server can be created based on https://github.com/uw-it-aca/spacescout_builds\\n\\nThis project is currently undergoing development.\\n\\n[Design Spec](https://github.com/uw-it-aca/spacescout-android/wiki/Design-Spec)\\n\\nGuides and docs in Wiki.\\n'},\n",
       " {'repo': 'davidawad/SpaceShare',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"# [SpaceShare](https://spaceshare.me)   [![Build Status](https://travis-ci.org/DavidAwad/SpaceShare.svg?branch=master)](https://travis-ci.org/DavidAwad/SpaceShare) [![Coverage Status](https://coveralls.io/repos/DavidAwad/SpaceShare/badge.svg)](https://coveralls.io/r/DavidAwad/SpaceShare) [![Code Climate](https://d3s6mut3hikguw.cloudfront.net/github/DavidAwad/SpaceShare/badges/gpa.svg)](https://d3s6mut3hikguw.cloudfront.net/github/DavidAwad/SpaceShare/) [![Gitter](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/DavidAwad/SpaceShare?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=body_badge)\\n\\n<!--\\n[![Deploy](https://www.herokucdn.com/deploy/button.png)](https://heroku.com/deploy?template=https://github.com/DavidAwad/SpaceShare) [![Deploy to Bluemix](https://bluemix.net/deploy/button.png)](https://bluemix.net/deploy?repository=https://github.com/davidawad/SpaceShare)\\n-->\\n\\n### This is a File sharing web service meant to simplify file sharing between persons and groups of people by removing the need to login. It is not secure, it's not meant to be, it's meant to be the extreme trade off between convenience and security.\\n\\n\\n#### You go to the app, upload your file, attach a number.\\n#### Others can go to the site knowing that number; or go to [spaceshare.me/upload/number](spaceshare.me/upload/number) and it will give you that file.\\n\\n## Requirements\\n- Python\\n- Flask\\n- MongoDB\\n- Redis\\n- Celery\\n- React\\n- Gulp\\n- bower\\n- npm\\n\\n## Development\\n#### You wanna run this hotness?\\n```shell\\n$git clone https://github.com/davidawad/spaceshare\\n$cd spaceshare\\n$docker-compose up\\n# awesome things\\n```\\n\\n###### fair warning, it doesn't fucking work yet.\\n\\n## Contributing\\nPlease do check out the [contributing](/CONTRIBUTING.md) guide if you're interested.\\n\\n## Special Thanks :\\n### [Joel Pena](https://github.com/jpena29), [Devon Peticolas](https://github.com/x), and [Wisdom Omuya](https://github.com/deafgoat), and of course StackOverflow made this app Possible.\\n\"},\n",
       " {'repo': 'post-kerbin-mining-corporation/StationPartsExpansionRedux',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# Stockalike Station Parts Expansion Redux\\n\\nA mod pack for Kerbal Space Program, delivering a high quality expansion to orbital and surface base construction.\\n\\n* [Features](#features)\\n* [Dependencies](#dependencies)\\n* [Installation](#installation)\\n* [Optional Patches](#optional-patches)\\n* [External Compatibility](#features)\\n* [Contributing](#contributing)\\n\\n## Features\\n\\nA complete and total rebuild of my old Stockalike Station Parts Expansion project. Everything has been redone, and nothing is left of the old mod!\\n\\n* **Small Station Parts:**  A full set of 1.25m station parts, with habitats, control rooms, connecting tubes, hubs and much more.\\n* **Medium Station Parts:** Expanding the 2.5m set that stock provides, you\\'ll find greenhouses, structural components, specialized connectors, orbital stowage bays...\\n* **Large Station Parts:**  Yup, large station parts! A full set in the 3.75m size class, from habs to labs to... well, lots more!\\n* **Reworks:** I\\'ve taken the time to rework the three stock station components (Cupola, MPL and Hitchiker) to match the new parts. They look the same but new!\\n* **Inflatable Habitats:**  A nice set of inflatable habitats for maximum crew space. They range from teeny to huge.\\n* **Centrifuges:** All size categories have at least one centrifuge habitat. They range from small and cramped to massive and spacious (one of the largest parts I\\'ve ever made).\\n* **Cargo Containers:** I really felt like making some multi-purpose cargo containers. These modules will adapt to what mods you have installed, and provide storage for resources from MKS, TAC-LS, USI-LS, EPL and probably a few more that I forgot.\\n* **Self-Levelling Base Frames**: To help align your bases on slightly bumpy terrain, use these base plates, which feature individually adjustable legs with a self-level function\\n* **Extensible Crew Connections:** Much like the Klaw, but better! Varying lengths and impressive looks!\\n* **Comprehensive Mod Support:** With the help of many forumgoers, this mod contains support for tons of mods, specifically those related to life support and colonization.\\n* **IVAs:** It almost killed me but everything has fully featured IVAs.\\n\\n## Dependencies\\n\\n### Required\\nThese components are required for the mod to function and are bundled as part of any download:\\n* [ModuleManager (4.2.1)](https://github.com/sarbian/ModuleManager)\\n* [B9PartSwitch (2.18.0)](https://github.com/blowfishpro/B9PartSwitch)\\n* [Near Future Props (0.7.1)](https://github.com/ChrisAdderley/NearFutureProps)\\n\\n## Installation\\n\\nTo install, place the GameData folder inside your Kerbal Space Program folder. If asked to overwrite files, please do so.\\n\\nNOTE: Do NOT rename or move folders within the GameData folder - this mod uses absolute paths to assets and will break if this happens.\\n\\n## Optional Patches\\n\\nSome extra patches are bundled that you can use to tweak your installation. To install them, drop the correct folder from the **Extras** folder into your KSP GameData Folder\\n\\n* **StationPartsExpansionReduxIVAs**: Adds IVAs to all parts.\\n\\n## External Mod Compatibility\\n\\nThis mod includes compatibility patches for the following mods:\\n* [KSP-AVC](https://github.com/CYBUTEK/KSPAddonVersionChecker): Provides version checking\\n* [Community Tech Tree](https://github.com/ChrisAdderley/CommunityTechTree): Provides an expanded, community-sourced technology tree for modders to use\\n* Kerbal Inventory System: Inventory added to particular parts\\n* Connected Life Support: Correctly marks parts as passable or impassable\\n* Extraplanetary Launch Pads: containers store mod resources\\n* KeepFit: Appropriate parts provide exercise\\n* Modular Kolonization System: containers store mod resources\\n* Snacks: sets up relevant life support for parts\\n* USI-LS:  sets up relevant life support for parts\\n\\nNote that the vast majority of this compatibility is community-sourced and not maintained by me.\\n\\n## Contributing\\n\\nI certainly accept pull requests. Please target all such things to the `dev` branch though!\\n\\n## Licensing\\n\\nThe art assets in this pack (all .dds, .png and .mu files) are distributed under an All Rights Reserved license. You may not redistribute or re-use these assets without express permission from the author.\\n\\nAny bundled mods are distributed under their own licenses:\\n* ModuleManager by ialdabaoth and sarbian is distributed under a Creative Commons Sharealike license. More details, including source code, can be found [here](http://forum.kerbalspaceprogram.com/threads/31342-0-20-ModuleManager-1-3-for-all-your-stock-modding-needs?p=528607&viewfull=1#post528607)\\n* The Community Resource Pack by RoverDude is also distributed under its own license. Please find source and more details [here](https://github.com/BobPalmer/CommunityResourcePack)\\n- B9PartSwitch by blowfish is also distributed under its own license. Please find source and more details [here](https://github.com/blowfishpro/B9PartSwitch)\\n\\nEverything else is distributed under the MIT license.\\n\\nCopyright (c) 2019 Chris Adderley\\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n'},\n",
       " {'repo': 'simonswain/deepspace',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Deep Space\\n\\nDeep Space is a science fiction simulation.\\n\\nIt creates stars, planets, economies, empires and spaceships.\\n\\nShips leave their home planet, colonise ungoverned words and spread\\ntheir empires as far as possible.\\n\\nDeep Space was first presented at JSConf.asia 2014. It is a study in\\nphysics, economics and emergent behaviour. Scientific accuracy is\\nsacrificed to make an interesting sim.\\n\\nYou can see a demo in operation at [http://simonswain.com/deepspace](http://simonswain.com/deepspace).\\n\\n## Install\\n\\nI run this on Ubuntu 14.04 desktop. Only really in Chrome, but it seems to work OK in Firefox.\\n\\nSeems to work fine on a mac with node installed using nvm.\\n\\nThis should get it installed:\\n\\n```bash\\ngit clone git@github.com:simonswain/deepspace.git\\ncd deepspace\\ncp config/index.sample.js config/index.js\\nnpm install\\nbower install\\nnode run\\n```\\n\\nWill run on `localhost:3002`\\n\\n## Key Control\\n\\n* `space` advances to next slide\\n* `left` and `Right` arrows change slide\\n* `esc` returns to title / index page\\n* `tab` toggles options on some screens\\n\\n## Screens\\n\\n### Make Planet\\n\\nObserve planet economy\\n\\n### Make System\\n\\nCreate a star and `n` planets with non space capable economies.\\n\\n### Make Ships\\n\\nCreate a star system and two empires that launch ships. Test Mode. No shooting.\\n\\n### Make Fight\\n\\nCreate a star system and two empires that launch ships. Test Mode. Shooting.\\n\\n### Make Colonies\\n\\nCreate a star system and one operational empire on a random planet.\\n\\n### Make War\\n\\nCreate a star system and two operational empires that fight for dominance.\\n\\n### Make Empires\\n\\nCreate a Sector and multiple competing empires.\\n\\n\\n## Reading and Viewing\\n\\n* Traveller (science fiction adventure in the far future)\\n* Ian M Banks Culture novels\\n* Red Mars, Green Mars, Blue Mars\\n* Atari vector games (Major Havoc, Tempest)\\n* Williams arcade games (Defender, Robotron, Joust)\\n* Tron (1992)\\n* Wargames (1983)\\n* Starship Troopers (1997)\\n\\n## History\\n\\n2014-11-25 0.0.1 Rough Cut'},\n",
       " {'repo': 'spacetimeengineer/spacetimeengine',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '![alt text](https://github.com/spacetimeengineer/spacetimeengine/blob/master/resources/spacetimeengine_logo.png)\\n\\nA Python utility built on Sympy (A symbolic mathematics library) which will analyze any given metric solution to the Einstein field equations. \\n\\n![equation](https://latex.codecogs.com/png.latex?%5Cdpi%7B100%7D%20%5Chuge%20G_%7B%5Cmu%5Cnu%7D%20&plus;%20%5CLambda%20g_%7B%5Cmu%5Cnu%7D%20%3D%20%5Cfrac%7B8%5Cpi%20G%7D%7Bc%5E4%7DT_%7B%5Cmu%5Cnu%7D)\\n\\nPrerequisites (Linux)\\n=====================\\n\\n1.) Install Python3\\n\\n    $ sudo apt install python3\\n\\n2.) Install pip3\\n\\n    $ sudo apt install python3-pip\\n\\n3.) Install git\\n\\n    $ sudo apt-get install git\\n    \\nPrerequisites (MacOS)\\n=====================\\n  \\n1.) Install homebrew  \\n    \\n    $ ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" brew doctor\\n    \\n2.) Set python as an enviornmental varible. \\n\\n    $ export PATH=\"/usr/local/opt/python/libexec/bin:$PATH\"\\n    \\n2.) Install git\\n\\n    $ brew install git\\n    \\n3.) Install python3 and pip3 (https://docs.python-guide.org/starting/install3/osx/)\\n\\n    $ brew install python3\\n    $ brew postinstall python3\\n    \\nPrerequisites (Windows)\\n=======================\\n1.) Install Python3 & pip3\\n\\n    Navigate to https://www.python.org/downloads/\\n    \\n3.) Install git\\n\\n    Navigate to https://gitforwindows.org/\\n    \\nInstallation with pip3\\n======================\\n\\n1.) Install with pip3\\n\\n    $ pip3 install spacetimeengine    \\n    \\n2.) Enter python shell\\n\\n    $ python3\\n    \\n3.) Import spacetimeengine\\n\\n    >> from spacetimeengine import *\\n    \\n4.) Create a SpaceTime object which describes the Schwarzschild spacetime\\n\\n    >> schwarzschild_spacetime = SpaceTime(Solution().schwarzschild())\\n    \\n5.) Enjoy watching the coefficients get computed.\\n\\nInstallation with git\\n=====================\\n\\n1.) Clone repository\\n\\n    $ git clone https://github.com/spacetimeengineer/spacetimeengine\\n\\n2.) Enter directory\\n\\n    $ cd spacetimeengine/spacetimeengine/samples\\n    \\n3.) Run example.py\\n\\n    $ python3 example.py\\n\\nSuggested Use\\n=============\\nIf you are a student or researcher, and you find yourself reading a publication based in General Relativity which provides metric solutions, then this utility can be used for working out the curvature coefficients which associate with the solution provided by the user. This can be a helpful utility as you read through the literature because you will be able to cross-reference the information provided by the literature with the values the spacetimeengine provides (this is why I developed it originally). More commonly, this utility can be used for error checking.\\n    \\n[Metric Tensor](https://en.wikipedia.org/wiki/Metric_tensor)\\n===============\\n\\nGenerally speaking, any metric solution to the Einstein field equations will be packaged into a geometric object known as the metric tensor. The metric tensor is often represented in matrix form and the spacetimeengine package adopts this representation.\\n\\n![equation](https://latex.codecogs.com/png.latex?%5Cdpi%7B100%7D%20%5Chuge%20g_%7B%5Cmu%5Cnu%7D%3D%5Cbegin%7Bbmatrix%7D%20%5Cleft%20%28%201-%5Cfrac%7B2GM%7D%7Brc%5E%7B2%7D%7D%20%5Cright%20%29%20%26%200%20%26%200%20%26%200%20%5C%5C%200%20%26%20-%5Cleft%20%28%201-%5Cfrac%7B2GM%7D%7Brc%5E%7B2%7D%7D%20%5Cright%20%29%5E%7B-1%7D%20%26%200%20%26%200%20%5C%5C%200%20%26%200%20%26%20-r%5E%7B2%7D%20%26%200%20%5C%5C%200%20%26%200%20%26%200%20%26%20-r%5E%7B2%7D%5Csin%5E%7B2%7D%5Ctheta%20%5Cend%7Bbmatrix%7D)\\n\\nThe spacetimeengine package employs the Sympy \\'Matrix\\' object for packaging the metric tensor and it serves as the essential parameter for constructing a \\'SpaceTime\\' object. The Solutions module currently stores some well-known metrics for study, but these can be used for understanding how to construct new solutions.\\n\\nConstructing a solution (In development)\\n=======================\\nCurrently, all metric solutions are packaged by specifying four key parameters and storing them in an array. These parameters include an index configuration for the given metric solution, the coordinates to define the metric in terms of, the metric itself, and the cosmological constant. It is important to note that a zero-valued cosmological constant indicates the employment of a classical formulation to the Einstein field equations. Below represents a valid definition of the Schwarzschild stationary black hole solution.\\n\\n    def schwarzschild(self):    \\n\\n        # Assigns meaning to the coordinates.\\n        x0, x1, x2, x3 = symbols(\\'t r theta phi\\')\\n        # Groups the coordinates in an array.\\n        coordinate_set = [x0, x1, x2, x3]\\n        \\n        \\n        # Constants required to describe the metric.\\n        G, M, c = symbols(\\'G M c\\')\\n        \\n        \\n        # Metric.\\n        metric = Matrix([    \\n                            [ (1-(2*G*M)/(x1*c**2)), 0, 0, 0 ], \\n                            [ 0, - (1-(2*G*M)/(x1*c**2))**(-1), 0, 0 ], \\n                            [ 0, 0, - x1**2, 0 ], \\n                            [ 0, 0, 0, - x1**2*sin(x2)**2 ]\\n                        ])\\n        \\n        # Describes the index configuration which which the metric represents.\\n        index_config = \"dd\"\\n        \\n        \\n        # Cosmological constant.\\n        cosmological_constant = 0\\n        \\n        \\n        # An array detailing the solution.\\n        solution_array = [ metric, coordinate_set, index_config, cosmological_constant ]\\n        \\n        \\n        # Returns solution\\n        return solution_array\\n                                        \\nIt may be helpful to store the solutions in a separate module. I prefer to keep my solutions in a \\'Solution()\\' class, which can be found in the \\'solutions\\' module. To construct a \\'SpaceTime\\' object just execute the command below, but first consider the given solution since high complexity solutions can take exponentially longer to process.\\n\\n    >>> spacetime = SpaceTime(Solution().schwarzschild())\\n\\nThe index configuration in this case is \"dd\" which represents a down-down configuration, which reflects a double covariant index configuration. These can be \"uu\", \"dd\", \"ud\", \"du\", but this library currently only supports certain index configurations depending on the quantity in question.\\n\\n[Stress-Energy-Momentum Tensor](https://en.wikipedia.org/wiki/Stress%E2%80%93energy_tensor)\\n=============================\\nThe Einstein field equations describe the equivilence of space-time curvature to mass-energy. The mass-energy is described by the coefficents encompassed within the stress-energy-momentum tensor denoted by T_{\\\\mu\\\\nu}. The cosmological constant denoted by Lambda is treated as an input parameter (since it is independent of the metric in most cases) and represents the dark energy thought to be responsble for the accellerated expansion of the cosmos. \\n\\n![equation](https://latex.codecogs.com/png.latex?%5Cdpi%7B100%7D%20%5Chuge%20T_%7B%5Cmu%20%5Cnu%20%7D%3D%7B%5Cfrac%20%7Bc%5E%7B4%7D%7D%7B8%5Cpi%20G%7D%7D%5Cleft%20%28%20G_%7B%5Cmu%20%5Cnu%20%7D&plus;%5CLambda%20g_%7B%5Cmu%20%5Cnu%20%7D%20%5Cright%20%29)\\n\\n\\n    >>> cosmological_constant = 0\\n    >>> mu = 0 # (dt)\\n    >>> nu = 1 # (dr)\\n    >>> index_config = \"dd\"\\n    >>> spacetime.print_stress_energy_coefficient(index_config, mu, nu, cosmological_constant)\\n    \\n    0\\n\\nSince the Schwarzschild solution is a vacuum solution, any stress energy coefficient will yield a zero.\\n\\n[The Einstein Tensor](https://en.wikipedia.org/wiki/Einstein_tensor)\\n=====================\\nThe Einstein tensor denoted by $G_{\\\\my\\\\nu}$ desribes the curvature of spacetime and allows the Einstein field equations to be written in concise form.\\n\\n![equation](https://latex.codecogs.com/png.latex?%5Cinline%20%5Cdpi%7B100%7D%20%5Chuge%20G_%7B%5Cmu%20%5Cnu%20%7D%3DR_%7B%5Cmu%20%5Cnu%20%7D-%7B%5Ctfrac%20%7B1%7D%7B2%7D%7DRg_%7B%5Cmu%20%5Cnu%20%7D)\\n\\n    >>> mu = 0 # (dt)\\n    >>> nu = 1 # (dr)\\n    >>> index_config = \"dd\"\\n    >>> spacetime.print_einstein_coefficient(index_config, mu, nu)\\n    \\n    G₀₁ = 0\\n\\n\\n[Ricci Tensor](https://en.wikipedia.org/wiki/Ricci_curvature)\\n===============\\nIn differential geometry, the Ricci curvature tensor represents the amount by which the volume of a narrow conical piece of a small geodesic ball in a curved Riemannian manifold deviates from that of the standard ball in Euclidean space. As such, it provides one way of measuring the degree to which the geometry determined by a given Riemannian metric might differ from that of ordinary Euclidean n-space.\\n\\n![equation](https://latex.codecogs.com/png.latex?%5Cinline%20%5Cdpi%7B100%7D%20%5Chuge%20R_%7Bij%7D%20%3D%20%7BR%5Ek%7D_%7Bikj%7D)\\n\\n    >>> mu = 0 # (dt)\\n    >>> nu = 1 # (dr)\\n    >>> index_config = \"dd\"\\n    >>> spacetime.print_ricci_coefficient(index_config, 3, 2)\\n    \\n    R₃₂ = 0\\n\\n\\n[Riemann Tensor](https://en.wikipedia.org/wiki/Riemann_curvature_tensor)\\n================\\nIn the mathematical field of differential geometry, the Riemann curvature tensor is the most common method used to express the curvature of Riemannian manifolds. It assigns a tensor to each point of a Riemannian manifold (i.e., it is a tensor field), that measures the extent to which the metric tensor is not locally isometric to that of Euclidean space.\\n\\n![equation](https://latex.codecogs.com/png.latex?%5Cinline%20%5Cdpi%7B100%7D%20%5Chuge%20R%5E%5Crho%7B%7D_%7B%5Csigma%5Cmu%5Cnu%7D%20%3D%20%5Cpartial_%5Cmu%5CGamma%5E%5Crho%7B%7D_%7B%5Cnu%5Csigma%7D%20-%20%5Cpartial_%5Cnu%5CGamma%5E%5Crho%7B%7D_%7B%5Cmu%5Csigma%7D%20&plus;%20%5CGamma%5E%5Crho%7B%7D_%7B%5Cmu%5Clambda%7D%5CGamma%5E%5Clambda%7B%7D_%7B%5Cnu%5Csigma%7D%20-%20%5CGamma%5E%5Crho%7B%7D_%7B%5Cnu%5Clambda%7D%5CGamma%5E%5Clambda%7B%7D_%7B%5Cmu%5Csigma%7D)\\n\\n\\n    >>> index_config = \"uddd\"\\n    >>> spacetime.print_reimann_coefficient(index_config, 3, 2, 2, 3)\\n    \\n            -2⋅G⋅M \\n    R³₂₂₃ = ───────\\n              2    \\n             c ⋅x₁ \\n\\n![equation](https://latex.codecogs.com/png.latex?%5Cinline%20%5Cdpi%7B100%7D%20%5Chuge%20%7B%5Cdisplaystyle%20R_%7Bik%5Cell%20m%7D%3D%7B%5Cfrac%20%7B1%7D%7B2%7D%7D%5Cleft%28%7B%5Cfrac%20%7B%5Cpartial%20%5E%7B2%7Dg_%7Bim%7D%7D%7B%5Cpartial%20x%5E%7Bk%7D%5Cpartial%20x%5E%7B%5Cell%20%7D%7D%7D&plus;%7B%5Cfrac%20%7B%5Cpartial%20%5E%7B2%7Dg_%7Bk%5Cell%20%7D%7D%7B%5Cpartial%20x%5E%7Bi%7D%5Cpartial%20x%5E%7Bm%7D%7D%7D-%7B%5Cfrac%20%7B%5Cpartial%20%5E%7B2%7Dg_%7Bi%5Cell%20%7D%7D%7B%5Cpartial%20x%5E%7Bk%7D%5Cpartial%20x%5E%7Bm%7D%7D%7D-%7B%5Cfrac%20%7B%5Cpartial%20%5E%7B2%7Dg_%7Bkm%7D%7D%7B%5Cpartial%20x%5E%7Bi%7D%5Cpartial%20x%5E%7B%5Cell%20%7D%7D%7D%5Cright%29&plus;g_%7Bnp%7D%5Cleft%28%5CGamma%20%5E%7Bn%7D%7B%7D_%7Bk%5Cell%20%7D%5CGamma%20%5E%7Bp%7D%7B%7D_%7Bim%7D-%5CGamma%20%5E%7Bn%7D%7B%7D_%7Bkm%7D%5CGamma%20%5E%7Bp%7D%7B%7D_%7Bi%5Cell%20%7D%5Cright%29%7D)\\n\\n    >>> spacetime.print_riemann_coefficient(\"dddd\", 2, 0, 2, 0)\\n\\n                ⎛         2  ⎞\\n            G⋅M⋅⎝2⋅G⋅M - c ⋅r⎠\\n    R₂₀₂₀ = ──────────────────\\n                   4  2       \\n                  c ⋅r        \\n\\n\\n[Christoffel symbols of the First Kind](https://en.wikipedia.org/wiki/Christoffel_symbols)\\n=======================================\\n\\nThe connection coefficients or \\'Christoffel symbol\\' are an array of numbers which represent the metric connection. The metric connection can be used to measure distances along curved manifolds. In General Relativity, the metric connection actually identifies the meaning of the gravitational field and can be used to track trajectories through spacetime.\\n\\n![equation](https://latex.codecogs.com/png.latex?%5Cinline%20%5Cdpi%7B100%7D%20%5Chuge%20%5CGamma%20_%7Bcab%7D%3D%7B%5Ctfrac%20%7B1%7D%7B2%7D%7D%5Cleft%28%7B%5Cfrac%20%7B%5Cpartial%20g_%7Bca%7D%7D%7B%5Cpartial%20x%5E%7Bb%7D%7D%7D&plus;%7B%5Cfrac%20%7B%5Cpartial%20g_%7Bcb%7D%7D%7B%5Cpartial%20x%5E%7Ba%7D%7D%7D-%7B%5Cfrac%20%7B%5Cpartial%20g_%7Bab%7D%7D%7B%5Cpartial%20x%5E%7Bc%7D%7D%7D%5Cright%29)\\n\\n    >>> spacetime.print_connection_coefficient(\"ddd\", 1, 0, 0)\\n\\n           -G⋅M \\n    Γ₁₀₀ = ─────\\n            2  2\\n           c ⋅r \\n\\n[Christoffel symbols of the Second Kind](https://en.wikipedia.org/wiki/Christoffel_symbols)\\n=======================================\\n\\n![equation](https://latex.codecogs.com/png.latex?%5Cinline%20%5Cdpi%7B100%7D%20%5Chuge%20%5CGamma%20%5E%7Bi%7D%7B%7D_%7Bkl%7D%3D%7B%5Ctfrac%20%7B1%7D%7B2%7D%7Dg%5E%7Bim%7D%5Cleft%28%7B%5Cfrac%20%7B%5Cpartial%20g_%7Bmk%7D%7D%7B%5Cpartial%20x%5E%7Bl%7D%7D%7D&plus;%7B%5Cfrac%20%7B%5Cpartial%20g_%7Bml%7D%7D%7B%5Cpartial%20x%5E%7Bk%7D%7D%7D-%7B%5Cfrac%20%7B%5Cpartial%20g_%7Bkl%7D%7D%7B%5Cpartial%20x%5E%7Bm%7D%7D%7D%5Cright%29)\\n\\n    >>> index_config = \"udd\"\\n    >>> spacetime.print_connection_coefficient(index_config, 1, 3, 3)\\n\\n           ⎛         2   ⎞    2    \\n           ⎝2⋅G⋅M - c ⋅x₁⎠⋅sin (x₂)\\n    Γ¹₃₃ = ────────────────────────\\n                       2           \\n                      c            \\n\\n[Weyl Tensor](https://en.wikipedia.org/wiki/Weyl_tensor)\\n=============\\n\\nIn differential geometry, the Weyl curvature tensor, named after Hermann Weyl, is a measure of the curvature of spacetime or, more generally, a pseudo-Riemannian manifold. Like the Riemann curvature tensor, the Weyl tensor expresses the tidal force that a body feels when moving along a geodesic. The Weyl tensor differs from the Riemann curvature tensor in that it does not convey information on how the volume of the body changes, but rather only how the shape of the body is distorted by the tidal force.\\n\\n![equation](https://latex.codecogs.com/png.latex?%5Cdpi%7B100%7D%20%5Chuge%20%7B%5Cdisplaystyle%20C_%7Bik%5Cell%20m%7D%3DR_%7Bik%5Cell%20m%7D&plus;%7B%5Cfrac%20%7B1%7D%7Bn-2%7D%7D%5Cleft%28R_%7Bim%7Dg_%7Bk%5Cell%20%7D-R_%7Bi%5Cell%20%7Dg_%7Bkm%7D&plus;R_%7Bk%5Cell%20%7Dg_%7Bim%7D-R_%7Bkm%7Dg_%7Bi%5Cell%20%7D%5Cright%29&plus;%7B%5Cfrac%20%7B1%7D%7B%28n-1%29%28n-2%29%7D%7DR%5Cleft%28g_%7Bi%5Cell%20%7Dg_%7Bkm%7D-g_%7Bim%7Dg_%7Bk%5Cell%20%7D%5Cright%29%2C%7D)\\n\\n    >>> index_config = \"dddd\"\\n    >>> spacetime.print_weyl_coefficient(index_config, 3, 2, 2, 3)\\n\\n                       2   \\n            2⋅G⋅M⋅r⋅sin (θ)\\n    C₃₂₂₃ = ───────────────\\n                    2      \\n                   c      \\n\\n[Schouten Tensor](https://en.wikipedia.org/wiki/Schouten_tensor) (Experimental)\\n=================\\n\\n![equation](https://latex.codecogs.com/png.latex?%5Cdpi%7B100%7D%20%5Chuge%20P_%7Bij%7D%20%3D%20%5Cfrac%7B1%7D%7Bn-2%7D%5Cleft%20%28%20R_%7Bij%7D%20-%20%5Cfrac%7BR%7D%7B2d-2%7D%5C%3A%20g_%7Bij%7D%20%5Cright%20%29)\\n\\n    >>> spacetime.get_schouten_coefficient(\"dd\",0,0)\\n\\n                                    2\\n              ⎛         2  ⎞ ⎛d    ⎞ \\n          G⋅M⋅⎝2⋅G⋅M - c ⋅r⎠⋅⎜──(t)⎟ \\n                             ⎝dt   ⎠ \\n    P₀₀ = ───────────────────────────\\n                      4  3           \\n                     c ⋅r         \\n\\n\\n[Geodesics parametrized by proper time](https://en.wikipedia.org/wiki/Geodesics_in_general_relativity#Mathematical_expression) (Experimental)\\n=======================================\\nThis is a measure of the local acceleration; that which could be measured by an accelerometer.\\n\\n![equation](https://latex.codecogs.com/png.latex?%5Cdpi%7B100%7D%20%5Chuge%20%5Cfrac%7Bd%5E%7B2%7Dx%5E%7B%5Clambda%7D%7D%7Bd%5Ctau%5E%7B2%7D%7D&plus;%5CGamma%5E%7B%5Clambda%7D_%7B%5Cmu%5Cnu%7D%5Cfrac%7Bdx%5E%7B%5Cmu%7D%7D%7Bd%5Ctau%7D%5Cfrac%7Bdx%5E%7B%5Cnu%7D%7D%7Bd%5Ctau%7D%3D0)\\n\\n    >>> spacetime.print_proper_acceleration(0)\\n\\n[Geodesics parametrized by coordinate time](https://en.wikipedia.org/wiki/Geodesics_in_general_relativity#Equivalent_mathematical_expression_using_coordinate_time_as_parameter) (Experimental)\\n===========================================\\nThis is a measure of the accelleration one observers another undergoing.\\n\\n![equation](https://latex.codecogs.com/png.latex?%5Cdpi%7B100%7D%20%5Chuge%20%5Cfrac%7Bd%5E%7B2%7Dx%5E%7B%5Clambda%7D%7D%7Bdt%5E%7B2%7D%7D%3D%5CGamma%5E%7B0%7D_%7B%5Cmu%5Cnu%7D%5Cfrac%7Bdx%5E%7B%5Cmu%7D%7D%7Bdt%7D%5Cfrac%7Bdx%5E%7B%5Cnu%7D%7D%7Bdt%7D%5Cfrac%7Bdx%5E%7B%5Clambda%7D%7D%7Bdt%7D%5C%3B-%5C%3B%5CGamma%5E%7B%5Clambda%7D_%7B%5Cmu%5Cnu%7D%5Cfrac%7Bdx%5E%7B%5Cmu%7D%7D%7Bdt%7D%5Cfrac%7Bdx%5E%7B%5Cnu%7D%7D%7Bdt%7D)\\n\\n    >>> spacetime.print_coordinate_acceleration(0)\\n\\n[Geodesic deviation equation](https://en.wikipedia.org/wiki/Geodesic_deviation#Mathematical_definition) (Experimental)\\n=============================\\nThis is a measure of how much two initial paralell geodesic paths will deviate or converge. \\n\\n![equation](https://latex.codecogs.com/png.latex?%5Cdpi%7B100%7D%20%5Chuge%20%5Cfrac%7Bd%5E%7B2%7D%5Cxi%5E%7B%5Clambda%7D%7D%7Bdt%5E%7B2%7D%7D%3D-%5C%3BR%5E%7B%5Clambda%7D_%7B%5Cmu%5Cnu%5Cell%7D%5Cfrac%7Bdx%5E%7B%5Cmu%7D%7D%7Bdt%7D%5Cfrac%7Bdx%5E%7B%5Cnu%7D%7D%7Bdt%7D%5Cxi%5E%7B%5Cell%7D)\\n\\n    >>> spacetime.print_separation_geodesic_acceleration(0)\\n\\nUsing with Jupyter Notebook (In development)\\n===========================\\nJupyter notebook has become very popular tool for python development in recent years.  It is great for science and research and this api is no exception. In order to use with Jupyter notebook the only thing to consider is the printing system. \\n\\nBuy me a cold brew\\n==================\\n[![Donate](https://img.shields.io/badge/Donate-PayPal-green.svg)](https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=NL2XB2BMMGT6G&currency_code=USD&source=url)\\n'},\n",
       " {'repo': 'FrancescoMancarelli/SpaceSHMUP',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': \"# SpaceSHMUP\\ngibson's Space Shooter in Unity\\n\"},\n",
       " {'repo': 'evesgf/SpaceGame', 'language': 'C#', 'readme_contents': ''},\n",
       " {'repo': 'openwisp/openwisp-ipam',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '=============\\nopenwisp-ipam\\n=============\\n\\n.. image:: https://github.com/openwisp/openwisp-ipam/actions/workflows/ci.yml/badge.svg?branch=master\\n  :target: https://github.com/openwisp/openwisp-ipam/actions/workflows/ci.yml?query=workflow%3AOpenWISP+Ipam+CI+Build%22\\n  :alt: CI Build status\\n\\n.. image:: https://coveralls.io/repos/openwisp/openwisp-ipam/badge.svg\\n  :target: https://coveralls.io/r/openwisp/openwisp-ipam\\n  :alt: Coverage\\n\\n.. image:: https://img.shields.io/pypi/v/openwisp-ipam\\n  :target: https://pypi.org/project/openwisp-ipam\\n  :alt: PyPI\\n\\n.. image:: https://img.shields.io/librariesio/release/github/openwisp/openwisp-ipam\\n  :target: https://libraries.io/github/openwisp/openwisp-ipam#dependencies\\n  :alt: Dependency monitoring\\n\\n.. image:: https://github.com/openwisp/openwisp-ipam/raw/docs/docs/subnet_demo.gif\\n  :alt: Feature Highlights\\n\\n.. contents:: **Table of Contents**:\\n   :backlinks: none\\n   :depth: 2\\n\\nAvailable Features\\n******************\\n\\n* IPv4 and IPv6 IP address management\\n* IPv4 and IPv6 Subnet management\\n* Automatic free space display for all subnets\\n* Visual display for a specific subnet\\n* IP request module\\n* REST API for CRUD operations and main features\\n* Possibility to search for an IP or subnet\\n* CSV Import and Export of subnets and their IPs\\n\\nProject Goals\\n*************\\n\\n* provide basic features of IP Address management as a reusable django app\\n* integrate this module in the rest of the OpenWISP ecosystem\\n* allow standalone usage (without the rest of OpenWISP)\\n* provide ways to extended the core features in order to create derivatives\\n\\nDependencies\\n************\\n\\n* Python 3.7 or higher\\n* Django 3.0 or higher\\n* Django REST Framework (for the REST API)\\n* openwisp-users\\n* swapper\\n\\nInstall stable version\\n**********************\\n\\n.. code-block:: shell\\n\\n    pip install openwisp-ipam\\n\\nInstall development version\\n***************************\\n\\nInstall tarball:\\n\\n.. code-block:: shell\\n\\n    pip install https://github.com/openwisp/openwisp-ipam/tarball/master\\n\\nAlternatively you can install via pip using git:\\n\\n.. code-block:: shell\\n\\n    pip install -e git+git://github.com/openwisp/openwisp-ipam#egg=openwisp-ipam\\n\\nInstallation for development\\n****************************\\n\\nInstall ``openwisp-ipam`` for development using following commands:\\n\\n.. code-block:: shell\\n\\n    git clone https://github.com/openwisp/openwisp-ipam.git\\n    cd openwisp-ipam\\n    pip install -e .\\n    pip install -r requirements-test.txt\\n\\nLaunch the development sever:\\n\\n.. code-block:: shell\\n\\n    cd tests/\\n    ./manage.py migrate\\n    ./manage.py createsuperuser\\n    ./manage.py runserver\\n\\nYou can access the admin interface at `http://127.0.0.1:8000/admin/`.\\n\\nRun Tests\\n=========\\n\\nInstall test requirements:\\n\\n.. code-block:: shell\\n\\n    pip install -r requirements-test.txt\\n\\nThen run the test suite:\\n\\n.. code-block:: shell\\n\\n    # options \"--keepdb\" & \"--parallel\" are optional but\\n    # improve time required for running tests.\\n    ./runtests.py --keepdb --parallel\\n    # Run tests for the sample_app\\n    SAMPLE_APP=1 ./runtests.py --keepdb --parallel\\n\\nVisual Display of subnets\\n*************************\\n\\nopenwisp-ipam provides a graphical representation of a subnet which shows the available free space under any subnet.\\n\\n.. image:: https://raw.githubusercontent.com/openwisp/openwisp-ipam/docs/docs/visual-display.png\\n\\nREST API\\n********\\n\\nLive documentation\\n==================\\n\\n.. image:: https://github.com/openwisp/openwisp-ipam/raw/docs/docs/api-docs.png\\n\\nA general live API documentation (following the OpenAPI specification) is available at ``/api/v1/docs/``.\\n\\nBrowsable web interface\\n=======================\\n\\n.. image:: https://github.com/openwisp/openwisp-ipam/raw/docs/docs/api-ui.png\\n\\nAdditionally, opening any of the endpoints `listed below <#list-of-endpoints>`_\\ndirectly in the browser will show the `browsable API interface of Django-REST-Framework\\n<https://www.django-rest-framework.org/topics/browsable-api/>`_,\\nwhich makes it even easier to find out the details of each endpoint.\\n\\nAuthentication\\n==============\\n\\nSee openwisp-users: `authenticating with the user token\\n<https://github.com/openwisp/openwisp-users#authenticating-with-the-user-token>`_.\\n\\nWhen browsing the API via the `Live documentation <#live-documentation>`_\\nor the `Browsable web page <#browsable-web-interface>`_, you can also use\\nthe session authentication by logging in the django admin.\\n\\nPagination\\n==========\\n\\nAll *list* endpoints support the ``page_size`` parameter that allows paginating\\nthe results in conjunction with the ``page`` parameter.\\n\\n.. code-block:: text\\n\\n    GET /api/v1/<api endpoint url>/?page_size=10\\n    GET /api/v1/<api endpoint url>/?page_size=10&page=2\\n\\nList of endpoints\\n=================\\n\\nSince the detailed explanation is contained in the `Live documentation <#live-documentation>`_\\nand in the `Browsable web page <#browsable-web-interface>`_ of each endpoint,\\nhere we\\'ll provide just a list of the available endpoints,\\nfor further information please open the URL of the endpoint in your browser.\\n\\nAPI Throttling\\n==============\\n\\nTo override the default API throttling settings, add the following to your ``settings.py`` file:\\n\\n.. code-block:: python\\n\\n    REST_FRAMEWORK = {\\n        \\'DEFAULT_THROTTLE_RATES\\': {\\n            \\'ipam\\': \\'100/hour\\',\\n        }\\n    }\\n\\nThe rate descriptions used in ``DEFAULT_THROTTLE_RATES`` may include\\n``second``, ``minute``, ``hour`` or ``day`` as the throttle period.\\n\\nGet Next Available IP\\n=====================\\n\\nA model method to fetch the next available IP address under a specific subnet. This method can also be accessed via a REST API: `openwisp_ipam/base/models.py <https://github.com/openwisp/openwisp-ipam/tree/master/tests/openwisp2/openwisp_ipam/base/models.py#L80>`_\\n\\nGET\\n---\\n\\nReturns the next available IP address under a subnet.\\n\\n.. code-block:: text\\n\\n    /api/v1/ipam/subnet/<subnet_id>/get-next-available-ip/\\n\\nRequest IP\\n^^^^^^^^^^\\n\\nA model method to create and fetch the next available IP address record under a subnet.\\n\\nPOST\\n----\\n\\nCreates a record for next available IP address and returns JSON data of that record.\\n\\n.. code-block:: text\\n\\n    POST /api/v1/ipam/subnet/<subnet_id>/request-ip/\\n\\n===========    ========================================\\nParam          Description\\n===========    ========================================\\ndescription    Optional description for the IP address\\n===========    ========================================\\n\\nResponse\\n^^^^^^^^\\n\\n.. code-block:: json\\n\\n\\n    {\\n        \"ip_address\": \"ip_address\",\\n        \"subnet\": \"subnet_uuid\",\\n        \"description\": \"optional description\"\\n    }\\n\\n\\nIpAddress-Subnet List and Create View\\n=====================================\\n\\nAn api endpoint to retrieve or create IP addresses under a specific subnet.\\n\\nGET\\n---\\n\\nReturns the list of IP addresses under a particular subnet.\\n\\n.. code-block:: text\\n\\n    /api/v1/ipam/subnet/<subnet_id>/ip-address/\\n\\nPOST\\n----\\n\\nCreate a new ``IP Address``.\\n\\n.. code-block:: text\\n\\n    /api/v1/ipam/subnet/<subnet_id>/ip-address/\\n\\n===========    ========================================\\nParam          Description\\n===========    ========================================\\nip_address     IPv6/IPv4 address value\\nsubnet         Subnet UUID\\ndescription    Optional description for the IP address\\n===========    ========================================\\n\\nSubnet List/Create View\\n=======================\\n\\nAn api endpoint to create or retrieve the list of subnet instances.\\n\\nGET\\n---\\n\\nReturns the list of ``Subnet`` instances.\\n\\n.. code-block:: text\\n\\n    /api/v1/ipam/subnet/\\n\\nPOST\\n----\\n\\nCreate a new ``Subnet``.\\n\\n.. code-block:: text\\n\\n    /api/v1/ipam/subnet/\\n\\n=============    ========================================\\nParam            Description\\n=============    ========================================\\nsubnet           Subnet value in CIDR format\\nmaster_subnet    Master Subnet UUID\\ndescription      Optional description for the IP address\\n=============    ========================================\\n\\nSubnet View\\n===========\\n\\nAn api endpoint for retrieving, updating or deleting a subnet instance.\\n\\nGET\\n---\\n\\nGet details of a ``Subnet`` instance\\n\\n.. code-block:: text\\n\\n    /api/v1/ipam/subnet/<subnet-id>/\\n\\nDELETE\\n------\\n\\nDelete a ``Subnet`` instance\\n\\n.. code-block:: text\\n\\n    /api/v1/ipam/subnet/<subnet-id>/\\n\\nPUT\\n---\\n\\nUpdate details of a ``Subnet`` instance.\\n\\n.. code-block:: text\\n\\n    /api/v1/ipam/subnet/<subnet-id>/\\n\\n=============    ========================================\\nParam            Description\\n=============    ========================================\\nsubnet           Subnet value in CIDR format\\nmaster_subnet    Master Subnet UUID\\ndescription      Optional description for the IP address\\n=============    ========================================\\n\\nIP Address View\\n===============\\n\\nAn api endpoint for retrieving, updating or deleting a IP address instance.\\n\\nGET\\n---\\n\\nGet details of an ``IP address`` instance.\\n\\n.. code-block:: text\\n\\n    /api/v1/ipam/ip-address/<ip_address-id>/\\n\\nDELETE\\n------\\n\\nDelete an ``IP address`` instance.\\n\\n.. code-block:: text\\n\\n    /api/v1/ipam/ip-address/<ip_address-id>/\\n\\nPUT\\n---\\n\\nUpdate details of an ``IP address`` instance.\\n\\n.. code-block:: text\\n\\n    /api/v1/ipam/ip-address/<ip_address-id>/\\n\\n===========    ========================================\\nParam          Description\\n===========    ========================================\\nip_address     IPv6/IPv4 value\\nsubnet         Subnet UUID\\ndescription    Optional description for the IP address\\n===========    ========================================\\n\\nExport Subnet View\\n==================\\n\\nView to export subnet data.\\n\\nPOST\\n----\\n\\n.. code-block:: text\\n\\n    /api/v1/ipam/subnet/<subnet-id>/export/\\n\\nImport Subnet View\\n==================\\n\\nView to import subnet data.\\n\\nPOST\\n----\\n\\n.. code-block:: text\\n\\n    /api/v1/ipam/import-subnet/\\n\\n\\nExporting and Importing Subnet\\n==============================\\n\\nOne can easily import and export `Subnet` data and it\\'s Ip Addresses using `openwisp-ipam`.\\nThis works for both IPv4 and IPv6 types of networks.\\n\\nExporting\\n---------\\n\\nData can be exported via the admin interface or by using a management command. The exported data is in `.csv` file format.\\n\\nFrom management command\\n^^^^^^^^^^^^^^^^^^^^^^^\\n\\n.. code-block:: shell\\n\\n    ./manage.py export_subnet <subnet value>\\n\\nThis would export the subnet if it exists on the database.\\n\\nFrom admin interface\\n^^^^^^^^^^^^^^^^^^^^\\n\\nData can be exported from the admin interface by just clicking on the export button on the subnet\\'s admin change view.\\n\\n.. image:: https://raw.githubusercontent.com/openwisp/openwisp-ipam/docs/docs/export.png\\n\\nImporting\\n---------\\n\\nData can be imported via the admin interface or by using a management command.\\nThe imported data file can be in `.csv` and `.xlsx` format. While importing\\ndata for ip addresses, the system checks if the subnet specified in the import file exists or not.\\nIf the subnet does not exists it will be created while importing data.\\n\\nFrom management command\\n^^^^^^^^^^^^^^^^^^^^^^^\\n\\n.. code-block:: shell\\n\\n    ./manage.py import_subnet --file=<file path>\\n\\nFrom admin interface\\n^^^^^^^^^^^^^^^^^^^^\\n\\nData can be imported from the admin interface by just clicking on the import button on the subnet view.\\n\\n.. image:: https://raw.githubusercontent.com/openwisp/openwisp-ipam/docs/docs/import.png\\n\\nCSV file format\\n===============\\n\\nFollow the following structure while creating `csv` file to import data.\\n\\n.. code-block:: text\\n\\n    Subnet Name\\n    Subnet Value\\n    Organization Slug\\n\\n    ip_address,description\\n    <ip-address>,<optional-description>\\n    <ip-address>,<optional-description>\\n    <ip-address>,<optional-description>\\n\\nSetup (integrate in an existing Django project)\\n***********************************************\\n\\nThe ``settings.py`` of your project should contain the following:\\n\\n.. code-block:: python\\n\\n    INSTALLED_APPS = [\\n        # openwisp2 modules\\n        \\'openwisp_users\\',\\n        \\'openwisp_ipam\\',\\n        # admin\\n        \\'admin_auto_filters\\',\\n        \\'django.contrib.admin\\',\\n        # rest framework\\n        \\'rest_framework\\',\\n        \\'drf_yasg\\',\\n    ]\\n\\n    AUTH_USER_MODEL = \\'openwisp_users.User\\'\\n\\nAdd the URLs to your main ``urls.py``:\\n\\n.. code-block:: python\\n\\n    from django.contrib import admin\\n    from django.urls import include, path\\n    from openwisp_users.api.urls import get_api_urls as get_users_api_urls\\n\\n    urlpatterns = [\\n        # admin URLs\\n        path(\\'admin/\\', admin.site.urls),\\n        # IPAM API\\n        path(\\'\\', include(\\'openwisp_ipam.urls\\')),\\n        # OpenAPI docs\\n        path(\\'api/v1/\\', include(\\'openwisp_utils.api.urls\\')),\\n        # Bearer Authentication API URL\\n        path(\\'api/v1/\\', include((get_users_api_urls(), \\'users\\'), namespace=\\'users\\')),\\n    ]\\n\\n\\nThen run:\\n\\n.. code-block:: shell\\n\\n    ./manage.py migrate\\n\\nExtending openwisp-ipam\\n***********************\\n\\nOne of the core values of the OpenWISP project is `Software Reusability <http://openwisp.io/docs/general/values.html#software-reusability-means-long-term-sustainability>`_,\\nfor this reason *openwisp-ipam* provides a set of base classes\\nwhich can be imported, extended and reused to create derivative apps.\\n\\nIn order to implement your custom version of *openwisp-ipam*,\\nyou need to perform the steps described in this section.\\n\\nWhen in doubt, the code in the `test project <https://github.com/openwisp/openwisp-ipam/tree/master/tests/openwisp2/>`_ and\\nthe `sample app <https://github.com/openwisp/openwisp-ipam/tree/master/tests/openwisp2/sample_ipam/>`_\\nwill serve you as source of truth:\\njust replicate and adapt that code to get a basic derivative of\\n*openwisp-ipam* working.\\n\\nIf you want to add new users fields, please follow the `tutorial to extend the\\nopenwisp-users <https://github.com/openwisp/openwisp-users/#extend-openwisp-users>`_.\\nAs an example, we have extended *openwisp-users* to *sample_users* app and\\nadded a field ``social_security_number`` in the `sample_users/models.py\\n<https://github.com/openwisp/openwisp-ipam/blob/master/tests/openwisp2/sample_users/models.py>`_.\\n\\n**Premise**: if you plan on using a customized version of this module,\\nwe suggest to start with it since the beginning, because migrating your data\\nfrom the default module to your extended version may be time consuming.\\n\\n1. Initialize your custom module\\n================================\\n\\nThe first thing you need to do is to create a new django app which will\\ncontain your custom version of *openwisp-ipam*.\\n\\nA django app is nothing more than a\\n`python package <https://docs.python.org/3/tutorial/modules.html#packages>`_\\n(a directory of python scripts), in the following examples we\\'ll call this django app\\n``myipam``, but you can name it how you want::\\n\\n    django-admin startapp myipam\\n\\nKeep in mind that the command mentioned above must be called from a directory\\nwhich is available in your `PYTHON_PATH <https://docs.python.org/3/using/cmdline.html#envvar-PYTHONPATH>`_\\nso that you can then import the result into your project.\\n\\nNow you need to add ``myipam`` to ``INSTALLED_APPS`` in your ``settings.py``,\\nensuring also that ``openwisp_ipam`` has been removed:\\n\\n.. code-block:: python\\n\\n    INSTALLED_APPS = [\\n        # ... other apps ...\\n        \\'openwisp_utils.admin_theme\\',\\n        # all-auth\\n        \\'django.contrib.sites\\',\\n        \\'allauth\\',\\n        \\'allauth.account\\',\\n        \\'allauth.socialaccount\\',\\n        # openwisp2 modules\\n        \\'openwisp_users\\',\\n        # \\'myipam\\',   <-- replace without your app-name here\\n        # admin\\n        \\'admin_auto_filters\\',\\n        \\'django.contrib.admin\\',\\n        # rest framework\\n        \\'rest_framework\\',\\n        # Other dependencies\\n        \\'reversion\\',\\n    ]\\n\\nFor more information about how to work with django projects and django apps,\\nplease refer to the `django documentation <https://docs.djangoproject.com/en/dev/intro/tutorial01/>`_.\\n\\n2. Install ``openwisp-ipam``\\n============================\\n\\nInstall (and add to the requirement of your project) openwisp-ipam::\\n\\n    pip install openwisp-ipam\\n\\n3. Add ``EXTENDED_APPS``\\n========================\\n\\nAdd the following to your ``settings.py``:\\n\\n.. code-block:: python\\n\\n    EXTENDED_APPS = (\\'openwisp_ipam\\',)\\n\\n4. Add ``openwisp_utils.staticfiles.DependencyFinder``\\n======================================================\\n\\nAdd ``openwisp_utils.staticfiles.DependencyFinder`` to\\n``STATICFILES_FINDERS`` in your ``settings.py``:\\n\\n.. code-block:: python\\n\\n    STATICFILES_FINDERS = [\\n        \\'django.contrib.staticfiles.finders.FileSystemFinder\\',\\n        \\'django.contrib.staticfiles.finders.AppDirectoriesFinder\\',\\n        \\'openwisp_utils.staticfiles.DependencyFinder\\',\\n    ]\\n\\n5. Add ``openwisp_utils.loaders.DependencyLoader``\\n==================================================\\n\\nAdd ``openwisp_utils.loaders.DependencyLoader`` to ``TEMPLATES``\\nin your ``settings.py``, but ensure it comes before\\n``django.template.loaders.app_directories.Loader``:\\n\\n.. code-block:: python\\n\\n    TEMPLATES = [\\n        {\\n            \\'BACKEND\\': \\'django.template.backends.django.DjangoTemplates\\',\\n            \\'OPTIONS\\': {\\n                \\'loaders\\': [\\n                    \\'django.template.loaders.filesystem.Loader\\',\\n                    \\'openwisp_utils.loaders.DependencyLoader\\',\\n                    \\'django.template.loaders.app_directories.Loader\\',\\n                ],\\n                \\'context_processors\\': [\\n                    \\'django.template.context_processors.debug\\',\\n                    \\'django.template.context_processors.request\\',\\n                    \\'django.contrib.auth.context_processors.auth\\',\\n                    \\'django.contrib.messages.context_processors.messages\\',\\n                ],\\n            },\\n        }\\n    ]\\n\\n6. Inherit the AppConfig class\\n==============================\\n\\nPlease refer to the following files in the sample app of the test project:\\n\\n- `sample_ipam/__init__.py <https://github.com/openwisp/openwisp-ipam/tree/master/tests/openwisp2/sample_ipam/__init__.py>`_.\\n- `sample_ipam/apps.py <https://github.com/openwisp/openwisp-ipam/tree/master/tests/openwisp2/sample_ipam/apps.py>`_.\\n\\nYou have to replicate and adapt that code in your project.\\n\\nFor more information regarding the concept of ``AppConfig`` please refer to\\nthe `\"Applications\" section in the django documentation <https://docs.djangoproject.com/en/dev/ref/applications/>`_.\\n\\n7. Create your custom models\\n============================\\n\\nFor the purpose of showing an example, we added a simple \"details\" field to the\\n`models of the sample app in the test project <https://github.com/openwisp/openwisp-ipam/tree/master/tests/openwisp2/sample_ipam/models.py>`_.\\n\\nYou can add fields in a similar way in your ``models.py`` file.\\n\\n**Note**: for doubts regarding how to use, extend or develop models please refer to\\nthe `\"Models\" section in the django documentation <https://docs.djangoproject.com/en/dev/topics/db/models/>`_.\\n\\n8. Add swapper configurations\\n=============================\\n\\nOnce you have created the models, add the following to your ``settings.py``:\\n\\n.. code-block:: python\\n\\n    # Setting models for swapper module\\n    OPENWISP_IPAM_IPADDRESS_MODEL = \\'myipam.IpAddress\\'\\n    OPENWISP_IPAM_SUBNET_MODEL = \\'myipam.Subnet\\'\\n\\nSubstitute ``myipam`` with the name you chose in step 1.\\n\\n9. Create database migrations\\n=============================\\n\\nCreate and apply database migrations::\\n\\n    ./manage.py makemigrations\\n    ./manage.py migrate\\n\\nFor more information, refer to the\\n`\"Migrations\" section in the django documentation <https://docs.djangoproject.com/en/dev/topics/migrations/>`_.\\n\\n\\n10. Create the admin\\n====================\\n\\nRefer to the `admin.py file of the sample app <https://github.com/openwisp/openwisp-ipam/tree/master/tests/openwisp2/sample_ipam/admin.py>`_.\\n\\nTo introduce changes to the admin, you can do it in two main ways which are described below.\\n\\n**Note**: for more information regarding how the django admin works, or how it can be customized,\\nplease refer to `\"The django admin site\" section in the django documentation <https://docs.djangoproject.com/en/dev/ref/contrib/admin/>`_.\\n\\n1. Monkey patching\\n------------------\\n\\nIf the changes you need to add are relatively small, you can resort to monkey patching.\\n\\nFor example:\\n\\n.. code-block:: python\\n\\n    from openwisp_ipam.admin import IpAddressAdmin, SubnetAdmin\\n\\n    SubnetAdmin.app_label = \\'sample_ipam\\'\\n\\n\\n2. Inheriting admin classes\\n---------------------------\\n\\nIf you need to introduce significant changes and/or you don\\'t want to resort to\\nmonkey patching, you can proceed as follows:\\n\\n.. code-block:: python\\n\\n    from django.contrib import admin\\n    from openwisp_ipam.admin import (\\n        IpAddressAdmin as BaseIpAddressAdmin,\\n        SubnetAdmin as BaseSubnetAdmin,\\n    )\\n    from swapper import load_model\\n\\n    IpAddress = load_model(\\'openwisp_ipam\\', \\'IpAddress\\')\\n    Subnet = load_model(\\'openwisp_ipam\\', \\'Subnet\\')\\n\\n    admin.site.unregister(IpAddress)\\n    admin.site.unregister(Subnet)\\n\\n    @admin.register(IpAddress)\\n    class IpAddressAdmin(BaseIpAddressAdmin):\\n        # add your changes here\\n\\n    @admin.register(Subnet)\\n    class SubnetAdmin(BaseSubnetAdmin):\\n        app_label = \\'myipam\\'\\n        # add your changes here\\n\\nSubstitute ``myipam`` with the name you chose in step 1.\\n\\n11. Create root URL configuration\\n=================================\\n\\n.. code-block:: python\\n\\n    from .sample_ipam import views as api_views\\n    from openwisp_ipam.urls import get_urls\\n\\n    urlpatterns = [\\n        # ... other urls in your project ...\\n        # openwisp-ipam urls\\n        # path(\\'\\', include(get_urls(api_views))) <-- Use only when changing API views (dicussed below)\\n        path(\\'\\', include(\\'openwisp_ipam.urls\\')),\\n    ]\\n\\nFor more information about URL configuration in django, please refer to the\\n`\"URL dispatcher\" section in the django documentation <https://docs.djangoproject.com/en/dev/topics/http/urls/>`_.\\n\\n12. Import the automated tests\\n==============================\\n\\nWhen developing a custom application based on this module, it\\'s a good\\nidea to import and run the base tests too, so that you can be sure the changes\\nyou\\'re introducing are not breaking some of the existing features of *openwisp-ipam*.\\n\\nIn case you need to add breaking changes, you can overwrite the tests defined\\nin the base classes to test your own behavior.\\n\\nSee the `tests of the sample app <https://github.com/openwisp/openwisp-ipam/tree/master/tests/openwisp2/sample_ipam/tests.py>`_\\nto find out how to do this.\\n\\nYou can then run tests with::\\n\\n    # the --parallel flag is optional\\n    ./manage.py test --parallel myipam\\n\\nSubstitute ``myipam`` with the name you chose in step 1.\\n\\nFor more information about automated tests in django, please refer to\\n`\"Testing in Django\" <https://docs.djangoproject.com/en/dev/topics/testing/>`_.\\n\\nOther base classes that can be inherited and extended\\n=====================================================\\n\\nThe following steps are not required and are intended for more advanced customization.\\n\\n1. Extending the API Views\\n--------------------------\\n\\nThe API view classes can be extended into other django applications as well. Note\\nthat it is not required for extending openwisp-ipam to your app and this change\\nis required only if you plan to make changes to the API views.\\n\\nCreate a view file as done in `views.py <https://github.com/openwisp/openwisp-ipam/tree/master/tests/openwisp2/sample_ipam/views.py>`_.\\n\\nFor more information about django views, please refer to the `views section in the django documentation <https://docs.djangoproject.com/en/dev/topics/http/views/>`_.\\n\\nContributing\\n************\\n\\nPlease refer to the `OpenWISP contributing guidelines <http://openwisp.io/docs/developer/contributing.html>`_.\\n\\n`Support channels <http://openwisp.org/support.html>`_ |\\n`Issue Tracker <https://github.com/openwisp/openwisp-ipam/issues>`_ |\\n`License <https://github.com/openwisp/openwisp-ipam/blob/master/LICENSE>`_\\n'},\n",
       " {'repo': 'SafeBreach-Labs/spacebin',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Spacebin\\n\\nSpacebin is a proof-of-concept malware that exfiltrates data (from No Direct Internet Access environments) via triggering AV on the endpoint and then communicating back from the AV\\'s cloud component.\\n\\nIt was released as part of the [THE ADVENTURES OF AV AND THE LEAKY SANDBOX](https://www.blackhat.com/us-17/briefings.html#the-adventures-of-av-and-the-leaky-sandbox) talk given at BlackHat USA 2017 conference and DEF CON 25 by Itzik Kotler and Amit Klein from [SafeBreach Labs](http://www.safebreach.com).\\n\\nSlides are availble [here](https://www.blackhat.com/docs/us-17/thursday/us-17-Kotler-The-Adventures-Of-Av-And-The-Leaky-Sandbox.pdf) and White paper is avialble [here](https://www.blackhat.com/docs/us-17/thursday/us-17-Kotler-The-Adventures-Of-Av-And-The-Leaky-Sandbox-wp.pdf)\\n\\n### Version\\n0.1.0\\n\\n### What\\'s Inside?\\n1. bingroundctrl is a directory with the server-side code.\\n\\n2. binrocket is a directory with client-side code. It\\'s the Python script that generates a rocket (i.e. C file that packs the binary satellite)\\n\\n3. binsatellite is a directory with more client-side code (i.e. Visual Studio 2015 Solution). It\\'s the actual binary satellite.\\n\\n### Instructions\\n\\n1. There\\'s a batch file called go.bat that does pretty much everything on the client side aspect. It takes optional command line argument (i.e. go.bat \"Secret Data\") that will be the payload. If not, \"Hello, world\" is the default.\\n\\n2. The go.bat assumes two things: That you\\'re running it from \"Developer Command Prompt\" (i.e. CL is in your PATH) and that you\\'re running it from the spacebin root directory. The latter is important because it uses relative-path to \"reference\" binsatellite Release binary.\\n\\n3. The results are rendered in a Web UI that is hosted on: http://YOUR_SITE:8080 the username is YOUR_USERNAME and the password is YOUR_PASSWORD the code this website is inside bingroundctrl and it\\'s a mixture of: tailon (Python app), nginx, tail, grep etc.\\n\\n4. To test that everything works as expected:\\n\\nopen the URL, login to the app. Afterward run go.bat with a string (i.e. go.bat \"Secret Secret\") and see that it appears on the Web UI.\\n\\n\\nLicense\\n----\\n\\nBSD 3-Clause\\n\\n\\n\\n'},\n",
       " {'repo': 'klb3713/sentence2vec',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"sentence2vec\\n============\\n\\nTools for mapping a sentence with arbitrary length to vector space\\n\\nWe provide an implementation of the Paragraph Vector in Quoc Le and Tomas Mikolov's paper: Distributed representations of Sentences and Documents.\\n\\nThis project is based on [gensim][1].\\n\\ninstall requires:\\n\\n - 'scipy >= 0.7.0'\\n - 'six >= 1.2.0'\\n\\n  [1]: https://github.com/piskvorky/gensim\\n\\n\\n2014-9-23 update: add test files for demo.\\n\"},\n",
       " {'repo': 'VinAIResearch/blur-kernel-space-exploring',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"# Exploring Image Deblurring via Encoded Blur Kernel Space\\n\\n## About the project\\n\\nWe introduce a method to encode the blur operators of an arbitrary dataset of sharp-blur image pairs into a blur kernel space. Assuming the encoded kernel space is close enough to in-the-wild blur operators, we propose an alternating optimization algorithm for blind image deblurring. It approximates an unseen blur operator by a kernel in the encoded space and searches for the corresponding sharp image. Due to the method's design, the encoded kernel space is fully differentiable, thus can be easily adopted in deep neural network models.\\n\\n![Blur kernel space](imgs/teaser.jpg)\\n\\nDetail of the method and experimental results can be found in [our following paper](https://arxiv.org/abs/2104.00317):\\n```\\n@inproceedings{m_Tran-etal-CVPR21, \\n\\u2003 author = {Phong Tran and Anh Tran and Quynh Phung and Minh Hoai}, \\n\\u2003 title = {Explore Image Deblurring via Encoded Blur Kernel Space}, \\n\\u2003 year = {2021}, \\n\\u2003 booktitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition (CVPR)} \\n}\\n```\\nPlease CITE our paper whenever this repository is used to help produce published results or incorporated into other software.\\n\\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1GDvbr4WQUibaEhQVzYPPObV4STn9NAot?usp=sharing)\\n\\n## Table of Content \\n\\n* [About the Project](#about-the-project)\\n* [Getting Started](#getting-started)\\n  * [Prerequisites](#prerequisites)\\n  * [Installation](#installation)\\n  * [Using the pretrained model](#Using-the-pretrained-model)\\n* [Training and evaluation](#Training-and-evaluation)\\n* [Model Zoo](#Model-zoo)\\n\\n## Getting started\\n\\n### Prerequisites\\n\\n* Python >= 3.7\\n* Pytorch >= 1.4.0\\n* CUDA >= 10.0\\n\\n### Installation\\n\\n``` sh\\ngit clone https://github.com/VinAIResearch/blur-kernel-space-exploring.git\\ncd blur-kernel-space-exploring\\n\\n\\nconda create -n BlurKernelSpace -y python=3.7\\nconda activate BlurKernelSpace\\nconda install --file requirements.txt\\n```\\n\\n## Training and evaluation\\n### Preparing datasets\\nYou can download the datasets in the [model zoo section](#model-zoo).\\n\\nTo use your customized dataset, your dataset must be organized as follow:\\n```\\nroot\\n├── blur_imgs\\n    ├── 000\\n    ├──── 00000000.png\\n    ├──── 00000001.png\\n    ├──── ...\\n    ├── 001\\n    ├──── 00000000.png\\n    ├──── 00000001.png\\n    ├──── ...\\n├── sharp_imgs\\n    ├── 000\\n    ├──── 00000000.png\\n    ├──── 00000001.png\\n    ├──── ...\\n    ├── 001\\n    ├──── 00000000.png\\n    ├──── 00000001.png\\n    ├──── ...\\n```\\nwhere `root`, `blur_imgs`, and `sharp_imgs` folders can have arbitrary names. For example, let `root, blur_imgs, sharp_imgs` be `REDS, train_blur, train_sharp` respectively (That is, you are using the REDS training set), then use the following scripts to create the lmdb dataset:\\n```sh\\npython create_lmdb.py --H 720 --W 1280 --C 3 --img_folder REDS/train_sharp --name train_sharp_wval --save_path ../datasets/REDS/train_sharp_wval.lmdb\\npython create_lmdb.py --H 720 --W 1280 --C 3 --img_folder REDS/train_blur --name train_blur_wval --save_path ../datasets/REDS/train_blur_wval.lmdb\\n```\\nwhere `(H, C, W)` is the shape of the images (note that all images in the dataset must have the same shape), `img_folder` is the folder that contains the images, `name` is the name of the dataset, and `save_path` is the save destination (`save_path` must end with `.lmdb`).\\n\\nWhen the script is finished, two folders `train_sharp_wval.lmdb` and `train_blur_wval.lmdb` will be created in `./REDS`.\\n\\n\\n### Training\\nTo do image deblurring, data augmentation, and blur generation, you first need to train the blur encoding network (The F function in the paper). This is the only network that you need to train. After creating the dataset, change the value of `dataroot_HQ` and `dataroot_LQ` in `options/kernel_encoding/REDS/woVAE.yml` to the paths of the sharp and blur lmdb datasets that were created before, then use the following script to train the model:\\n```\\npython train.py -opt options/kernel_encoding/REDS/woVAE.yml\\n```\\n\\nwhere `opt` is the path to yaml file that contains training configurations. You can find some default configurations in the `options` folder. Checkpoints, training states, and logs will be saved in `experiments/modelName`. You can change the configurations (learning rate, hyper-parameters, network structure, etc) in the yaml file.\\n\\n### Testing\\n#### Data augmentation\\nTo augment a given dataset, first, create an lmdb dataset using `scripts/create_lmdb.py` as before. Then use the following script:\\n```\\npython data_augmentation.py --target_H=720 --target_W=1280 \\\\\\n\\t\\t\\t    --source_H=720 --source_W=1280\\\\\\n\\t\\t\\t    --augmented_H=256 --augmented_W=256\\\\\\n                            --source_LQ_root=datasets/REDS/train_blur_wval.lmdb \\\\\\n                            --source_HQ_root=datasets/REDS/train_sharp_wval.lmdb \\\\\\n\\t\\t\\t    --target_HQ_root=datasets/REDS/test_sharp_wval.lmdb \\\\\\n                            --save_path=results/GOPRO_augmented \\\\\\n                            --num_images=10 \\\\\\n                            --yml_path=options/data_augmentation/default.yml\\n```\\n`(target_H, target_W)`, `(source_H, source_W)`, and `(augmented_H, augmented_W)` are the desired shapes of the target images, source images, and augmented images respectively. `source_LQ_root`, `source_HQ_root`, and `target_HQ_root` are the paths of the lmdb datasets for the reference blur-sharp pairs and the input sharp images that were created before. `num_images` is the size of the augmented dataset. `model_path` is the path of the trained model. `yml_path` is the path to the model configuration file. Results will be saved in `save_path`.\\n\\n![Data augmentation examples](imgs/results/augmentation.jpg)\\n\\n#### Generate novel blur kernels\\nTo generate a blur image given a sharp image, use the following command:\\n```sh\\npython generate_blur.py --yml_path=options/generate_blur/default.yml \\\\\\n\\t\\t        --image_path=imgs/sharp_imgs/mushishi.png \\\\\\n\\t\\t\\t--num_samples=10\\n\\t\\t\\t--save_path=./res.png\\n```\\nwhere `model_path` is the path of the pre-trained model, `yml_path` is the path of the configuration file. `image_path` is the path of the sharp image. After running the script, a blur image corresponding to the sharp image will be saved in `save_path`. Here is some expected output:\\n![kernel generating examples](imgs/results/generate_blur.jpg)\\n**Note**: This only works with models that were trained with `--VAE` flag. The size of input images must be divisible by 128.\\n\\n#### Generic Deblurring\\nTo deblur a blurry image, use the following command:\\n```sh\\npython generic_deblur.py --image_path imgs/blur_imgs/blur1.png --yml_path options/generic_deblur/default.yml --save_path ./res.png\\n```\\nwhere `image_path` is the path of the blurry image. `yml_path` is the path of the configuration file. The deblurred image will be saved to `save_path`.\\n\\n![Image deblurring examples](imgs/results/general_deblurring.jpg)\\n\\n#### Deblurring using sharp image prior\\n[mapping]: https://drive.google.com/uc?id=14R6iHGf5iuVx3DMNsACAl7eBr7Vdpd0k\\n[synthesis]: https://drive.google.com/uc?id=1TCViX1YpQyRsklTVYEJwdbmK91vklCo8\\n[pretrained model]: https://drive.google.com/file/d/1PQutd-JboOCOZqmd95XWxWrO8gGEvRcO/view\\nFirst, you need to download the pre-trained styleGAN or styleGAN2 networks. If you want to use styleGAN, download the [mapping] and [synthesis] networks, then rename and copy them to `experiments/pretrained/stylegan_mapping.pt` and `experiments/pretrained/stylegan_synthesis.pt` respectively. If you want to use styleGAN2 instead, download the [pretrained model], then rename and copy it to `experiments/pretrained/stylegan2.pt`.\\n\\nTo deblur a blurry image using styleGAN latent space as the sharp image prior, you can use one of the following commands:\\n```sh\\npython domain_specific_deblur.py --input_dir imgs/blur_faces \\\\\\n\\t\\t    --output_dir experiments/domain_specific_deblur/results \\\\\\n\\t\\t    --yml_path options/domain_specific_deblur/stylegan.yml  # Use latent space of stylegan\\npython domain_specific_deblur.py --input_dir imgs/blur_faces \\\\\\n\\t\\t    --output_dir experiments/domain_specific_deblur/results \\\\\\n\\t\\t    --yml_path options/domain_specific_deblur/stylegan2.yml  # Use latent space of stylegan2\\n```\\nResults will be saved in `experiments/domain_specific_deblur/results`.\\n**Note**: Generally, the code still works with images that have the size divisible by 128. However, since our blur kernels are not uniform, the size of the kernel increases as the size of the image increases. \\n\\n![PULSE-like Deblurring examples](imgs/results/domain_specific_deblur.jpg)\\n\\n## Model Zoo\\nPretrained models and corresponding datasets are provided in the below table. After downloading the datasets and models, follow the instructions in the [testing section](#testing) to do data augmentation, generating blur images, or image deblurring.\\n\\n[REDS]: https://seungjunnah.github.io/Datasets/reds.html\\n[GOPRO]: https://seungjunnah.github.io/Datasets/gopro\\n\\n[REDS woVAE]: https://drive.google.com/file/d/12ZhjXWcYhAZjBnMtF0ai0R5PQydZct61/view?usp=sharing\\n[GOPRO woVAE]: https://drive.google.com/file/d/1WrVALP-woJgtiZyvQ7NOkaZssHbHwKYn/view?usp=sharing\\n[GOPRO wVAE]: https://drive.google.com/file/d/1QMUY8mxUMgEJty2Gk7UY0WYmyyYRY7vS/view?usp=sharing\\n[GOPRO + REDS woVAE]: https://drive.google.com/file/d/169R0hEs3rNeloj-m1rGS4YjW38pu-LFD/view?usp=sharing\\n\\n|Model name              | dataset(s)      | status                   |\\n|:-----------------------|:---------------:|-------------------------:|\\n|[REDS woVAE]            | [REDS]          | :heavy_check_mark:       |\\n|[GOPRO woVAE]           | [GOPRO]         | :heavy_check_mark:       |\\n|[GOPRO wVAE]            | [GOPRO]         | :heavy_check_mark:       |\\n|[GOPRO + REDS woVAE]    | [GOPRO], [REDS] | :heavy_check_mark:       |\\n\\n\\n## Notes and references\\nThe training code is borrowed from the EDVR project: https://github.com/xinntao/EDVR\\n\\nThe backbone code is borrowed from the DeblurGAN project: https://github.com/KupynOrest/DeblurGAN\\n\\nThe styleGAN code is borrowed from the PULSE project: https://github.com/adamian98/pulse\\n\\nThe stylegan2 code is borrowed from https://github.com/rosinality/stylegan2-pytorch\\n\"},\n",
       " {'repo': 'tmpvar/polygon.js',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': \"# polygon.js\\n\\n## Install\\n\\n\\n__nodejs__\\n\\n`npm install polygon`\\n\\n## Usage\\n\\nCreate a new polygon:\\n\\n```javascript\\nvar p = new Polygon([\\n  Vec2(0, 0),\\n  Vec2(10, 0),\\n  Vec2(0, 10)\\n]);\\n\\n```\\n\\nYou can pass an array of `Vec2`s, arrays `[x, y]`, or objects `{ x: 10, y: 20 }`\\n\\n\\n## Supported Methods\\n\\n* `each(function(prev, current, next, idx) {})`\\n* `point(idx)` - returns the point at index `idx`. note: this will wrap in both directions\\n* `dedupe(returnNew)` - ensure all of the points are unique\\n* `insert(vec2, index)` - insert `vec2` at the specified index\\n* `remove(vecOrIndex)` - remove the specified `vec2` or numeric index from this polygon\\n* `clean(returnNew)` - removes contiguous points that are the same\\n* `winding()` - returns the direction in which a polygon is wound (true === clockwise)\\n* `rewind(bool)` - rewinds the polygon in the specified direction (true === clockwise)\\n* `area()` - computes the area of the polygon\\n* `closestPointTo(vec2)` - finds the closest point in this polygon to `vec2`\\n* `center()` - returns a `Vec2` at the center of the AABB\\n* `scale(amount, origin, returnNew)` - scales this polygon around `origin` (default is `this.center()`) and will return a new polygon if requested with `returnNew`\\n* `containsPoint(vec2)` - returns true if `vec2` is inside the polygon\\n* `containsPolygon(poly)` - returns true if `poly` is completely contained in this polygon\\n* `aabb()` - returns an object `{x:_, y:_, w:_, h:_}` representing the axis-aligned bounding box of this polygyon\\n* `offset(amount)` - performs an offset/buffering operation on this polygon and returns a new one\\n* `line(index)` - return an array `[startpoint, endpoint]` representing the line at the specified `index`\\n* `lines(function(start, end, index) {})` - iterate over the lines in this polygon\\n* `selfIntersections` - find self-intersections and return them as a new polygon\\n* `pruneSelfIntersections` - remove self intersections from this polygon.  returns an array of polygons\\n* `length` - returns the number of points in this polygon\\n* `clone` - return a new instance of this polygon\\n* `rotate(rads, vec2, returnNew)` - rotate by origin `vec2` (default `this.center()`) by radians `rads` and return a clone if `returnNew` is specified\\n* `translate(vec2, returnNew)` - translate by `vec2` and return a clone if `returnNew` is specified\\n* `equal(poly)` - return true if this polygon has the same components and the incoming `poly`\\n* `contains(thing)` - works with an array of vec2's, an object containing a `.position` and `.radius`, an object populated with x1,y1,x2,y2, an object populated with x,y,w,h, and an object populated with x,y,width,height.  See the tests for more info\\n* `union(polygon)` returns a new polygon representing the boolean union of `this` and the incoming `polygon`\\n* `cut(polygon)` returns a new polygon representing the boolean cut of `polygon` from `this`\\n* `toArray()` convert this polygon into an array of arrays (`[[x, y]]`)\\n\\n## license\\n\\nMIT\\n\"},\n",
       " {'repo': 'ferram4/Kerbal-Joint-Reinforcement',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': 'Kerbal Joint Reinforcement, v3.3.3\\n==========================\\n\\nPhysics stabilizer plugin for Kerbal Space Program\\n\\nSource available at: https://github.com/ferram4/Kerbal-Joint-Reinforcement\\n\\n***************************************************\\n****** INSTALLING KERBAL JOINT REINFORCEMENT ******\\n***************************************************\\n\\nMerge the GameData folder with the existing one in your KSP directory.  KSP will then load it as an add-on.\\nThe source folder simply contains the source code (in C#) for the plugin.  If you didn\\'t already know what it was, you don\\'t need to worry about it; don\\'t copy it over.\\n\\n\\n********************************\\n****** EXCITING FEATURES! ******\\n********************************\\n\\n\\n\\n-- Physics Easing\\n\\n\\t- Slowly dials up external forces (gravity, centrifugal, coriolis) when on the surface of a planet, reducing the initial stress during loading\\n\\t- All parts and joints are strengthened heavily during physics loading (coming off of rails) to prevent Kraken attacks on ships\\n\\n-- Launch Clamp Easing\\n\\n\\t- Prevents launch clamps from shifting on load, which could destroy the vehicle on the pad\\n\\n-- Stiffen interstage connections\\n\\n\\t- Parts connected to a decoupler will be connected to each other, reducing flex at the connection to reasonable levels\\n\\n\\n-- Stiffen launch clamp connections\\n\\n\\t- Less vehicle movement on vessel initialization\\n\\t- Warning: may cause spontaneous rocket disintegration if rocket is too large and overconstrained (far too many lanuch clamps; their connections will fight each other and give rise to phantom forces)\\n\\n\\n-- Increase stiffness and strengths of connections\\n\\n\\t- Larger parts will have stiffer connections to balance their larger masses / sizes\\n\\t- Sequential parts in a stack will be connected with a stiff, but weak connection to add even more stiffness and counteract wobble\\n\\n-- Option to make connection strengths weaker to counteract increases in stiffness\\n\\n\\n-- Joint Stiffness parameters can be tweaked in included config.xml file\\n\\n\\t- config value documentation:\\n\\n\\nGeneral Values\\n\\n\\tType\\tName\\t\\t\\t\\t\\tDefault Value\\t\\tAction\\n\\n\\tbool\\treinforceAttachNodes\\t\\t\\t1\\t\\t\\t--Toggles stiffening of all vessel joints\\n\\tbool\\tmultiPartAttachNodeReinforcement\\t1\\t\\t\\t--Toggles additional stiffening by connecting parts in a stack one part further, but at a weaker strength\\n\\tbool\\treinforceDecouplersFurther\\t\\t1\\t\\t\\t--Toggles stiffening of interstage connections\\n\\tbool\\treinforceLaunchClampsFurther\\t\\t1\\t\\t\\t--Toggles stiffening of launch clamp connections\\n\\tbool\\tuseVolumeNotArea\\t\\t\\t1\\t\\t\\t--Switches to calculating connection area based on volume, not area; not technically correct, but allows a better approximation of very large rockets\\n\\tbool\\tdebug\\t\\t\\t\\t\\t0\\t\\t\\t--Toggles debug output to log; please activate and provide log if making a bug report\\n\\tfloat\\tmassForAdjustment\\t\\t\\t0.01\\t\\t\\t--Parts below this mass will not be stiffened\\n\\tfloat\\tstiffeningExtensionMassRatioThreshold\\t5\\t\\t\\t--Sets mass ratio needed between parts to extend Decoupler Stiffening one part further than it normally would have gone; essentially, if the code would have stopped at part A, but part B that it is connected to is >5 times as massive as part A, include part B\\n\\tfloat\\tdecouplerAndClampJointStrength\\t\\t-1\\t\\t\\t--Sets breaking strength for joints involved in decoupler and clamp additional strengthening; -1 makes them unbreakable\\n\\nAngular \"Drive\" Values (universally scales angular strength of connections)\\n\\n\\tType\\tName\\t\\t\\t\\tDefault Value\\t\\tAction\\n\\n\\tfloat\\tangularDriveSpring\\t\\t5e12\\t\\t\\t--Factor used to scale stiffness of angular connections\\n\\tfloat\\tangularDriveDamper\\t\\t25\\t\\t\\t--Factor used to scale damping of motion in angular connections\\n\\tfloat\\tangularMaxForceFactor\\t\\t-1\\t\\t\\t--Factor used to scale maximum force that can be applied before connection \"gives out\"; does not control joint strength; -1 makes this value infinite\\n\\nJoint Strength Values\\n\\n\\tType\\tName\\t\\t\\t\\tDefault Value\\t\\tAction\\n\\n\\tfloat\\tbreakForceMultiplier\\t\\t1\\t\\t\\t--Factor scales the failure strength (for forces) of joint connections; 1 gives stock strength\\n\\tfloat\\tbreakTorqueMultiplier\\t\\t1\\t\\t\\t--Factor scales the failure strength (for torque) of joint connections; 1 gives stock strength\\n\\tfloat\\tbreakStrengthPerArea\\t\\t1500\\t\\t\\t--Overrides above values if not equal to 1; joint strength is based on the area of the part and failure strength is equal to this value times connection area\\n\\tfloat\\tbreakTorquePerMOI\\t\\t6000\\t\\t\\t--Same as above value, but for torques rather than forces and is based on the moment of inertia, not area\\n\\nPart and Module Exemptions\\n\\n\\tType\\tName\\t\\t\\t\\tDefault Value\\t\\tAction\\n\\n\\tstring\\texemptPartType0\\t\\t\\tMuMechToggle\\t\\t--Part stiffening not applied to this type of \"Part\"; exemption to avoid interference with Infernal Robotics\\n\\tstring\\texemptPartType1\\t\\t\\tMuMechServo\\t\\t--Part stiffening not applied to this type of \"Part\"; exemption to avoid interference with Infernal Robotics\\n\\n\\tstring\\texemptModuleType0\\t\\tWingManipulator\\t\\t--Part stiffening not applied to parts with this type of PartModule; exemption to prevent problems with pWings\\n\\tstring\\texemptModuleType1\\t\\tSingleGroupMan\\t\\t--Part stiffening not applied to parts with this type of PartModule; exemption to prevent problems with procedural adapter included with pWings\\n\\tstring\\texemptModuleType2\\t\\tKerbalEVA\\t\\t--Part stiffening not applied to parts with this type of PartModule; exemption to prevent problems with Kerbals in command seats\\n\\tstring\\texemptModuleType3\\t\\tMuMechToggle\\t\\t--Part stiffening not applied to parts with this type of PartModule; exemption to prevent problems with Kerbals in command seats\\n\\tstring\\texemptModuleType4\\t\\tWingProcedural\\t\\t--Part stiffening not applied to parts with this type of PartModule; exemption to prevent problems with Kerbals in command seats\\n\\nFurther part and module exemptions can be added using the same formating and changing the number\\n\\n\\nDecoupler Stiffening Extension Types\\n\\n\\tType\\tName\\t\\t\\t\\t\\tDefault Value\\t\\tAction\\n\\n\\tstring\\tdecouplerStiffeningExtensionType0\\tModuleEngines\\t\\t--Decoupler stiffening will look for parts beyond this part type to add to stiffening\\n\\tstring\\tdecouplerStiffeningExtensionType1\\tModuleEnginesFX\\t\\t--Decoupler stiffening will look for parts beyond this part type to add to stiffening\\n\\tstring\\tdecouplerStiffeningExtensionType2\\tModuleHybridEngine\\t--Decoupler stiffening will look for parts beyond this part type to add to stiffening\\n\\tstring\\tdecouplerStiffeningExtensionType3\\tModuleHybridEngines\\t--Decoupler stiffening will look for parts beyond this part type to add to stiffening\\n\\tstring\\tdecouplerStiffeningExtensionType4\\tModuleEngineConfigs\\t--Decoupler stiffening will look for parts beyond this part type to add to stiffening\\n\\nThese types are currently not used, but removing the a in front of them will cause KJR to make use of them again; their lack should not affect stiffening appreciably but does help reduce overhead and strange stiffening situations\\n\\n\\tstring\\tadecouplerStiffeningExtensionType5\\tModuleDecouple\\t\\t--Decoupler stiffening will look for parts beyond this part type to add to stiffening\\n\\tstring\\tadecouplerStiffeningExtensionType6\\tModuleAnchoredDecoupler\\t--Decoupler stiffening will look for parts beyond this part type to add to stiffening\\n\\tstring\\tadecouplerStiffeningExtensionType7\\tProceduralFairingBase\\t--Decoupler stiffening will look for parts beyond this part type to add to stiffening\\n\\n\\n***********************\\n****** CHANGELOG ******\\n***********************\\nv3.3.3  \\n\\tFeatures  \\n\\t--Recompile against KSP 1.3, ensure CompatChecker compatibility with 1.3  \\n\\nv3.3.2  \\n\\tBugfixes  \\n\\t--Fix multijoints breaking IR joints and any other exempted parts from moving  \\n\\nv3.3.1  \\n\\tBugfixes  \\n\\t--Fix a critical bug involving unphysical forces applied to vessels on load / unload of other vessels and SOI switches  \\n\\nv3.3.0  \\n\\tFeatures  \\n\\t--Recompile to fix for KSP 1.2  \\n\\t--Update method of handling multi-part-joints to ensure compatibility with Konstruction mod  \\n\\t--Removal of old symmetry-based multi-part stabilization due to ineffectiveness in all situations to reduce overhead  \\n\\t--Implementation of new vessel-part-tree leaf-based stabilization for greater stability on space stations and other convoluted shapes  \\n\\nv3.2.0  \\n\\tFeatures  \\n\\t--Recompile to ensure KSP 1.1.3 compatibility  \\n\\t--Change multi-part-joint system to stabilize space stations and similar vehicles with very large masses connected by very flexy parts  \\n\\nv3.1.7  \\n\\tFeatures  \\n\\t--Recompile to ensure KSP 1.1.2 compatibility, especially within CompatibilityChecker utility  \\n\\nv3.1.6  \\n\\tFeatures  \\n\\t--Update to ensure KSP 1.1.1 compatibility  \\n\\t--Minor optimization in joint setups  \\n\\t--Remove B9 pWings from stiffening exemption, as it is unnecessary  \\n\\nv3.1.5  \\n\\tFeatures  \\n\\t--Updated to be compatible with KSP 1.1  \\n\\t--Very minor efficiency improvements in physics easing and stiffening of joints  \\n\\t--Fully exempt EVAs from all KJR effects  \\n\\t--Update config parameters to function with stock fixing of never-breakable joints bug  \\n\\nv3.1.4  \\n\\tMisc  \\n\\t--Fixed issue with .version file and compatible KSP versions\\n\\nv3.1.3  \\n\\tFeatures  \\n\\t--Updated compatibility for KSP 1.0\\n\\nv3.1.2  \\n\\tFeatures  \\n\\t--Added code to slightly stiffen connections between symmetrically-connected parts attached to a central part; should reduce some physics weirdness\\n\\n\\tBugFixes  \\n\\t--Fixed issue where undocking was impossible.\\n\\nv3.1.1  \\n\\tBugFixes  \\n\\t--Fixed a serious lock-to-worldspace issue involving multipart joints and physicsless parts\\n\\nv3.1  \\n\\tFeatures  \\n\\t--Set multipart joints to account for large mass ratios in choosing which parts to join  \\n\\t--Set Decoupler Stiffenning to require the connection of immediate decoupler children to stiffen things even further\\n\\n\\tBugFixes  \\n\\t--Fixed a decoupling issues with multipart joints  \\n\\t--Fixed multipart joint lock-to-worldspace issues  \\n\\t--Fixed some issues on loading very large, heavy parts\\n\\nv3.0.1  \\n\\tBugFixes  \\n\\t--Fix some issues involving multipart joints  \\n\\t--More null checking for situations that shouldn\\'t happen, but might  \\n\\nv3.0  \\n\\tFeatures  \\n\\t--MultiPart joints: weak, but stiff connections along a stack that will add even more stiffness without making the connection cheatingly strong  \\n\\t--Proper, guaranteed application of stiffened properties, regardless of stock joint parameters  \\n\\t--Updated default config values for greater sanity  \\n\\t--Refactoring of code for sanity\\n\\n\\tBugFixes\\n\\t--Longstanding issue with radially-attached parts that were larger than their parent are now fixed  \\n\\t--Many NREs from bad events or bad states now avoided\\n\\t\\nv2.4.5\\n\\tFeatures\\n\\t--KSP 0.90 compatibility\\n\\t--Include some extra checks to prevent errors from occurring\\n\\nv2.4.4\\n\\tFeatures\\n\\t--KSP 0.25 compatibility  \\n\\t--Update CompatibilityChecker  \\n\\t--Shutdown functionality if CompatibilityChecker returns warnings\\n\\nv2.4.3\\n\\tFeatures\\n\\t--KSP 0.24.2 compatibility\\n\\nv2.4.2\\n\\tFeatures\\n\\t--KSP 0.24.1 compatibility\\n\\t\\nv2.4.1\\n\\n\\tBugfixes:\\n\\t--Included JsonFx.dll, which is required by ModStats\\n\\t--Relabeled ModStatistics.dll to allow simple overwriting for ModStats updates\\n\\nv2.4\\n\\n\\tFeatures\\n\\t--KSP 0.24 compatibility\\n\\n\\tBugfixes\\n\\t--Fixed some interference with infernal robotics\\n\\nv2.3\\n\\n\\tFeatures\\n\\t--Updated attach node reinforcement to use properties closer to stock joint performance, but stiffer.\\n\\t--Decoupler and clamp stiffening increased in strength for use in Real Solar System\\n\\t--Removed unused config values\\n\\nv2.2\\n\\n\\tFeatures\\n\\t--Updated to function with KSP ARM Patch (KSP 0.23.5)\\n\\t--Removed inertia tensor fix, as it is now stock\\n\\t--Main stiffening / strengthening is now disabled by default due to stock joint improvements\\n\\t--Decoupler stiffening is now disabled by default due to stock joint improvements\\n\\n\\tBugfixes:\\n\\t--Vessels can no longer become permanently indestructible\\n\\nv2.1\\n\\n\\tFeatures\\n\\t--Reduced extent of decoupler stiffening joint creation; this should reduce physics overhead\\n\\t--Code refactoring for additional performance gains\\n\\t--Removed physics easing effect on inertia tensors; was unnecessary and added more overhead\\n\\t--Workaround for the stock \"Launch Clamps shift on the pad and overstress your ship\" bug that is particularly noticeable with RSS\\n\\t--Clamp connections are stiffer; now allowed by above workaround\\n\\n\\tBugfixes\\n\\t--KAS struts no longer break on load\\n\\n\\nv2.0\\n\\n\\tFeatures\\n\\t--Full release of proper inertia tensors!  Massive parts will feel more massive.\\n\\t--Full release of greater physics easing!  Landed and pre-launch crafts will have gravitational, centrifugal and coriolis forces slowly added to them, reducing the initial physics jerk tremendously\\n\\t--Launch clamps are now much stiffer when connected to more-massive-than-stock mod parts\\n\\t--Tightened up default joint settings more\\n\\t--Decoupler Stiffening Extension will now extend one part further if it\\'s final part is much less massive than its parent / child part\\n\\t--Added Majiir\\'s CompatibilityChecker; this will simply warn the user if they are not using a compatible version of KSP\\n\\n\\tBugfixes\\n\\t--Joints during physics easing strengthened\\n\\nv2.0x2\\n\\n\\tFeatures\\n\\t--Elaborated physics easing: joints\\' flexion range is initially great and decreases, and gravitational + rotating ref frame forces are cancelled out to resolve internal joint stresses ere loading the rocket\\n\\t--Greatly tightened default joint settings\\n\\n\\tBugfixes\\n\\t--Non-zero angular limits no longer wrongly reorient parts.\\n\\n\\nv2.0x1\\n\\n\\tFeatures\\n\\t--Fixed part inertia tensors: heavy, large objects should now \"feel\" more massive, and their connections should better behave. Thanks to a.g. for finding this one.\\n\\t--Slightly stiffened Launch Clamps\\n\\t--Removed v1.7\\'s improper stiffening for stretchy tanks, which the ability to stretch stretchy tanks makes unnecessary\\n\\n\\tBugfixes\\n\\t--Non-zero angular limits no longer wrongly reorient parts.\\n\\n\\nv1.7\\n\\n\\tFeatures\\n\\t--Connection area can be from volume instead of connection area calculated; for very, very large vehicles that the standard settings cannot handle\\n\\t--Default joint parameters stiffened\\n\\t--Stretchy tanks stiffened--a better solution is being developed while this one helps RSS\\n\\n\\tBugfixes\\n\\t--Decoupling no longer further stiffens joints being deleted from non-staged decouplers during decoupling / partial crashing\\n\\n\\nv1.6\\n\\n\\tFeatures\\n\\t--BreakStrengthPerUnitArea will not override large breakForces, easing I-beams and structural elements\\' use\\n\\n\\tBugfixes\\n\\t--Fixed decoupler-dockingport combination parts from causing strange disassembly when undocking\\n\\n\\nv1.5\\n\\n\\tFeatures\\n\\t--Updated to KSP 0.23\\n\\t--Joint breaking strength can be set to increase with connection area so that large part connections can have realistically large strength; on by default\\n\\t--Vessels are further strengthened for the first 30 physics frames after coming off rails or loading, reducing initialization jitters.\\n\\n\\tBugfixes:\\n\\t--Launch clamps after staging remain clamped to the ground.\\n\\t--Kraken no longer throws launchpads at orbiting craft\\n\\n\\nv1.4.2\\n\\n\\tBugfixes\\n\\t--Wobble reduced\\n\\t--General tweaks to reduce wobbling further\\n\\n\\nv1.4.1\\n\\n\\tBugfixes\\n\\t--Maximum joint forces correctly calculated\\n\\t--Docking no longer causes exceptions to be thrown and cause lag\\n\\n\\nv1.4\\n\\n\\tFeatures\\n\\t--Increased calculation of surface-attached connection area\\'s accuracy\\n\\n\\tBugfixes\\n\\t--Wobble between stack-attached parts of very different sizes greatly reduced\\n\\n\\nv1.3\\n\\n\\tFeatures\\n\\t--Better solution for failure to apply decoupler ejection forces\\n\\t--Will not stiffen parts below a given mass, which can be changed in config\\n\\t--Properly updates on docking\\n\\n\\tBugfixes\\n\\t--Launch clamps no longer to the surface lock ships\\n\\n\\nv1.2\\n\\n\\tFeatures\\n\\t--Workaround for stock KSP bug where struts would prevent decoupler ejection forces from being applied\\n\\n\\tTweaks\\n\\t--Reduced default maxForceFactors to be more reasonable levels\\n\\n\\tBugFixes\\n\\t--Struts properly disconnect\\n\\t--Decouplers properly function\\n\\n\\nv1.1\\n\\n\\tFeatures\\n\\t--Stiffness of joint no longer erroneously dependent on breakForce / breakTorque\\n\\t--Decoupler stiffening function made more comprehensive\\n\\n\\tBugFixes\\n\\t--Further decoupler stiffening affects radial decouplers\\n\\t--Decoupler further stiffening no longer causes Nulls to be thrown when attached to physics-disabled parts\\n\\t--Procedural fairings no longer locked to rockets\\n\\t--Infernal Robotics parts function\\n\\t--Temporary stopgap measure: stiffening not applied to pWings to prevent ultra-flexy wings\\n\\n\\tKnown Issues\\n\\t--Decouplers exert no detach force with extra decoupler stiffening enabled\\n\\tSame issues as strut attachment bug\\n\\n\\nv1.0\\n\\n\\tRelease'},\n",
       " {'repo': 'searxng/searx-space',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# searx-stats2\\n\\nStatistics on the [searx](https://searx.github.io/searx/) instances: https://searx.space ([Onion URL](http://searxspbitokayvkhzhsnljde7rqmn7rvoga6e4waeub3h7ug3nghoad.onion/))\\n\\n## Installation\\n\\n### Download and run cryptcheck-backend\\n\\nsearx-stats2 expects [cryptcheck-backend](https://github.com/dalf/cryptcheck-backend) to respond on localhost:7000:\\n\\n```sh\\ndocker run --rm -p 7000:7000 dalf/cryptcheck-backend:latest\\n```\\n\\nNote: cryptcheck-backend is used to get the TLS grade.\\n\\n### Install system packages\\n\\nInstall packages (for Ubuntu):\\n\\n```sh\\napt install firefox wget git build-essential python3-dev virtualenv python3-virtualenv libxslt-dev zlib1g-dev libffi-dev libssl-dev libyaml-dev python3-ldns python3-venv tor\\n```\\n\\nFor Debian, `firefox` should be replaced with `firefox-esr`.\\n\\n### Get the project\\n\\nInstall searxstats:\\n\\n```sh\\ncd /usr/local\\nsudo git clone https://github.com/searx/searx-stats2\\nsudo useradd searxstats -d /usr/local/searx-stats2\\nsudo chown searxstats:searxstats -R /usr/local/searx-stats2\\n```\\n\\n### Project install\\n\\n```sh\\nsudo -u searxstats -i\\ncd /usr/local/searx-stats2\\npython3 -m venv --system-site-packages ve\\n. ./ve/bin/activate\\npip install -r requirements.txt\\n./utils/install-geckodriver\\nmkdir cache\\nmkdir html/data\\ntouch html/data/instances.json\\n```\\n\\n### Run\\n\\nRun (it takes between 30 minutes and 1 hour):\\n\\n```sh\\npython3 -m searxstats --cache /usr/local/searx-stats2/cache --all\\n```\\n\\nOutput in `html/data/instances.json`.\\n\\nTo display all options:\\n\\n```sh\\npython3 -m searxstats --help\\n```\\n'},\n",
       " {'repo': 'wasidennis/AdaptSegNet',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Learning to Adapt Structured Output Space for Semantic Segmentation\\n\\nPytorch implementation of our method for adapting semantic segmentation from the synthetic dataset (source domain) to the real dataset (target domain). Based on this implementation, our result is ranked 3rd in the [VisDA Challenge](http://ai.bu.edu/visda-2017/).\\n\\nContact: Yi-Hsuan Tsai (wasidennis at gmail dot com) and Wei-Chih Hung (whung8 at ucmerced dot edu)\\n\\n## Paper\\n[Learning to Adapt Structured Output Space for Semantic Segmentation](https://arxiv.org/abs/1802.10349) <br />\\n[Yi-Hsuan Tsai](https://sites.google.com/site/yihsuantsai/home)\\\\*, [Wei-Chih Hung](https://hfslyc.github.io/)\\\\*, [Samuel Schulter](https://samschulter.github.io/), [Kihyuk Sohn](https://sites.google.com/site/kihyuksml/), [Ming-Hsuan Yang](http://faculty.ucmerced.edu/mhyang/index.html) and [Manmohan Chandraker](http://cseweb.ucsd.edu/~mkchandraker/) <br />\\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018 (**spotlight**) (\\\\* indicates equal contribution).\\n\\nPlease cite our paper if you find it useful for your research.\\n\\n```\\n@inproceedings{Tsai_adaptseg_2018,\\n  author = {Y.-H. Tsai and W.-C. Hung and S. Schulter and K. Sohn and M.-H. Yang and M. Chandraker},\\n  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\n  title = {Learning to Adapt Structured Output Space for Semantic Segmentation},\\n  year = {2018}\\n}\\n```\\n\\n## Example Results\\n\\n![](figure/result_git.png)\\n\\n## Quantitative Reuslts\\n\\n![](figure/iou_comparison_v2.png)\\n\\n## Installation\\n* Install PyTorch from http://pytorch.org with Python 2 and CUDA 8.0\\n\\n* **NEW** Add the LS-GAN objective to improve the performance\\n  - Usage: add `--gan LS` option during training (see below for more details)\\n\\n* PyTorch 0.4 with Python 3 and CUDA 8.0\\n  - Usage: replace the training and evaluation codes with the ones in the `pytorch_0.4` folder\\n  - Update: tensorboard is provided by adding `--tensorboard` in the command\\n  - Note: the single-level model works as expected, while the multi-level model requires smaller weights, e.g., `--lambda-adv-target1 0.00005 --lambda-adv-target2 0.0005`. We will investigate this issue soon.\\n\\n* Clone this repo\\n```\\ngit clone https://github.com/wasidennis/AdaptSegNet\\ncd AdaptSegNet\\n```\\n## Dataset\\n* Download the [GTA5 Dataset](https://download.visinf.tu-darmstadt.de/data/from_games/) as the source domain, and put it in the `data/GTA5` folder\\n\\n* Download the [Cityscapes Dataset](https://www.cityscapes-dataset.com/) as the target domain, and put it in the `data/Cityscapes` folder\\n\\n## Pre-trained Models\\n* Please find our-pretrained models using ResNet-101 on three benchmark settings [here](https://www.dropbox.com/s/gpzm15ipyt01mis/DA_Seg_models.zip?dl=0)\\n\\n* They include baselines (without adaptation and with feature adaptation) and our models (single-level and multi-level)\\n\\n## Testing\\n* **NEW** Update results using LS-GAN and using [Synscapes](https://7dlabs.com/synscapes-overview) as the source domain\\n  - Performance: check the appendix of the updated [arXiv paper](https://arxiv.org/abs/1802.10349) (updated on 10/17/2019)\\n  - [Pre-trained models](https://www.dropbox.com/s/sif9cd6ad4s9y5d/AdaptSegNet_LSGAN_models.zip?dl=0)\\n\\n* Download the pre-trained multi-level [GTA5-to-Cityscapes model](http://vllab.ucmerced.edu/ytsai/CVPR18/GTA2Cityscapes_multi-ed35151c.pth) and put it in the `model` folder\\n\\n* Test the model and results will be saved in the `result` folder\\n\\n```\\npython evaluate_cityscapes.py --restore-from ./model/GTA2Cityscapes_multi-ed35151c.pth\\n```\\n\\n* Or, test the VGG-16 based model [Model Link](http://vllab.ucmerced.edu/ytsai/CVPR18/GTA2Cityscapes_vgg-ac4ac9f6.pth)\\n\\n```\\npython evaluate_cityscapes.py --model DeeplabVGG --restore-from ./model/GTA2Cityscapes_vgg-ac4ac9f6.pth\\n```\\n\\n* Compute the IoU on Cityscapes (thanks to the code from [VisDA Challenge](http://ai.bu.edu/visda-2017/))\\n```\\npython compute_iou.py ./data/Cityscapes/data/gtFine/val result/cityscapes\\n```\\n\\n## Training Examples\\n* **NEW** Train the GTA5-to-Cityscapes model (single-level with LS-GAN)\\n\\n```\\npython train_gta2cityscapes_multi.py --snapshot-dir ./snapshots/GTA2Cityscapes_single_lsgan \\\\\\n                                     --lambda-seg 0.0 \\\\\\n                                     --lambda-adv-target1 0.0 --lambda-adv-target2 0.01 \\\\\\n                                     --gan LS\\n```\\n\\n* Train the GTA5-to-Cityscapes model (multi-level)\\n\\n```\\npython train_gta2cityscapes_multi.py --snapshot-dir ./snapshots/GTA2Cityscapes_multi \\\\\\n                                     --lambda-seg 0.1 \\\\\\n                                     --lambda-adv-target1 0.0002 --lambda-adv-target2 0.001\\n```\\n\\n* Train the GTA5-to-Cityscapes model (single-level)\\n\\n```\\npython train_gta2cityscapes_multi.py --snapshot-dir ./snapshots/GTA2Cityscapes_single \\\\\\n                                     --lambda-seg 0.0 \\\\\\n                                     --lambda-adv-target1 0.0 --lambda-adv-target2 0.001\\n```\\n\\n## Related Implementation and Dataset\\n* Y.-H. Tsai, K. Sohn, S. Schulter, and M. Chandraker. Domain Adaptation for Structured Output via Discriminative Patch Representations. In ICCV, 2019. (Oral) [[paper]](https://arxiv.org/abs/1901.05427v3) [[project]](http://www.nec-labs.com/~mas/adapt-seg/adapt-seg.html) [[Implementation Guidance]](https://docs.google.com/document/d/1w235D1vonIl6ER7AEfOOp8T0OFUiLwXCDFUdAra62RU/edit?usp=sharing)\\n* W.-C. Hung, Y.-H Tsai, Y.-T. Liou, Y.-Y. Lin, and M.-H. Yang. Adversarial Learning for Semi-supervised Semantic Segmentation. In BMVC, 2018. [[paper]](https://arxiv.org/abs/1802.07934) [[code]](https://github.com/hfslyc/AdvSemiSeg)\\n* Y.-H. Chen, W.-Y. Chen, Y.-T. Chen, B.-C. Tsai, Y.-C. Frank Wang, and M. Sun. No More Discrimination: Cross City Adaptation of Road Scene Segmenters. In ICCV 2017. [[paper]](https://arxiv.org/abs/1704.08509) [[project]](https://yihsinchen.github.io/segmentation_adaptation/)\\n\\n## Acknowledgment\\nThis code is heavily borrowed from [Pytorch-Deeplab](https://github.com/speedinghzl/Pytorch-Deeplab).\\n## Note\\nThe model and code are available for non-commercial research purposes only.\\n* 10/2019: update performance and training/evaluation codes for using LS-GAN and Synscapes (especially thanks to [Yan-Ting Liu](https://github.com/wheatdog) for helping experiments)\\n* 01/2019: upate the training code for PyTorch 0.4\\n* 07/23/2018: update evaluation code for PyTorch 0.4\\n* 06/04/2018: update pretrained VGG-16 model \\n* 02/2018: code released\\n\\n\\n\\n\\n'},\n",
       " {'repo': 'SpaceNetChallenge/SpaceNet8',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Algorithmic Baseline for SpaceNet 8 Flood Detection Challenge \\n\\n\\nEach year, natural disasters such as hurricanes, tornadoes, earthquakes and floods significantly damage infrastructure and result in loss of life, property and billions of dollars. As these events become more frequent and severe, there is an increasing need to rapidly develop maps and analyze the scale of destruction to better direct resources and first responders. To help address this need, the SpaceNet 8 Flood Detection Challenge will focus on infrastructure and flood mapping related to hurricanes and heavy rains that cause route obstructions and significant damage. The goal of SpaceNet 8 is to leverage the existing repository of datasets and algorithms from SpaceNet Challenges 1-7 (https://spacenet.ai/datasets/) and apply them to a real-world disaster response scenario, expanding to multiclass feature extraction and characterization for flooded roads and buildings and predicting road speed. \\n\\n\\n### Setup\\n1. Download this repo  \\n\\n2. Build docker image  \\n`nvidia-docker build -t sn8/baseline:1.0 /path/to/sn8_baseline/docker`\\n\\n3. Create and run the container (mount volumes to access your data, etc. see https://docs.docker.com/engine/reference/commandline/run/ for options)  \\n`nvidia-docker run -it --rm sn8/baseline:1.0 bash`\\n\\n\\n### Data Preparation\\nFollow these steps to prepare data for training and validation. \\n\\n1. Clean the geojson labels and add speed values to the roads based on road type, number of lanes, and surface type.  \\n`python baseline/data_prep/geojson_prep.py --root_dir /path/to/spacenet8/aws/data/download --aoi_dirs Germany_Training_Public Louisiana-East_Training_Public`\\n\\n\\n    The cleaning step here also catches geometry problems, makes a single commom schema, and moves roads and buildings to seperate geojsons/shps. It will output a few additional files that are used in the subsequent step for creating image masks. The following new files will be written to the AOI annotations directory:  \\n        - `{AOI}/annotations/prepped_cleaned/roads_cleaned_{x}_{y}_{id}.geojson`  \\n        - `{AOI}/annotations/prepped_cleaned/buildings_cleaned_{x}_{y}_{id}.geojson`  \\n        - `{AOI}/annotations/prepped_cleaned/roads_speed_{x}_{y}_{id}.geojson`  \\n        - `{AOI}/annotations/prepped_cleaned/roads_speed_{x}_{y}_{id}.shp`  \\n        - `{AOI}/annotations/prepped_cleaned/buildings_{x}_{y}_{id}.shp`  \\n\\n2. Create training and validation masks from the geojsons to use during training and validation.  \\n`python baseline/data_prep/create_masks.py --root_dir /path/to/spacenet8/aws/data/download --aoi_dirs Germany_Training_Public Louisiana-East_Training_Public`\\n\\n    Four masks are generated during this process:  \\n        - binary building mask (0 non-building, 1 building)  \\n        - binary road mask (0 non-building, 1 building)  \\n        - 8-channel road speed mask  \\n        - 4-channel flood mask  \\n\\n3. Create a random train/val split to train the models on\\n`python baseline/data_prep/generate_train_val_test_csvs.py --root_dir /path/to/spacenet8/aws/data/download --aoi_dirs Germany_Training_Public Louisiana-East_Training_Public --out_csv_basename sn8_data --val_percent 0.15 --out_dir /path/to/output/folder/for/train/val/csvs`\\n\\n    This will create a csv file with filepaths to training images and labels and csv file with filepaths to validation images. It will do a random train/val split. These csvs are used by the dataloader during training. \\n\\n### Train/Validate Foundation Features Network  \\n`python baseline/train_foundation_features.py --train_csv /path/to/train.csv --val_csv /path/to/val.csv --save_dir /path/to/save/directory/foundation --model_name resnet34 --lr 0.0001 --batch_size 2 --n_epochs 50 --gpu 0`\\n\\n### Inference with Foundation Features Network  \\n1. Write prediction tiffs to be used for postprocessing and generating the submission .csv  \\n`python baseline/foundation_eval.py --model_path /path/to/saved/foundation/best_model.pth --in_csv /path/to/val/or/test.csv --save_preds_dir /path/to/output/foundation/eval_test --gpu 0 --model_name resnet34`  \\n\\n2. Write prediction .pngs for visual inspection of predictions  \\n`python baseline/foundation_eval.py --model_path /path/to/saved/foundation/best_model.pth --in_csv /path/to/val/or/test.csv --save_fig_dir /path/to/output/foundation/eval_test/pngs --gpu 0 --model_name resnet34`  \\n\\n### Train/Validate Flood Features Network  \\n`python baseline/train_flood.py --train_csv /path/to/train.csv --val_csv /path/to/val.csv --save_dir /path/to/save/directory/flood --model_name resnet34_siamese --lr 0.0001 --batch_size 2 --n_epochs 50 --gpu 0`  \\n\\n### Inference With Flood Features Network\\n1. Write prediction tiffs to be used for postprocessing and generating the submission .csv  \\n    `python baseline/flood_eval.py --model_path /path/to/saved/flood/best_model.pth --in_csv /path/to/val/or/test.csv --save_preds_dir /path/to/output/flood/eval_test --gpu 0 --model_name resnet34_siamese`    \\n\\n2. Write prediction .pngs for visual inspection of predictions  \\n    `python lib/flood_eval.py --model_path /path/to/saved/flood/best_model.pth --in_csv /path/to/val/or/test.csv --save_fig_dir /path/to/output/flood/eval_test/pngs --gpu 0 --model_name resnet34_siamese`    \\n\\n### Postprocessing\\nAny of the following postprocessing steps require that you have run inference with the flood features network and foundation features network. As input, the postprocessing requires the geotiff predictions.  \\n\\n##### Roads\\n1. Skeletonize road raster predictions and convert to vector data.  \\n    `python baseline/postprocessing/roads/vectorize_roads.py --im_dir /path/to/road/prediction/geotiffs --out_dir /path/to/road/prediction/geotiffs --write_shps --write_graphs --write_csvs --write_skeletons`\\n2. Generate road network graph from vector data  \\n    `python baseline/postprocessing/roads/wkt_to_G.py --wkt_submission /path/to/output/csv/from/last/step --graph_dir /output/path/to/save/simplified/graphs --log_file /path/to/log`\\n3. Assign road speed predictions to road network graph linestrings  \\n    `python baseline/postprocessing/roads/infer_speed.py `\\n4. Generate the road submission .csv  \\n    `python baseline/postprocessing/roads/create_submission.py`\\n\\nTo run all these steps, see the script `/baseline/postprocessing/roads/road_post.sh`. Change only the variables:  \\n    EVAL_CSV  \\n    ROAD_PRED_DIR  \\n    FLOOD_PRED_DIR  \\n\\n##### Buildings\\n1. Merge the building predictions from the foundation features network with the flooded predictions from the flood features network to attribute buildings as flooded or non-flooded. Polygonize the building prediction raster, remove polygons below a certain area threshold and simplify polygon geometries.  \\n    `python baseline/postprocessing/buildings/building_postprocessing.py --root_dir /path/to/foundation/features/building/prediction/geotiffs --flood_dir /path/to/flood/prediction/geotiffs --out_submission_csv /path/to/output/building_submission.csv --out_shapefile_dir /path/to/output/building/pred_shps --square_size 5 --simplify_tolerance 0.75 --min_area 5 --perc_positive 0.5`\\n\\n### Submission\\nYour submission files should be in the following format.\\n\\n| ImageId | Object | Flooded | Wkt_Pix | Wkt_Geo |\\n| ------ | ------ | ------ | ------ | ------ |\\n\\nImageId is the reference image for the prediction. Object should be either \"Building\" or \"Road\". Flooded should be set to \"True\" or \"False\". Wkt_Pix is well-known-text format for coordinates as (row, col) of the predictions in reference to the image. Wkt_Geo is well-known-text coordinates of the predictions in WGS84 (x, y). \\n\\nExample for buildings: \\n| ImageId | Object | Flooded | Wkt_Pix | Wkt_Geo |\\n| ------ | ------ | ------ | ------ | ------ |\\n| 104001006504F400_0_11_20 | Building | Null | POLYGON EMPTY | POLYGON EMPTY |\\n| 104001006504F400_0_10_29 | Building | FALSE | POLYGON ((6 706, 0 708, 0 715, 9 719, 12 706, 6 706)) | POLYGON ((-90.4706194788189 29.8296995794957,-90.4706358281571 29.8296930397604,-90.4706390980247 29.8296668808194,-90.4706063993484 29.8296570712164,-90.4705965897455 29.8296963096281,-90.4706194788189 29.8296995794957)) |  \\n\\n\\n### References and Resources\\n1. SpaceNet-8 workshop paper: https://openaccess.thecvf.com/content/CVPR2022W/EarthVision/papers/Hansch_SpaceNet_8_-_The_Detection_of_Flooded_Roads_and_Buildings_CVPRW_2022_paper.pdf\\n\\n2. The preprocessing for road speed labels, road speed training, and road speed post-processing leverages: https://github.com/avanetten/cresi  \\n\\n3. See the following paper for more information on the road speed segmentation: https://openaccess.thecvf.com/content_WACV_2020/html/Van_Etten_City-Scale_Road_Extraction_from_Satellite_Imagery_v2_Road_Speeds_and_WACV_2020_paper.html  \\n\\n3. SpaceNet 5 blogs for road speed estimation: https://spacenet.ai/sn5-challenge/\\n'},\n",
       " {'repo': 'zero-to-mastery/starwars-spaceships',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# starwars-spaceships\\n'},\n",
       " {'repo': 'fluid-project/infusion-docs',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': \"# Infusion Documentation\\n\\n[![Netlify Status](https://api.netlify.com/api/v1/badges/c9061766-d760-4eda-922f-da3b838a6013/deploy-status)](https://app.netlify.com/sites/fluid-infusion-docs/deploys)\\n\\n## Browse the Infusion Documentation\\n\\nYou can [browse the Infusion Documentation website](http://docs.fluidproject.org/infusion/), or if\\nyou prefer, you can [browse the source documentation files directly on GitHub](src/documents).\\n\\n## Working with the Infusion Documentation locally\\n\\nYou must have [Node and NPM](https://nodejs.org/en/download/) installed in order to work on the Infusion Documentation\\nlocally (the LTS version is recommended).\\n\\nTo install the dependencies for this project:\\n\\n```shell\\nnpm install\\n```\\n\\nTo serve the site locally in development mode with live reloading using [Eleventy](https://11ty.dev):\\n\\n```shell\\nnpm run start\\n```\\n\\nThen, point your browser to: `http://localhost:8080/`\\n\\nTo statically build the site using [Eleventy](https://11ty.dev):\\n\\n```shell\\nnpm run build\\n```\\n\\n## Deploying the Infusion Documentation website\\n\\nThe [Infusion Documentation website](http://docs.fluidproject.org/infusion/) is published with [Netlify](https://netlify.com)\\nevery time new content is pushed to the `main` branch of this repository. [Deploy previews](https://docs.netlify.com/site-deploys/overview/#deploy-preview-controls)\\nare also generated for every pull request. For more information, please review Netlify's [documentation](https://docs.netlify.com).\\n\\n## Generating a Docker image\\n\\nYou can serve the website from a [Docker](https://docs.docker.com/get-docker) container.\\n\\nOnce you have Docker installed, run the following commands to build a Docker image and start a container:\\n\\n* Build the image: `docker build -t infusion-docs .`\\n* Run the container: `docker run --name infusion-docs -p 8000:80 infusion-docs`\\n\\nThe documentation will be available at [http://localhost:8000](http://localhost:8000)\\n\\n* To stop and remove the container: `docker rm -f infusion-docs`\\n\\nIf you make changes to the documentation, repeat the steps to build the image and start a new container.\\n\"},\n",
       " {'repo': 'openrazer/openrazer',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"# [OpenRazer](https://openrazer.github.io/)\\n\\nA collection of Linux drivers for Razer devices - providing kernel drivers, DBus services and Python bindings to interact with the DBus interface.\\n\\n## Something not working?\\n\\nSometimes there are problems with the driver installation due to missing kernel modules or secure boot. Please refer to the [Troubleshooting wiki page](https://github.com/openrazer/openrazer/wiki/Troubleshooting) for guidance.\\n\\nIf the troubleshooting guide did not pinpoint the problem, [try searching open/closed issues](https://github.com/openrazer/openrazer/issues?q=is%3Aissue+) before creating a new one.\\n\\n## Device Support\\n\\nThe list below contains latest devices supported on this branch (usually **master**). These might not be released yet, so check the [stable branch](https://github.com/openrazer/openrazer/tree/stable) for what should be available in the packages for your distribution.\\n\\nThe devices below are fully feature supported by OpenRazer, which means all available USB controllable functions for that product are made available by the library.\\n\\n### Keyboards\\n| Device                                        | USB VID:PID |\\n| --------------------------------------------- | ----------- |\\n| Razer BlackWidow Ultimate 2012                |  1532:010D  |\\n| Razer BlackWidow Classic (Alternate)          |  1532:010E  |\\n| Razer Anansi                                  |  1532:010F  |\\n| Razer BlackWidow Ultimate 2013                |  1532:011A  |\\n| Razer BlackWidow Stealth                      |  1532:011B  |\\n| Razer BlackWidow Tournament Edition 2014      |  1532:011C  |\\n| Razer DeathStalker Expert                     |  1532:0202  |\\n| Razer BlackWidow Chroma                       |  1532:0203  |\\n| Razer DeathStalker Chroma                     |  1532:0204  |\\n| Razer Blade Stealth                           |  1532:0205  |\\n| Razer BlackWidow Tournament Edition Chroma    |  1532:0209  |\\n| Razer Blade QHD                               |  1532:020F  |\\n| Razer Blade Pro (Late 2016)                   |  1532:0210  |\\n| Razer BlackWidow Chroma (Overwatch)           |  1532:0211  |\\n| Razer BlackWidow Ultimate 2016                |  1532:0214  |\\n| Razer BlackWidow X Chroma                     |  1532:0216  |\\n| Razer BlackWidow X Ultimate                   |  1532:0217  |\\n| Razer BlackWidow X Tournament Edition Chroma  |  1532:021A  |\\n| Razer Ornata Chroma                           |  1532:021E  |\\n| Razer Ornata                                  |  1532:021F  |\\n| Razer Blade Stealth (Late 2016)               |  1532:0220  |\\n| Razer BlackWidow Chroma V2                    |  1532:0221  |\\n| Razer Blade (Late 2016)                       |  1532:0224  |\\n| Razer Blade Pro (2017)                        |  1532:0225  |\\n| Razer Huntsman Elite                          |  1532:0226  |\\n| Razer Huntsman                                |  1532:0227  |\\n| Razer BlackWidow Elite                        |  1532:0228  |\\n| Razer Cynosa Chroma                           |  1532:022A  |\\n| Razer Cynosa Chroma Pro                       |  1532:022C  |\\n| Razer Blade Stealth (Mid 2017)                |  1532:022D  |\\n| Razer Blade Pro FullHD (2017)                 |  1532:022F  |\\n| Razer Blade Stealth (Late 2017)               |  1532:0232  |\\n| Razer Blade 15 (2018)                         |  1532:0233  |\\n| Razer Blade Pro 17 (2019)                     |  1532:0234  |\\n| Razer BlackWidow Lite (2018)                  |  1532:0235  |\\n| Razer BlackWidow Essential                    |  1532:0237  |\\n| Razer Blade Stealth (2019)                    |  1532:0239  |\\n| Razer Blade 15 Advanced (2019)                |  1532:023A  |\\n| Razer Blade 15 Base (2018)                    |  1532:023B  |\\n| Razer Cynosa Lite                             |  1532:023F  |\\n| Razer Blade 15 Mercury (2018)                 |  1532:0240  |\\n| Razer BlackWidow (2019)                       |  1532:0241  |\\n| Razer Huntsman Tournament Edition             |  1532:0243  |\\n| Razer Blade 15 Mercury (Mid 2019)             |  1532:0245  |\\n| Razer Blade 15 Base (Mid 2019)                |  1532:0246  |\\n| Razer Blade Stealth (Late 2019)               |  1532:024A  |\\n| Razer Blade Pro (Late 2019)                   |  1532:024C  |\\n| Razer Blade 15 Studio Edition (2019)          |  1532:024D  |\\n| Razer BlackWidow V3                           |  1532:024E  |\\n| Razer Blade Stealth (Early 2020)              |  1532:0252  |\\n| Razer Blade 15 Advanced (2020)                |  1532:0253  |\\n| Razer Blade 15 Base (Early 2020)              |  1532:0255  |\\n| Razer Blade Pro 17 (Early 2020)               |  1532:0256  |\\n| Razer Huntsman Mini                           |  1532:0257  |\\n| Razer BlackWidow V3 Mini Hyperspeed (Wired)   |  1532:0258  |\\n| Razer Blade Stealth (Late 2020)               |  1532:0259  |\\n| Razer BlackWidow V3 Pro (Wired)               |  1532:025A  |\\n| Razer Ornata V2                               |  1532:025D  |\\n| Razer Cynosa V2                               |  1532:025E  |\\n| Razer Huntsman V2 Analog                      |  1532:0266  |\\n| Razer Huntsman Mini (JP)                      |  1532:0269  |\\n| Razer Book 13 (2020)                          |  1532:026A  |\\n| Razer Huntsman V2 TKL                         |  1532:026B  |\\n| Razer Huntsman V2                             |  1532:026C  |\\n| Razer Blade 15 Advanced (Early 2021)          |  1532:026D  |\\n| Razer Blade 17 Pro (Early 2021)               |  1532:026E  |\\n| Razer Blade 15 Base (Early 2021)              |  1532:026F  |\\n| Razer Blade 14 (2021)                         |  1532:0270  |\\n| Razer BlackWidow V3 Mini Hyperspeed (Wireless)|  1532:0271  |\\n| Razer Blade 15 Advanced (Mid 2021)            |  1532:0276  |\\n| Razer Blade 17 Pro (Mid 2021)                 |  1532:0279  |\\n| Razer Huntsman Mini Analog                    |  1532:0282  |\\n| Razer Blade 15 Advanced (Early 2022)          |  1532:028A  |\\n| Razer Blade 17 (2022)                         |  1532:028B  |\\n| Razer DeathStalker V2                         |  1532:0295  |\\n| Razer BlackWidow V3 Tenkeyless                |  1532:0A24  |\\n\\n### Mice\\n| Device                                        | USB VID:PID |\\n| --------------------------------------------- | ----------- |\\n| Razer Orochi 2011                             |  1532:0013  |\\n| Razer DeathAdder 3.5G                         |  1532:0016  |\\n| Razer Abyssus 1800                            |  1532:0020  |\\n| Razer Mamba 2012 (Wired)                      |  1532:0024  |\\n| Razer Mamba 2012 (Wireless)                   |  1532:0025  |\\n| Razer DeathAdder 3.5G Black                   |  1532:0029  |\\n| Razer Naga 2012                               |  1532:002E  |\\n| Razer Imperator 2012                          |  1532:002F  |\\n| Razer Ouroboros 2012                          |  1532:0032  |\\n| Razer Taipan                                  |  1532:0034  |\\n| Razer Naga Hex (Red)                          |  1532:0036  |\\n| Razer DeathAdder 2013                         |  1532:0037  |\\n| Razer DeathAdder 1800                         |  1532:0038  |\\n| Razer Orochi 2013                             |  1532:0039  |\\n| Razer Naga Epic Chroma (Wired)                |  1532:003E  |\\n| Razer Naga Epic Chroma (Wireless)             |  1532:003F  |\\n| Razer Naga 2014                               |  1532:0040  |\\n| Razer Naga Hex                                |  1532:0041  |\\n| Razer Abyssus 2014                            |  1532:0042  |\\n| Razer DeathAdder Chroma                       |  1532:0043  |\\n| Razer Mamba (Wired)                           |  1532:0044  |\\n| Razer Mamba (Wireless)                        |  1532:0045  |\\n| Razer Mamba Tournament Edition                |  1532:0046  |\\n| Razer Orochi (Wired)                          |  1532:0048  |\\n| Razer Diamondback Chroma                      |  1532:004C  |\\n| Razer DeathAdder 2000                         |  1532:004F  |\\n| Razer Naga Hex V2                             |  1532:0050  |\\n| Razer Naga Chroma                             |  1532:0053  |\\n| Razer DeathAdder 3500                         |  1532:0054  |\\n| Razer Lancehead (Wired)                       |  1532:0059  |\\n| Razer Lancehead (Wireless)                    |  1532:005A  |\\n| Razer Abyssus V2                              |  1532:005B  |\\n| Razer DeathAdder Elite                        |  1532:005C  |\\n| Razer Abyssus 2000                            |  1532:005E  |\\n| Razer Lancehead Tournament Edition            |  1532:0060  |\\n| Razer Atheris (Receiver)                      |  1532:0062  |\\n| Razer Basilisk                                |  1532:0064  |\\n| Razer Basilisk Essential                      |  1532:0065  |\\n| Razer Naga Trinity                            |  1532:0067  |\\n| Razer Abyssus Elite (D.Va Edition)            |  1532:006A  |\\n| Razer Abyssus Essential                       |  1532:006B  |\\n| Razer Mamba Elite (Wired)                     |  1532:006C  |\\n| Razer DeathAdder Essential                    |  1532:006E  |\\n| Razer Lancehead Wireless (Receiver)           |  1532:006F  |\\n| Razer Lancehead Wireless (Wired)              |  1532:0070  |\\n| Razer DeathAdder Essential (White Edition)    |  1532:0071  |\\n| Razer Mamba Wireless (Receiver)               |  1532:0072  |\\n| Razer Mamba Wireless (Wired)                  |  1532:0073  |\\n| Razer Pro Click (Receiver)                    |  1532:0077  |\\n| Razer Viper                                   |  1532:0078  |\\n| Razer Viper Ultimate (Wired)                  |  1532:007A  |\\n| Razer Viper Ultimate (Wireless)               |  1532:007B  |\\n| Razer DeathAdder V2 Pro (Wired)               |  1532:007C  |\\n| Razer DeathAdder V2 Pro (Wireless)            |  1532:007D  |\\n| Razer Pro Click (Wired)                       |  1532:0080  |\\n| Razer Basilisk X HyperSpeed                   |  1532:0083  |\\n| Razer DeathAdder V2                           |  1532:0084  |\\n| Razer Basilisk V2                             |  1532:0085  |\\n| Razer Basilisk Ultimate (Wired)               |  1532:0086  |\\n| Razer Basilisk Ultimate (Receiver)            |  1532:0088  |\\n| Razer Viper Mini                              |  1532:008A  |\\n| Razer DeathAdder V2 Mini                      |  1532:008C  |\\n| Razer Naga Left-Handed Edition                |  1532:008D  |\\n| Razer Naga Pro (Wired)                        |  1532:008F  |\\n| Razer Naga Pro (Wireless)                     |  1532:0090  |\\n| Razer Viper 8KHz                              |  1532:0091  |\\n| Razer Orochi V2 (Receiver)                    |  1532:0094  |\\n| Razer Orochi V2 (Bluetooth)                   |  1532:0095  |\\n| Razer Naga X                                  |  1532:0096  |\\n| Razer DeathAdder Essential (2021)             |  1532:0098  |\\n| Razer Basilisk V3                             |  1532:0099  |\\n| Razer DeathAdder V2 X HyperSpeed              |  1532:009C  |\\n| Razer Viper V2 Pro (Wired)                    |  1532:00A5  |\\n| Razer Viper V2 Pro (Wireless)                 |  1532:00A6  |\\n| Razer DeathAdder V3 Pro (Wired)               |  1532:00B6  |\\n| Razer DeathAdder V3 Pro (Wireless)            |  1532:00B7  |\\n\\n### Mousemats\\n| Device                                        | USB VID:PID |\\n| --------------------------------------------- | ----------- |\\n| Razer Firefly Hyperflux                       |  1532:0068  |\\n| Razer Firefly                                 |  1532:0C00  |\\n| Razer Goliathus                               |  1532:0C01  |\\n| Razer Goliathus Extended                      |  1532:0C02  |\\n| Razer Firefly v2                              |  1532:0C04  |\\n\\n### Headsets\\n| Device                                        | USB VID:PID |\\n| --------------------------------------------- | ----------- |\\n| Razer Kraken 7.1                              |  1532:0501  |\\n| Razer Kraken 7.1 Chroma                       |  1532:0504  |\\n| Razer Kraken 7.1                              |  1532:0506  |\\n| Razer Kraken 7.1 V2                           |  1532:0510  |\\n| Razer Kraken Ultimate                         |  1532:0527  |\\n| Razer Kraken Kitty Edition                    |  1532:0F19  |\\n\\n### Misc\\n| Device                                        | USB VID:PID |\\n| --------------------------------------------- | ----------- |\\n| Razer Mouse Dock                              |  1532:007E  |\\n| Razer Nostromo                                |  1532:0111  |\\n| Razer Orbweaver                               |  1532:0113  |\\n| Razer Tartarus                                |  1532:0201  |\\n| Razer Orbweaver Chroma                        |  1532:0207  |\\n| Razer Tartarus Chroma                         |  1532:0208  |\\n| Razer Core                                    |  1532:0215  |\\n| Razer Tartarus V2                             |  1532:022B  |\\n| Razer Nommo Chroma                            |  1532:0517  |\\n| Razer Nommo Pro                               |  1532:0518  |\\n| Razer Chroma Mug Holder                       |  1532:0F07  |\\n| Razer Base Station Chroma                     |  1532:0F08  |\\n| Razer Chroma Hardware Development Kit (HDK)   |  1532:0F09  |\\n| Razer Raptor 27                               |  1532:0F12  |\\n| Razer Mouse Bungee V3 Chroma                  |  1532:0F1D  |\\n| Razer Base Station V2 Chroma                  |  1532:0F20  |\\n| Razer Thunderbolt 4 Dock Chroma               |  1532:0F21  |\\n| Razer Charging Pad Chroma                     |  1532:0F26  |\\n\\n#### Determining the Device ID\\nRazer devices use a USB VID (Vendor ID) of `1532`. You can identify the USB PID (Product ID) by typing:\\n\\n    lsusb | grep '1532:'\\n\\nThis will output something similar to this:\\n\\n    Bus 003 Device 005: ID 1532:0203 Razer USA, Ltd\\n\\n\\n---\\n\\n## Installation\\n\\nOfficial packages are available for these distributions (and their derivatives):\\n\\n* [Ubuntu](https://openrazer.github.io/#ubuntu)\\n* [Debian](https://openrazer.github.io/#debian)\\n* [Arch Linux](https://openrazer.github.io/#arch)\\n* [Fedora](https://openrazer.github.io/#fedora)\\n* [openSUSE](https://openrazer.github.io/#opensuse)\\n* [Mageia](https://openrazer.github.io/#mageia)\\n\\nCommunity supported packages are available for:\\n\\n* [Gentoo](https://openrazer.github.io/#gentoo)\\n* [NixOS](https://openrazer.github.io/#nixos)\\n* [Red Hat / CentOS](https://openrazer.github.io/#redhat)\\n* [Solus](https://openrazer.github.io/#solus)\\n* [Slackware](https://openrazer.github.io/#slackware)\\n* [Void Linux](https://openrazer.github.io/#voidlinux)\\n\\n## Applications\\n\\nThe following applications complement and interact with this driver:\\n\\n* [Polychromatic](https://github.com/polychromatic/polychromatic) - a graphical management tool and tray applet to managing Razer peripherals.\\n* [RazerGenie](https://github.com/z3ntu/RazerGenie) - Qt application for configuring your Razer devices under GNU/Linux.\\n* [razerCommander](https://github.com/GabMus/razerCommander) - Simple GUI written in Gtk3\\n* [Snake](http://bithatch.co.uk/snake.html) - a stylised tool and tray applet for configuring Razer devices on Linux, written in Java.\\n* [Chroma Feedback](https://github.com/redaxmedia/chroma-feedback) - Turn your Razer keyboard, mouse or headphone into a extreme feedback device\\n\\n---\\n\\nThe project is licensed under the GPL and is not officially endorsed by [Razer, Inc](http://www.razerzone.com/).\\n\"},\n",
       " {'repo': 'Bamblehorse/tiny',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# @bamblehorse/tiny\\n\\n[![npm (scoped)](https://img.shields.io/npm/v/@bamblehorse/tiny.svg)](https://www.npmjs.com/package/@bamblehorse/tiny)\\n[![npm bundle size (minified)](https://img.shields.io/bundlephobia/min/@bamblehorse/tiny.svg)](https://www.npmjs.com/package/@bamblehorse/tiny)\\n\\nRemoves all spaces from a string.\\n\\n## Install\\n\\n```\\n$ npm install @bamblehorse/tiny\\n```\\n\\n## Usage\\n\\n```js\\nconst tiny = require(\"@bamblehorse/tiny\");\\n\\ntiny(\"So much space!\");\\n//=> \"Somuchspace!\"\\n\\ntiny(1337);\\n//=> Uncaught TypeError: Tiny wants a string!\\n//    at tiny (<anonymous>:2:41)\\n//    at <anonymous>:1:1\\n```\\n'},\n",
       " {'repo': 'zitron-git/KSPSerialIO', 'language': 'C#', 'readme_contents': ''},\n",
       " {'repo': 'ukupat/tabs-or-spaces',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Tabs or spaces\\n\\nModule for analysing which whitespace types are used by the top starred repositories in GitHub.\\n\\n[Check out the results](http://ukupat.github.io/tabs-or-spaces/)\\n\\n## License\\n\\n[MIT](//github.com/ukupat/tabs-or-spaces/blob/master/LICENSE)\\n'},\n",
       " {'repo': 'varkenvarken/spacetree',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"spacetree\\n=========\\n\\nA Blender add-on to create tree objects with the space colonization algorithm\\n\\nCurrently part of Blender contrib:\\n\\nhttps://svn.blender.org/svnroot/bf-extensions/contrib/py/scripts/addons/add_mesh_space_tree/\\n\\nDocumentation (work in progress):\\n\\nhttp://wiki.blender.org/index.php/Extensions:2.6/Py/Scripts/Add_Mesh/Add_Space_Tree\\n\\nCurrent developments can be followed on BlenderArtists:\\n\\nhttp://blenderartists.org/forum/showthread.php?282550-A-new-tree-add-on\\n\\nNote\\n====\\n\\na commercial version of this add-on is available on Blender Market:\\nhttps://blendermarket.com/creators/varkenvarken\\n\\nINSTALLATION\\n============\\n\\nIf you have installed Blender from a daily build, this add-on is already bundled. If you want to try a newer version you have to make sure that the distributed version is removed first:\\n\\n- quit Blender\\n- go to the installation directory of the contributed addons, e.g. <BlenderInstallDir>\\\\2.69\\\\scripts\\\\addons_contrib\\n- remove the add_mesh_space_tree directory\\n- download add_mesh_space_tree.zip from the release directory (do not use the download as .zip for the complete repository, that won't work: the .zip in the release dir is a package Blender cn use directly)\\n- open Blender\\n- choose File->User preferences->Addons->Install from file and select the downloaded .zip, click install\\n- don't forget to check the enable addon checkbox once it it is installed\\n\\nLikewise, when installing an even newer version (i.e. after you have replaced the distributed version) you need to make sure nothing remains in the user data folder, so you'll then have to perform the steps aboce for the directory:\\n\\nC:\\\\Users\\\\<username>\\\\AppData\\\\Roaming\\\\Blender Foundation\\\\Blender\\\\2.69\\\\scripts\\\\addons\\n\\n(on Windows taht is, I am not sure where this lives exactly on Unix systems)\\n\\nIf you want to enable the addon later, it lives in the Add Mesh section of the addons and is called 'SCA Tree generator'.\\n\\nTo use the addon (in the 3DVIEW) click Add->Mesh->Add tree to scene, the options are in the toolbar panel (Ctrl-T)\\n\\nNOTE: the tree is generated at the position of the 3d cursor. If you don't see the tree, check that you can see the cursor.\\n\\nNOTE: generating a tree can take quite some time, therefore the tree does NOT change immediately if you tweak an option. You have to click the 'update tree' button to generate a new tree after you changed the settings. \\n\\n\\n\\n\"},\n",
       " {'repo': 'philicious/spacewalk-scripts',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '- **getDebianAnnouncements.py** By https://github.com/rpasche This downloads all security announcements of debian from the current year and the year before and uses html2text to transform it to ascii text\\n- **parseUbuntu.py** parses https://lists.ubuntu.com/archives/ubuntu-security-announce/$DATE.txt.gz into an XML which can be read by errata-import.pl / errata-import.py\\n- **parseDebian.py** By https://github.com/rpasche the same as parseUbuntu.py, but parses all security announcements downloaded with getDebianAnnouncements.py and writes this to an XML file for later use with errata-import-debian.py\\n- **errata-import.pl** originally by Steve Meier http://cefs.steve-meier.de/ I just modified it slightly to work with Ubuntu USN.\\n- **errata-import.py** By https://github.com/pandujar Ported version of the previous one. Includes some enhancenments like date, author and better package processing. Its quite faster than the Perl version.\\n- **errata-import-debian.py** By https://github.com/rpasche This is the modified version of errata-import.py for Debian\\n- **errata.py** is the missing \"action\" for rhn_check so it can apply Errata. Copy it to /usr/share/rhn/actions \\nIts just a copy of https://github.com/spacewalkproject/spacewalk/tree/master/client/rhel/yum-rhn-plugin/actions\\n- **spacewalk-errata.sh** is a Bash script which downloads the compressed security announces, calls parseUbuntu.py on them and finally calls errata-import.py to import the Errata. This script can be run as a Cronjob to automate things.\\n- **errataToSlack.py** reports all errata affecting at least one system to a Slack channel or group\\n- **getSystemUpdatesHistory.py** Lists all packages installed on a given node after a datetime or after X hours before now\\nIf no datetime is given, packages installed in past 24h are listed.\\n- **import-old.sh** imports all errata from Jan 2012 to the month just before today when run; in effect provides constantly up-to-date ubuntu-errata.xml file\\n- **debianSync.py** Ported version of https://github.com/stevemeier/spacewalk-debian-sync . Its a drop-in replacement, meaning all arguments are the same\\n'},\n",
       " {'repo': 'Steven-Cannavan/URP_ScreenSpacePlanarReflections',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': \"# URP_ScreenSpacePlanarReflections\\nSimple example of implementing screen space planar reflections as a RenderFeature for URP\\n\\n![Screen Space Reflections in URP](/Images/PuddleScreenshot.png)\\n\\nBased upon Remi Genin's work: [Screen Space Plane Indexed Reflection In Ghost Recon Wildlands](http://remi-genin.fr/blog/screen-space-plane-indexed-reflection-in-ghost-recon-wildlands/)\\n\\nThis has been created for educational purposes, not for production. Use this at your own risk.\\n\\nKnown Issues:\\n* Doesn't work for Android\\n* Still in progress :)\\n\\n## How To Use\\n\\nAdd the ScreenSpacePlanarReflectionFeature to your Feature list\\n\\n![SSPR Render Feature](/Images/Feature.png)\\n\\nAdjust the position and rotation of the reflection plane by adjusting 'Plane Rotation' & 'Plane Location'\\n\\nThe edge stretch option will stretch the edges of the reflection make them fit better, in my expereience this looks terrible in VR as it breaks perspective but looks pretty good in general.\\n\\nThe blur option should really always be on, at somepoint if i get round to implementing the temporal history I have it as an option to switch between. However right now it will blur the pixels to help cover any gaps, which can be quite significant depending on the angle.\\n\\nRender reflective layer is an option to do another opaque pass on objects which have the specific layer mask selected (Reflective Surface Layer), remember to remove that layer from the default layer mask, if you only intend to use this on transparent materials then I wouldnt worry about enabling this.\\n\\nStencil optimization should only be used if you have a reflective surface layer and your happy to only generate reflections where theyre on screen.\\n\\nAn Example material that uses this is in Assets/Materials/Puddle which uses the example URPExample/SSPR_Lit shader\\n\\n## How it works\\n\\nThis feature will inject upto 3 passes into URP\\n\\n1. \\\\[Optional\\\\] Stencil Pass - Before Opaques\\n\\nThe stencil pass is there to render out any 'Surfaces' that are in the reflective pass with the intention of using that stencil information to exclude rendering out pixels that wont be reflected.\\n\\n2. Reflection Pass - After Skybox\\n\\nThe reflection pass using a mixture of compute and pixel shader will render the reflection into a globa texture called \\n_\\\\_ScreenSpacePlanarReflectionTexture_ if the device does not support compute we will not render anything, this means with this implementation the reflection is only valid for after this pass\\n\\n3. \\\\[Optional\\\\] Render Reflectives Pass  - After Skybox\\nRenderes all renderers in the Reflective Surface Layer, follows opaque rules (forward to back sorting etc)\\n\\n\\n## In Progress\\n* Roughness / Kawase blur sampling\\n\\n## TODO\\n* Implement Temporal History Buffer\\n* Stereo Support\\n* Test Support for Consoles\\n* Fix RenderDoc Bug ( Material is lost when you load render doc)\\n* Switch to RTHandle\\n* Deferred Support\\n\\n## Wishlist / Maybes\\n* 2D Renderer Support\\n* Get working for Android GLES 3\\n\"},\n",
       " {'repo': 'colbyfayock/space-jelly-store-workshop',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# 🛒 Create an eCommerce Store with Next.js and Stripe Checkout\\n\\n* Demo: https://space-jelly-store.netlify.app/\\n\\nIn this workshop you\\'ll learn how to build a new ecommerce store from scratch using Next.js and Stripe Checkout.\\n\\n* [Workshop Overview](#workshop-overview)\\n* [Who Am I?](#who-am-i)\\n* [Before the Workshop](#before-the-workshop)\\n* [During the Workshop](#during-the-workshop)\\n\\n## 🔍 Workshop Overview\\nExcited to learn? You should be! Here\\'s some of the stuff we\\'ll learn in this workshop:\\n* Create a new Next.js app from scratch\\n* Add and manage a dynamic grid of products\\n* Set up and configure products in Stripe\\n* Manage local state with React\\'s useState hook\\n* Create a custom React hook\\n* Use the React Context API to manage global state\\n* Store and persist app state with localstorage\\n* Integrate Stripe Checkout to let people purchase your products\\n* Deploy an app to Vercel\\n\\n## 👨\\u200d🚀 Who Am I?\\nI\\'m Colby Fayock! 👋\\n\\nLearning by doing is the best way to learn something new and that’s the approach I take when I help others learn about Javascript, React, and the static web through [writing](https://www.freecodecamp.org/news/author/colbyfayock/), [videos on Youtube](https://www.youtube.com/colbyfayock), and [courses on egghead.io](https://egghead.io/instructors/colby-fayock?af=atzgap).\\n\\nI try to bring this passion from my work tackling challenges like high scale video streaming services, ecommerce with [ThinkGeek](https://twitter.com/thinkgeek), satellite dashboards, and working as a Developer Advocate for [Applitools](https://applitools.com/).\\n\\nIn addition to all of my other work, I\\'ve published two books [Jamstack Handbook](https://jamstackhandbook.com/) — which includes everything you need to know about the Jamstack with 3 step-by-step tutorials — and [50 Projects for React and the Static Web](https://50reactprojects.com/) — which is a free ebook that includes 50 project ideas complete with project briefs, resources, and even design layout ideas.\\n\\nYou can also find me on Twitter at [@colbyfayock](https://twitter.com/colbyfayock)!\\n\\n## 🧰 Before the Workshop\\n\\nThere\\'s really one thing you would ideally have done before we jump into the workshop:\\n* Make sure you\\'re set up with the minimum environment requirements\\n\\n### Environment Requirements\\nThere\\'s also only one thing that is probably considered a \"strict\" requirement for the workshop:\\n* [node](https://nodejs.org/en/)\\n\\nThe **operating system** you use shouldn\\'t matter as long as you can run commands with npm (or yarn if that\\'s your thing).\\n\\nIf you can run `npm -v`, you should be good to go.\\n\\n### Setting Up the Project\\nOnce you verified you have all of the environment requirements, we can get started by cloning this repository.\\n* Navigate to the folder you\\'d like to work in\\n* Run:\\n```\\ngit clone https://github.com/colbyfayock/space-jelly-store-workshop\\ncd space-jelly-store-workshop\\n```\\n\\nAll of the lessons are contained in their own individual directory within the lessons folder.\\n\\nFor each lesson, you can start from scratch by navigating to that lesson\\'s directory and running:\\n```\\nnpm install\\n```\\n\\nThis will install all of the required dependencies to get started for the lesson.\\n\\n### Starting Up the Project\\nAfter all of the dependencies are installed for the lesson, you should be good to go!\\n* Run:\\n```\\nnpm run dev\\n```\\n\\nThis should kick off a new server available at http://localhost:3000.\\n\\nIf you\\'ve successfully started up your development server, you should be ready to go for the workshop!\\n\\n### What\\'s In This Project?\\nWe\\'ll start off by creating a new app from scratch with Create Next App. In our new app, we\\'ll add products and dynamically manage them using an external json file as our data source.\\n\\nUsing React\\'s APIs like useState and the Context API, we\\'ll create a shopping cart that allows customers to add and manage products in that cart. We\\'ll also use localstorage to store that shopping cart and persist it when our customers reload the page.\\n\\nWith our app, we\\'ll set up and configure products in Stripe, which will allow our customers to purchase those products using Stripe Checkout.\\n\\nFinally, we\\'ll deploy this app to Vercel so our customers can easily purchase our products!\\n\\n## 🖥 The Workshop\\n\\n### Lessons Structure\\n\\nEach lesson is broken out into it\\'s own directory under the lessons folder. Each contain\\'s its own set of dependencies (though most are the same) as well as its own project code that we\\'ll use to work through the exercise.\\n\\nYou\\'ll find a README.md at the top of each lesson. This will give you an overview and instructions for that particular lesson as well as what files you\\'ll need to work through.\\n\\nAs you\\'re working through the lesson, you can use designated keys to find locations in the code where changes should be made. These keys can be used by searching for them in your code editor. The keys available are:\\n* `@lesson-##-todo`\\n* `@lesson-##-answer`\\n\\nFor example, if we want to find all of the spots where we should be working for lesson 2, we can use the `@lesson-02-todo` key to find those files and locations.\\n\\n### Lesson Plan\\n\\n* [00 - Introduction](https://github.com/colbyfayock/space-jelly-store-workshop/tree/main/lessons/00%20-%20Introduction)\\n* [01 - Create a New React Application with Next.js](https://github.com/colbyfayock/space-jelly-store-workshop/tree/main/lessons/01%20-%20Create%20a%20New%20React%20Application%20with%20Next.js)\\n* [02 - Add a Grid of Products with Images to a Next.js React App](https://github.com/colbyfayock/space-jelly-store-workshop/tree/main/lessons/02%20-%20Add%20a%20Grid%20of%20Products%20with%20Images%20to%20a%20Next.js%20React%20App)\\n* [03 - Add and Configure Products in the Stripe Dashboard for an Online Store](https://github.com/colbyfayock/space-jelly-store-workshop/tree/main/lessons/03%20-%20Add%20and%20Configure%20Products%20in%20the%20Stripe%20Dashboard%20for%20an%20Online%20Store)\\n* [04 - Dynamically Manage a Grid of Products in an Online Store with a JSON Document](https://github.com/colbyfayock/space-jelly-store-workshop/tree/main/lessons/04%20-%20Dynamically%20Manage%20a%20Grid%20of%20Products%20in%20an%20Online%20Store%20with%20a%20JSON%20Document)\\n* [05 - Host & Deploy a Next.js React app on Vercel](https://github.com/colbyfayock/space-jelly-store-workshop/tree/main/lessons/05%20-%20Host%20%26%20Deploy%20a%20Next.js%20React%20app%20on%20Vercel)\\n* [06 - Configure a Stripe Checkout Domain for Client-Only Integration](https://github.com/colbyfayock/space-jelly-store-workshop/tree/main/lessons/06%20-%20Configure%20a%20Stripe%20Checkout%20Domain%20for%20Client-Only%20Integration)\\n* [07 - Add a Stripe API Key as an Environment Variable in Next.js & Vercel](https://github.com/colbyfayock/space-jelly-store-workshop/tree/main/lessons/07%20-%20Add%20a%20Stripe%20API%20Key%20as%20an%20Environment%20Variable%20in%20Next.js%20%26%20Vercel)\\n* [08 - Add a Buy Now Button and Integrate Stripe Checkout to Purchase Products in a Next.js Online Store](https://github.com/colbyfayock/space-jelly-store-workshop/tree/main/lessons/08%20-%20Add%20a%20Buy%20Now%20Button%20and%20Integrate%20Stripe%20Checkout%20to%20Purchase%20Products%20in%20a%20Next.js%20Online%20Store)\\n* [09 - Create a Shopping Cart with the useState React Hook to Manage Product Quantity and Total](https://github.com/colbyfayock/space-jelly-store-workshop/tree/main/lessons/09%20-%20Create%20a%20Shopping%20Cart%20with%20the%20useState%20React%20Hook%20to%20Manage%20Product%20Quantity%20and%20Total)\\n* [10 - Create a Custom React Hook to Manage Cart State](https://github.com/colbyfayock/space-jelly-store-workshop/tree/main/lessons/10%20-%20Create%20a%20Custom%20React%20Hook%20to%20Manage%20Cart%20State)\\n* [11 - Use the React Context API to Globally Manage Cart State in a Next.js App](https://github.com/colbyfayock/space-jelly-store-workshop/tree/main/lessons/11%20-%20Use%20the%20React%20Context%20API%20to%20Globally%20Manage%20Cart%20State%20in%20a%20Next.js%20App)\\n* [12 - Store and Load Cart State from Local Storage to Persist the Shopping Cart When Reloading the Page](https://github.com/colbyfayock/space-jelly-store-workshop/tree/main/lessons/12%20-%20Store%20and%20Load%20Cart%20State%20from%20Local%20Storage%20to%20Persist%20the%20Shopping%20Cart%20When%20Reloading%20the%20Page)\\n* [13 - Use Next.js Dynamic Routes to Create Product Pages for an Online Store](https://github.com/colbyfayock/space-jelly-store-workshop/tree/main/lessons/13%20-%20Use%20Next.js%20Dynamic%20Routes%20to%20Create%20Product%20Pages%20for%20an%20Online%20Store)\\n* [14 - Create a Shopping Cart Page to Manage Products to Purchase in a Next.js App](https://github.com/colbyfayock/space-jelly-store-workshop/tree/main/lessons/14%20-%20Create%20a%20Shopping%20Cart%20Page%20to%20Manage%20Products%20to%20Purchase%20in%20a%20Next.js%20App)\\n* [15 - Add a Quantity Input to the Cart Page to Add or Remove Items from a Shopping Cart in Next.js](https://github.com/colbyfayock/space-jelly-store-workshop/tree/main/lessons/15%20-%20Add%20a%20Quantity%20Input%20to%20the%20Cart%20Page%20to%20Add%20or%20Remove%20Items%20from%20a%20Shopping%20Cart%20in%20Next.js)\\n\\n## 🎓 After the Workshop\\n\\n\\n### Feedback\\n\\nThe good, the bad -- I want to know! I would really appreciate you taking any time to hop in this survey and be honest about your experience with the workshop. Please take into consideration both how much you\\'ve enjoyed the workshop and how much you felt you\\'ve learned.\\n\\n[Take the survey!](https://forms.gle/Lt2Z5hrGm6UWAApa6)\\n\\nOr https://forms.gle/Lt2Z5hrGm6UWAApa6\\n'},\n",
       " {'repo': 'robinsloan/sentence-space',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Welcome to sentence space\\n\\nYou can find an introduction to this project [here](https://www.robinsloan.com/notes/voyages-in-sentence-space).\\n\\nThis is a server designed to provide a couple of interesting artifacts. The core of it is a variational autoencoder that embeds sentences into a continuous space; as a result, you can select a point anywhere in that space and get a (more or less) coherent sentence back.\\n\\nOnce you\\'ve established this continuous sentence space, what can you get from it?\\n\\n1. *Sentence gradients*: smooth interpolations between two input sentences.\\n2. *Sentence neighborhoods*: clouds of alternative sentences closely related to an input sentence.\\n\\nThese are very weird artifacts! If you try to write a sentence gradient by hand, you\\'ll find it\\'s very difficult. Is it useful? Possibly not. Is it _interesting_? Definitely!\\n\\nAgain, you\\'ll find a ton more context and exploration in [this post](https://www.robinsloan.com/voyages-in-sentence-space).\\n\\n## Running the server\\n\\nThis code isn\\'t quite turnkey, but if you\\'re willing to tinker, you should be able to train your own models and serve your own gradients, neighborhoods, and who-knows-what-else.\\n\\nThe requirements are:\\n\\n* Python 2.7\\n* Flask\\n* Numpy 1.12.1\\n* Theano 0.9 (plus Nvidia\\'s CUDA and cudnn)\\n* Pandas 0.20.1\\n* Matplotlib 2.0.2\\n* [`sentencepiece`](https://github.com/google/sentencepiece) (optional but nice to have)\\n* [`wordfilter`](https://github.com/dariusk/wordfilter)\\n\\nOne way to get started would be to use Anaconda:\\n\\n```\\nconda create -n sentence-space python=2.7\\nsource activate sentence-space\\nconda install flask\\nconda install numpy=1.12.1\\nconda install theano=0.9.0\\nconda install pandas=0.20.1\\nconda install matplotlib=2.0.2\\n\\npip install wordfilter\\npip install sentencepiece\\n```\\n\\nIf you have those requirements installed, as well as CUDA and cudnn (which is A Whole Other Thing), it _should_ be possible to run `bash serve.sh` and get a server running. If that\\'s not the case, open an issue and let me know. I definitely want to streamline this over time, and improve this documentation as well.\\n\\nThe name of the trained model you want to serve is specified near the top of `textproject_server.py`. (Sorry it\\'s not a command-line option; I just couldn\\'t be bothered.) Try `sample_no_sp_2` to start.\\n\\nOnce the server is running, the API is simple:\\n\\n* `/gradient?s1=Your%20first%20sentence&s2=Your%20second%20sentence`\\n* `/neighborhood?s1=Your%20sentence&mag=0.2`\\n\\nBoth endpoints return a JSON array of results. The code is currently configured to provide seven sentences in each gradient or neighborhood, but you could make that three or 128.\\n\\n## Contributors\\n\\nThis project is forked from [`stas-semeniuta/textvae`](https://github.com/stas-semeniuta/textvae), which is the code for the paper [\"A Hybrid Convolutional Variational Autoencoder for Text Generation\"](https://arxiv.org/abs/1702.02390) by Stanislau Semeniuta, Aliaksei Severyn, and Erhardt Barth. I\\'m indebted to Semeniuta, et. al., for their skill and generosity. If I have tinkered slightly, it is because I stood on the shoulders of smart people.\\n\\nI\\'m indebted also to [`@richardassar`](https://github.com/richardassar), whose improvements allow this server to provide results at interactive speeds.\\n\\nYou can find Semeniuta et. al.\\'s original README in (you guessed it) `ORIGINAL-README.md`.\\n'},\n",
       " {'repo': 'YichenGong/Densely-Interactive-Inference-Network',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Densely Interactive Inference Network (DIIN)\\n\\nThis is the code to reproduce the model in [Natural Language Inference over Interaction Space](https://arxiv.org/abs/1709.04348).\\n\\n## Environment\\n\\tpython 3.6\\n\\ttensorflow = 1.3\\n\\n\\n## Setup\\n\\t$ git clone https://github.com/YichenGong/Densely-Interactive-Inference-Network.git\\n\\t$ cd Densely-Interactive-Inference-Network\\n\\t$ pip install -r requirements.txt\\n\\n## Download Data\\nFirst, run `download.py` for the datasets and the preprocessed file:\\n\\n```\\n$ cd data\\n$ python download.py\\n```\\n\\nThen, manually download download MultiNLI 0.9\\n[matched](https://www.kaggle.com/c/multinli-matched-open-evaluation/data)\\nand [mismatched](https://www.kaggle.com/c/multinli-mismatched-open-evaluation/data)\\ntest set under data/multinli_0.9 folder\\n\\nIf any of the auto download fails, you can manually download them from:\\n* [SNLI 1.0](https://nlp.stanford.edu/projects/snli/)\\n* [MultiNLI 0.9](https://www.nyu.edu/projects/bowman/multinli/)\\n* [Glove](https://nlp.stanford.edu/projects/glove/)\\n* [shared.jsonl](https://drive.google.com/file/d/0B6CTyAhSHoJTa3ZSSE5QQUJrb3M/view?usp=sharing)\\n\\n\\nWhen you finish downloading, your data folder should look like this:\\n\\n```\\n    $ tree data\\n    data\\n    ├── download.py\\n    ├── glove.840B.300d.txt\\n    ├── multinli_0.9\\n    │\\xa0\\xa0 ├── Icon\\\\015\\n    │\\xa0\\xa0 ├── multinli_0.9_dev_matched.jsonl\\n    │\\xa0\\xa0 ├── multinli_0.9_dev_matched.txt\\n    │\\xa0\\xa0 ├── multinli_0.9_dev_mismatched.jsonl\\n    │\\xa0\\xa0 ├── multinli_0.9_dev_mismatched.txt\\n    │\\xa0\\xa0 ├── multinli_0.9_test_matched_sample_submission.csv\\n    │\\xa0\\xa0 ├── multinli_0.9_test_mismatched_sample_submission.csv\\n    │\\xa0\\xa0 ├── multinli_0.9_train.jsonl\\n    │\\xa0\\xa0 ├── multinli_0.9_train.txt\\n    │\\xa0\\xa0 └── paper.pdf\\n    ├── shared.jsonl\\n    └── snli_1.0\\n        ├── Icon\\\\015\\n        ├── README.txt\\n        ├── snli_1.0_dev.jsonl\\n        ├── snli_1.0_dev.txt\\n        ├── snli_1.0_test.jsonl\\n        ├── snli_1.0_test.txt\\n        ├── snli_1.0_train.jsonl\\n        └── snli_1.0_train.txt\\n```\\n\\nI don\\'t recommend you to use multinli_1.0 here because the id doesn\\'t match the id in preprocessed sample id.\\n\\n## To run the code\\n\\t$ cd python \\n\\t# on MultiNLI\\n\\t$ PYTHONHASHSEED=0 python train_mnli.py DIIN demo_testing \\n\\t# on SNLI\\n\\t$ PYTHONHASHSEED=0 python train_mnli.py DIIN demo_testing_SNLI --training_completely_on_snli\\n\\n\\n\\n\\n## License\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use these files except in compliance with the License. You may obtain a copy of the License at\\n\\n[http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\\n\\n'},\n",
       " {'repo': 'EtienneLamoureux/TQVaultAE',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# TQVaultAE\\n[![Steam](https://img.shields.io/badge/steam-link-lightgrey.svg)](https://steamcommunity.com/sharedfiles/filedetails/?id=1136716167)\\n[![Release](https://img.shields.io/badge/stable-4.3.0-blue.svg)](https://github.com/EtienneLamoureux/TQVaultAE/releases)\\n[![License](https://img.shields.io/badge/license-MIT-green.svg)](https://github.com/EtienneLamoureux/TQVaultAE/blob/master/LICENSE)\\n\\nTQVaultAE is an external tool for [Titan Quest Anniversary Edition](https://www.thqnordic.com/games/titan-quest) that allows you to store and search your items outside the game.\\nWorks with all expansions!\\n\\n![TQVaultAE screenshot](https://raw.githubusercontent.com/EtienneLamoureux/TQVaultAE/master/documentation/screenshot.PNG \"Hey, I can see my inventory from here!\")\\n\\n## Features\\n- **Infinite bank space**\\n- [Powerful search](documentation/ADVANCEDSEARCH.md)\\n- Cheats \\n    - Items\\n        - [Extract relic/charm from items at no cost, keeping both](documentation/AFFIXES.md#relics-removal)\\n        - [Modify the relic/charm/artefact completion bonus](documentation/AFFIXES.md#relic-and-charm-completion-bonus-change)\\n        - [Complete relic/charm from a single piece](documentation/AFFIXES.md#relic-and-charm-completion)\\n        - [Craft an artifact from its recipe](documentation/AFFIXES.md#artefact-creation)\\n        - [Change item seed](documentation/AFFIXES.md#item-seed-change)\\n        - [Create missing set pieces](documentation/AFFIXES.md#create-missing-set-pieces)\\n        - [Craft custom items](documentation/FORGE.md)\\n        - [Change items affixes](documentation/AFFIXES.md#prefix-change)\\n        - Duplicate any item\\n    - Characters\\n        - Redisribute attribute points\\n        - Unlock difficulties\\n        - Level up\\n- [Support of Titan Quest 2006](documentation/TQORIGINAL.md)\\n- QOL\\n    - [Cloud saving](documentation/GITBACKUP.md)\\n    - Bulk item transfer (<kbd>CTRL</kbd>+click, right-click)\\n    - [Highlight items](documentation/HIGHLIGHT.md)\\n    - Combine stacks (potions, relics and charms) by dropping them onto each other\\n    - Split potion stacks apart\\n    - Keyboard shortcuts\\n        - <kbd>CTRL</kbd>+<kbd>F</kbd>  : Open search form\\n        - <kbd>CTRL</kbd>+<kbd>+</kbd> : Increase vault size\\n        - <kbd>CTRL</kbd>+<kbd>-</kbd> : Reduce vault size\\n        - <kbd>CTRL</kbd>+<kbd>Home</kbd> : Default vault size\\n        - <kbd>CTRL</kbd>+<kbd>A</kbd> : Select all items in the vault\\n        - <kbd>CTRL</kbd>+<kbd>D</kbd> : De-select all selected items\\n        - <kbd>BACKSPACE</kbd> : Deletes currently hightlighted item\\n        - <kbd>C</kbd> : Copies currently hightlighted item and <b>randomizes the item seed</b> on the new item\\n        - <kbd>D</kbd> : Duplicates currently hightlighted item and <b>keeps the same item seed</b> on the new item\\n        - <kbd>&#8592;</kbd> : Moves currently hightlighted item(s) to vault panel\\n        - <kbd>&#8594;</kbd> : Moves currently hightlighted item(s) to player/secondary vault panel\\n        - <kbd>&#8595;</kbd> : Moves currently hightlighted item(s) to stash panel\\n        - <kbd>CTRL</kbd>+click : Activate multi selection\\n        - <kbd>SHIFT</kbd>+drag : Activate mouse lasso for multi selection\\n- [Character management](documentation/CHARMANAGE.md)\\n- Character backups\\n    - If an error occurs, backups are located at `My Documents\\\\My Games\\\\Titan Quest\\\\TQVaultData\\\\Backup`\\n- External tools\\n    - [ARZ Explorer](documentation/ARZEXPLORER.md) : Game resource file exploration\\n    - [Save File Explorer](documentation/SAVEFILEEXPLORER.md) : Game save file exploration\\n\\n## Installation\\n### Installer\\n1. Navigate to the [release page](https://github.com/EtienneLamoureux/TQVaultAE/releases)\\n2. Download the latest release\\'s `.exe` file.\\n    - **Please note:** If you opt for a pre-release, be aware that they are alpha builds.\\n3. Double-click the `.exe`.\\n4. Navigate to the folder where you installed TQVaultAE. Double-click `TQVaultAE.exe`\\n5. Enjoy!\\n\\n### DIY Archive\\n1. Navigate to the [release page](https://github.com/EtienneLamoureux/TQVaultAE/releases)\\n2. Download the latest release\\'s `.zip` file.\\n    - **Please note:** If you opt for a pre-release, be aware that they are alpha builds.\\n3. Extract the content of the archive on your computer.\\n4. Navigate to the folder where you extracted the artefacts. Double-click `TQVaultAE.exe`\\n5. Enjoy!\\n\\n### Configuration\\nThe \"Configure\" button (top-left) opens up the configuration menu. That\\'s where you can change:\\n- The language used by the application\\n- The paths where the vault files are located\\n- The paths where the game files are located\\n- The cheats (To enable/disable these options, see the F.A.Q. below)\\n- Every vault button can be customized using in-game pictures\\n\\n## Troubleshooting and F.A.Q.\\n**Q. Does TQVaultAE modify my items? The stats I see are not the same as the ones ingame.**\\n\\n*A. No, unless you specifically use the cheats, TQVaultAE doesn\\'t alter items in any way. The difference you see is simply due to the way stats are generated in Titan Quest: each item has base stats and a unique seed that modifies those stats. TQVaultAE only displays the base stats (and not the modifications due to the RNG).*\\n\\n**Q. Can I use TQVaultAE while playing the game?**\\n\\n*A. Only when using the \"Allow hot reload features\" setting, otherwise it is not safe to do so. Even then, be aware that any unsaved changes made in TQVaultAE will be lost when interacting with in-game inventories.*\\n\\n**Q. What happened to my items, I transferred items to my character and they are not there in game?**\\n\\n*A. If you are using the Steam version of the game, make sure Steam Cloud synchronization is disabled as it will overwrite local game saves modified by TQVaultAE with cloud older saves.*\\n\\n**Q. How to enable/disable the cheats (character edition, item edition, item copy)?**\\n\\n*A. There is a dedicated checkbox in the tool settings window.*\\n\\n**Q. Can TQVaultAE use my old vault files?**\\n\\n*A. Yes, TQVaultAE is compatible with the legacy TQvault vault files.*\\n\\n**Q. Error Loading Resources. This may be caused by a bad language or game path setting.**\\n\\n*A. Follow these steps:*\\n1. *Navigate the the installation folder of TQVaultAE*\\n2. *Open `UserConfig.xml` in a text editor (i.e. notepad, **not Microsoft Word**)*\\n3. *Replace the following sections:*\\n\\n```xml\\n<AutoDetectGamePath>1</AutoDetectGamePath>\\n...\\n<TQITPath></TQITPath>\\n<TQPath></TQPath>\\n```\\n\\n*by (replace the path to the correct one for your computer)*\\n\\n```xml\\n<AutoDetectGamePath>0</AutoDetectGamePath>\\n...\\n<TQITPath>C:\\\\examplePath\\\\Titan Quest Anniversary Edition</TQITPath>\\n<TQPath>C:\\\\examplePath\\\\Titan Quest Anniversary Edition</TQPath>\\n```\\n\\n4. *Open TQVaultAE*\\n    - *You might be greeted with a warning dialog about the vault path not being set. Click OK.*\\n5. *Open the configuration menu by clicking the top-left button*\\n6. *Validate the vault path and the game paths shown*\\n7. *Click OK to close the configuration menu*\\n\\n**Q. I have this game as a stand-alone (i.e. not through Steam or GOG). How can I make TQVaultAE work?**\\n\\n*A. See the answer to \"**Error Loading Resources. This may be caused by a bad language or game path setting.**\" above*\\n\\n**Q. Does TQVaultAE work with the Immortal Throne expansion?**\\n\\n*A. Yes*\\n\\n**Q. Does TQVaultAE work with the Ragnarok expansion?**\\n\\n*A. Yes*\\n\\n**Q. Does TQVaultAE work with the Atlantis expansion?**\\n\\n*A. Yes*\\n\\n**Q. Does TQVaultAE work with the Eternal Embers expansion?**\\n\\n*A. Yes*\\n\\n**Q. Can I still earn achievements while using TQVaultAE?**\\n\\n*A. Yes*\\n\\n**Q. How can i change my vault icons?**\\n<br />\\n<img src=\"./documentation/TQVaultAE_changeIcon.png\" width=\"700\" alt=\"Open the wizard\" />\\n<br />\\n<img src=\"./documentation/TQVaultAE_CustomIcon.png\" width=\"700\" alt=\"Customize your vault\" />\\n\\n**Q. How can i adjust the volume?**\\n\\n*A. You can enable/Disable the sounds in the tool settings window or adjust the volume via Windows Volume Mixer.*\\n\\n**Q. I have a problem not listed here. What can I do?**\\n\\n*A. There are several things you can do:*\\n- *Close TQVaultAE and open it up again. It may fix your problem*\\n- *Look up if your problem is featured in [our previously answered questions](https://github.com/EtienneLamoureux/TQVaultAE/issues?q=+is%3Aissue+label%3Aquestion+)*\\n- *Look up if your problem is featured in [TQVault\\'s documentation](https://github.com/EtienneLamoureux/TQVaultAE/blob/master/documentation/TQVault%20common%20issues.pdf)*\\n- *Create an issue in [our issue tracking board](https://github.com/EtienneLamoureux/TQVaultAE/issues)*\\n\\n## Contributors\\nThis project could not go on without the continued volunteer contributions of the Titan Quest community. If you\\'re thinking about contributing, please read our [contributing guidelines](/CONTRIBUTING.md).\\n\\n### TQVaultAE\\n- [Open-source contributors](https://github.com/EtienneLamoureux/TQVaultAE/graphs/contributors)\\n\\n### TQVault\\n- Brandon \"bman654\" Wallace, *original author*\\n- saydc, *item stats*\\n- Jesse \"VillageIdiot/EJFudd\" Calhoun, *item stats & ARZExplorer util*\\n- AvunaOs, *new UI*\\n\\n#### Translation team\\n- FOE, *german*\\n- Jean, *French*\\n- Vifarc, *French*\\n- Cygi, *Polish*\\n- Xelat, *Russian*\\n- Kurrus, *Spanish*\\n- Klauhs, *Portuguese*\\n\\n## Disclaimer\\nTitan Quest, THQ and their respective logos are trademarks and/or registered trademarks of THQ Nordic AB. This non-commercial project is in no way associated with THQ Nordic AB.\\n'},\n",
       " {'repo': 'streetmix/streetmix',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '<p align=\"center\">\\n  <a href=\"http://streetmix.net/\">\\n    <img width=\"450\" alt=\"Streetmix\" src=\"https://user-images.githubusercontent.com/2553268/62242301-dc18c800-b3a8-11e9-9960-6f6cbac50234.png\">\\n  </a>\\n</p>\\n\\n<p align=\"center\">\\n  <b>Streetmix is a collaborative process for communities and city planners to improve public spaces.</b> \\n  <br>Design, remix, and share your neighborhood street at <a href=\"https://streetmix.net/\">streetmix.net</a>.\\n</p>\\n\\n<p align=\"center\">\\n  :couple: :palm_tree: :oncoming_automobile: :oncoming_bus: :palm_tree: :dancer:\\n</p>\\n\\n<p align=\"center\">\\n  <br><b><a href=\"https://strt.mx/discord\">Join our community on Discord!</a></b>\\n</p>\\n\\n<p align=\"center\">\\n  <b>We welcome contributions!</b>\\n  <br>Please see our <a href=\"https://docs.streetmix.net/contributing/intro\">contributor guidelines</a>.\\n</p>\\n\\n<p align=\"center\">\\n  <a href=\"https://github.com/streetmix/streetmix/actions/workflows/ci.yml\"><img alt=\"Build status\" src=\"https://img.shields.io/github/workflow/status/streetmix/streetmix/Continuous%20integration%20(CI)%20testing/main?style=for-the-badge\"></a>\\n  <a href=\"https://codeclimate.com/github/streetmix/streetmix\"><img alt=\"Code Climate\" src=\"https://img.shields.io/codeclimate/tech-debt/streetmix/streetmix.svg?style=for-the-badge\"></a>\\n  <a href=\"https://codecov.io/gh/streetmix/streetmix\"><img alt=\"Code coverage\" src=\"https://img.shields.io/codecov/c/gh/streetmix/streetmix.svg?label=test%20coverage&style=for-the-badge\"></a>\\n</p>\\n\\n<hr>\\n\\n<p align=\"center\">\\n  <img src=\"https://github.com/streetmix/streetmix/raw/main/docs/static/screenshot.png\" alt=\"screenshot\">\\n</p>\\n\\n## About\\n\\n#### What are street sections?\\n\\nA \"section\" is shortened way of saying \"cross-section view\", a type of 2D non-perspectival drawing commonly used in engineering and architecture to show what something looks like when you take a slice of it and look at it head-on. Similarly, a street section is a cross section view of a street, showing the widths and placement of vehicle lanes, bike lanes, sidewalks, trees, street furniture or accessories (like benches or street lamps), as well as engineering information like how the road is sloped to facilitate drainage, or the locations of underground utilities. Although sections can be simplified line drawings, urban designers and landscape architects have created very colorful illustrative street sections, removing most of the engineering particulars to communicate how a street could be designed to feel safe, walkable or habitable.\\n\\n![example-sections](docs/static/thumb_sections.png \"Left to Right: (1) Existing conditions section of Market Street, from the Better Market Street Plan, San Francisco (2) Proposed one-way cycletrack design of Second Street, from the Great Second Street Plan, San Francisco (3)Example of an illustrative section, courtesy of Lou Huang\")\\n\\n#### Why does Streetmix exist?\\n\\nWhen city planners seek input from community meetings from the public on streetscape improvements, one common engagement activity is to create paper cut-outs depicting different street components (like bike lanes, sidewalks, trees, and so on) and allow attendees to reassemble them into their desired streetscape. Planners and city officials can then take this feedback to determine a course of action for future plans. By creating an web-based version of this activity, planners can reach a wider audience than they could at meetings alone, and allow community members to share and remix each other\\'s creations.\\n\\nThe goal is to promote two-way communication between planners and the public, as well. Streetmix intends to communicate not just feedback to planners but also information and consequences of actions to the users that are creating streets. Kind of like SimCity did with its in-game advisors!\\n\\nStreetmix can be used as a tool to promote and engage citizens around streetscape and placemaking issues, such as [Complete Streets][completestreets] or the Project for Public Spaces\\' [Rightsizing Streets Guide][rightsizing].\\n\\n[completestreets]: https://smartgrowthamerica.org/program/national-complete-streets-coalition/\\n[rightsizing]: http://www.pps.org/reference/rightsizing/\\n\\n#### Why the name \"Streetmix\"?\\n\\n\"Streets\" + \"remix\" :-)\\n\\n#### How did this project start?\\n\\nStreetmix started as a [Code for America][cfa] hackathon project in January 2013, inspired by community meetings like the one described above.\\n\\n[cfa]: https://codeforamerica.org/\\n\\n#### How do I install / set up Streetmix myself?\\n\\nStreetmix is a [Node.js](https://nodejs.org/) based project. Set up your own by [following these instructions](https://docs.streetmix.net/contributing/code/local-setup)!\\n\\n## Sponsors\\n\\n<p align=\"center\">\\n  <a href=\"https://opencollective.com/streetmix\"><img src=\"https://opencollective.com/streetmix/sponsors.svg\" alt=\"Become a sponsor\"></a>\\n</p>\\n\\n## Copyright\\n\\nCopyright (c) 2013-2018 Code for America and contributors.  \\nCopyright (c) 2019-2023 Streetmix LLC.  \\nSee [LICENSE][] for details.\\n\\n[license]: https://github.com/streetmix/streetmix/blob/main/LICENSE\\n\\nStreetmix is maintained by [Bad Idea Factory](https://biffud.com/) with the support of many contributors.\\n'},\n",
       " {'repo': 'Mukosame/Zooming-Slow-Mo-CVPR-2020',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Zooming-Slow-Mo (CVPR-2020)\\n\\nBy [Xiaoyu Xiang<sup>\\\\*</sup>](https://engineering.purdue.edu/people/xiaoyu.xiang.1), [Yapeng Tian<sup>\\\\*</sup>](http://yapengtian.org/), [Yulun Zhang](http://yulunzhang.com/), [Yun Fu](http://www1.ece.neu.edu/~yunfu/), [Jan P. Allebach<sup>+</sup>](https://engineering.purdue.edu/~allebach/), [Chenliang Xu<sup>+</sup>](https://www.cs.rochester.edu/~cxu22/) (<sup>\\\\*</sup> equal contributions, <sup>+</sup> equal advising)\\n\\nThis is the official Pytorch implementation of _Zooming Slow-Mo: Fast and Accurate One-Stage Space-Time Video Super-Resolution_.\\n\\n#### [Paper](https://arxiv.org/abs/2002.11616) | [Journal Version](https://arxiv.org/abs/2104.07473) | [Demo (YouTube)](https://youtu.be/8mgD8JxBOus) | [1-min teaser (YouTube)](https://www.youtube.com/watch?v=C1o85AXUNl8) | [1-min teaser (Bilibili)](https://www.bilibili.com/video/BV1GK4y1t7nb/)\\n\\n<table>\\n  <thead>\\n    <tr>\\n      <td>Input&nbsp;&nbsp;&nbsp;&nbsp;</td>\\n      <td>Output</td>\\n    </tr>\\n  </thead>\\n  <tr>\\n    <td colspan=\"2\">\\n      <a href=\"https://youtu.be/8mgD8JxBOus\">\\n        <img src=\"dump/demo720.gif\" alt=\"Demo GIF\">\\n        </img>\\n      </a>\\n    </td>\\n  </tr>\\n</table>\\n\\n## Updates\\n\\n- 2020.3.13 Add meta-info of datasets used in this paper\\n- 2020.3.11 Add new function: video converter\\n- 2020.3.10: Upload the complete code and pretrained models\\n\\n## Contents\\n\\n0. [Introduction](#introduction)\\n1. [Prerequisites](#Prerequisites)\\n2. [Get Started](#Get-Started)\\n   - [Installation](#Installation)\\n   - [Training](#Training)\\n   - [Testing](#Testing)\\n   - [Colab Notebook](#Colab-Notebook)\\n3. [Citations](#citations)\\n4. [Contact](#Contact)\\n5. [License](#License)\\n6. [Acknowledgments](#Acknowledgments)\\n\\n## Introduction\\n\\nThe repository contains the entire project (including all the preprocessing) for one-stage space-time video super-resolution with Zooming Slow-Mo.\\n\\nZooming Slow-Mo is a recently proposed joint video frame interpolation (VFI) and video super-resolution (VSR) method, which directly synthesizes an HR slow-motion video from an LFR, LR video. It is going to be published in [CVPR 2020](http://cvpr2020.thecvf.com/). The most up-to-date paper with supplementary materials can be found at [arXiv](https://arxiv.org/abs/2002.11616).\\n\\nIn Zooming Slow-Mo, we firstly temporally interpolate features of the missing LR frame by the proposed feature temporal interpolation network. Then, we propose a deformable ConvLSTM to align and aggregate temporal information simultaneously. Finally, a deep reconstruction network is adopted to predict HR slow-motion video frames. If our proposed architectures also help your research, please consider citing our paper.\\n\\nZooming Slow-Mo achieves state-of-the-art performance by PSNR and SSIM in Vid4, Vimeo test sets.\\n\\n![framework](./dump/framework.png)\\n\\n## Prerequisites\\n\\n- Python 3 (Recommend to use [Anaconda](https://www.anaconda.com/download/#linux))\\n- [PyTorch >= 1.1](https://pytorch.org/)\\n- NVIDIA GPU + [CUDA](https://developer.nvidia.com/cuda-downloads)\\n- [Deformable Convolution v2](https://arxiv.org/abs/1811.11168), we adopt [CharlesShang\\'s implementation](https://github.com/CharlesShang/DCNv2) in the submodule.\\n- Python packages: `pip install numpy opencv-python lmdb pyyaml pickle5 matplotlib seaborn`\\n\\n## Get Started\\n\\n### Installation\\n\\nInstall the required packages: `pip install -r requirements.txt`\\n\\nFirst, make sure your machine has a GPU, which is required for the DCNv2 module.\\n\\n1. Clone the Zooming Slow-Mo repository. We\\'ll call the directory that you cloned Zooming Slow-Mo as ZOOMING_ROOT.\\n\\n```Shell\\ngit clone --recursive https://github.com/Mukosame/Zooming-Slow-Mo-CVPR-2020.git\\n```\\n\\n2. Compile the DCNv2:\\n\\n```Shell\\ncd $ZOOMING_ROOT/codes/models/modules/DCNv2\\nbash make.sh         # build\\npython test.py    # run examples and gradient check\\n```\\n\\nPlease make sure the test script finishes successfully without any errors before running the following experiments.\\n\\n### Training\\n\\n#### Part 1: Data Preparation\\n\\n1. Download the original training + test set of `Vimeo-septuplet` (82 GB).\\n\\n```Shell\\nwget http://data.csail.mit.edu/tofu/dataset/vimeo_septuplet.zip\\napt-get install unzip\\nunzip vimeo_septuplet.zip\\n```\\n\\n2. Split the `Vimeo-septuplet` into a training set and a test set, make sure you change the dataset\\'s path to your download path in script, also you need to run for the training set and test set separately:\\n\\n```Shell\\ncd $ZOOMING_ROOT/codes/data_scripts/sep_vimeo_list.py\\n```\\n\\nThis will create `train` and `test` folders in the directory of **`vimeo_septuplet/sequences`**. The folder structure is as follows:\\n\\n```\\nvimeo_septuplet\\n├── sequences\\n    ├── 00001\\n        ├── 0266\\n            ├── im1.png\\n            ├── ...\\n            ├── im7.png\\n        ├── 0268...\\n    ├── 00002...\\n├── readme.txt\\n├──sep_trainlist.txt\\n├── sep_testlist.txt\\n```\\n\\n3. Generate low resolution (LR) images. You can either do this via MATLAB or Python (remember to configure the input and output path):\\n\\n```Matlab\\n# In Matlab Command Window\\nrun $ZOOMING_ROOT/codes/data_scripts/generate_LR_Vimeo90K.m\\n```\\n\\n```Shell\\npython $ZOOMING_ROOT/codes/data_scripts/generate_mod_LR_bic.py\\n```\\n\\n4. Create the LMDB files for faster I/O speed. Note that you need to configure your input and output path in the following script:\\n\\n```Shell\\npython $ZOOMING_ROOT/codes/data_scripts/create_lmdb_mp.py\\n```\\n\\nThe structure of generated lmdb folder is as follows:\\n\\n```\\nVimeo7_train.lmdb\\n├── data.mdb\\n├── lock.mdb\\n├── meta_info.txt\\n```\\n\\n#### Part 2: Train\\n\\n**Note:** In this part, we assume you are in the directory **`$ZOOMING_ROOT/codes/`**\\n\\n1. Configure your training settings that can be found at [options/train](./codes/options/train). Our training settings in the paper can be found at [train_zsm.yml](https://github.com/Mukosame/Zooming-Slow-Mo-CVPR-2020/blob/master/codes/options/train/train_zsm.yml). We\\'ll take this setting as an example to illustrate the following steps.\\n\\n2. Train the Zooming Slow-Mo model.\\n\\n```Shell\\npython train.py -opt options/train/train_zsm.yml\\n```\\n\\nAfter training, your model `xxxx_G.pth` and its training states, and a corresponding log file `train_LunaTokis_scratch_b16p32f5b40n7l1_600k_Vimeo_xxxx.log` are placed in the directory of `$ZOOMING_ROOT/experiments/LunaTokis_scratch_b16p32f5b40n7l1_600k_Vimeo/`.\\n\\n### Testing\\n\\nWe provide the test code for both standard test sets (Vid4, SPMC, etc.) and custom video frames.\\n\\n#### Pretrained Models\\n\\nOur pretrained model can be downloaded via [GitHub](https://github.com/Mukosame/Zooming-Slow-Mo-CVPR-2020/blob/master/experiments/pretrained_models/xiang2020zooming.pth) or [Google Drive](https://drive.google.com/open?id=1xeOoZclGeSI1urY6mVCcApfCqOPgxMBK).\\n\\n#### From Video\\n\\nIf you have installed ffmpeg, you can convert any video to a high-resolution and high frame-rate video using [video_to_zsm.py](./codes/video_to_zsm.py). The corresponding commands are:\\n\\n```Shell\\ncd $ZOOMING_ROOT/codes\\npython video_to_zsm.py --video PATH/TO/VIDEO.mp4 --model PATH/TO/PRETRAINED/MODEL.pth --output PATH/TO/OUTPUT.mp4\\n```\\n\\nWe also write the above commands to a Shell script, so you can directly run:\\n\\n```Shell\\nbash zsm_my_video.sh\\n```\\n\\n#### From Extracted Frames\\n\\nAs a quick start, we also provide some example images in the [test_example](./test_example) folder. You can test the model with the following commands:\\n\\n```Shell\\ncd $ZOOMING_ROOT/codes\\npython test.py\\n```\\n\\n- You can put your own test folders in the [test_example](./test_example) too, or just change the input path, the number of frames, etc. in [test.py](codes/test.py).\\n\\n- Your custom test results will be saved to a folder here: `$ZOOMING_ROOT/results/your_data_name/`.\\n\\n#### Evaluate on Standard Test Sets\\n\\nThe [test.py](codes/test.py) script also provides modes for evaluation on the following test sets: `Vid4`, `SPMC`, etc. We evaluate PSNR and SSIM on the Y-channels in YCrCb color space. The commands are the same with the ones above. All you need to do is the change the data_mode and corresponding path of the standard test set.\\n\\n### Colab Notebook\\n\\nPyTorch Colab notebook (provided by [@HanClinto](https://github.com/HanClinto)): [HighResSlowMo.ipynb](https://gist.github.com/HanClinto/49219942f76d5f20990b6d048dbacbaf)\\n\\n## Citations\\n\\nIf you find the code helpful in your resarch or work, please cite the following papers.\\n\\n```BibTex\\n@misc{xiang2021zooming,\\n  title={Zooming SlowMo: An Efficient One-Stage Framework for Space-Time Video Super-Resolution},\\n  author={Xiang, Xiaoyu and Tian, Yapeng and Zhang, Yulun and Fu, Yun and Allebach, Jan P and Xu, Chenliang},\\n  archivePrefix={arXiv},\\n  eprint={2104.07473},\\n  year={2021},\\n  primaryClass={cs.CV}\\n}\\n\\n@InProceedings{xiang2020zooming,\\n  author = {Xiang, Xiaoyu and Tian, Yapeng and Zhang, Yulun and Fu, Yun and Allebach, Jan P. and Xu, Chenliang},\\n  title = {Zooming Slow-Mo: Fast and Accurate One-Stage Space-Time Video Super-Resolution},\\n  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n  pages={3370--3379},\\n  month = {June},\\n  year = {2020}\\n}\\n\\n@InProceedings{tian2018tdan,\\n  author={Yapeng Tian, Yulun Zhang, Yun Fu, and Chenliang Xu},\\n  title={TDAN: Temporally Deformable Alignment Network for Video Super-Resolution},\\n  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n  pages={3360--3369},\\n  month = {June},\\n  year = {2020}\\n}\\n\\n@InProceedings{wang2019edvr,\\n  author    = {Wang, Xintao and Chan, Kelvin C.K. and Yu, Ke and Dong, Chao and Loy, Chen Change},\\n  title     = {EDVR: Video restoration with enhanced deformable convolutional networks},\\n  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\\n  month     = {June},\\n  year      = {2019},\\n}\\n```\\n\\n## Contact\\n\\n[Xiaoyu Xiang](https://engineering.purdue.edu/people/xiaoyu.xiang.1) and [Yapeng Tian](http://yapengtian.org/).\\n\\nYou can also leave your questions as issues in the repository. We will be glad to answer them.\\n\\n## License\\n\\nThis project is released under the [GNU General Public License v3.0](https://github.com/Mukosame/Zooming-Slow-Mo-CVPR-2020/blob/master/LICENSE).\\n\\n## Acknowledgments\\n\\nOur code is inspired by [TDAN-VSR](https://github.com/YapengTian/TDAN-VSR) and [EDVR](https://github.com/xinntao/EDVR).\\n'},\n",
       " {'repo': 'XanaduAI/MrMustard',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '![Logo](https://github.com/XanaduAI/MrMustard/blob/main/mm_white.png#gh-light-mode-only)\\n![Logo](https://github.com/XanaduAI/MrMustard/blob/main/mm_dark.png#gh-dark-mode-only)\\n\\n[![Apache-2.0](https://img.shields.io/badge/License-Apache--2.0-blue)](https://opensource.org/licenses/Apache-2.0)\\n[![Actions Status](https://github.com/XanaduAI/MrMustard/workflows/Tests/badge.svg)](https://github.com/XanaduAI/MrMustard/actions)\\n[![Python version](https://img.shields.io/pypi/pyversions/mrmustard.svg?style=popout-square)](https://pypi.org/project/MrMustard/)\\n\\nMr Mustard is a differentiable simulator with a sophisticated built-in optimizer, that operates across phase space and Fock space.\\nIt is built on top of an agnostic autodiff interface, to allow for plug-and-play backends (TensorFlow by default, PyTorch coming soon).\\n\\nMr Mustard supports:\\n- Phase space representation of Gaussian states and Gaussian channels on an arbitrary number of modes\\n- Exact Fock representation of any Gaussian circuit and any Gaussian state up to an arbitrary cutoff\\n- Riemannian optimization on the symplectic group (for Gaussian transformations) and on the orthogonal group (for interferometers)\\n- Adam optimizer for euclidean parameters.\\n- single-mode gates (parallelizable):\\n    - squeezing, displacement, phase rotation, attenuator, amplifier, additive noise\\n- two-mode gates:\\n    - beam splitter, Mach-Zehnder interferometer, two-mode squeezing, CX, CZ, CPHASE\\n- N-mode gates (with dedicated Riemannian optimization):\\n    - interferometer (orthogonal), Gaussian transformation (symplectic)\\n- single-mode states (parallelizable):\\n    - vacuum, coherent, squeezed, displaced-squeezed, thermal\\n- two-mode states:\\n    - two-mode squeezed vacuum\\n- N-mode states:\\n    - Gaussian state\\n- Photon number moments and entropic measures\\n- PNR detectors and Threshold detectors with trainable quantum efficiency and dark counts\\n- Homodyne, Heterodyne and Generaldyne measurements\\n- Composable circuits\\n- Plug-and-play backends (TensorFlow as default)\\n- An abstraction layer `XPTensor` for seamless symplectic algebra (experimental)\\n\\n# The lab module\\nThe lab module contains things you\\'d find in a lab: states, transformations, measurements, circuits. States can be used at the beginning of a circuit as well as at the end, in which case a state is interpreted as a measurement (a projection onto that state). Transformations are usually parametrized and map states to states. The action on states is differentiable with respect to the state and to the gate parameters.\\n\\n\\n## 1. States and Gates\\nHere are a few examples of states and gates:\\n```python\\nimport numpy as np\\nfrom mrmustard.lab import *\\n\\nvac = Vacuum(num_modes=2)        # 2-mode vacuum state\\ncoh = Coherent(x=0.1, y=-0.4)    # coh state |alpha> with alpha = 0.1 - 0.4j\\nsq  = SqueezedVacuum(r=0.5)      # squeezed vacuum state\\ng   = Gaussian(num_modes=2)      # 2-mode Gaussian state with zero means\\nfock4 = Fock(4)                  # fock state |4>\\n\\nD  = Dgate(x=1.0, y=-0.4)         # Displacement by 1.0 along x and -0.4 along y\\nS  = Sgate(r=0.5)                 # Squeezer with r=0.5\\n\\nBS = BSgate(theta=np.pi/4)          # 50/50 beam splitter\\nS2 = S2gate(r=0.5)                  # two-mode squeezer\\nMZ = MZgate(phi_a=0.3, phi_b=0.1)   # Mach-Zehnder interferometer\\nI  = Interferometer(8)              # 8-mode interferometer\\nL  = Attenuator(0.5)                # pure lossy channel with 50% transmissivity\\nA  = Amplifier(gain=2.0, nbar=1.0)  # noisy amplifier with 200% gain\\n```\\n\\nThe `repr` of single-mode states shows the Wigner function:\\n<img width=\"571\" alt=\"Screen Shot 2021-12-06 at 1 31 17 PM\" src=\"https://user-images.githubusercontent.com/8944955/144902008-8d26d59c-8600-4391-9144-ffcc1b2215c2.png\">\\n\\n```python\\ncat_amps = Coherent(2.0).ket([20]) + Coherent(-2.0).ket([20])\\ncat_amps = cat_amps / np.linalg.norm(cat_amps)\\ncat = State(ket=cat_amps)\\ncat\\n```\\n<img width=\"538\" alt=\"Screen Shot 2021-12-06 at 8 27 06 PM\" src=\"https://user-images.githubusercontent.com/8944955/144949009-ebf7bbf8-9240-406c-ab99-bf8c36acd3f7.png\">\\n\\nStates (even those in Fock representation) are always compatible with gates:\\n```python\\ncat >> Sgate(0.5)  # squeezed cat\\n```\\n<img width=\"479\" alt=\"Screen Shot 2021-12-07 at 2 03 14 PM\" src=\"https://user-images.githubusercontent.com/8944955/145090219-298ca2ab-92e9-4ac2-beab-33ee33770fb2.png\">\\n\\n\\n\\n\\n## 2. Gates and the right shift operator `>>`\\n\\nApplying gates to states looks natural, thanks to python\\'s right-shift operator `>>`:\\n```python\\ndisplaced_squeezed = Vacuum(1) >> Sgate(r=0.5) >> Dgate(x=1.0)\\n```\\n\\nIf you want to apply a gate to specific modes, use the `getitem` format. Here are a few examples:\\n```python\\nD = Dgate(y=-0.4)\\nS = Sgate(r=0.1, phi=0.5)\\nstate = Vacuum(2) >> D[1] >> S[0]  # displacement on mode 1 and squeezing on mode 0\\n\\nBS = BSgate(theta=1.1)\\nstate = Vacuum(3) >> BS[0,2]  # applying a beamsplitter to modes 0 and 2\\nstate = Vacuum(4) >> S[0,1,2]  # applying the same Sgate in parallel to modes 0, 1 and 2 but not to mode 3\\n```\\n\\n## 3. Circuit\\nWhen chaining just gates with the right-shift `>>` operator, we create a circuit:\\n```python\\nX8 = Sgate(r=[1.0] * 4) >> Interferometer(4)\\noutput = Vacuum(4) >> X8\\n\\n# lossy X8\\nnoise = lambda: np.random.uniform(size=4)\\nX8_realistic = (Sgate(r=0.9 + 0.1*noise(), phi=0.1*noise())\\n                >> Attenuator(0.89 + 0.01*noise())\\n                >> Interferometer(4)\\n                >> Attenuator(0.95 + 0.01*noise())\\n               )\\n\\n# 2-mode Bloch Messiah decomposition\\nbloch_messiah = Sgate(r=[0.1,0.2]) >> BSgate(-0.1, 2.1) >> Dgate(x=[0.1, -0.4])\\nmy_state = Vacuum(2) >> bloch_messiah\\n```\\n\\n## 4. Measurements\\nIn order to perform a measurement, we use the left-shift operator, e.g. `coh << sq` (think of the left-shift on a state as \"closing\" the circuit).\\n```python\\nleftover = Vacuum(4) >> X8 << SqueezedVacuum(r=10.0, phi=np.pi)[2]  # a homodyne measurement of p=0.0 on mode 2\\n```\\n\\nTransformations can also be applied in the dual sense by using the left-shift operator `<<`:\\n```python\\nAttenuator(0.5) << Coherent(0.1, 0.2) == Coherent(0.1, 0.2) >> Amplifier(2.0)\\n```\\nThis has the advantage of modelling lossy detectors without applying the loss channel to the state going into the detector, which can be overall faster e.g. if the state is kept pure by doing so.\\n\\n## 5. Detectors\\nThere are two types of detectors in Mr Mustard. Fock detectors (PNRDetector and ThresholdDetector) and Gaussian detectors (Homodyne, Heterodyne). However, Gaussian detectors are a thin wrapper over just Gaussian states, as Gaussian states can be used as projectors (i.e. `state << DisplacedSqueezed(...)` is how Homodyne performs a measurement).\\n\\nThe PNR and Threshold detectors return an array of unnormalized measurement results, meaning that the elements of the array are the density matrices of the leftover systems, conditioned on the outcomes:\\n```python\\nresults = Gaussian(2) << PNRDetector(efficiency = 0.9, modes = [0])\\nresults[0]  # unnormalized dm of mode 1 conditioned on measuring 0 in mode 0\\nresults[1]  # unnormalized dm of mode 1 conditioned on measuring 1 in mode 0\\nresults[2]  # unnormalized dm of mode 1 conditioned on measuring 2 in mode 0\\n# etc...\\n```\\nThe trace of the leftover density matrices will yield the success probability. If multiple modes are measured then there is a corresponding number of indices:\\n```python\\nresults = Gaussian(3) << PNRDetector(efficiency = [0.9, 0.8], modes = [0,1])\\nresults[2,3]  # unnormalized dm of mode 2 conditioned on measuring 2 in mode 0 and 3 in mode 1\\n# etc...\\n```\\nSet a lower `settings.PNR_INTERNAL_CUTOFF` (default 50) to speed-up computations of the PNR output.\\n\\n## 6. Comparison operator `==`\\nStates support the comparison operator:\\n```python\\n>>> bunched = (Coherent(1.0) & Coherent(1.0)) >> BSgate(np.pi/4)\\n>>> bunched.get_modes(1) == Coherent(np.sqrt(2.0))\\nTrue\\n```\\nAs well as transformations (gates and circuits):\\n```python\\n>>> Dgate(np.sqrt(2)) >> Attenuator(0.5) == Attenuator(0.5) >> Dgate(1.0)\\nTrue\\n```\\n\\n## 7. State operations and properties\\nStates can be joined using the `&` (and) operator:\\n```python\\nCoherent(x=1.0, y=1.0) & Coherent(x=2.0, y=2.0)  # A separable two-mode coherent state\\n\\ns = SqueezedVacuum(r=1.0)\\ns4 = s & s & s & s   # four squeezed states\\n```\\n\\nSubsystems can be accessed via `get_modes`:\\n```python\\njoint = Coherent(x=1.0, y=1.0) & Coherent(x=2.0, y=2.0)\\njoint.get_modes(0)  # first mode\\njoint.get_modes(1)  # second mode\\n\\nswapped = joint.get_modes([1,0])\\n```\\n\\n## 8. Fock representation\\nThe Fock representation of a State is obtained via `.ket(cutoffs)` or `.dm(cutoffs)`. For circuits and gates it\\'s `.U(cutoffs)` or `.choi(cutoffs)`. The Fock representation is exact (with minor caveats) and it doesn\\'t break differentiability. This means that one can define cost functions on the Fock representation and backpropagate back to the phase space representation.\\n\\n```python\\n# Fock representation of a coherent state\\nCoherent(0.5).ket(cutoffs=[5])   # ket\\nCoherent(0.5).dm(cutoffs=[5])    # density matrix\\n\\nDgate(x=1.0).U(cutoffs=[15])  # truncated unitary op\\nDgate(x=1.0).choi(cutoffs=[15])  # truncated choi op\\n```\\n\\nStates can be initialized in Fock representation and used as any other state:\\n```python\\nmy_amplitudes = np.array([0.5, 0.25, -0.5, 0.25, 0.25, 0.5, -0.25] + [0.0]*23)  # notice the buffer\\nmy_state = State(ket=my_amplitudes)\\nmy_state >> Sgate(r=0.5)  # just works\\n```\\n<img width=\"542\" alt=\"Screen Shot 2021-12-06 at 1 44 38 PM\" src=\"https://user-images.githubusercontent.com/8944955/144903799-5b6c1524-4357-4be0-9778-e1f0de6943c1.png\">\\n\\nAlternatively,\\n```python\\nmy_amplitudes = np.array([0.5, 0.25, -0.5, 0.25, 0.25, 0.5, -0.25])  # no buffer\\nmy_state = State(ket=my_amplitudes)\\nmy_state._cutoffs = [42]  # force the cutoff\\nmy_state >> Sgate(r=0.5)  # works too\\n```\\n\\n# The physics module\\nThe physics module contains a growing number of functions that we can apply to states directly. These are made out of the functions that operate on the _representation_ of the state:\\n\\n- If the state is in Gaussian representation, then internally the physics functions utilize the [physics.gaussian](https://github.com/XanaduAI/MrMustard/blob/main/mrmustard/physics/gaussian.py) module.\\n- If the state is in Fock representation, then internally the physics functions utilize the [physics.fock](https://github.com/XanaduAI/MrMustard/blob/main/mrmustard/physics/fock.py) module.\\n\\n\\n# The math module\\nThe math module is the backbone of Mr Mustard, which consists in the [`Math`](https://github.com/XanaduAI/MrMustard/blob/main/mrmustard/math/math_interface.py) interface\\nMr Mustard comes with a plug-and-play backends through a math interface. You can use it as a drop-in replacement for tensorflow or pytorch and your code will be plug-and-play too!\\n```python\\nfrom mrmustard import settings\\nfrom mrmustard.math import Math\\nmath = Math()\\n\\nmath.cos(0.1)  # tensorflow\\n\\nsettings.BACKEND = \\'torch\\'\\n\\nmath.cos(0.1)  # pytorch (upcoming)\\n```\\n\\n### Optimization\\nThe `Optimizer` (available in `mrmustard.training` uses Adam underneath the hood for Euclidean parameters and a custom symplectic optimizer for Gaussian gates and states and an orthogonal optimizer  for interferometers.\\n\\nWe can turn any simulation in Mr Mustard into an optimization by marking which parameters we wish to be trainable. Let\\'s take a simple example: synthesizing a\\ndisplaced squeezed state.\\n\\n```python\\nfrom mrmustard.lab import Dgate, Ggate, Attenuator, Vacuum, Coherent, DisplacedSqueezed\\nfrom mrmustard.physics import fidelity\\nfrom mrmustard.training import Optimizer\\n\\nD = Dgate(x = 0.1, y = -0.5, x_trainable=True, y_trainable=True)\\nL = Attenuator(transmissivity=0.5)\\n\\n# we write a function that takes no arguments and returns the cost\\ndef cost_fn_eucl():\\n    state_out = Vacuum(1) >> D >> L\\n    return 1 - fidelity(state_out, Coherent(0.1, 0.5))\\n\\nG = Ggate(num_modes=1, symplectic_trainable=True)\\ndef cost_fn_sympl():\\n    state_out = Vacuum(1) >> G >> D >> L\\n    return 1 - fidelity(state_out, DisplacedSqueezed(r=0.3, phi=1.1, x=0.4, y=-0.2))\\n\\nopt = Optimizer(symplectic_lr=0.1, euclidean_lr=0.01)\\nopt.minimize(cost_fn_eucl, by_optimizing=[D])  # using Adam for D\\n\\nopt = Optimizer(symplectic_lr=0.1, euclidean_lr=0.01)\\nopt.minimize(cost_fn_sympl, by_optimizing=[G,D])  # using Adam for D and the symplectic opt for G\\n```'},\n",
       " {'repo': 'iblasquez/tdd_spaceInvaders',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '# Space Invaders en TDD\\n\\n**L\\'objectif** de ce mini-projet est de développer un jeu de **space invaders** en **TDD**.\\n\\nDans ce README, vous trouverez les rubriques suivantes :\\n\\n- [A propos du jeu *Space Invaders*](#aProposSpaceInvaders)   \\n- [A propos du TDD (*T*est *D*riven *D*evelopment)](#aProposTDD)\\n- [Organisation du mini-projet (accès aux différents sprints)](#organisation)\\n- [Références](#references) \\n- [Commentaires, remarques et suggestions](#commentaires)  \\n- [Licence](#licence)\\n\\n## A propos du jeu *Space Invaders* <a id=\"aProposSpaceInvaders\"></a>\\n\\nSpace Invader est un **jeu de tir spatial** (*shoot\\'em up*).\\n\\nLe système du jeu est le suivant (Extrait Wikipédia):\\n\\n> [Space Invaders est un jeu fixe en deux dimensions.    \\n> Le joueur contrôle un canon laser qu\\'il peut déplacer horizontalement, au bas de l\\'écran.    \\n> Dans les airs, des rangées d\\'aliens se déplacent latéralement tout en se rapprochant progressivement du sol et en lançant des missiles.    \\n> L\\'objectif est de détruire avec le canon laser une vague ennemie, qui se compose de cinq rangées de onze aliens chacune, avant qu\\'elle n\\'atteigne le bas de l\\'écran.    \\n> Le joueur gagne des points à chaque fois qu\\'il détruit un envahisseur.   \\n> Le jeu n\\'autorise qu\\'un tir à la fois et permet d\\'annuler ceux des ennemis en tirant dessus.    \\n> La vitesse et la musique s\\'accélèrent au fur et à mesure que le nombre d\\'aliens diminue.     \\n> L\\'élimination totale de ces derniers amène une nouvelle vague ennemie plus difficile, et ce indéfiniment.   \\n> Le jeu ne se termine que lorsque le joueur perd, ce qui en fait le premier jeu sans fin.\\n> Les aliens tentent de détruire le canon en tirant dessus pendant qu\\'ils s\\'approchent du bas de l\\'écran.\\n> S\\'ils l\\'atteignent ou arrivent jusqu\\'au sol, ils ont réussi leur invasion et le jeu est fini.  \\n> De temps en temps, un vaisseau spatial apparait tout en haut de l\\'écran et fait gagner des points bonus s\\'il est détruit. \\n> Quatre bâtiments destructibles permettent au joueur de se protéger des tirs ennemis.   \\n> Ces défenses se désintègrent progressivement sous l\\'effet des projectiles adverses et de ceux du joueur.   \\n> Le nombre de bâtiments n\\'est pas le même d\\'une version à l\\'autre.](https://fr.wikipedia.org/wiki/Space_Invaders)\\n\\n\\n\\n\\n*Remarques* :   \\n\\n  \\n- Le site [http://www.classicgaming.cc/classics/space-invaders](http://www.classicgaming.cc/classics/space-invaders) est dédié à **Space Invaders**, on y trouve : [un descriptif détaillé du jeu](http://www.classicgaming.cc/classics/space-invaders/play-guide), des ressources graphiques, des ressources de son, ...\\n- Une définition de **jeu de tir spatial** peut être consulté dans le *Vocabulaire du jeu vidéo de Yolande Perron*  disponible [ici](https://www.oqlf.gouv.qc.ca/ressources/bibliotheque/dictionnaires/20120701_jeu_video.pdf).  \\n\\n\\n| <img src=\"https://upload.wikimedia.org/wikipedia/commons/8/80/Space_Invaders_style.png\" alt=\"Space Invaders\" width=\"400\">\\n<img src=\"http://www.classicgaming.cc/classics/space-invaders/images/space-invaders-screenshot-points-sm.jpg\" alt=\"Space Invaders sur Classic Game\" width=\"400\">  |  \\nSources : images extraites de [Wikipédia](https://fr.wikipedia.org/wiki/Space_Invaders) et de [ClassicGaming](http://www.classicgaming.cc/classics/space-invaders/play-guide)\\n\\n\\n\\n## A propos du TDD (*T*est *D*riven *D*evelopment) <a id=\"aProposTDD\"></a>\\n\\nLe ***développement dirigé par les tests*** (**T**est **D**riven **D**evelopment ou **TDD**) est une approche itérative et incrémentale de codage piloté par les tests unitaires. Un cycle de développement TDD se compose de trois étapes : \\n\\n\\n<img src=\"http://iblasquez.github.io/presentation_TDD_CodingDojo/TDD_Mantra.png\" alt=\"Mantra\" width=\"400\">\\n\\n\\n\\n- La première étape (**RED**) consiste à écrire un nouveau test unitaire et vérifier qu\\'il échoue : ce test apporte ainsi un nouveau comportement.\\n\\n- La deuxième étape (**GREEN**) consiste à écrire **au plus vite** un code de production pour faire passer le test précédent ainsi que les tests antérieurs. \\n\\n- La troisième étape (**REFACTOR**) est une phase de [refactoring](https://refactoring.com/) qui vise à faire émerger une [conception simple](http://referentiel.institut-agile.fr/yagni.html) afin d\\'améliorer la qualité de code.   \\n[4 critères de simplicité](http://referentiel.institut-agile.fr/simplicite.html)  ont été énoncés par Kent Beck (dans le cadre d\\'eXtreme Programming) :  \\n  - le code est doté de tests unitaires et fonctionnels et tous ces tests passent\\n  - le code ne fait apparaître aucune duplication\\n  - le code fait apparaître séparément chaque responsabilité distincte\\n  - le code contient le nombre minimum d\\'élément (classes, méthodes, lignes) compatible avec les trois premiers critères  \\n Gardez à l\\'esprit ces critères lorsque vous êtes dans une phase de refactoring.\\n\\nRemarque : Une présentation rapide du TDD est disponible [ici](http://iblasquez.github.io/presentation_TDD_CodingDojo)\\n\\n\\n## Organisation du mini-projet <a id=\"organisation\"></a>\\n\\nCe mini-projet est découpé en plusieurs **fonctionnalités** priorisées et regroupées en trois objectifs :\\n\\n\\n**Objectif n° 1 :  \\nUn Space Invaders *minimum*  : un vaisseau, un missile, un envahisseur (MVP)**\\n\\n\\n- [Fonctionnalité n°1 : Déplacer le vaisseau dans l\\'espace de jeu](enonces/SpaceInvaders_S1_DeplacerVaisseau.md)\\n- [Fonctionnalité n°2 : Dimensionner le vaisseau](enonces/SpaceInvaders_S2_DimensionnerVaisseau.md)\\n- [Fonctionnalité n°3 : Choisir la vitesse du vaisseau](enonces/SpaceInvaders_S3_ChoisirVitesseVaisseau.md)\\n- [Fonctionnalité n°4 : Tirer un missile depuis le vaisseau](enonces/SpaceInvaders_S4_TirerMissileDepuisVaisseau.md)\\n- [Fonctionnalité n°5 : Ajouter un envahisseur dans le jeu](enonces/SpaceInvaders_S5_Envahisseur.md)\\n- [Fonctionnalité n°6 : Détecter une collision entre deux sprites](enonces/SpaceInvaders_S6_DetecterCollision.md)\\n- [Fonctionnalité n°7 : Terminer la partie](enonces/SpaceInvaders_S7_TerminerPartie.md)\\n\\n**Objectif n° 2 :  \\nVers un Space Invaders plus *classique* (Améliorations du MVP)**\\n  \\n- [Fonctionnalité n°8 : Permettre au vaisseau de tirer plusieurs missiles](enonces/SpaceInvaders_S8_TirerPlusieursMissiles.md)   \\n- [Fonctionnalité n°9 : Envoyer une *ligne* d\\'envahisseurs](enonces/SpaceInvaders_S9_EnvoyerLigneEnvahisseurs.md)  \\n- [Fonctionnalité n°10 : Gérer un score](enonces/SpaceInvaders_S10_GererScore.md) \\n- [Fonctionnalité n°11 : Tirer un missile depuis un envahisseur de manière aléatoire](enonces/SpaceInvaders_S11_TirerMissileDepuisEnvahisseur.md)\\n- [Fonctionnalité n°12 : Envoyer une *horde* d\\'envahisseurs](enonces/SpaceInvaders_S12_EnvoyerHordeEnvahisseurs.md)   \\n\\n**Objectif n° 3 : \\nLe Space Invaders de vos rêves :smile:**\\n\\n- [Fonctionnalité n°13 & co : Toute amélioration possible pour réaliser le Space Invader de vos rêves](enonces/SpaceInvaders_S13_SpaceInvadersDeReve.md)\\n\\n\\nLes quatre premières fonctionnalités sont écrites sous la forme de tutoriel et sont *extrêmement* détaillées : vous serez guidés pas à pas afin de vous plonger dans la démarche TDD et apprendre peu à peu à prendre en main votre IDE :smile:\\n\\nAu fil des fonctionnalités, vous aurez de plus en plus d\\'autonomie pour développer votre mini-projet qui devra bien sûr respecter au mieux les bonnes pratiques de qualité de code ...\\n\\nOutre les fonctionnalités, d\\'autres tâches *techniques* vont être nécessaire pour mener à bien le premier objectif :\\n\\n- Le Sprint 0 (selon la terminologie agile) en tout début de projet va permettre :\\n\\t- [Installation du socle technique](enonces/SpaceInvaders_S0_SocleTechnique.md)\\n\\t- [Rapide Analyse du problème](enonces/SpaceInvaders_S0_QuickDesignSession.md)  \\n- La [mise en place d\\'un moteur graphique au sein de notre jeu](enonces/SpaceInvaders_Spike_MoteurGraphique.md) sera consacrée à la prise en main d\\'un moteur graphique simplifié et à son intégration au jeu. Idéalement, cette tâche devrait être réalisée après la fonctionnalité 2 lorsque le vaisseau a une dimension. En réalité jusqu\\'à la livraison, le moteur graphique n\\'est pas vraiment nécessaire pour le développement de notre application puisque le comportement du jeu est vérifié et validé par les tests ! \\n\\nUne fois l\\'objectif n°1 terminé (et avant de commencer l\\'objectif n°2), nous verrons également comment livrer ce projet via la : \\n \\n- [Création d\\'un .JAR du projet](enonces/SpaceInvaders_JAR.md)  \\n\\n\\n\\n**Remarque pour ce projet dans le cadre du module M2104 :**  \\n- Suivant la vitesse à laquelle vous avancerez, plusieurs *fonctionnalités* pourront être réalisées pendant une séance de TP. Vous *tirerez* ainsi les fonctionnalités au fur et à mesure de vos besoins, chaque binôme avançant à son propre rythme.    \\n- La séance consacrée à la mise en place du moteur graphique sera réalisée après La fonctionnalité n°2 : soyez patient, et continuez tranquillement les fonctionnalités en attendant de mettre en place ce rendu visuel ;-)  \\n- Pour le module M2104, l\\'objectif n°1 est demandé.\\n\\n\\nHave fun !\\n\\n\\n\\nNous commencerons donc ce mini-projet par l\\'installation du socle technique : c\\'est par [ici](enonces/SpaceInvaders_S0_SocleTechnique.md).\\n\\n\\n## Références <a id=\"references\"></a>\\n\\nCe mini-projet est en relation avec les enseignements suivants :\\n\\n- Cours : [Quid du Test dans un développement logiciel ?](https://github.com/iblasquez/enseignement-iut-m2104/blob/master/slides/7_Tests.pdf)  \\n- Cours : [Sensibilisation aux bonnes pratiques de la programmation (qualité logicielle)](https://github.com/iblasquez/enseignement-iut-m2104/blob/master/slides/8_QualiteLogicielle_CleanCode.pdf)\\n- Présentation : [Coding Dojo : une aide à la pratique du TDD](http://iblasquez.github.io/presentation_TDD_CodingDojo)\\n- Atelier TDDlego : [Sensibilisation aux bonnes pratiques techniques du Software Craftsmanship : Lego® à la rescousse !](https://github.com/iblasquez/atelier-bonnes-pratiques-tdd-lego)\\n- Et plus généralement [tous les enseignements du module de conception (M2104)](https://github.com/iblasquez/enseignement-iut-m2104-conception) \\n\\n<!-- Tutoriel autour de [l\\'exemple *simplifié* du premier chapitre du livre Refactoring, Improving the Design of Existing Code de Martin Fowler](https://github.com/iblasquez/Refactoring_PremierExempleFowler) -->\\n<!-- Les références sont disponibles [ici](SpaceInvaders_References.md) -->\\n\\n\\n## Commentaires, remarques et suggestions <a id=\"commentaires\"></a>\\nPour les discussions, c\\'est par là : [https://github.com/iblasquez/tdd_spaceInvaders/issues](https://github.com/iblasquez/tdd_spaceInvaders/issues)  \\nPour les propositions de contenu, de modification par ici : [https://github.com/iblasquez/tdd_spaceInvaders/pulls](https://github.com/iblasquez/tdd_spaceInvaders/pulls)\\n\\nEt bien sûr, n\\'hésitez pas à personnaliser vos messages avec des [emojis](http://www.webpagefx.com/tools/emoji-cheat-sheet/) :smile:\\n\\n\\n## Licence <a id=\"licence\"></a>\\nTous les documents de ce dépôt sont placés sous licence CC BY-NC-SA :  [Creative Commons\\nAttribution - Pas d\\'Utilisation Commerciale - Partage dans les Mêmes Conditions](https://creativecommons.org/licenses/by-nc-sa/4.0/)\\n\\n<img src=\"https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png\" width=\"100\">\\n\\nEn savoir plus sur [les licences Creative Commons](https://creativecommons.org/licenses/?lang=fr-FR) ...\\n\\nToutefois, toute personne enseignant ou ayant enseignée au département Informatique de l\\'IUT du Limousin doit demander une autorisation préalable par écrit si elle souhaite utiliser les documents de ce dépôt. :smile:'},\n",
       " {'repo': 'puniverse/spaceships-demo',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': \"# Spaceships\\n\\nThis is a demo of the [SpaceBase real-time spatial database](http://paralleluniverse.co). It simulates tens of thousands of spaceships in battle.\\n\\nA blog post discussing the demo is [here](http://blog.paralleluniverse.co/post/44146699200/spaceships).\\n\\nIf you'd like to discuss SpaceBase, our [forum is here](https://groups.google.com/forum/?fromgroups#!forum/spacebase-user).\\n\\nThis code is available under the terms of the [MIT license](http://opensource.org/licenses/MIT).\\n\\n## Setup\\n\\nJDK 7 is required. We use Gradle build system. In order to build and run type in the project directory:\\n```sh\\n./gradlew\\n```\\nor in windows:\\n```\\ngradlew.bat\\n```\\nThis will download and install gradle build system and the build and run the project.\\nEnjoy !\\n\\n## Usage\\nUse the arrow keys or mouse scrolling to move the viewport. To zoom in/out use `+` `-` or Ctrl+mouse-scroll.\\n\"},\n",
       " {'repo': 'deepqmc/deepqmc',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# DeepQMC\\n\\n![checks](https://img.shields.io/github/actions/workflow/status/deepqmc/deepqmc/tests.yaml?label=tests)\\n[![coverage](https://img.shields.io/codecov/c/github/deepqmc/deepqmc.svg)](https://codecov.io/gh/deepqmc/deepqmc)\\n![python](https://img.shields.io/pypi/pyversions/deepqmc.svg)\\n[![pypi](https://img.shields.io/pypi/v/deepqmc.svg)](https://pypi.org/project/deepqmc/)\\n[![commits since](https://img.shields.io/github/commits-since/deepqmc/deepqmc/latest.svg)](https://github.com/deepqmc/deepqmc/releases)\\n[![last commit](https://img.shields.io/github/last-commit/deepqmc/deepqmc.svg)](https://github.com/deepqmc/deepqmc/commits/master)\\n[![license](https://img.shields.io/github/license/deepqmc/deepqmc.svg)](https://github.com/deepqmc/deepqmc/blob/master/LICENSE)\\n[![code style](https://img.shields.io/badge/code%20style-black-202020.svg)](https://github.com/ambv/black)\\n[![doi](https://img.shields.io/badge/doi-10.5281%2Fzenodo.3960826-blue)](http://doi.org/10.5281/zenodo.3960826)\\n\\nDeepQMC implements variational quantum Monte Carlo for electrons in molecules, using deep neural networks as trial wave functions. The package is based on [JAX](https://github.com/google/jax) and [Haiku](https://github.com/deepmind/dm-haiku). Besides the core functionality, it contains an implementation of the [PauliNet](https://doi.org/ghcm5p) ansatz.\\n\\n### Installing\\n\\nInstall and update using [Pip](https://pip.pypa.io/en/stable/quickstart/):\\n\\n```\\npip install -U deepqmc -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\\n```\\n\\nTo install DeepQMC from a local Git repository run:\\n\\n```\\ngit clone https://github.com/deepqmc/deepqmc\\ncd deepqmc\\npip install -e .[dev] -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\\n```\\n\\n### Documentation and exemplary usage\\n\\nFor further information about the DeepQMC package and tutorials covering the basic usage visit the [documentation](https://deepqmc.github.io).\\n'},\n",
       " {'repo': 'VincentDubois/SpaceInvader',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': 'SpaceInvader\\n============\\n\\nVersion initiale pour le projet Android (DUT SRC S3)'},\n",
       " {'repo': 'sugardose/SpaceEngineers',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': \"Sam's Autopilot Manager v2\\n[Steam](https://steamcommunity.com/sharedfiles/filedetails/?id=1653875433)\\n\\nSam's Autopilot Manager v2 vMod (SCBionicle - modified version)\\n[Steam](https://steamcommunity.com/sharedfiles/filedetails/?id=1941423134)\\n\\nSam's Autopilot Manager v1 (deprecated)\\n[Steam](https://steamcommunity.com/sharedfiles/filedetails/?id=1224507423)\\n\"},\n",
       " {'repo': 'Thinkful-Ed/spaced-repetition',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Spaced Repetition Capstone\\n\\n## Setup\\n\\nTo setup the application\\n\\n1. Fork and clone the project to your machine\\n2. `npm install`. This will also install the application *Cypress.io* for running browser integration tests\\n\\nThe project expects you have the Spaced repetition API project setup and running on http://localhost:8000.\\n\\nFind instructions to setup the API here https://github.com/Thinkful-Ed/spaced-repetition-api.\\n\\n## Running project\\n\\nThis is a `create-react-app` project so `npm start` will start the project in development mode with hot reloading by default.\\n\\n## Running the tests\\n\\nThis project uses [Cypress IO](https://docs.cypress.io) for integration testing using the Chrome browser.\\n\\nCypress has the following expectations:\\n\\n- You have cypress installed (this is a devDependency of the project)\\n- You have your application running at http://localhost:3000.\\n  - You can change the address of this expectation in the `./cypress.json` file.\\n- Your `./src/config.js` is using http://localhost:8000/api as the `API_ENDPOINT`\\n\\nTo start the tests run the command:\\n\\n```bash\\nnpm run cypress:open\\n```\\n\\nOn the first run of this command, the cypress application will verify its install. Any other runs after this, the verification will be skipped.\\n\\nThe command will open up the Cypress application which reads tests from the `./cypress/integration/` directory. You can then run individual tests by clicking on the file names or run all tests by clicking the \"run all tests\" button in the cypress GUI.\\n\\nTests will assert against your running localhost client application.\\n\\nYou can also start all of the tests in the command line only (not using the GUI) by running the command:\\n\\n```bash\\nnpm run cypress:run\\n```\\n\\nThis will save video recordings of the test runs in the directory `./cypress/videos/`.\\n'},\n",
       " {'repo': 'aidam38/roamsr',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# 🗃️ roam/sr - Spaced Repetition in Roam Research\\n\\nFor more info, see: https://roamresearch.com/#/app/roam-depot-developers/page/uQSCwVKx0\\n\\n## Contributing\\n\\n- Contact me on Twitter: https://twitter.com/adam_krivka or email krivka.adam@gmail.com.\\n- Create issues and pull requests in this repository.\\n'},\n",
       " {'repo': 'angrox/spacewalk-api-scripts',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': 'spacewalk-api-scripts\\n=====================\\n\\n2021-10-12: EOL. This repo is deprecated and archived. \\n\\nWhat is this?\\n-------------\\nThese are a few scripts which utilize the spacewalk api to perform various tasks. \\nFor we all have (mostly) the same problems or requirements I will publish all scripts\\nI wrote to perform recurring or annoying tasks. \\n\\n\\nScripts\\n-------\\n* spacewalk-rhn-sync -- Sync packages from the Red Hat Network and pushes it to the spacewalk server. Uses mrepo! \\n* spacewalk-orgclone-channel -- Clones a channel and its errata to a new channel. This works even if the channel is shared from another organization.\\n* spacewalk-create-yumrepo -- Creates a yum repository out of an spacewalk channel\\n* spacewalk-compare-packages -- Compares packages of an host for a different channel to check for updates \\n* spacewalk-clone-errata -- Clones errata from one channel to another\\n* spacewalk-remove-old-packages -- Deletes packages without channel OR outdated packages from one channel\\n* spacewalk-schedule-scriptrun -- Schedules a remote command for one client\\n* uln-clone-errata -- Fetches errata information for oracles \"unbreakable\" red hat clone and pushes it into spacewalk\\n   -- Update 20130618: It seems this script is outdated - ULN errata comes with an XML file within the repo and spacewalk-repo-sync honors that file\\n* spacewalk-generate-reinstall-key -- Generates a reinstall activation key for a given system\\n\\n\\n\\n\\n'},\n",
       " {'repo': 'TriggerAu/KerbalAlarmClock',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': 'KerbalAlarmClock\\n================\\nA management and utility plugin for [Kerbal Space Program](https://www.kerbalspaceprogram.com/)\\n\\nThe Kerbal Alarm Clock is a plugin that allows you to create reminder alarms at future periods to help you manage your flights and not warp past important times\\n\\nForum Thread: [KerbalAlarmClock](https://forum.kerbalspaceprogram.com/topic/22809-kerbal-alarm-clock)\\nAuthor: TriggerAu\\n'},\n",
       " {'repo': 'ApmeM/android-flowlayout',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '# Android flow layout\\n\\n## Introduction\\n\\nExtended linear layout that wrap its content when there is no place in the current line.\\n\\n[![Maven Central](https://maven-badges.herokuapp.com/maven-central/org.apmem.tools/layouts/badge.svg?style=flat)](https://maven-badges.herokuapp.com/maven-central/org.apmem.tools/layouts/)\\n[![Travis-CI](https://travis-ci.org/ApmeM/android-flowlayout.svg?branch=master)] (https://travis-ci.org/ApmeM/android-flowlayout)\\n\\n## Demonstration\\n\\nOrientation: HORIZONTAL, Gravity: FILL, LayoutDirection: LTR\\n\\n![](https://github.com/ApmeM/android-flowlayout/raw/master/img/LANDSCAPE_LTR_FILL_HORIZONTAL_DEBUG.png)\\n![](https://github.com/ApmeM/android-flowlayout/raw/master/img/PORTRAIT_LTR_FILL_HORIZONTAL_DEBUG.png)\\n\\nOrientation: HORIZONTAL, Gravity: RIGHT & BOTTOM, LayoutDirection: RTL\\n\\n![](https://github.com/ApmeM/android-flowlayout/raw/master/img/LANDSCAPE_RTL_RIGHTBOTTOM_HORIZONTAL_DEBUG.png)\\n![](https://github.com/ApmeM/android-flowlayout/raw/master/img/PORTRAIT_RTL_RIGHTBOTTOM_HORIZONTAL_DEBUG.png)\\n\\nOrientation: VERTICAL, Gravity: CENTER, LayoutDirection: LTR\\n\\n![](https://github.com/ApmeM/android-flowlayout/raw/master/img/LANDSCAPE_LTR_CENTER_VERTICAL_DEBUG.png)\\n![](https://github.com/ApmeM/android-flowlayout/raw/master/img/PORTRAIT_LTR_CENTER_VERTICAL_DEBUG.png)\\n\\nDebug is switched off:\\n\\n![](https://github.com/ApmeM/android-flowlayout/raw/master/img/LANDSCAPE_LTR_FILL_HORIZONTAL_NODEBUG.png)\\n![](https://github.com/ApmeM/android-flowlayout/raw/master/img/PORTRAIT_LTR_FILL_HORIZONTAL_NODEBUG.png)\\n\\n## Installation and usage\\n\\nTake from maven repository (<http://search.maven.org/#search%7Cga%7C1%7Corg.apmem.tools>, <http://mvnrepository.com/search.html?query=org.apmem.tools>) or add FlowLayout and other components to your solution\\n\\nAdd it as dependency in Gradle as:\\n\\n\\tcompile \\'org.apmem.tools:layouts:1.10@aar\\'\\n\\nOr maven\\n\\n        <dependency>\\n            <groupId>org.apmem.tools</groupId>\\n            <artifactId>layouts</artifactId>\\n            <version>1.10</version>\\n            <scope>provided</scope>\\n        </dependency>\\n\\nAdd the following xml code into your layout/something.xml:\\n\\n\\t<org.apmem.tools.layouts.FlowLayout\\n\\t\\txmlns:android=\"http://schemas.android.com/apk/res/android\"\\n\\t\\tandroid:layout_width=\"fill_parent\"\\n\\t\\tandroid:layout_height=\"wrap_content\"\\n\\t>\\n\\t</org.apmem.tools.layouts.FlowLayout>\\n\\nTo change default direction use the following code\\n\\n\\tandroid:orientation=\"vertical\"\\n\\nTo change layout direction use the following code\\n\\n\\txmlns:f=\"http://schemas.android.com/apk/res/your.namespace\"\\n\\tf:layoutDirection=\"rtl\"\\n\\t\\nAndroid gravity now supported (in combination with elements weight):\\n\\n        f:weightDefault=\"1.0\"\\n        android:gravity=\"fill\"\\n\\nTo override default spacing between elements use default android margins in the child View element:\\n\\n\\tandroid:layout_marginTop=\"32dip\"\\n\\tandroid:layout_marginRight=\"32dip\"\\n\\nAlso if you need to break line before some object even if there is enough space for it in the previous line - use the following LayoutParameter in the child view element:\\n\\n\\tf:layout_newLine=\"true\"\\n\\n## Detailed parameters\\n\\nLayout parameters:\\n\\n\\t* android:orientation - line direction. Use one of the following values:\\n\\n\\t\\t* horizontal - line will be in horizontal direction, linebreak will create new line\\n\\n\\t\\t* vertical - line will be in vertical direction, linebreak will create new column\\n\\n        * android:gravity - standard android gravity supported\\n\\n\\t* debugDraw - draw debug information\\n\\n        * weightDefault - default weight value for child elements. Used to fill line in case of Gravity.FILL_HORIZONTAL | Gravity.FILL_VERTICAL\\n\\n        * layoutDirection - direction of inner child elements:\\n\\n                *  ltr - left to right direction\\n\\n                *  rtl - right to left direction\\n\\nChild layout parameters:\\n\\n\\t* android:layout_margin* - override default spacings\\n\\n\\t* android:layout_gravity - standard android gravity supported\\n\\n        * layout_weight - weight of the element. If not specified \"layout.defaultWight\" is used.\\n\\n\\t* layout_newLine - break line before current element even if there is enough place in the current line.\\n\\n## Copyrights\\n\\n   Copyright 2011, Artem Votincev (apmem.org)\\n \\n   Licensed under the Apache License, Version 2.0 (the \"License\");\\n   you may not use this file except in compliance with the License.\\n   You may obtain a copy of the License at\\n\\n       http://www.apache.org/licenses/LICENSE-2.0\\n\\n   Unless required by applicable law or agreed to in writing, software\\n   distributed under the License is distributed on an \"AS IS\" BASIS,\\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n   See the License for the specific language governing permissions and\\n   limitations under the License.\\n'},\n",
       " {'repo': 'Adivise/NanoSpacePlus',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '<p align=\"center\">\\n<img src=\"https://capsule-render.vercel.app/api?type=waving&color=gradient&height=200&section=header&text=NanoSpacePlus&fontSize=80&fontAlignY=35&animation=twinkling&fontColor=gradient\"/> </a> \\n</p>\\n\\n<p align=\"center\"> \\n  <a href=\"https://discord.gg/SNG3dh3MbR\" target=\"_blank\"> <img src=\"https://discordapp.com/api/guilds/903043706410643496/widget.png?style=banner2\"/> </a> \\n</p>\\n\\n<p align=\"center\"> \\n  <a href=\"https://ko-fi.com/nanotect\" target=\"_blank\"> <img src=\"https://ko-fi.com/img/githubbutton_sm.svg\"/> </a> \\n</p>\\n\\n## 📑 Feature\\n- [x] Music System (AutoComplete [Play, Playskip, Playtop])\\n- [x] Playlists System (AutoComplete, [All])\\n- [x] Premium System (Ko-Fi Support!)\\n- [x] Setup Request System\\n- [x] Multi Language\\n- [x] Slash Command (Base, Group, Sub)\\n- [x] Context Message Menu\\n- [x] Custom Filters\\n- [x] Easy to use\\n\\n## 🎶 Support Source\\n- [x] Youtube\\n- [x] SoundCloud\\n- [x] Twitch\\n- [x] Bandcamp\\n- [x] Vimeo\\n- [x] Https (Radio)\\n\\n<details><summary>📃 Plugins (More Support Source) [CLICK ME]</summary>\\n<p>\\n\\n## 📃 Plugins (More Support Source) (Require: LavaLink v3.6.x)\\n- [x] [LavaSrc](https://github.com/TopiSenpai/LavaSrc)\\n- Spotify\\n- Deezer\\n- Apple\\n- Yandex\\n\\n- [x] [skybot-lavalink-plugin](https://github.com/DuncteBot/skybot-lavalink-plugin)\\n- Mixcloud\\n- Ocremix\\n- Clyp\\n- Reddit\\n- Getyarn\\n- TikTok\\n- PornHub\\n- Soundgasm\\n\\n</p>\\n</details>\\n\\n<details><summary>📎 Requirements [CLICK ME]</summary>\\n<p>\\n\\n## 📎 Requirements\\n\\n- [x] Node.js v16+ **[Download](https://nodejs.org/en/download/)**\\n- [x] Discord Bot Token **[Guide](https://discordjs.guide/preparations/setting-up-a-bot-application.html#creating-your-bot)**\\n- [x] LavaLink **[Guide](https://github.com/freyacodes/lavalink)** (*Dev Version!* **[Download](https://ci.fredboat.com/repository/)**)\\n- [My Application File](https://cdn.discordapp.com/attachments/1010784573061349496/1038914440734715994/application.yml)\\n- [x] MongoDB **[Download](https://www.mongodb.com/try/download/community)** (Download & install = Finish!)\\n\\n## 🛑 Super Requirements \\n\\n- Java 11-13 **[Download JDK13](http://www.mediafire.com/file/m6gk7aoq96db8g0/file)** (i use this version) for LAVALINK!\\n\\n</p>\\n</details>\\n\\n## 📚 Installation\\n\\n```\\ngit clone https://github.com/Adivise/NanoSpacePlus\\ncd NanoSpacePlus\\nnpm install\\n```\\n\\n<details><summary>📄 Configuration [CLICK ME]</summary>\\n<p>\\n\\n## 📄 Configuration\\n\\nCopy or Rename `.env.example` to `.env` and fill out the values:\\n\\n```.env\\n# Bot\\nTOKEN=REPLACE_HERE\\nNP_REALTIME=false\\nLEAVE_TIMEOUT=120000\\nLANGUAGE=en\\nEMBED_COLOR=#000001\\n\\n# Dev\\nOWNER_ID=REPLACE_HERE\\n\\n# Database\\nMONGO_URI=mongodb://127.0.0.1:27017/nanospaceplus\\nLIMIT_TRACK=50\\nLIMIT_PLAYLIST=10\\n\\n# Nodes\\nNODE_HOST=localhost\\nNODE_PORT=5555\\nNODE_PASSWORD=123456\\n```\\nAfter installation or finishes all you can use `node .` to start the bot. or `Run Start.bat`\\n\\n</p>\\n</details>\\n\\n<details><summary>🔩 Features & Commands [CLICK ME]</summary>\\n<p>\\n\\n## 🔩 Features & Commands\\n\\n> Note: The default prefix is \\'/\\'\\n\\n💬 **Context Menu**\\n- Play (Right-Click & Apps > Context | Play) \\n- Skip (Right-Click & Apps > Context | Skip) \\n- Stop (Right-Click & Apps > Context | Stop) \\n- Shuffle (Right-Click & Apps > Context | Shuffle) \\n- Loop (Right-Click & Apps > Context | Loop) \\n\\n💬 **Extra Commands!**\\n- Play (/extra play) \\n\\n🎶 **Music Commands!** \\n\\n- Play (/play [song/url])\\n- Search (/search [songname])\\n- Nowplaying (/nowplaying)\\n- Queue (/music queue [page])\\n- Loop (/music loop [current, queue])\\n- LoopQueue (/music loopqueue)\\n- Shuffle (/music shuffle)\\n- Volume (/music volume [10 - 100])\\n- Pause (/music pause)\\n- Resume (/music resume)\\n- Skip (/music skip)\\n- Skipto (/music skipto [position])\\n- Clear (/musicclear)\\n- Join (/music join)\\n- Leave (/music leave)\\n- Forward (/music forward [second])\\n- Seek (/music seek [second])\\n- Rewind (/music rewind [second])\\n- Replay (/music replay)\\n- TwentyFourSeven (/music 247)\\n- Previous (/music previous)\\n- Autoplay (/music autoplay)\\n- Move (/music move [song] [position])\\n- Remove (/music remove [song])\\n- PlaySkip (/music playskip [song/url])\\n- SearchSkip (/music searchskip [songname])\\n- PlayTop (/music playtop [song/url])\\n- SearchTop (/music searchtop [songname])\\n- SearchSkip (/music searchskip [song/url])\\n- PlayTop (/music playtop [song/url])\\n- SearchTop (/music searchtop [song/url])\\n- Charts (/charts [global/guild])\\n\\n⏺ **Filter Commands!**\\n- Bass (/filter bass)\\n- Superbass (/filter superbass)\\n- Pop (/filter pop)\\n- Treblebass (/filter treblebass)\\n- Soft (/filter soft)\\n- Earrape (/filter earrape)\\n- Equalizer (/filter equalizer [14 bands])\\n- Speed (/filter speed [amount])\\n- Picth (/filter pitch [amount])\\n- Vaporwave (/filter vaporwave)\\n- Nightcore (/filter nightcore)\\n- Bassboost (/filter bassboost [-10 - 10])\\n- Rate (/filter rate)\\n- Reset (/filter reset)\\n- 3d (/filter 3d)\\n- China (/filter china)\\n- Chipmunk (/filter chipmunk)\\n- Darthvader (/filter darthvader)\\n- DoubleTime (/filter doubletime)\\n- SlowMotion (/filter slowmotion)\\n- Tremolo (/filter tremolo)\\n- Vibrate (/filter vibrate)\\n- Vibrato (/filter vibrato)\\n- Daycore (/filter daycore)\\n- Television (/filter Television)\\n\\t\\n📦 **Playlist Commands!**\\n- Create (/playlist create [name])\\n- Add (/playlist add [name] [link])\\n- Private (/playlist private [name])\\n- Public (/playlist public [name])\\n- Delete (/playlist delete [name])\\n- Import (/playlist import [name])\\n- Detail (/playlist detail [name])\\n- Remove (/playlist remove [name] [position])\\n- SaveCurrent (/playlist savecurrent [name])\\n- SaveQueue (/playlist savequeue [name])\\n- View (/playlist view)\\n\\t\\n💎 **Premium Commands!**\\n- Profile (/profile)\\n- Generate (/premium generate [plan] [amount]) // (OWNER ONLY)\\n- Redeem (/redeem [code])\\n- Setup (/premium setup)\\n- Transaction (/premium transaction [id])\\n- Remove (/premium remove [mention]) // (OWNER ONLY)\\n\\t\\n📑 **Utility Commands!**\\n- Shutdown (/utility shutdown) // (OWNER ONLY)\\n- Language (/utility language [language] ) // Example: en, th\\n- Help (/help)\\n- CommandStats (/commandstats)\\n- Vps (/utility vps)\\n- LavaLink (/utility lavalink)\\n\\n</p>\\n</details>\\n\\n## ❣ Contributors\\n\\n<a href=\"https://github.com/Adivise/NanoSpacePlus/graphs/contributors\">\\n  <img src=\"https://contributors-img.web.app/image?repo=Adivise/NanoSpacePlus\" />\\n</a>\\n'},\n",
       " {'repo': 'microsoft/deep-space',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# deep-space\\nSample web application written in Java and uses AngularJS.\\n'},\n",
       " {'repo': 'Robin-jiangyufeng/SpaceTextWatcher',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': \"# SpaceTextWatcher\\nandroid输入框输入银行卡,输入手机,输入身份证格式化的实现\\n# 项目地址\\n[SpaceTextWatcher](https://github.com/Robin-jiangyufeng/SpaceTextWatcher)\\n# 实现方式\\n```java\\n    @Override\\n\\tpublic void beforeTextChanged(CharSequence s, int start, int count,\\n\\t\\t\\tint after) {\\n\\t\\tbeforeTextLength = s.length();\\n\\t\\tif (buffer.length() > 0) {\\n\\t\\t\\tbuffer.delete(0, buffer.length());\\n\\t\\t}\\n\\t\\tspaceNumberA = 0;\\n\\t\\tfor (int i = 0; i < s.length(); i++) {\\n\\t\\t\\tif (s.charAt(i) == ' ') {\\n\\t\\t\\t\\tspaceNumberA++;\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n\\n\\t@Override\\n\\tpublic void onTextChanged(CharSequence s, int start, int before, int count) {\\n\\t\\tonTextLength = s.length();\\n\\t\\tbuffer.append(s.toString());\\n\\t\\tif (onTextLength == beforeTextLength || onTextLength > maxLenght\\n\\t\\t\\t\\t|| isChanged) {\\n\\t\\t\\tisChanged = false;\\n\\t\\t\\treturn;\\n\\t\\t}\\n\\t\\tisChanged = true;\\n\\t}\\n\\n\\t@Override\\n\\tpublic void afterTextChanged(Editable s) {\\n\\t\\tif (isChanged) {\\n\\t\\t\\tlocation = editText.getSelectionEnd();\\n\\t\\t\\tint index = 0;\\n\\t\\t\\twhile (index < buffer.length()) { // 删掉所有空格\\n\\t\\t\\t\\tif (buffer.charAt(index) == ' ') {\\n\\t\\t\\t\\t\\tbuffer.deleteCharAt(index);\\n\\t\\t\\t\\t} else {\\n\\t\\t\\t\\t\\tindex++;\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n\\n\\t\\t\\tindex = 0;\\n\\t\\t\\tint spaceNumberB = 0;\\n\\t\\t\\twhile (index < buffer.length()) { // 插入所有空格\\n\\t\\t\\t\\tspaceNumberB = insertSpace(index, spaceNumberB);\\n\\t\\t\\t\\tindex++;\\n\\t\\t\\t}\\n\\n\\t\\t\\tString str = buffer.toString();\\n\\n\\t\\t\\t// 下面是计算光位置的\\n\\t\\t\\tif (spaceNumberB > spaceNumberA) {\\n\\t\\t\\t\\tlocation += (spaceNumberB - spaceNumberA);\\n\\t\\t\\t\\tspaceNumberA = spaceNumberB;\\n\\t\\t\\t}\\n\\t\\t\\tif (isSetText) {\\n\\t\\t\\t\\tlocation = str.length();\\n\\t\\t\\t\\tisSetText = false;\\n\\t\\t\\t} else if (location > str.length()) {\\n\\t\\t\\t\\tlocation = str.length();\\n\\t\\t\\t} else if (location < 0) {\\n\\t\\t\\t\\tlocation = 0;\\n\\t\\t\\t}\\n\\n\\t\\t\\teditText.setText(str);\\n\\t\\t\\ttry {\\n\\t\\t\\t\\teditText.setSelection(location);\\n\\t\\t\\t} catch (Exception e) {\\n\\t\\t\\t\\te.printStackTrace();\\n\\t\\t\\t}\\n\\t\\t\\tisChanged = false;\\n\\t\\t}\\n\\t}\\n\\t\\n    /**\\n\\t * 根据类型插入空格\\n\\t * \\n\\t * @param index\\n\\t * @param spaceNumberAfter\\n\\t * @return\\n\\t * @see [类、类#方法、类#成员]\\n\\t */\\n\\tprivate int insertSpace(int index, int spaceNumberAfter) {\\n\\t\\tswitch (spaceType) {\\n\\t\\tcase defaultType:// 相隔四位空格\\n\\t\\t\\tif (index > 3\\n\\t\\t\\t\\t\\t&& (index % (4 * (spaceNumberAfter + 1)) == spaceNumberAfter)) {\\n\\t\\t\\t\\tbuffer.insert(index, ' ');\\n\\t\\t\\t\\tspaceNumberAfter++;\\n\\t\\t\\t}\\n\\t\\t\\tbreak;\\n\\t\\tcase bankCardNumberType:\\n\\t\\t\\tif (index > 3\\n\\t\\t\\t\\t\\t&& (index % (4 * (spaceNumberAfter + 1)) == spaceNumberAfter)) {\\n\\t\\t\\t\\tbuffer.insert(index, ' ');\\n\\t\\t\\t\\tspaceNumberAfter++;\\n\\t\\t\\t}\\n\\t\\t\\tbreak;\\n\\t\\tcase mobilePhoneNumberType:\\n\\t\\t\\tif (index == 3\\n\\t\\t\\t\\t\\t|| ((index > 7) && ((index - 3) % (4 * spaceNumberAfter) == spaceNumberAfter))) {\\n\\t\\t\\t\\tbuffer.insert(index, ' ');\\n\\t\\t\\t\\tspaceNumberAfter++;\\n\\t\\t\\t}\\n\\t\\t\\tbreak;\\n\\t\\tcase IDCardNumberType:\\n\\t\\t\\tif (index == 6\\n\\t\\t\\t\\t\\t|| ((index > 10) && ((index - 6) % (4 * spaceNumberAfter) == spaceNumberAfter))) {\\n\\t\\t\\t\\tbuffer.insert(index, ' ');\\n\\t\\t\\t\\tspaceNumberAfter++;\\n\\t\\t\\t}\\n\\t\\t\\tbreak;\\n\\t\\tdefault:\\n\\t\\t\\tif (index > 3\\n\\t\\t\\t\\t\\t&& (index % (4 * (spaceNumberAfter + 1)) == spaceNumberAfter)) {\\n\\t\\t\\t\\tbuffer.insert(index, ' ');\\n\\t\\t\\t\\tspaceNumberAfter++;\\n\\t\\t\\t}\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t\\treturn spaceNumberAfter;\\n\\t}\\n\\n\\t/***\\n\\t * 计算需要的空格数\\n\\t * \\n\\t * @return 返回添加空格后的字符串长度\\n\\t * @see [类、类#方法、类#成员]\\n\\t */\\n\\tprivate int computeSpaceCount(CharSequence charSequence) {\\n\\t\\tbuffer.delete(0, buffer.length());\\n\\t\\tbuffer.append(charSequence.toString());\\n\\t\\tint index = 0;\\n\\t\\tint spaceNumberB = 0;\\n\\t\\twhile (index < buffer.length()) { // 插入所有空格\\n\\t\\t\\tspaceNumberB = insertSpace(index, spaceNumberB);\\n\\t\\t\\tindex++;\\n\\t\\t}\\n\\t\\tbuffer.delete(0, buffer.length());\\n\\t\\treturn index;\\n\\t}\\n```\\n# 使用方法\\n```java\\n        AddSpaceTextWatcher[] asEditTexts=new AddSpaceTextWatcher[3];\\n        EditText[] editTexts=new EditText[3];\\n        editTexts[0]=(EditText)findViewById(R.id.editText);//银行卡\\n        editTexts[1]=(EditText)findViewById(R.id.editText2);//手机号\\n        editTexts[2]=(EditText)findViewById(R.id.editText3);//身份证\\n        asEditTexts[0]=new AddSpaceTextWatcher(editTexts[0],48);//银行卡\\n        asEditTexts[0].setSpaceType(AddSpaceTextWatcher.SpaceType.bankCardNumberType);\\n        asEditTexts[1]=new AddSpaceTextWatcher(editTexts[1],13);//手机号\\n        asEditTexts[1].setSpaceType(AddSpaceTextWatcher.SpaceType.mobilePhoneNumberType);\\n        asEditTexts[2]=new AddSpaceTextWatcher(editTexts[2],21);//身份证\\n        asEditTexts[2].setSpaceType(AddSpaceTextWatcher.SpaceType.IDCardNumberType);\\n````\\n\\n# 关于作者Robin\\n* 屌丝程序员\\n* GitHub: [Robin-jiangyufeng](https://github.com/Robin-jiangyufeng)\\n* QQ:429257411\\n* 交流QQ群 236395044\"},\n",
       " {'repo': 'cuiliang/Caps2CtrlSpace',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# Caps2CtrlSpace\\nCapsLock 切换输入法中英文\\n# 原理\\n监听按键，如果是Capslock，转换成Ctrl+Space\\n\\n# 开发环境\\n VS2019\\n \\n#  Exe下载\\nhttps://github.com/cuiliang/Caps2CtrlSpace/releases\\n  \\n#  使用\\n如需提权（在以管理员身份运行的程序中生效），请将其复制到`c:\\\\program files`目录中使用。\\n启动后，会自动隐藏窗口显示在系统托盘内一个桔黄色的小点的图标。 这时候可以按capslock来切换中英文了。\\n \\n*自动启动*\\n \\n双击系统托盘图标，在打开的窗口中选中自动启动的选项即可。\\n \\n'},\n",
       " {'repo': 'ytiurin/tetris',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '  ![Demo GIF](https://ytiurin.github.io/tetris/public/demo.gif)\\n\\n# [Play :video_game: TETRIS](https://ytiurin.github.io/tetris/)\\n\\nI made this small project to simulate the original 1984 version of TETRIS game. I saw a [Youtube video](https://www.youtube.com/watch?v=O0gAgQQHFcQ) showing the gameplay of this classic run on [DVK-2](https://en.wikipedia.org/wiki/DVK) computer and thought I could implement it in browser and have some fun in the process.\\n\\nTo make it look similar to the old game, I made it entirely text based, meaning that every frame of the game animation is rendered into a string of text with 25 rows of 80 chars and looks like this:\\n\\n```\\n\\nROWS HIT:             11    ‹! . . . . . . . . . .!›                            \\nSCORE:               980    ‹! . . . . . . . . . .!›      UP ARROW: ROTATE      \\nLEVEL:                 2    ‹! . . . . . . . . . .!›    DOWN ARROW: SOFT DROP   \\n                            ‹! . . . .▮▮ . . . . .!›      SPACEBAR: HARD DROP   \\n                            ‹! . . . .▮▮▮▮ . . . .!›        ESC, P: PAUSE       \\n                            ‹! . . . .▮▮ . . . . .!›                            \\n                            ‹! . . . . . . . . . .!›                            \\n                            ‹! . . . . . . . . . .!›                            \\n                            ‹! . . . . . . . . . .!›                            \\n                            ‹! . . . . . . . . . .!›                            \\n                    ▮▮▮▮▮▮▮▮‹! . . . . . . . . . .!›                            \\n                            ‹! . . . . . . . . . .!›                            \\n                            ‹! . . . . . . . . . .!›                            \\n                            ‹! . . . . . . . . . .!›                            \\n                            ‹! . . . . . . . . . .!›                            \\n                            ‹! . . . . . . . . . .!›                            \\n                            ‹! . . . . . . . . . .!›                            \\n                            ‹! . . . . . . .▮▮▮▮▮▮!›                            \\n                            ‹!▮▮▮▮ . . .▮▮ .▮▮▮▮▮▮!›                            \\n                            ‹!▮▮▮▮ .▮▮▮▮▮▮▮▮▮▮▮▮▮▮!›                            \\n                            ‹!====================!›                            \\n                              \\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/\\\\/                              \\n\\n```\\n\\nBasic setup code:\\n\\n```javascript\\n\\nTETRIS.on({\\n  nextFrame: function( frame ) {\\n    // replace HTML special chars\\n    frame = frame.replace( /[ <>]|\\\\n\\\\r/g, function( m ) { return {\\n      \" \": \"&nbsp;\",\\n      \"<\" : \"&lsaquo;\",\\n      \">\" : \"&rsaquo;\",\\n      \"\\\\n\\\\r\" : \"<br>\" }[ m ] })\\n\\n    document.body.innerHTML = frame\\n  }\\n})\\n\\naddEventListener( \"keydown\", function( e ) {\\n  TETRIS.pressKey( e.keyCode )\\n})\\n\\n```\\n\\nCheck the [play.js](https://github.com/ytiurin/tetris/blob/master/src/play.js) for more advanced code.\\n\\n## Audio\\n\\nI extracted some audio effects from the original video and integrated them into the demo using [Web Audio API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API).\\n\\n## Progressive Web App\\n\\nThe [play page](https://ytiurin.github.io/tetris/) is served as a [PWA](https://developers.google.com/web/progressive-web-apps/), so you can play the game offline.\\n\\n## Leaderboard\\n\\nI also made a simple microservice to store best scores of the [play page](https://ytiurin.github.io/tetris/).\\n'},\n",
       " {'repo': 'Murali-group/GraphSpace',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': 'GraphSpace \\n================\\n\\nGraphSpace is running at http://graphspace.org\\n\\nRequirements\\n===================================\\n1. [Python v2.7.10](https://www.python.org/downloads/release/python-2710/)\\n2. [postgreSQL](https://github.com/Murali-group/GraphSpace/wiki/PostgreSQL-Installation)\\n3. [virtualenv](https://virtualenv.pypa.io/en/stable/)\\n4. [bower](https://bower.io/)\\n5. [ElasticSearch](https://github.com/Murali-group/GraphSpace/wiki/Steps-for-setting-up-ElasticSearch-on-AWS)\\n\\nInstallation Instructions\\n===================================\\n* [Running GraphSpace Locally](https://github.com/Murali-group/GraphSpace/wiki/Running-GraphSpace-locally)\\n* [Running GraphSpace on Apache](https://github.com/Murali-group/GraphSpace/wiki/Running-GraphSpace-on-Apache)\\n\\nDocumentation\\n=================\\n\\nGraphSpace has extensive documentation on the [user interface](http://docs.graphspace.org/en/latest/Quick_Tour_of_GraphSpace.html#welcome-screen), the [REST API](http://docs.graphspace.org/en/latest/Programmers_Guide.html#graphspace-rest-api) and a [Python package for programmatic interaction](http://manual.graphspace.org/projects/graphspace-python/en/latest/tutorial/index.html).\\n\\n\\nContributing\\n=================\\n\\nFeel free to fork and send us pull requests. Here are the [guidelines for contribution](https://github.com/Murali-group/GraphSpace/blob/master/CONTRIBUTING.md) in GraphSpace.\\n\\n\\nContact\\n=================\\n\\nIf you have questions or suggestions about GraphSpace, please contact\\n\\n- **T.M. Murali ([@tmmurali](https://github.com/tmmurali))**\\n- **Aditya Bharadwaj ([@adbharadwaj](https://github.com/adbharadwaj))**\\n\\n\\nLicense\\n=================\\n\\nGraphSpace is available under the GNU General Public License v2.0 license. See [LICENSE.md](https://github.com/Murali-group/GraphSpace/blob/master/LICENSE.md) for more information.\\n'},\n",
       " {'repo': 'Adivise/DisSpaceX',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '<p align=\"center\">\\n<img src=\"https://capsule-render.vercel.app/api?type=waving&color=gradient&height=200&section=header&text=DisSpaceX&fontSize=80&fontAlignY=35&animation=twinkling&fontColor=gradient\"/> </a> \\n</p>\\n\\n<p align=\"center\"> \\n  <a href=\"https://discord.gg/SNG3dh3MbR\" target=\"_blank\"> <img src=\"https://discordapp.com/api/guilds/903043706410643496/widget.png?style=banner2\"/> </a> \\n</p>\\n\\n<p align=\"center\"> \\n  <a href=\"https://ko-fi.com/nanotect\" target=\"_blank\"> <img src=\"https://ko-fi.com/img/githubbutton_sm.svg\"/> </a> \\n</p>\\n\\n## 📑 Feature\\n- [x] Music System\\n- [x] Filters System\\n- [x] AutoComplete (Play, PlaySkip, PlayTop)\\n- [x] Song Request Channel\\n- [x] Context Message Menu\\n- [x] Message Button\\n- [x] Database (Json)\\n- [x] Easy to use\\n\\n## 🎶 Support Source\\n- [x] Youtube\\n- [x] SoundCloud\\n- [x] Spotify\\n\\n## 🚨 Have a Problem\\n\\n✈ Join Discord:  [NanoSpace ♪♪](https://discord.gg/SNG3dh3MbR)\\n   mention me in chat #general or #javascript and ask problem okay! 👌\\n\\n## 🛑 Requirements\\n\\nNode.js **[Download](https://nodejs.org/dist/v17.0.1/node-v17.0.1-x64.msi)**\\n\\nDiscord Bot Token **[Guide](https://discordjs.guide/preparations/setting-up-a-bot-application.html#creating-your-bot)**\\n\\n## 💌 Installation\\n\\n```\\ngit clone https://github.com/Adivise/DisSpaceX\\ncd DisSpaceX\\nnpm install\\n```\\nAfter installation finishes you can use `node .` to start the bot. or `Run Start.bat`\\n\\n## 📚 Configuration\\n\\nCopy or Rename `.env.example` to `.env` and fill out the values:\\n\\n```.env\\nTOKEN=REPLACE_HERE\\nOWNER_ID=REPLACE_HERE\\nEMBED_COLOR=#000001\\n```\\n\\n## 📄 Features & Commands\\n\\n> Note: The default prefix is \\'/\\'\\n\\n💬 **Context Menu**\\n- Play (Right-Click & Apps > Context | Play) \\n- Skip (Right-Click & Apps > Context | Skip) \\n- Stop (Right-Click & Apps > Context | Stop) \\n- Shuffle (Right-Click & Apps > Context | Shuffle) \\n- Loop (Right-Click & Apps > Context | Loop) \\n\\n🎶 **Music Commands!** \\n- Play (/play [song/url])\\n- Nowplaying (/music nowplaying)\\n- Queue (/music queue [page])\\n- Repeat (/music loop)\\n- Loopqueue (/music loopqueue)\\n- Shuffle (/music shuffle)\\n- Volume control (/music volume [10 - 100])\\n- Pause (/music pause)\\n- Resume (/music resume)\\n- Skip (/music skip)\\n- Skipto (/music skipto [position])\\n- ClearQueue (/music clearqueue)\\n- Join (/music join)\\n- Leave (/music leave)\\n- Forward (/music forward [second])\\n- Seek (/music seek [second])\\n- Rewind (/music rewind [second])\\n- Replay (/music replay)\\n- 247 (/music 247)\\n- Previous (/music previous)\\n- Autoplay (/music autoplay)\\n- Move (/music move [song] [position])\\n- Remove (/music remove [song])\\n- PlaySkip (/music playskip [song/url])\\n- PlayTop (/music playtop [song/url])\\n\\n⏺ **Filter Commands!**\\n- Vaporwave (/filter vaporwave)\\n- Earwax (/filter earwax)\\n- Nightcore (/filter nightcore)\\n- 3d (/filter 3d)\\n- Echo (/filter echo)\\n- Flanger (/filter flanger)\\n- Gate (/filter gate)\\n- Haas (/filter hass)\\n- Karaoke (/filter karaoke)\\n- Mcopand (/filter mcopand)\\n- Phaser (/filter phaser)\\n- Reverse (/filter reverse)\\n- Surround (/filter surround)\\n- Tremolo (/filter tremolo)\\n- Bassboost (/filter bassboost)\\n- Earrape (/filter earrape)\\n- Custom (/filter custom [args])\\n- Reset (/filter reset)\\n\\n📑 **Utilities Commands!**\\n- Help (/help)\\n- Invite (/utilities invite)\\n- Restart (/utilities restart)\\n- Setup (/utilities setup)\\n'},\n",
       " {'repo': 'danielgtaylor/html5-space-fighter',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': 'HTML5 Space Fighter\\n===================\\n\\nAn HTML5 game written using GameJS.\\n\\nLive Version\\n------------\\n\\n[Play the game online](http://programmer-art.org/dropbox/fighter-static/index.html \"Click here to play!\")\\n\\nDevelopment\\n-----------\\n\\nIn order to develop please download [GameJS](http://gamejs.org/) first, then\\nclone this repository into e.g. `gamejs/examples/html5-space-fighter`. When\\nrunning the GameJS development server you will now see the game listed and\\nplayable. For example:\\n\\n    cd gamejs/examples\\n    git clone git://github.com/danielgtaylor/html5-space-fighter.git\\n    cd ..\\n    ./gjs-server.sh\\n\\nThen go to [localhost:8080](http://localhost:8080/)\\n\\nDeployment\\n----------\\n\\nYou can deploy the game as follows, assuming an installation like the steps\\ngiven above:\\n\\n    cd gamejs\\n    ./gjs-statify.sh examples/html5-space-fighter ~/Desktop/fighter-static\\n\\nThen you can upload the `fighter-static` folder on your desktop to a web server\\nto make it publicly accessible.\\n\\nLicense\\n-------\\nCopyright (c) 2011 Daniel G. Taylor\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in\\nall copies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\\nTHE SOFTWARE.\\n\\n'},\n",
       " {'repo': 'qkmaxware/Spaceworks',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# Spaceworks #\\n\\nSpaceworks is work-in-progress c# library designed to be an \"all-in-one\" library for the Unity3D Game Engine to aid people in overcoming many of the inherent difficulties with making space based games. This library including many different systems which all of which help programmers manage and design new systems. Some of these systems include: a planetary body renderer, keplerian orbit system, floating origin manager, multi-threading, resource pooling and more. The ```/docs``` folder contains a code of conduct document, contribution guidelines document, issue template, and pull request template. \\n\\n## Motivation ##\\n\\nI have always loved space, and by extension space themed games. Space can create many interesting challenges to game designers and programmers that can make them very difficult to complete. I have noticed that while many of these issues are addressed in academic articles and discussions, there are very few publicly available libraries which implement the techniques. \\n\\n## How To Install ##\\n\\nSimply download the source code from this GIT repository and import it anywhere in the Assets folder of an existing Unity3D project.\\n\\n## Project Structure ##\\n\\n1. docs\\n\\t* Github related documentation\\n2. Materials\\n\\t* Pre-configured test materials\\n3. Primitives\\n\\t* C# classes that are utilized by each of the systems within Spaceworks\\n\\t1. Components\\n\\t\\t* MonoBehaviour based components to aid in testing systems\\n\\t2. Data Structures\\n\\t\\t* Pure data structures that are used to store or encapsulate other information or ideas.\\n\\t\\t1. Math\\n\\t\\t2. Meshes\\n\\t\\t3. Noise\\n\\t\\t4. Octree\\n\\t\\t5. Quadtree\\n\\t\\t6. Stores\\n4. Scenes\\n\\t* Unity3d scenes designed to test specific systems of Spaceworks\\n5. Screenshots\\n\\t* Screenshots used for demonstrations\\n6. Shaders\\n\\t* Pre-made shaders related to rendering specific objects in space.\\n5. Systems\\n\\t* C# scripts relating to various parts of Spaceworks\\n\\t1. Floating Origin\\n\\t\\t* Move objects in the scene to keep \"nearby\" objects close to the scene origin and therefor in the highest level of floating point precision.\\n\\t2. Modular Construction\\n\\t\\t* Use modular parts and allow them to \"clip\" together in well defined ways. \\n\\t3. Orbits\\n\\t\\t* Describe motion of planetary bodies in space. \\n\\t4. Planet Name Generator\\n\\t\\t* Pseudo-randomly generate names for planets, creatures, or more.\\n\\t5. Planet Renderer\\n\\t\\t* Create and render planetary geometry using a quad-tree to store levels of detail\\n\\t6. Pooling\\n\\t\\t* Save resources by reusing objects for later.\\n\\t7. Radar3d\\n\\t\\t* 3d radar implementation\\n\\t8. Solar System\\n\\t\\t* Create full sized solar systems by creating a small \"model\" that is easy to modify. \\n\\t9. Threading\\n\\t\\t* Classes for creating async tasks and resolve them using a pool of threads.\\n\\n## Screenshots ##\\n\\n![Close-up Image](Screenshots/planet.png)\\n![Far-away Image](Screenshots/planet2.png)'},\n",
       " {'repo': 'spacetelescope/jwql',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '<p align=\"center\">\\n  <img src=\"logos/jwql_logo_full_transparent.png\" width=\"400\"/>\\n</p>\\n\\n# The JWST Quicklook Application (`JWQL`)\\n\\n[![Current Release](https://img.shields.io/github/release/spacetelescope/jwql.svg)](https://github.com/spacetelescope/jwql/releases/latest/)\\n[![PyPI - License](https://img.shields.io/pypi/l/Django.svg)](https://github.com/spacetelescope/jwql/blob/master/LICENSE)\\n[![Python](https://img.shields.io/badge/Python-3.7-blue.svg)](https://www.python.org/)\\n[![Build Status](https://github.com/spacetelescope/jwql/workflows/JWQL%20CI/badge.svg)](https://github.com/spacetelescope/jwql/actions)\\n[![Documentation Status](https://readthedocs.org/projects/jwql/badge/?version=latest)](https://jwql.readthedocs.io/en/latest/?badge=latest)\\n[![STScI](https://img.shields.io/badge/powered%20by-STScI-blue.svg?colorA=707170&colorB=3e8ddd&style=flat)](http://www.stsci.edu)\\n[![DOI](https://zenodo.org/badge/109727729.svg)](https://zenodo.org/badge/latestdoi/109727729)\\n[![codecov](https://codecov.io/gh/spacetelescope/jwql/branch/develop/graph/badge.svg)](https://codecov.io/gh/spacetelescope/jwql)\\n\\n\\nThe JWST Quicklook Application (`JWQL`) is a database-driven web application and automation framework for use by the JWST instrument teams to monitor and trend the health, stability, and performance of the JWST instruments.  The system is comprised of the following:\\n1. A network file system that stores all uncalibrated and calibrated data products on disk in a centrally-located area, accessible to instrument team members (i.e. the MAST data cache)\\n2. A relational database that stores observational metadata allowing for data discovery via relational queries (MAST database API).\\n3. A software library that provides tools to support an automation framework in which to build automated instrument monitoring routines.\\n4. A web application that allows users to visually inspect new and archival JWST data as well as instrument-specific monitoring and performance results.\\n\\nOfficial API documentation can be found on [ReadTheDocs](https://jwql.readthedocs.io)\\n\\nThe `jwql` application is available at [https://jwql.stsci.edu](https://jwql.stsci.edu).  Please note that the application is currently restricted to specific JWST instrument team members.\\n\\n## Installation for Users\\n\\nTo install `jwql`, simply use `pip`:\\n\\n```\\npip install jwql\\n```\\n\\nThe section below describes a more detailed installation for users that wish to contribute to the `jwql` repository.\\n\\n## Installation for Contributors\\n\\nGetting `jwql` up and running on your own computer requires four steps, detailed below:\\n1. Cloning the GitHub repository\\n2. Installing the `conda`environment\\n3. Installing the python package\\n4. Setting up the configuration file\\n\\n### Prerequisites\\n\\nIt is highly suggested that contributors have a working installation of `anaconda` or `miniconda` for Python 3.8.  Downloads and installation instructions are  available here:\\n\\n- [Miniconda](https://conda.io/miniconda.html)\\n- [Anaconda](https://www.continuum.io/downloads)\\n\\nRequirements for contributing to the `jwql` package will be included in the `jwql` `conda` environment, which is included in our installation instructions below. Further package requirements will be provided for `jwql` by a `setup.py` script included in the repository.\\n\\n### Clone the `jwql` repo\\n\\nYou first need to clone the current version of `jwql`. The simplest way to do this is to go to the directory you want your copy of the repository to be in and clone the repository there. Once you are in the directory you can do the following:\\n\\n```\\ngit clone https://github.com/spacetelescope/jwql.git\\ncd jwql\\n```\\n\\nor, if you would rather use `SSH` instead of `https`, type\\n```\\ngit clone git@github.com:spacetelescope/jwql.git\\ncd jwql\\n```\\ninstead, and then proceed as stated.\\n\\n### Environment Installation\\n\\nFollowing the download of the `jwql` repository, contributors can then install the `jwql` `conda` environment via the environment yaml file, which contains all of the dependencies for the project. First, if necessary, [install `conda`](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html). Next, ensure that your version of `conda` is up to date:\\n\\n```\\nconda update conda\\n```\\n\\nNext, activate the `base` or `root` environment (depending on your version of `conda`):\\n\\n```\\nsource activate base/root\\n```\\n\\n**Note:** If you have added a step activating conda to your default terminal/shell (e.g. the `.bashrc`, `.zshrc`, or `.profile` file) then you don\\'t need to do the above step.\\n\\nLastly, create the `jwql` environment via one of the `environment.yml` files (currently `environment_python_3_8.yml`, for python 3.8, and `environment_python_3.9.yml`, for python 3.9, are supported by `jwql`):\\n\\n```\\nconda env create -f environment_python_3_8.yml\\n```\\n\\nor\\n\\n```\\nconda env create -f environment_python_3_9.yml\\n```\\n\\n### Configuration File\\n\\nMuch of the `jwql` software depends on the existence of a `config.json` file within the `jwql` directory.  This file contains data that may be unique to users and/or contain sensitive information.  Please see the [Config File wiki page](https://github.com/spacetelescope/jwql/wiki/Config-file) for instructions on how to provide this file.\\n\\n\\n## Citation\\n\\nIf you use `JWQL` for work/research presented in a publication (whether directly,\\nor as a dependency to another package), we recommend and encourage the following acknowledgment:\\n\\n```\\n  This research made use of the open source Python package \\'jwql\\' (Bourque et al, 2020).\\n```\\n\\nwhere (Bourque et al, 2020) is a citation of the Zenodo record available using the DOI badge above. By using the `Export` box in the lower right corner of the Zenodo page, you can export the citation in the format most convenient for you.\\n\\n\\n## Software Contributions\\n\\nThere are two current pages to review before you begin contributing to the `jwql` development. The first is our [style guide](https://github.com/spacetelescope/jwql/blob/main/style_guide/README.md) and the second is our [suggested git workflow page](https://github.com/spacetelescope/jwql/wiki/git-&-GitHub-workflow-for-contributing), which contains an in-depth explanation of the workflow.\\n\\nContributors are also encouraged to check out the [Checklist for Contributors Guide](https://github.com/spacetelescope/jwql/wiki/Checklist-for-Contributors-and-Reviewers-of-Pull-Requests) to ensure the pull request contains all of the necessary changes.\\n\\nThe following is a bare-bones example of a best work flow for contributing to the project:\\n\\n1. Create a fork off of the `spacetelescope` `jwql` repository.\\n2. Make a local clone of your fork.\\n3. Ensure your personal fork is [pointing `upstream` properly](https://docs.github.com/en/free-pro-team@latest/github/collaborating-with-issues-and-pull-requests/configuring-a-remote-for-a-fork).\\n4. Create a branch on that personal fork.\\n5. Make your software changes.\\n6. Push that branch to your personal GitHub repository (i.e. `origin`).\\n7. On the `spacetelescope` `jwql` repository, create a pull request that merges the branch into `spacetelescope:develop`.\\n8. Assign a reviewer from the team for the pull request.\\n9. Iterate with the reviewer over any needed changes until the reviewer accepts and merges your branch.\\n10. Delete your local copy of your branch.\\n\\n\\n## Issue Reporting / Feature Requests\\n\\nUsers who wish to report an issue or request a new feature may do so through the following channels:\\n\\n1. Submit a new issue on GitHub (preferred method): https://github.com/spacetelescope/jwql/issues\\n2. Submit a new ticket on Jira: https://jira.stsci.edu/projects/JWQL/\\n\\n\\n## Code of Conduct\\n\\nUsers and contributors to the `jwql` repository should adhere to the [Code of Conduct](https://github.com/spacetelescope/jwql/blob/main/CODE_OF_CONDUCT.md).  Any issues or violations pertaining to the Code of Conduct should be brought to the attention of a `jwql` team member or to `jwql@stsci.edu`.\\n\\n## Questions\\n\\nAny questions about the `jwql` project or its software can be directed to `jwql@stsci.edu`.\\n\\n\\n## Current Development Team\\n- Bryan Hilbert (Project Manager, INS) [@bilhbert4](https://github.com/bhilbert4)\\n- Mees Fix (Technical Lead, INS) [@mfixstsci](https://github.com/mfixstsci)\\n- Misty Cracraft (INS) [@cracraft](https://github.com/cracraft)\\n- Mike Engesser (INS) [@mengesser](https://github.com/mengesser)\\n- Shannon Osborne (INS) [@shanosborne](https://github.com/shanosborne)\\n- Maria Pena-Guerrero [@penaguerrero](https://github.com/penaguerrero)\\n- Ben Sunnquist (INS) [@bsunnquist](https://github.com/bsunnquist)\\n- Brian York (INS) [@york-stsci](https://github.com/york-stsci)\\n\\n## Past Development Team Members\\n- Matthew Bourque (INS) [@bourque](https://github.com/bourque)\\n- Lauren Chambers (INS) [@laurenmarietta](https://github.com/laurenmarietta)\\n- Joe Filippazzo (INS) [@hover2pi](https://github.com/hover2pi)\\n- Graham Kanarek (INS) [@gkanarek](https://github.com/gkanarek)\\n- Teagan King (INS) [@tnking97](https://github.com/tnking97)\\n- Sara Ogaz (DMD) [@SaOgaz](https://github.com/SaOgaz)\\n- Catherine Martlin (INS) [@catherine-martlin](https://github.com/catherine-martlin)\\n- Johannes Sahlmann (INS) [@Johannes-Sahlmann](https://github.com/johannes-sahlmann)\\n\\n\\n## Acknowledgments:\\n- Faith Abney (DMD)\\n- Joshua Alexander (DMD) [@obviousrebel](https://github.com/obviousrebel)\\n- Anastasia Alexov (DMD)\\n- Sara Anderson (DMD)\\n- Tracy Beck (INS)\\n- Francesca Boffi (INS) [@frboffi](https://github.com/frboffi)\\n- Clara Brasseur (DMD) [@ceb8](https://github.com/ceb8)\\n- Matthew Burger (DMD)\\n- Steven Crawford (DMD) [@stscicrawford](https://github.com/stscicrawford)\\n- James Davies (DMD) [@jdavies-st](https://github.com/jdavies-st)\\n- Rosa Diaz (INS) [@rizeladiaz](https://github.com/rizeladiaz)\\n- Van Dixon (INS)\\n- Larry Doering (ITSD)\\n- Tom Donaldson (DMD) [@tomdonaldson](https://github.com/tomdonaldson)\\n- Kim DuPrie (DMD)\\n- Jonathan Eisenhamer (DMD) [@stscieisenhamer](https://github.com/stscieisenhamer)\\n- Ben Falk (DMD) [@falkben](https://github.com/falkben)\\n- Ann Feild (OPO)\\n- Mike Fox (DSMO) [@mfox22](https://github.com/mfox22)\\n- Scott Friedman (INS)\\n- Alex Fullerton (INS) [@awfullerton](https://github.com/awfullerton)\\n- Macarena Garcia Marin (INS)\\n- Lisa Gardner (DMD)\\n- Vera Gibbs (ITSD)\\n- Catherine Gosmeyer (INS) [@cgosmeyer](https://github.com/cgosmeyer)\\n- Phil Grant (ITSD)\\n- Dean Hines (INS)\\n- Sherie Holfeltz (INS) [@stholfeltz](https://github.com/stholfeltz)\\n- Joe Hunkeler (DMD) [@jhunkeler](https://github.com/jhunkeler)\\n- Catherine Kaleida (DMD) [@ckaleida](https://github.com/ckaleida)\\n- Deborah Kenny (DMD)\\n- Jenn Kotler (DMD) [@jenneh](https://github.com/jenneh)\\n- Daniel Kühbacher (Goddard) [@DanielKuebi](https://github.com/DanielKuebi)\\n- Mark Kyprianou (DMD) [@mkyp](https://github.com/mkyp)\\n- Stephanie La Massa (INS)\\n- Matthew Lallo (INS)\\n- Karen Levay (DMD)\\n- Crystal Mannfolk (SCOPE) [@cmannfolk](https://github.com/cmannfolk)\\n- Greg Masci (ITSD)\\n- Jacob Matuskey (DMD) [@jmatuskey](https://github.com/jmatuskey)\\n- Margaret Meixner (INS)\\n- Christain Mesh (DMD) [@cam72cam](https://github.com/cam72cam)\\n- Prem Mishra (ITSD)\\n- Don Mueller (ITSD)\\n- Maria Antonia Nieto-Santisteban (SEITO)\\n- Brian O\\'Sullivan (INS)\\n- Joe Pollizzi (JWSTMO)\\n- Lee Quick (DMD)\\n- Anupinder Rai (ITSD)\\n- Matt Rendina (DMD) [@rendinam](https://github.com/rendinam)\\n- Massimo Robberto (INS) [@mrobberto](https://github.com/mrobberto)\\n- Mary Romelfanger (DMD)\\n- Elena Sabbi (INS)\\n- Bernie Shiao (DMD)\\n- Matthew Sienkiewicz (ITSD)\\n- Arfon Smith (DSMO) [@arfon](https://github.com/arfon)\\n- Linda Smith (INS)\\n- Patrick Taylor (ITSD)\\n- Dave Unger (ITSD)\\n- Jeff Valenti (JWSTMO) [@JeffValenti](https://github.com/JeffValenti)\\n- Jeff Wagner (ITSD)\\n- Thomas Walker (ITSD)\\n- Geoff Wallace (DMD)\\n- Lara Wilkinson (OPO)\\n- Alex Yermolaev (ITSD) [@alexyermolaev](https://github.com/alexyermolaev)\\n- Joe Zahn (ITSD)\\n'},\n",
       " {'repo': 'Votyn/SpaceBoat',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Space Boat\\nDiscord.js Multifunction bot primarily made for the [Space Engine Discord Server](https://discord.gg/spaceengine)\\n## Planned features\\n - ~~Moderation features such as mute, kick, ban, \"roleremove\", and some other role addition commands.~~\\n - ~~Logging functionality to log user actions such as leaving and entering the server, information of which is stored in a logging channel.~~\\n - Role stasis, where if a user leaves the server and comes back less than a week after, their roles remain. (mainly to prevent someone from getting rid of their roles)\\n - An \"experience\" and autorole system.\\n - An adaptable \"Starboard\", where a channel can be made into a starboard, that listens to a set of channels given to it, and the \"starring\" of messages in said channel.\\n\\n## Getting the official bot on your server\\nThis is currently unavailable as the bot is still highly in development! For now, you can try out the bot by installing it yourself, as shown below!\\n\\n## Installation and Running of the bot.\\nThis bot requires [node.js](https://nodejs.org/en/download/), [git](https://git-scm.com/downloads), and [yarn](https://yarnpkg.com/en/docs/install). \\n\\nTo download the **stable** version of the bot; i.e. the version the official bot is running, run the following in a terminal:\\n```bash\\ngit clone https://github.com/Votyn/SpaceBoat.git\\n```\\nTo download the **indev** version of the bot that I am currently working on, you have to clone from the `dev` branch:\\n```bash\\ngit clone -b dev https://github.com/Votyn/SpaceBoat.git\\n```\\nHaving downloaded the bot, download the node dependencies:\\n```bash\\nyarn install\\n```\\nThen to start the bot (This uses `nodemon`):\\n```bash\\nyarn start\\n```\\nIf you want to run it in the background with `screen` you can use `yarn background` instead.\\n'},\n",
       " {'repo': 'codepoetpbowden/ConnectedLivingSpace',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': \"ConnectedLivingSpace\\n====================\\n\\nOverview\\n--------\\nKSP addon to identify and make use of living spaces that are connected to each other on a vessel.\\n\\nOriginally by [**codepoetbowden**](https://github.com/codepoetpbowden/ConnectedLivingSpace), later maintained by **PapaJoesSoup**.\\n\\nCurrently maintained by [**Papa_Joe**](https://github.com/mwerle/ConnectedLivingSpace).\\n\\nInterim maintainance by [Micha](https://github.com/mwerle/ConnectedLivingSpace). Covers releases during Papa_Joe's absence\\n\\nChanges\\n-------\\n\\nPlease see the [Changelog](CHANGELOG.md) for a version history and overview of changes. For the full details, please see the GitHub commit log.\\n\\nContributing\\n------------\\n\\nPlease see the [Contribution Guidelines for this project](Source/Contributing.md) which provide an overview of how this project is laid out and how to support it.\\n\\nAlso make sure you agree with the [Project License](License.txt) before starting any work.\\n\\nA list of contributors can be [found here](CONTRIBUTORS.md).\\n\\nResources\\n---------\\n\\n - [PapaJoe's Release Forum Thread](http://forum.kerbalspaceprogram.com/index.php?showtopic=109972) is the main discussion and announcements location for this mod.\\n   - [Micha's forum thread](http://forum.kerbalspaceprogram.com/index.php?showtopic=192130) contains support info during Papa_Joe's absence.\\n - The official [Wiki](https://github.com/codepoetpbowden/ConnectedLivingSpace/wiki) contains lots more information, especially for developers wishing to integrate their mod with CLS.\\n\\nRequirements\\n------------\\n\\nThis mod relies on [ModuleManager](https://github.com/sarbian/ModuleManager) for adding functionality to stock parts and mod parts which don't have CLS configurations built in.\\n\\nDownload Links\\n--------------\\n\\n - [GitHub](https://github.com/codepoetpbowden/ConnectedLivingSpace/releases) - primary download location.\\n - [SpaceDock](https://spacedock.info/mod/190)\\n - [CurseForge](https://www.curseforge.com/kerbal/ksp-mods/connectedlivingspace)\\n - And via the [CKAN](http://forum.kerbalspaceprogram.com/index.php?90246) tool.\\n\"},\n",
       " {'repo': 'SpaceNetChallenge/SpaceNet7_Multi-Temporal_Solutions',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '<p align=\"center\">\\n<a href=\"https://spacenet.ai\"><img src=\"_figs/sn_logo.png\" width=\"350\" alt=\"SpaceNet LLC\"></a>\\n</p>\\n<h1 align=\"center\">The SpaceNet 7 Multi-temporal Urban Development Challenge </h1>\\n<h2 align=\"center\">Winning Solutions</h2>\\n<br>\\n\\n## Summary\\n\\nThe five subdirectories in this repository comprise the code for the winning solutions of SpaceNet 7 hosted by TopCoder. Each subdirectory contains the competitors\\' written descriptions of their solution to the challenge. See the blog post on CosmiQ Works\\' blog [The DownlinQ](?) for an additional summary.  Baseline code can be found [here](https://github.com/CosmiQ/CosmiQ_SN7_Baseline).\\n\\nData is hosted on aws at:\\n```\\n\\ts3://spacenet-dataset/spacenet/SN7_buildings/\\n```\\n\\nWinning model weights are hosted at:\\n```\\n\\ts3://spacenet-dataset/spacenet-model-weights/spacenet-7/\\n```\\n\\nThe winning solutions all use Docker, and assume SpaceNet 7 data is mounted in the `/data/` directory.  \\n\\nPerformance of the algorithms on the SpaceNet 7 final test set is shown below:\\n\\n![alt text](_figs/table1.png)\\n\\n![alt text](_figs/scot_rate_plot.png)\\n\\n---------\\n\\nQuestions about SpaceNet? Check out our website at [https://spacenet.ai](https://spacenet.ai).\\n\\n\\n\\n'},\n",
       " {'repo': 'keijiro/KinoObscurance',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': 'KinoObscurance\\n==============\\n\\n**Obscurance** is a screen-space ambient obscurance (SSAO) image effect for\\nUnity.\\n\\n![Screenshot](http://i.imgur.com/OdTiiIx.png)\\n![Screenshot](http://i.imgur.com/zSdPL8U.png)\\n\\nSystem Requirements\\n-------------------\\n\\nUnity 5.5 or later versions.\\n\\nObscurance runs on most of the platforms supported by Unity (including mobile\\nplatforms). Please [open an issue] if there is any compatibility problem.\\n\\n[open an issue]: https://github.com/keijiro/KinoObscurance/issues\\n\\nInstallation\\n------------\\n\\nDownload one of the unitypackage files from the [Releases] page and import it\\nto a project.\\n\\n[Releases]: https://github.com/keijiro/KinoObscurance/releases\\n\\nLicense\\n-------\\n\\n[MIT](LICENSE.md)\\n'},\n",
       " {'repo': 'multiformats/multicodec',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# multicodec\\n\\n[![](https://img.shields.io/badge/made%20by-Protocol%20Labs-blue.svg?style=flat-square)](http://ipn.io)\\n[![](https://img.shields.io/badge/project-multiformats-blue.svg?style=flat-square)](https://github.com/multiformats/multiformats)\\n[![](https://img.shields.io/badge/freenode-%23ipfs-blue.svg?style=flat-square)](https://webchat.freenode.net/?channels=%23ipfs)\\n[![](https://img.shields.io/badge/readme%20style-standard-brightgreen.svg?style=flat-square)](https://github.com/RichardLitt/standard-readme)\\n\\n> Canonical table of of codecs used by various multiformats\\n\\n## Table of Contents\\n\\n- [Motivation](#motivation)\\n- [Description](#description)\\n- [Examples](#examples)\\n- [Multicodec table](#multicodec-table)\\n  - [Adding new multicodecs to the table](#adding-new-multicodecs-to-the-table)\\n- [Implementations](#implementations)\\n- [Reserved Code Ranges](#reserved-code-ranges)\\n- [FAQ](#faq)\\n- [Contribute](#contribute)\\n- [License](#license)\\n\\n## Motivation\\n\\nMulticodec is an agreed-upon codec table. It is designed for use in binary representations, such as keys or identifiers (i.e [CID](https://github.com/ipld/cid)).\\n\\n## Description\\n\\nThe code of a multicodec is usually encoded as unsigned varint as defined by [multiformats/unsigned-varint](https://github.com/multiformats/unsigned-varint). It is then used as a prefix to identify the data that follows.\\n\\n## Examples\\n\\nMulticodec is used in various [Multiformats](https://github.com/multiformats/multiformats). In [Multihash](https://github.com/multiformats/multihash) it is used to identify the hashes, in the machine-readable [Multiaddr](https://github.com/multiformats/multiaddr) to identify components such as IP addresses, domain names, identities, etc.\\n\\n## Multicodec table\\n\\nFind the canonical table of multicodecs at [table.csv](/table.csv). There\\'s also a sortable [viewer](https://ipfs.io/ipfs/QmXec1jjwzxWJoNbxQF5KffL8q6hFXm9QwUGaa3wKGk6dT/#title=Multicodecs&src=https://raw.githubusercontent.com/multiformats/multicodec/master/table.csv).\\n\\n### Status\\n\\nEach multicodec is marked with a status:\\n\\n* draft - this codec has been reserved but may be reassigned if it doesn\\'t gain wide adoption.\\n* permanent - this codec has been widely adopted and may not reassigned.\\n\\nNOTE: Just because a codec is marked draft, don\\'t assume that it can be re-assigned. Check to see if it ever gained wide adoption and, if so, mark it as permanent.\\n\\n### Adding new multicodecs to the table\\n\\nThe process to add a new multicodec to the table is the following:\\n\\n1. Fork this repo\\n2. Add your codecs to the table. Each newly proposed codec must have:\\n  1. A unique codec.\\n  2. A unique name.\\n  3. A category.\\n  4. A status of \"draft\".\\n3. Submit a Pull Request\\n\\nThis [\"first come, first assign\"](https://github.com/multiformats/multicodec/pull/16#issuecomment-260146609) policy is a way to assign codes as they are most needed, without increasing the size of the table (and therefore the size of the multicodecs) too rapidly.\\n\\nThe first 127 bits are encoded as a single-byte varint, hence they are reserved for the most widely used multicodecs. So if you are adding your own codec to the table, you most likely would want to ask for a codec bigger than `0x80`.\\n\\nCodec names should be easily convertible to constants in common programming languages using basic transformation rules (e.g. upper-case, conversion of `-` to `_`, etc.). Therefore they should contain alphanumeric characters, with the first character being alphabetic. The primary delimiter for multi-part names should be `-`, with `_` reserved for cases where a secondary delimiter is required. For example: `bls12_381-g1-pub` contains 3 parts: `bls_381`, `g1` and `pub`, where `bls_381` is \"BLS 381\" which is not commonly written as \"BLS381\" and therefore requires a secondary separator.\\n\\nThe `validate.py` script can be used to validate the table once it\\'s edited.\\n\\n## Implementations\\n\\n- [go](https://github.com/multiformats/go-multicodec/)\\n- [JavaScript](https://github.com/multiformats/js-multicodec)\\n- Python\\n  - [py-multicodec](https://github.com/multiformats/py-multicodec)\\n  - `multicodec` sub-module of Python module [multiformats](https://github.com/hashberg-io/multiformats)\\n- [Haskell](https://github.com/multiformats/haskell-multicodec)\\n- [Elixir](https://github.com/nocursor/ex-multicodec)\\n- [Scala](https://github.com/fluency03/scala-multicodec)\\n- [Ruby](https://github.com/sleeplessbyte/ruby-multicodec)\\n- [Java](https://github.com/richardbergquist/java-multicodec)\\n- [Add yours today!](https://github.com/multiformats/multicodec/edit/master/table.csv)\\n\\n## Reserved Code Ranges\\n\\nThe following code ranges have special meaning and may only have meanings assigned to as specified in their description:\\n\\n### Private Use Area\\n\\n*Range*: `0x300000 – 0x3FFFFF`\\n\\nCodes in this range are reserved for internal use by applications and will never be assigned any meaning as part of the Multicodec specification.\\n\\n## FAQ\\n\\n> Why varints?\\n\\nSo that we have no limitation on protocols.\\n\\n> What kind of varints?\\n\\nAn Most Significant Bit unsigned varint, as defined by the [multiformats/unsigned-varint](https://github.com/multiformats/unsigned-varint).\\n\\n> Don\\'t we have to agree on a table of protocols?\\n\\nYes, but we already have to agree on what protocols themselves are, so this is not so hard. The table even leaves some room for custom protocol paths, or you can use your own tables. The standard table is only for common things.\\n\\n> Where did multibase go?\\n\\nFor a period of time, the [multibase](https://github.com/multiformats/multibase) prefixes lived in this table. However, multibase prefixes are *symbols* that may map to *multiple* underlying byte representations (that may overlap with byte sequences used for other multicodecs). Including them in a table for binary/byte identifiers lead to more confusion than it solved.\\n\\nYou can still find the table in [multibase.csv](https://github.com/multiformats/multibase/blob/master/multibase.csv).\\n\\n> Can I use multicodec for my own purpose?\\n\\nSure, you can use multicodec whenever you have the need for self-identifiable data. Just prefix your own data with the corresponding varint encodec multicodec.\\n\\n## Contribute\\n\\nContributions welcome. Please check out [the issues](https://github.com/multiformats/multicodec/issues).\\n\\nCheck out our [contributing document](https://github.com/multiformats/multiformats/blob/master/contributing.md) for more information on how we work, and about contributing in general. Please be aware that all interactions related to multiformats are subject to the IPFS [Code of Conduct](https://github.com/ipfs/community/blob/master/code-of-conduct.md).\\n\\nSmall note: If editing the README, please conform to the [standard-readme](https://github.com/RichardLitt/standard-readme) specification.\\n\\n## License\\n\\nThis repository is only for documents. All of these are licensed under the [CC-BY-SA 3.0](https://ipfs.io/ipfs/QmVreNvKsQmQZ83T86cWSjPu2vR3yZHGPm5jnxFuunEB9u) license © 2016 Protocol Labs Inc. Any code is under a [MIT](LICENSE) © 2016 Protocol Labs Inc.\\n'},\n",
       " {'repo': 'IBM/spring-boot-microservices-on-kubernetes',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '[![构建状态](https://travis-ci.org/IBM/spring-boot-microservices-on-kubernetes.svg?branch=master)](https://travis-ci.org/IBM/spring-boot-microservices-on-kubernetes)\\r\\n![Bluemix 部署](https://metrics-tracker.mybluemix.net/stats/13404bda8d87a6eca2c5297511ae9a5e/badge.svg)\\r\\n\\r\\n#  在 Kubernetes 上构建和部署 Java Spring Boot 微服务\\r\\n\\r\\n*阅读本文的其他语言版本：[English](README.md)。*\\r\\n\\r\\nSpring Boot 是常用 Java 微服务框架之一。Spring Cloud 拥有一组丰富的、良好集成的 Java 类库，用于应对 Java 应用程序堆栈中发生的运行时问题；而 Kubernetes 则提供了丰富的功能集来运行多语言微服务。这些技术彼此互补，为 Spring Boot 应用程序提供了强大的平台。\\r\\n\\r\\n在此代码中，我们演示了如何在 Kubernetes 上部署一个简单的 Spring Boot 应用程序。此应用程序称为 Office Space，它模仿了电影[上班一条虫 (Office Space)](http://www.imdb.com/title/tt0151804/) 中 Michael Bolton 的虚构应用程序创意。该应用程序利用了这样一个金融方案：通常不满一分钱的分币会四舍五入，而此方案将这部分币值转移到一个独立的银行账户中来计算交易利息。\\r\\n\\r\\n该应用程序使用 Java 8/Spring Boot 微服务计算利息，然后将分币存入数据库。另一个 Spring Boot 微服务是通知服务。当账户余额超过 50,000 美元时，它会发送电子邮件。它是由计算利息的 Spring Boot Web 服务器触发的。前端使用 Node.js 应用程序来显示 Spring Boot 应用程序累积的当前账户余额。后端使用 MySQL 数据库来存储账户余额。\\r\\n\\r\\n![spring-boot-kube](images/spring-boot-kube.png)\\r\\n\\r\\n## 前提条件\\r\\n\\r\\n使用 [Minikube](https://kubernetes.io/docs/getting-started-guides/minikube) 创建一个 Kubernetes 集群用于本地测试，使用 [IBM Cloud Private](https://github.com/IBM/Kubernetes-container-service-GitLab-sample/blob/master/docs/deploy-with-ICP.md) 或者 [IBM Cloud Container Service](https://github.com/IBM/container-journey-template) 创建一个 Kubernetes 集群以部署到云中。本文中的代码使用 Travis 定期使用[基于 IBM Cloud Container Service 的 Kubernetes 集群](https://console.ng.bluemix.net/docs/containers/cs_ov.html#cs_ov) 进行测试。\\r\\n\\r\\n\\r\\n## 步骤\\r\\n1.[创建数据库服务](#1-create-the-database-service)  \\r\\n1.1 [在容器中使用 MySQL](#11-use-mysql-in-container) 或者  \\r\\n1.2 [使用 IBM Cloud MySQL服务](#12-use-bluemix-mysql)  \\r\\n2.[创建 Spring Boot 微服务](#2-create-the-spring-boot-microservices)  \\r\\n2.1 [使用 Maven 构建项目](#21-build-your-projects-using-maven)  \\r\\n2.2 [构建和推送 Docker 镜像](#22-build-your-docker-images-for-spring-boot-services)  \\r\\n2.3 [为 Spring Boot 服务修改 yaml 文件](#23-modify-compute-interest-apiyaml-and-send-notificationyaml-to-use-your-image)  \\r\\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.3.1 [在通知服务中使用默认电子邮件服务](#231-use-default-email-service-gmail-with-notification-service) 或者  \\r\\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.3.2 [在通知服务中使用 OpenWhisk Actions](#232-use-openwhisk-action-with-notification-service)  \\r\\n2.4 [部署 Spring Boot 微服务](#24-deploy-the-spring-boot-microservices)  \\r\\n3.[创建前端服务](#3-create-the-frontend-service)  \\r\\n4.[创建交易生成器服务](#4-create-the-transaction-generator-service)  \\r\\n5.[访问应用程序](#5-access-your-application)\\r\\n\\r\\n#### [故障排除](#troubleshooting-1)\\r\\n\\r\\n# 1.创建数据库服务\\r\\n\\r\\n后端包含 MySQL 数据库和 Spring Boot 应用程序。每一项\\r\\n微服务都包含一个 Kubernetes Deployment 和一个 Kubernetes Service。Kubernetes Deployment 用于管理每一项微服务所启动的 pod。Kubernetes Service 用于\\r\\n为每一项微服务创建一个稳定的 DNS 记录，以便它们可以\\r\\n根据域名相互引用。\\r\\n\\r\\n* 创建 MySQL 数据库后端的方法有两种：\\r\\n  **[在容器中使用 MySQL](#11-use-mysql-in-container)** *或者*\\r\\n  **[使用 IBM Cloud MySQL 服务](#12-use-bluemix-mysql)**\\r\\n\\r\\n## 1.1 在容器中使用 MySQL\\r\\n```bash\\r\\n$ kubectl create -f account-database.yaml\\r\\nservice \"account-database\" created\\r\\ndeployment \"account-database\" created\\r\\n```\\r\\n默认认证信息已使用 base64 在 secrets.yaml 中进行了编码。\\r\\n> base64 编码不会加密或隐藏您的密钥。请勿将其上传至您的 Github 仓库中。\\r\\n\\r\\n```\\r\\n$ kubectl apply -f secrets.yaml\\r\\nsecret \"demo-credentials\" created\\r\\n```\\r\\n\\r\\n下一步请参考[步骤 2](#2-create-the-spring-boot-microservices) 。\\r\\n\\r\\n## 1.2 使用 IBM Cloud MySQL 服务\\r\\n通过 https://console.ng.bluemix.net/catalog/services/compose-for-mysql 在 IBM Cloud 中为 MySQL 提供 Provision Compose\\r\\n转至 Service credentials 并查看您的凭证。包括 MySQL 主机名、端口、用户名和密码等信息位于凭证 URI 下，如下所示：\\r\\n![images](images/mysqlservice.png)\\r\\n您将需要应用这些凭证作为 Kubernetes 集群中的密钥。这些信息应已被 `base64` 编码。\\r\\n使用脚本 `./scripts/create-secrets.sh`。系统将提示您输入自己的凭证。这将对您输入的凭证进行编码，并创建 Kubenetes Secret 对象。\\r\\n```bash\\r\\n$ ./scripts/create-secrets.sh\\r\\nEnter MySQL username:\\r\\nadmin\\r\\nEnter MySQL password:\\r\\npassword\\r\\nEnter MySQL host:\\r\\nhostname\\r\\nEnter MySQL port:\\r\\n23966\\r\\nsecret \"demo-credentials\" created\\r\\n```\\r\\n\\r\\n_您也可以编辑 `secrets.yaml` 文件，将其中的数据值编辑为自己的 base64 编码的凭证。然后执行 `kubectl apply -f secrets.yaml`。_\\r\\n\\r\\n下一步请参考[步骤 2](#2-create-the-spring-boot-microservices) 。\\r\\n\\r\\n# 2.创建 Spring Boot 微服务\\r\\n您需要[安装 Maven](https://maven.apache.org/index.html) 工具。\\r\\n如果要修改 Spring Boot 应用程序，请在构建 Java 项目和 Docker 镜像之前完成修改。\\r\\n\\r\\nSpring Boot 微服务包括 **Compute-Interest-API** 和 **Send-Notification**。\\r\\n\\r\\n**Compute-Interest-API** 是一个需要使用 MySQL 数据库的 Spring Boot 应用程序。相关配置位于 `spring.datasource*` 中的 application.properties 中。\\r\\n\\r\\n*compute-interest-api/src/main/resources/application.properties*\\r\\n```\\r\\nspring.datasource.url = jdbc:mysql://${MYSQL_DB_HOST}:${MYSQL_DB_PORT}/dockercon2017\\r\\n\\r\\n# Username and password\\r\\nspring.datasource.username = ${MYSQL_DB_USER}\\r\\nspring.datasource.password = ${MYSQL_DB_PASSWORD}\\r\\n```\\r\\n\\r\\n`application.properties` 配置为使用 MYSQL_DB_* 环境变量。这些变量在 `compute-interest-api.yaml` 文件中定义。\\r\\n*compute-interest-api.yaml*\\r\\n```yaml\\r\\nspec:\\r\\n  containers:\\r\\n  - image: anthonyamanse/compute-interest-api:secrets\\r\\n    imagePullPolicy: Always\\r\\n    name: compute-interest-api\\r\\n    env:\\r\\n      - name: MYSQL_DB_USER\\r\\n        valueFrom:\\r\\n          secretKeyRef:\\r\\n            name: demo-credentials\\r\\n            key: username\\r\\n      - name: MYSQL_DB_PASSWORD\\r\\n        valueFrom:\\r\\n          secretKeyRef:\\r\\n            name: demo-credentials\\r\\n            key: password\\r\\n      - name: MYSQL_DB_HOST\\r\\n        valueFrom:\\r\\n          secretKeyRef:\\r\\n            name: demo-credentials\\r\\n            key: host\\r\\n      - name: MYSQL_DB_PORT\\r\\n        valueFrom:\\r\\n          secretKeyRef:\\r\\n            name: demo-credentials\\r\\n            key: port\\r\\n    ports:\\r\\n    - containerPort: 8080\\r\\n```\\r\\n\\r\\nYAML 文件已配置为从先前创建的 Kubernetes Secret 中获取值。这些信息将最终写入`application.properties`并最终为 Spring Boot 应用程序所用。 \\r\\n\\r\\n**Send-Notification** 可配置为通过 Gmail 和/或 Slack 发送通知。通知仅在 MySQL 数据库中的账户余额超过 50,000 美元时推送一次。默认设置为使用 Gmail 。通知。您还可以使用事件驱动技术（在本例中为 [OpenWhisk](http://openwhisk.org/)） 来发送电子邮件和 Slack 消息。要将 OpenWhisk 与您的通知微服务配合使用，请在构建和部署微服务映像之前遵循[此处](#232-use-openwhisk-action-with-notification-service) 的步骤进行操作。否则，只有在选择仅使用电子邮件通知后才能继续。\\r\\n\\r\\n## 2.1.使用 Maven 构建项目\\r\\n\\r\\n当 Maven 成功构建 Java 项目后，您需要使用在其相应文件夹中提供的 **Dockerfile** 构建 Docker 镜像。\\r\\n> 备注：compute-interest-api 会将分币乘以 100,000，用于执行模拟。您可以编辑/移除 `src/main/java/officespace/controller/MainController.java` 中的 `remainingInterest *= 100000` 行。当余额超过 50,000 美元时，程序还会发送通知， 您可以编辑 `if (updatedBalance > 50000 && emailSent == false )` 行中的数字。保存更改后，就可以构建项目了。\\r\\n\\r\\n```bash\\r\\nGo to containers/compute-interest-api\\r\\n$ mvn package\\r\\n\\r\\nGo to containers/send-notification\\r\\n$ mvn package\\r\\n\\r\\n```\\r\\n*我们将使用 IBM Cloud 容器镜像仓库来保存镜像（由此进行映像命名），也可以使用 [Docker Hub](https://docs.docker.com/datacenter/dtr/2.2/guides/user/manage-images/pull-and-push-images) 保存镜像。*\\r\\n## 2.2 为 Spring Boot 服务构建 Docker 映像\\r\\n> 备注：本文使用 IBM Cloud 容器镜像库中保存镜像。\\r\\n\\r\\n如果您计划使用 IBM Cloud 容器镜像库，需要首先设置帐户。请遵循[此处](https://developer.ibm.com/recipes/tutorials/getting-started-with-private-registry-hosted-by-ibm-bluemix/) 的教程进行操作。\\r\\n\\r\\n您也可以使用  [Docker Hub](https://hub.docker.com)  保存镜像。\\r\\n\\r\\n```bash\\r\\n$ docker build -t registry.ng.bluemix.net/<namespace>/compute-interest-api .\\r\\n$ docker build -t registry.ng.bluemix.net/<namespace>/send-notification .\\r\\n$ docker push registry.ng.bluemix.net/<namespace>/compute-interest-api\\r\\n$ docker push registry.ng.bluemix.net/<namespace>/send-notification\\r\\n```\\r\\n## 2.3 为使用您所构建的镜像，需要修改 *compute-interest-api.yaml* 和 *send-notification.yaml* 文件\\r\\n\\r\\n成功推送镜像后，您将需要修改 yaml 文件以使用自己的镜像。\\r\\n```yaml\\r\\n// compute-interest-api.yaml\\r\\n  spec:\\r\\n    containers:\\r\\n    - image: registry.ng.bluemix.net/<namespace>/compute-interest-api # replace with your image name\\r\\n```\\r\\n\\r\\n```yaml\\r\\n// send-notification.yaml\\r\\n  spec:\\r\\n    containers:\\r\\n    - image: registry.ng.bluemix.net/<namespace>/send-notification # replace with your image name\\r\\n```\\r\\n\\r\\n存在两种可能的通知方式，请参见：\\r\\n[2.3.1 使用默认电子邮件服务](#231-use-default-email-service-gmail-with-notification-service)\\r\\n**或**\\r\\n[2.3.2 使用 OpenWhisk Actions](#232-use-openwhisk-action-with-notification-service)。\\r\\n\\r\\n### 2.3.1 使用默认电子邮件服务 (Gmail) 来处理通知服务\\r\\n\\r\\n\\r\\n您将需要修改 `send-notification.yaml` 中的 **环境变量**：\\r\\n```yaml\\r\\n    env:\\r\\n    - name: GMAIL_SENDER_USER\\r\\n       value: \\'username@gmail.com\\' # change this to the gmail that will send the email\\r\\n    - name: GMAIL_SENDER_PASSWORD\\r\\n       value: \\'password\\' # change this to the the password of the gmail above\\r\\n    - name: EMAIL_RECEIVER\\r\\n       value: \\'sendTo@gmail.com\\' # change this to the email of the receiver\\r\\n```\\r\\n\\r\\n现在，您可以继续执行[步骤 2.4](#24-deploy-the-spring-boot-microservices)。\\r\\n\\r\\n### 2.3.2 使用 OpenWhisk Action 来处理通知服务\\r\\n本部分的要求：\\r\\n* 您的 Slack 团队中具有 [Slack Incoming Webhook](https://api.slack.com/incoming-webhooks)。\\r\\n* **IBM Cloud帐户**，以便使用 [OpenWhisk CLI](https://console.ng.bluemix.net/openwhisk/)。\\r\\n\\r\\n\\r\\n#### 2.3.2.1 创建 Actions\\r\\n本代码库的根目录中包含您创建 OpenWhisk Actions 时所需的代码。\\r\\n如果您尚未安装 OpenWhisk CLI，请转至[此处](https://console.ng.bluemix.net/openwhisk/)。\\r\\n您可以使用 `wsk` 命令来创建 OpenWhisk Actions。创建操作使用以下语法：`wsk action create < action_name > < source code for action> [add --param for optional Default parameters]`\\r\\n* 创建用于发送 **Slack 通知** 的 Action\\r\\n```bash\\r\\n$ wsk action create sendSlackNotification sendSlack.js --param url https://hooks.slack.com/services/XXXX/YYYY/ZZZZ\\r\\nReplace the url with your Slack team\\'s incoming webhook url.\\r\\n```\\r\\n* 创建用于发送 **Gmail 通知** 的 Action\\r\\n```bash\\r\\n$ wsk action create sendEmailNotification sendEmail.js\\r\\n```\\r\\n\\r\\n#### 2.3.2.2 测试 Actions\\r\\n您可以使用 `wsk action invoke [action name] [add --param to pass  parameters]` 测试 OpenWhisk Actions\\r\\n* 调用 Slack 通知\\r\\n```bash\\r\\n$ wsk action invoke sendSlackNotification --param text \"Hello from OpenWhisk\"\\r\\n```\\r\\n* 调用电子邮件通知\\r\\n```bash\\r\\n$ wsk action invoke sendEmailNotification --param sender [sender\\'s email] --param password [sender\\'s password]--param receiver [receiver\\'s email] --param subject [Email subject] --param text [Email Body]\\r\\n```\\r\\n至此，您应分别收到一条 Slack 消息和一封电子邮件。\\r\\n\\r\\n#### 2.3.2.3 为 Actions 创建 REST API\\r\\n您可以使用 `wsk api create` 为创建的 Action 映射 REST API 端点。其语法为 `wsk api create [base-path] [api-path] [verb (GET PUT POST etc)] [action name]`\\r\\n* 创建用于 **Slack 通知** 的 REST API 端点\\r\\n```bash\\r\\n$ wsk api create /v1 /slack POST sendSlackNotification\\r\\nok: created API /v1/email POST for action /_/sendEmailNotification\\r\\nhttps://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/slack\\r\\n```\\r\\n* 创建用于 **Gmail 通知** 的 REST API 端点\\r\\n```bash\\r\\n$ wsk api create /v1 /email POST sendEmailNotification\\r\\nok: created API /v1/email POST for action /_/sendEmailNotification\\r\\nhttps://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/email\\r\\n```\\r\\n\\r\\n您可以使用以下命令查看 API 列表：\\r\\n```bash\\r\\n$ wsk api list\\r\\nok: APIs\\r\\nAction                                      Verb  API Name  URL\\r\\n/Anthony.Amanse_dev/sendEmailNotificatio    post       /v1  https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/email\\r\\n/Anthony.Amanse_dev/testDefault             post       /v1  https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/slack\\r\\n```\\r\\n\\r\\n请记录 这些 API URL，稍后我们将使用它们 。\\r\\n\\r\\n#### 2.3.2.4 测试 REST API URL\\r\\n\\r\\n* 测试用于 **Slack 通知** 的 REST API 端点。这里请使用您自己的 API URL。\\r\\n```bash\\r\\n$ curl -X POST -d \\'{ \"text\": \"Hello from OpenWhisk\" }\\' https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/slack\\r\\n```\\r\\n![Slack 通知](images/slackNotif.png)\\r\\n* 测试用于 **Gmail 通知** 的 REST API 端点。这里请使用您自己的 API URL。将参数 **sender、password、receiver 和 subject** 的值替换为您自己的值。\\r\\n```bash\\r\\n$ curl -X POST -d \\'{ \"text\": \"Hello from OpenWhisk\", \"subject\": \"Email Notification\", \"sender\": \"testemail@gmail.com\", \"password\": \"passwordOfSender\", \"receiver\": \"receiversEmail\" }\\' https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/email\\r\\n```\\r\\n![电子邮件通知](images/emailNotif.png)\\r\\n\\r\\n#### 2.3.2.5 将 REST API URL 添加到 yaml 文件中\\r\\n一旦确认您的 API 运行正常，就可以将这些 URL 放入 `send-notification.yaml` 文件中了\\r\\n```yaml\\r\\nenv:\\r\\n- name: GMAIL_SENDER_USER\\r\\n  value: \\'username@gmail.com\\' # the sender\\'s email\\r\\n- name: GMAIL_SENDER_PASSWORD\\r\\n  value: \\'password\\' # the sender\\'s password\\r\\n- name: EMAIL_RECEIVER\\r\\n  value: \\'sendTo@gmail.com\\' # the receiver\\'s email\\r\\n- name: OPENWHISK_API_URL_SLACK\\r\\n  value: \\'https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/slack\\' # your API endpoint for slack notifications\\r\\n- name: SLACK_MESSAGE\\r\\n  value: \\'Your balance is over $50,000.00\\' # your custom message\\r\\n- name: OPENWHISK_API_URL_EMAIL\\r\\n  value: \\'https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/email\\' # your API endpoint for email notifications\\r\\n```\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n## 2.4 部署 Spring Boot 微服务\\r\\n```bash\\r\\n$ kubectl create -f compute-interest-api.yaml\\r\\nservice \"compute-interest-api\" created\\r\\ndeployment \"compute-interest-api\" created\\r\\n```\\r\\n```bash\\r\\n$ kubectl create -f send-notification.yaml\\r\\nservice \"send-notification\" created\\r\\ndeployment \"send-notification\" created\\r\\n```\\r\\n\\r\\n# 3.创建前端服务\\r\\n此用户界面是 Node.js 应用程序，可显示账户总余额。\\r\\n**如果您在 IBM Cloud 中使用 MySQL 数据库，请记得填充 `account-summary.yaml` 文件中环境变量的值，否则请将其留空。这是在[步骤 1](#1-create-the-database-service) 中执行的操作。**\\r\\n\\r\\n\\r\\n* 创建 **Node.js** 前端：\\r\\n```bash\\r\\n$ kubectl create -f account-summary.yaml\\r\\nservice \"account-summary\" created\\r\\ndeployment \"account-summary\" created\\r\\n```\\r\\n\\r\\n# 4.创建交易生成器服务\\r\\n交易生成器是 Python 应用程序，可使用累积利息生成随机交易。\\r\\n* 创建交易生成器 **Python** 应用程序：\\r\\n```bash\\r\\n$ kubectl create -f transaction-generator.yaml\\r\\nservice \"transaction-generator\" created\\r\\ndeployment \"transaction-generator\" created\\r\\n```\\r\\n\\r\\n# 5.访问应用程序\\r\\n您可以通过 Kubernetes Cluster IP 和 NodePort 访问应用程序。NodePort 应为 **30080**。\\r\\n\\r\\n* 要查找 Cluster IP，请执行以下命令：\\r\\n```bash\\r\\n$ bx cs workers <cluster-name>\\r\\nID                                                 Public IP        Private IP      Machine Type   State    Status   \\r\\nkube-dal10-paac005a5fa6c44786b5dfb3ed8728548f-w1   169.47.241.213   10.177.155.13   free           normal   Ready  \\r\\n```\\r\\n\\r\\n* 要查找账户摘要 (account-summary) 服务的 NodePort，请执行以下命令：\\r\\n```bash\\r\\n$ kubectl get svc\\r\\nNAME                    CLUSTER-IP     EXTERNAL-IP   PORT(S)                                                                      AGE\\r\\n...\\r\\naccount-summary         10.10.10.74    <nodes>       80:30080/TCP                                                                 2d\\r\\n...\\r\\n```\\r\\n* 在您的浏览器上，转至 `http://<your-cluster-IP>:30080`\\r\\n![账户余额](images/balance.png)\\r\\n\\r\\n## 故障排除\\r\\n* 要从头开始，请删除所有内容：`kubectl delete svc,deploy -l app=office-space`\\r\\n\\r\\n\\r\\n## 参考资料\\r\\n* [John Zaccone](https://github.com/jzaccone) - [通过 Docker 部署的 Office Space 应用程序](https://github.com/jzaccone/office-space-dockercon2017) 的原始作者。\\r\\n* Office Space 应用程序是以 1999 年运用此理念的同名电影为基础编写的。\\r\\n\\r\\n## 许可\\r\\n[Apache 2.0](http://www.apache.org/licenses/LICENSE-2.0)\\r\\n\\r\\n# 隐私声明\\r\\n\\r\\n可以配置包含这个包的样本 Kubernetes Yaml 文件，以跟踪对 [IBM Cloud ](https://www.bluemix.net/) 和其他 Kubernetes 平台的部署。每次部署时，都会将以下信息发送到 [Deployment Tracker](https://github.com/IBM/metrics-collector-service) 服务：\\r\\n\\r\\n* Kubernetes 集群提供者（`IBM Cloud、Minikube 等`）\\r\\n* Kubernetes 机器 ID (`MachineID`)\\r\\n* 这个 Kubernetes 作业中的环境变量。\\r\\n\\r\\n此数据是从样本应用程序的 yaml 文件中的 Kubernetes Job 收集而来。IBM 使用此数据来跟踪将样本应用程序部署到 IBM Cloud 相关的指标，以度量我们的示例的实用性，从而让我们能够持续改进为您提供的内容。仅跟踪那些包含代码以对 Deployment Tracker 服务执行 ping 操作的样本应用程序的部署过程。\\r\\n\\r\\n## 禁用部署跟踪\\r\\n\\r\\n请注释掉/移除 `account-summary.yaml` 文件末尾的 Kubernetes Job 部分。\\r\\n'},\n",
       " {'repo': 'python-astrodynamics/spacetrack',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"spacetrack\\n-------------\\n\\n|PyPI Version| |Documentation| |CI Status| |Coverage| |Python Version| |MIT License|\\n\\nspacetrack is a python module for `Space-Track <https://www.space-track.org>`__\\n\\nInstallation\\n~~~~~~~~~~~~\\n\\n.. code:: bash\\n\\n    $ pip install spacetrack\\n\\nExample\\n~~~~~~~\\n\\n.. code:: python\\n\\n   >>> from spacetrack import SpaceTrackClient\\n   >>> st = SpaceTrackClient('identity', 'password')\\n\\n   >>> print(st.tle_latest(norad_cat_id=[25544, 41335], ordinal=1, format='tle'))\\n   1 25544U 98067A   16179.00000000  .00000000  00000-0  00000-0 0  0000\\n   2 25544  00.0000   0.0000 0000000  00.0000 000.0000 00.00000000  0000\\n   1 41335U 16011A   16179.00000000  .00000000  00000-0  00000-0 0  0000\\n   2 41335  00.0000   0.0000 0000000  00.0000 000.0000 00.00000000  0000\\n\\n   >>> # Operators, to save manual string formatting.\\n   >>> import spacetrack.operators as op\\n   >>> drange = op.inclusive_range(dt.datetime(2016, 6, 26),\\n   ...                             dt.datetime(2016, 6, 27))\\n\\n   >>> # Streaming downloads line by line\\n   >>> lines = st.tle_publish(iter_lines=True, publish_epoch=drange, orderby='TLE_LINE1', format='tle')\\n   >>> with open('tle.txt', 'w') as fp:\\n   ...     for line in lines:\\n   ...         fp.write(line)\\n\\n   # Streaming downloads in chunk (note file is opened in binary mode)\\n   >>> content = st.download(iter_content=True, file_id=..., format='stream')\\n   >>> with open('file.txt', 'wb') as fp:\\n   ...     for chunk in content:\\n   ...         fp.write(chunk)\\n\\n   >>> # Parameter checking, using Space-Track's modeldef API\\n   >>> st.tle_latest(norad_cat_id=25544)\\n   TypeError: 'tle_latest' got an unexpected argument 'onrad_cat_id'\\n\\n   >>> # Automatic rate limiting\\n   >>> for satno in my_satnos:\\n   ...     # Gets limited to <20 requests per minute automatically by blocking\\n   ...     st.tle(...)\\n\\nAuthors\\n~~~~~~~\\n- Frazer McLean <frazer@frazermclean.co.uk>\\n\\nDocumentation\\n~~~~~~~~~~~~~\\n\\nFor in-depth information, `visit the\\ndocumentation <http://spacetrack.readthedocs.org/en/latest/>`__!\\n\\nDevelopment\\n~~~~~~~~~~~\\n\\nspacetrack uses `semantic versioning <http://semver.org>`__\\n\\n.. |CI Status| image:: https://github.com/python-astrodynamics/spacetrack/workflows/CI/badge.svg?branch=master\\n   :target: https://github.com/python-astrodynamics/spacetrack/actions?workflow=CI\\n.. |PyPI Version| image:: http://img.shields.io/pypi/v/spacetrack.svg?style=flat-square\\n   :target: https://pypi.python.org/pypi/spacetrack/\\n.. |Python Version| image:: https://img.shields.io/badge/python-3.6%2B-brightgreen.svg?style=flat-square\\n   :target: https://www.python.org/downloads/\\n.. |MIT License| image:: http://img.shields.io/badge/license-MIT-blue.svg?style=flat-square\\n   :target: https://raw.githubusercontent.com/python-astrodynamics/spacetrack/master/LICENSE.txt\\n.. |Coverage| image:: https://img.shields.io/codecov/c/github/python-astrodynamics/spacetrack/master.svg?style=flat-square\\n   :target: https://codecov.io/github/python-astrodynamics/spacetrack?branch=master\\n.. |Documentation| image:: https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat-square\\n   :target: http://spacetrack.readthedocs.org/en/latest/\\n\"},\n",
       " {'repo': 'helloworld1/AnyMemo',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': 'AnyMemo Readme\\n==============\\n\\nWhat is AnyMemo\\n---------------\\n\\nAnyMemo is a free open-sourced spaced repetition flashcard learning software similar to SuperMemo for Android mobile phones.\\nIt implements an advanced adaptive scheduling algorithm based on modified Mnemosyne algorithm (Enhanced SuperMemo SM2 algorithm) to maximize the learning efficient.\\n\\nAnyMemo will help you learn various languages like Arabic, Chinese, English, German, Spanish, French, Japanese, Italian, Korean, Esperanto.\\nAlso you can learn histories, computer related topics, religion, life styles using AnyMemo too!\\n\\nSee detailed info at https://anymemo.org\\n\\nDownload\\n--------\\n\\n<a href=\"https://f-droid.org/en/packages/org.liberty.android.fantastischmemo/\" target=\"_blank\">\\n<img src=\"https://f-droid.org/badge/get-it-on.png\" alt=\"Get it on F-Droid\" height=\"80\"/></a>\\n<a href=\"https://play.google.com/store/apps/details?id=org.liberty.android.fantastischmemo\" target=\"_blank\">\\n<img src=\"https://play.google.com/intl/en_us/badges/images/generic/en-play-badge.png\" alt=\"Get it on Google Play\" height=\"80\"/></a>\\n\\nDirectory structure\\n-------------------\\n\\n* app: The app source\\n* gradle: The gradle wrapper\\n* scripts: The scripts that automate some tasks\\n\\nHow to compile\\n--------------\\n\\nAnyMemo is built using gradle. You need to make sure the following are installed in\\norder to build:\\n* Android SDK tools: http://developer.android.com/sdk/index.html\\n* JDK: http://www.oracle.com/technetwork/java/javase/downloads/index.html\\n\\nIn Android SDK tools\\' \"Android SDK Manager\", you need the following items:\\n* Android 8.0 SDK platform\\n* Android SDK tools\\n* Android SDK Build-tools version 24\\n* Android support repository\\n* Android support library\\n\\nOnce the dependency is satisfied, you need to set the ANDROID_HOME environment variable\\nto the Android SDK installation location. E. g.\\n```\\nexport ANDROID_HOME=~/android-sdk-linux/\\n```\\n\\n\\ncd to the AnyMemo directory and Use gradle to compile the project\\n\\nMove AMSecrets.java.template in `src/org/liberty/android/fantastischmemo/common/` to AMSecrets.\\nProvide the values to the fields in AMSecrets.java.\\n\\nThem compile using gradle:\\n```\\ncd AnyMemo\\n./gradlew assembleFreeApi16Debug\\n```\\nTo build pro version, use\\n```\\n./gradlew assembleProApi16Debug\\n```\\nAnyMemo has a dev flavor that changes the min API level to 21 to make the build much faster.\\nIt is recommended for development only.\\n```\\n./gradlew assembleDevApi21Debug\\n```\\n\\nThe build artifacts are in\\n```\\nbuild/apk/\\n```\\n\\nInstrumentation tests\\n---------------------\\nThere are hundreds of instrumentation tests that ensure the basic function of\\nAnyMemo. To run these tests against free version, you need to run:\\n```\\n./gradlew installFreeApi16DebugTest\\n./gradlew connectedInstrumentTestFreeApi16Debug\\n```\\n\\nTo run against pro version:\\n```\\n./gradlew installProApi16DebugTest\\n./gradlew connectedInstrumentTestProApi16Debug\\n```\\n\\n\\nEclipse users\\n-------------\\n\\nSorry, the support of Eclipse ended since Google does not support it well.\\n\\nAndroid studio user\\n-------------------\\n\\nIt is easy! Install all the dependencies listed in the \"How to compile\" section\\nand you can import directly into Android studio.\\n\\n'},\n",
       " {'repo': 'e-dog/ProceduralFairings',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# Procedural Fairings\\n\\nProcedural Fairings mod for Kerbal Space Program.\\n\\n[Forum thread](http://forum.kerbalspaceprogram.com/index.php?/topic/36371-110-procedural-fairings-316-april-20/)\\n\\n[Download](https://github.com/e-dog/ProceduralFairings/releases)\\n\\n[License](http://creativecommons.org/licenses/by/3.0/)\\n\\n\\n## Installation\\nRemove old version of the mod.\\n\\nCopy ProceduralFairings into \"Gamedata\" in your KSP folder.\\n\\n## Installation Notes\\n\\nIf you downloaded KSP from Squad\\'s website, then the KSP folder is where you unzipped it when you first downloaded the game\\n\\nIf you downloaded KSP from Steam, then right-clicking KSP in your Steam library, select \"properties,\" switching to the \"local files\" tab, and pressing \"browse local files\" opens the game folder.\\n\\nIn the KSP main folder a \"GameData\" folder contains all add-ons; without any add-ons, it contains only the \"Squad\" and \"NASAMission\" sub-folders - the stock \"add-ons\" from the developers of the game. Unzip the ProceduralFairings folder into your Gamedata folder.\\n\\n## Tutorial\\n[Pictures](http://imgur.com/a/xCF0q)\\n\\n### Steps\\n1. Put a fairing base under your payload (all Procedural Fairings parts are in the Aerodynamics tab) and a decoupler if necessary.\\n2. Attached fairings automatically reshape for your payload.\\n3. Enabling symmetry on fairings will encapsulate your payload\\n4. Rearrange stages to jettison fairings at the proper stage.\\n\\n### Inline Fairings\\n- Flipping another fairing base over and adding it above the payload will cause side fairings to stick to it instead of creating a nose cone, thereby creating inline fairings between two bases.\\n- Procedural Fairings includes low-profile base rings intended for inline fairings.\\n\\n### Controls\\nRight-click parts and use tweakables.\\n\\n## Career mode\\nMaximum (and minimum) part size is limited by tech. See GameData/ProceduralFairings/common.cfg for details.\\n\\n## Version history\\n**3.00**\\n- First release on GitHub.\\n- Moved files up to GameData folder (no Keramzit folder anymore). Make sure to delete old mod before installing (which is a good practice anyway).\\n- Added new resizable fairing bases with configurable number of side nodes.\\n- Old parts (bases and adapter) are deprecated. Launched vessels should be fine, but you might have trouble loading old designs in VAB/SPH in career mode.\\n- Added new part: Thrust Plate Multi-Adapter.\\n- Using KSPAPIExtensions by Swamp-Ig for better tweakables.\\n- Removed old keyboard-based tweaks - use new tweakables.\\n- Tweaking outer diameter (with fairings), instead of inner radius.\\n- Added fairing decoupler torque tweakable.\\n- Side nodes (for attaching fairings) get larger with the base size to make them more sturdy in KSP 0.23.5+\\n- Tech limits are not checked in sandbox mode anymore.\\n- Extra payload radius is now zero by default.\\n- Fixed interstage adapter decoupling with fuselage fairings.\\n'},\n",
       " {'repo': 'mazipan/mazipan.space',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# mazipan.space\\n\\n[![js-standard-style](https://img.shields.io/badge/code%20style-standard-brightgreen.svg)](http://standardjs.com) ![Website Up](https://img.shields.io/website-up-down-brightgreen-red/https/mazipan.space.svg)\\n\\n👿 Codebase for personal blog ([mazipan.space](https://mazipan.space/)), built with Next.js and Tailwind CSS\\n\\n## Live\\n\\n- [mazipan.space](https://mazipan.space/)\\n\\n## Tech Stack\\n\\n- Next.js\\n- Tailwind\\n\\n## Features\\n\\n- Dark mode\\n- Simple pagination\\n- Featured article\\n- Article tags\\n- Dual language support (ID and EN)\\n- Generate RSS feed and sitemap.xml\\n- Code highlighting, support line highlighting\\n- Image optimization, mostly using [Next-Image](https://nextjs.org/docs/api-reference/next/image)\\n- 3rd party comment system using Github Issue\\n- Auto generating web perf report using [psi-gh-action](https://github.com/mazipan/psi-gh-action)\\n\\n## Blog comments\\n\\nhttps://github.com/mazipan/blog-comments/issues\\n\\n## Previous version\\n\\nIf you are looking for Gatsby version, please visit branch `gatsby`.\\n\\n## Use as a template\\n\\nYou can use this repo to bootstrap a same blog for yourself.\\nFeel free, just click **\"Use this template\"** button in this repo.\\n\\n----\\n\\nCopyright © 2019 by Irfan Maulana\\n'},\n",
       " {'repo': 'tangeping/spacecraft_demos',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': 'Spacecraft_unity3d_war\\n=============\\n\\n## ����Ŀ��ΪKBEngine���������Ŀͻ�����ʾ��д��������Unity5.x\\n\\nhttp://www.kbengine.org\\n\\n## �ٷ���̳\\n\\n\\thttp://bbs.kbengine.org\\n\\n\\n## QQ����Ⱥ\\n\\n\\t461368412\\n\\n\\n## KBE����ĵ�\\n\\n\\tAssets\\\\Plugins\\\\kbengine_unity3d_plugins\\\\README.md\\n\\n## GO!\\n\\n\\t1. ȷ���Ѿ����ع�KBEngine��������棬���û��������������\\n\\t\\t���ط����Դ��(KBEngine)��\\n\\t\\t\\thttps://github.com/kbengine/kbengine/releases/latest\\n\\n\\t\\t����(KBEngine)��\\n\\t\\t\\thttps://www.comblockengine.com/docs/1.0/install/index/\\n\\n\\t\\t��װ(KBEngine)��\\n\\t\\t\\thttps://www.comblockengine.com/docs/1.0/install/index/\\n\\n\\t2. ����������ʲ���\"kbengine_spaceship_demos_assets\"������������Ŀ¼\"kbengine/\"֮�£�����ͼ��\\n![demo_configure](https://github.com/tangeping/spacecraft_demos/raw/master/img/screenshots/kebengine_demo_assert_copy.png)\\n\\n\\t3. ͨ��������ʲ�������KBE�ͻ��˲������ѡ��Ĭ���Ѿ�����һ�ݣ����Ƿ���������ظĶ�����Ҫ�ٴ����ɣ�\\n\\t\\t1: ˫������ kbengine/kbengine_demos_asset/gensdk.bat\\n\\t\\t2: ����kbengine_unity3d_plugins��kbengine_unity3d_SpaceShip_demo\\\\Assets\\\\Plugins\\\\            \\n\\n    4.��װ�����ο���https://github.com/kbengine/kbengine_unity3d_demo readme�������ķ�����kbengine_demos_assetsֱ����kbengine_spaceship_demos_assets�������\\n        \\n\\n## ����Demo(��ѡ):\\n\\t�ı��¼IP��ַ��˿ڣ�ע�⣺���ڷ���˶˿ڲ��ֲο�http://www.kbengine.org/cn/docs/installation.html��:\\n![demo_configure](https://github.com/tangeping/spacecraft_demos/raw/master/img/screenshots/spacecraft_unity_config.png)\\n\\n## ����������:\\n\\n\\t�ȿ��������\\n\\t\\tWindows:\\n\\t\\t\\tkbengine\\\\kbengine_spaceship_demos_assets\\\\start_server.bat\\n\\n\\t\\tLinux:\\n\\t\\t\\tkbengine\\\\kbengine_spaceship_demos_assets\\\\start_server.sh\\n\\n\\t�������״̬:\\n\\t\\t��������ɹ���������־���ҵ�\"Components::process(): Found all the components!\"��\\n\\t\\t�κ��������������־������\"ERROR\"�ؼ��֣����ݴ����������Խ����\\n\\t\\t(More: http://www.kbengine.org/cn/docs/startup_shutdown.html)\\n        \\n## Spacecraft��ͼ\\n\\n![Spacecraft��Ŀ����Ч��](https://github.com/tangeping/spacecraft_demos/raw/master/img/screenshots/login.png)\\n![Spacecraft��Ŀ����Ч��](https://github.com/tangeping/spacecraft_demos/raw/master/img/screenshots/pick.png)\\n![Spacecraft��Ŀ����Ч��](https://github.com/tangeping/spacecraft_demos/raw/master/img/screenshots/fight1.png)\\n![Spacecraft��Ŀ����Ч��](https://github.com/tangeping/spacecraft_demos/raw/master/img/screenshots/fight2.jpg)\\n![Spacecraft��Ŀ����Ч��](https://github.com/tangeping/spacecraft_demos/raw/master/img/screenshots/settle.png)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'},\n",
       " {'repo': 'wwwtyro/space-2d',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': 'Procedural generation of 2D space scenes. View the demo [here](https://wwwtyro.github.io/space-2d).\\n'},\n",
       " {'repo': 'hundredrabbits/Verreciel',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Verreciel\\n\\nVerreciel is a space exploration game happening in a small capsule.\\nCode is formatted with [standard.js](https://standardjs.com/).\\n\\n## Setup\\n\\n```\\ncd desktop\\nnpm install\\nnpm start\\n```\\n\\nIf you wish to test a specific mission, use:\\n\\n```\\n./node_modules/.bin/electron main.js mission:10 \\n```\\n\\n## Extras\\n\\n- This application supports the [Ecosystem Theme](https://github.com/hundredrabbits/Themes).\\n- Support this project through [Patreon](https://patreon.com/100).\\n- See the [License](LICENSE.md) file for license rights and limitations (MIT).\\n- Pull Requests are welcome!\\n'},\n",
       " {'repo': 'michalbe/mibbu',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': 'Mibbu\\n========\\n\\n#### First Javascript Game MicroFramework ####\\n\\nMibbu gives you everything you need for fast prototyping your Javascript game in less than 2.5KB of gzipped code. Games created with Mibbu can be displayed using Canvas or DOM mode (you can change it with one single function, or use feature detection to use DOM where it is no canvas, like in IE family). Mibbu supports also CSS animations in Webkit ([blogpost](http://michalbe.blogspot.com/2011/05/css3-animations-in-mibbu.html)) and in Firefox BETA (5.0).\\n\\n\\n[Documentation](http://mibbu.eu)\\n\\n\\n### Features of Mibbu ###\\n* Rendering game using both - Canvas or DOM\\n* Animation of the sprites (using Canvas, DOM or CSS Animation in Webkit & Firefox 5+)\\n* Collisions detection with collision zones\\n* Simple background manager\\n* Callbacks after given number of frames\\n* Method\\'s chaining\\n* Check [example](https://github.com/michalbe/mibbu/tree/master/example) for more\\n\\n### Change Log ###\\n\\n2011 08 09 - **0.3.2/nagasaki** (5.62 KB, gzip 2.29 KB)\\n\\n* one pixel background margin bug in Canvas Mode fixed.\\n\\n2011 08 07 - **0.3.1/victor** (5.91 KB, gzip 2.36 KB) by [end3r](https://github.com/end3r)\\n\\n* it\\'s now possible to switch background image using .img() function\\n\\n2011 06 12 - **0.3/arkansas** (5.89 KB, gzip 2.37 KB)\\n\\n* every method now return itself when you call it with parameters (like .speed(3)) so it\\'s possile to connect multiple methods into one chain (like background.speed(4).direction(-20)). Methods called without arguments returns value of given metod (like background.speed() return the speed of the background) \\n\\n2011 06 12 - **0.2.4/annefrank** (5.57 KB, gzip 2.30 KB) by [MartinDoms](https://github.com/MartinDoms)\\n\\n* background direction could be now specified in both, strings (\"N\", \"S\", \"E\", \"W\") and numbers (radians)\\n* it is possible to change background direction \\'on the fly\\' without resetting the background position\\n\\n2011 06 11 - **0.2.3/kamehameha**  (5.42 KB, gzip 2.12 KB)\\n\\n* FPS module optimization\\n\\n2011 06 05 - **v0.2.2/blackwater** (5.43 KB, gzip 2.12 KB)\\n\\n* Animation structure changed according to Marek Stepien\\'s research on CSS Animations in Firefox [[blogpost](http://michalbe.blogspot.com/2011/06/css-animation-in-firefox.html#update)]\\n\\n2011 06 04 - **v0.2.1/birthdayAfterparty** (5.46 KB, gzip 2.12 KB)\\n\\n* CSS3 Animations support in Firefox (tested in 5.0/BETA) \\n* .indexOf() name changed for minimization and compatibility with other solutions [[blogpost](http://michalbe.blogspot.com/2011/06/css-animation-in-firefox.html)]\\n\\n2011 05 23 - **v0.2/suiko** (5.54 KB, gzip 2.13 KB) [[blogpost](http://michalbe.blogspot.com/2011/05/css3-animations-in-mibbu.html)]\\n\\n* CSS3 Animations support in Webkit browsers (tested on Chrome 11 & Safari 5.03) \\n* .cssAnimationOff() function added (works the same as .canvasOff() but for CSS Animations)\\n* some major & minor bug fixes\\n\\n2011 05 19 - **v0.1.2/guanabhadra** (4.36 KB, gzip: 1.81 KB)\\n\\n* Changes in Array#indexOf - we don\\'t need full spec implementation here. This one is faster and smaller.\\n\\n2011 05 09 - **v0.1.1/atilla** (4.45 KB, gzip: 1.83 KB)\\n\\n* Array#indexOf fix by [killdream](https://github.com/killdream)\\n* Proper declaration of MB_mainCanvasStyle variable\\n \\n2011 05 05 - **v0.1/odoacer** (4.35 KB, gzip: 1.81 KB)\\n\\n* First release\\n\\n----\\n\\n### License ###\\n\\n####The MIT License####\\n\\nCopyright (c) 2011 [Michal Budzynski](https://profiles.google.com/michal.budzynski.js/about). All rights reserved.\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in\\nall copies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\\nTHE SOFTWARE.\\n'},\n",
       " {'repo': 'sjsdfg/dl4j-tutorials',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '# dl4j-tutorials\\n\\ndeeplearning4j 教程\\n\\n视频教程列表：[Deeplearning4j - 入门视频](https://www.jianshu.com/p/566fc3db676b)\\n\\n哔哩哔哩直达地址：https://space.bilibili.com/327018681/#/\\n\\n - **交流群： 289058486**\\n - **入群问题： Deeplearning4j 源码在 github的地址（mac 系统QQ看不到群问题，入群记得添加答案）**\\n\\nDeepLearning4J（DL4J）是一套基于Java语言的神经网络工具包，可以构建、定型和部署神经网络。DL4J与Hadoop和Spark集成，支持分布式CPU和GPU，为商业环境（而非研究工具目的）所设计。Skymind是DL4J的商业支持机构。\\n\\nDeeplearning4j拥有先进的技术，以即插即用为目标，通过更多预设的使用，避免多余的配置，让非企业也能够进行快速的原型制作。DL4J同时可以规模化定制。DL4J遵循Apache 2.0许可协议，一切以其为基础的衍生作品均属于衍生作品的作者。\\n\\n# Give Me a Favor\\n\\n<center>\\n<img src=\"http://static.zybuluo.com/ZzzJoe/yflamvkjh2i7zn5qcp9wpj61/%E5%AF%92%E6%B2%A7.jpg\" />\\n</center>\\n\\n---\\n\\n## 注意\\n因为使用的maven管理项目，所以第一次使用的时候更改maven配置。更改仓库地址为国内的阿里云\\n\\n- [Deeplearning4j入门（零）- maven环境配置 - 寒沧](https://www.bilibili.com/video/av25768162)\\n- [settings.xml 文件下载](https://github.com/sjsdfg/dl4j-tutorials/blob/master/src/main/resources/setting/settings.xml)\\n\\n```xml\\n<mirror>\\n\\t<id>nexus-aliyun</id>\\n\\t<mirrorOf>central</mirrorOf>\\n\\t<name>Nexus aliyun</name>\\n\\t<url>http://maven.aliyun.com/nexus/content/groups/public</url>\\n</mirror> \\n```\\n\\n### 使用maven把jar包导出为外部\\n```bash\\nmvn dependency:copy-dependencies -DoutputDirectory=target/lib\\n```\\n\\n- [deeplearning4j-1.0.0beta离线jar包---百度云](https://pan.baidu.com/s/1pxuEmzypSvlguCftsMaZ3g)\\n\\n\\n## dl4j概览\\n\\n1. [dl4j快速索引：网络层，功能和类](https://github.com/deeplearning4j/deeplearning4j-docs/blob/gh-pages/quickref.md)\\n2. [dl4j-example 概览](https://github.com/deeplearning4j/deeplearning4j-docs/blob/gh-pages/examples-tour.md)\\n3. [dl4j 神经网络评估](https://deeplearning4j.org/docs/latest/deeplearning4j-nn-evaluation)\\n4. [dl4j 版本发布日志](https://github.com/deeplearning4j/deeplearning4j-docs/blob/releasenotes_100a/releasenotes.md)\\n5. [Java api文档](https://deeplearning4j.org/api/v1.0.0-beta2/)\\n6. [skymind 官方博客](https://blog.skymind.ai/)\\n7. [Quickstart with Deeplearning4J](http://www.dubs.tech/guides/quickstart-with-dl4j/)\\n8. [旧版本官网github](https://github.com/deeplearning4j/deeplearning4j-docs/tree/gh-pages)\\n9. [skymind ai wiki](https://skymind.ai/wiki/)\\n10. [skymind开源数据集集合](https://skymind.ai/wiki/open-datasets)\\n11. [Java Deep Learning Projects: Implement 10 real-world deep learning using Deeplearning4j and opensource APIs](https://pan.baidu.com/s/1Y2VoO6kLd6RIHCVqpVKzlQ)\\n\\n## 调参\\n\\n1. [我搭的神经网络不work该怎么办！看看这11条新手最容易犯的错误](https://mp.weixin.qq.com/s?__biz=MzIxMjAzNDY5Mg==&mid=2650791830&idx=1&sn=da81a253d4753e78d0ad5040ecf3ca29&chksm=8f474a7db830c36bf083c8e22414b2b2ee0fd38dd60175921952ae01c669ce4a3d105f972b09&mpshare=1&scene=1&srcid=0913NEV8u5Rz7fdaIDjAPnvs#rd)\\n2. [nd4j 和 DeepLearning4j 性能调优 debug](https://deeplearning4j.org/docs/latest/deeplearning4j-config-performance-debugging)\\n3. [神经网络训练问题排查](https://deeplearning4j.org/cn/troubleshootingneuralnets)\\n\\n## lesson1 nd4j基础操作\\n\\n参考资料：\\n\\n 1. [一天搞懂深度学习](https://pan.baidu.com/s/1FW8zqzE4rK7pCOsC46dhIQ) \\n 1. [Deep Learning A Practitioner’s Approach](https://pan.baidu.com/s/1C1s2xMuDYJBd3kCB8bxlxA)\\n 2. https://nd4j.org/userguide\\n 3. [nd4j方法快速索引](https://www.jianshu.com/p/c4f6284946bf)\\n\\n## lesson2 简易线性回归\\n\\n参考资料：\\n\\n 1. [深度神经网络简介][2]\\n 2. [译-第四章 可视化证明神经网络可以计算任何函数](http://www.jianshu.com/p/1d80023119cc)\\n 3. [A visual proof that neural nets can compute any function](http://neuralnetworksanddeeplearning.com/chap4.html)\\n 4. [神经网络中Epoch、Iteration、Batchsize相关理解和说明](https://blog.csdn.net/program_developer/article/details/78597738)\\n\\n## lesson3 简易数据分类\\n参考资料：\\n\\n 1. [ETL用户指南][3]\\n 2. [MNIST基础教程，包含一些分类知识][4]\\n 3. [Deeplearning4j Smote 样本均衡实现](https://zhuanlan.zhihu.com/p/52258279)\\n\\n## lesson4 Minst手写数字分类\\n\\n参考资料：\\n\\n 1. [MINST数据集](http://yann.lecun.com/exdb/mnist/)\\n 2. [神经网络学习的可视化、监测及调试方法](https://deeplearning4j.org/cn/visualization)\\n\\n\\n## lesson5 模型保存与读取\\n\\n参考资料：\\n\\n 1. [HDFS模型保存][5]\\n 2. [SparkDl4jMultiLayer模型存储](https://github.com/sjsdfg/deeplearning4j-issues/blob/master/markdown/deeplearning4j%E7%9B%B8%E5%85%B3/SparkNetwork%E6%A8%A1%E5%9E%8B%E5%AD%98%E5%82%A8.md)\\n\\n## lesson6 Minst手写数字模型改进-CNN\\n\\n参考资料：\\n 1. [关于深度学习之CNN经典论文原文(1950~2018)简介][9]\\n 2. [Visualizing and Understanding CNNs.pdf](https://github.com/sjsdfg/deeplearning4j-issues/blob/master/Visualizing%20and%20Understanding%20CNNs.pdf)\\n 3. [deep learning for computer vision with python(3 本)](https://pan.baidu.com/s/17UMo76p75piTcArqu0wXJQ) 密码：vr0r\\n 4. [对ResNet本质的一些思考](https://zhuanlan.zhihu.com/p/60668529)\\n 5. [DL4J之CNN对今日头条文本分类](https://my.oschina.net/u/1778239/blog/3089890)\\n\\n在使用 GPU 加速之前请务必确认一下几点：\\n 1. 电脑是否为 **英伟达** GPU，即 GTX 系列，使用 AMD 显卡无法使用 GPU 加速\\n 2. 电脑是否安装了 cuda ，如果安装了 cuda 请确认安装的 cuda 版本和你 pom 中引入的 `nd4j.backend` 版本是否对应\\n 3. 电脑安装 cuda 之后请确保你的 IDE 已经感知到环境变量的变化，在 IDE 中的 `terminal` 使用 `nvcc -V` 命令查看。如不确定直接重启电脑即可\\n \\n以下为 GPU 安装和使用教程：\\n 1. [Deeplearning4j-使用Cuda 9.1和 Cudnn7.1 加速模型训练](https://www.jianshu.com/p/8a7533c2c79a)\\n 2. [在Deeplearning4j中使用cuDNN](https://blog.csdn.net/u011669700/article/details/79028821)\\n 3. [【视频】Deeplearning4j入门 - （十）GPU加速训练 - 寒沧](https://www.bilibili.com/video/av24603590)\\n \\n如想确定 DeepLearning4j 已经支持的 cuda 和 cudnn 的配套版本，请打开如下链接：\\n 1. [Using Deeplearning4j with cuDNN](https://deeplearning4j.org/cudnn) ：搜索 `CUDA Version` 字眼\\n\\n## lesson7 RNN循环神经网络\\n\\n参考资料\\n 1. 理解LSTM网络：https://www.jianshu.com/p/9dc9f41f0b29\\n 2. 循环网络和LSTM教程：https://deeplearning4j.org/cn/recurrentnetwork\\n 3. DL4J中的循环网络：https://deeplearning4j.org/cn/usingrnns\\n 4. [DeepLearning4j: LSTM Network Example](https://deeplearning4j.org/programmingguide/05_lstm)\\n \\n\\n## ObjectDetection 目标检测\\n\\n参考资料：\\n 1. [DeepLearning4j-使用Java训练YOLO模型](https://blog.csdn.net/u011669700/article/details/79886619)\\n 2. [Java构建汽车无人驾驶：汽车目标检测](https://blog.csdn.net/u011669700/article/details/79432195)\\n 3. [基于深度学习的目标检测技术演进：R-CNN、Fast R-CNN、Faster R-CNN、YOLO、SSD](https://www.julyedu.com/question/big/kp_id/26/ques_id/2103)\\n 4. [【中文】Yolo v1全面深度解读 目标检测论文](https://www.bilibili.com/video/av23354360)\\n 5. [【中文】Mask R-CNN 深度解读与源码解析 目标检测 物体检测 RCNN object detection 语义分割](https://www.bilibili.com/video/av24795835)\\n 6. 目标检测自定义数据集：https://pan.baidu.com/s/1u5yYv5SmK_vgd1zq1PsteQ\\n <div align=\"center\"> <img src=\"https://upload-images.jianshu.io/upload_images/2137832-f04063fbdfdaab6e.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" /> </div>\\n \\n\\n\\n## tensorflow 导入tf模型\\n\\n参考资料：\\n 1. https://blog.csdn.net/u011669700/article/details/80025161\\n \\n ****\\n\\n## JStarCraft,一个涵盖核心编程,人工智能,推荐与搜索领域的轻量级框架\\n\\n群友 [@HongZhaoHua](https://github.com/HongZhaoHua) 实现\\n\\n1. JStarCraft Core：提供了缓存，ORM，编解码，资源管理，脚本，监控，通讯，事务8个方面的核心特性，是整个框架的核心。\\n[https://github.com/HongZhaoHua/jstarcraft-core](https://github.com/HongZhaoHua/jstarcraft-core)\\n2. JStarCraft AI：包含了多种矩阵和向量计算的实现，支持硬件加速和并行计算，是目前最快最全的Java矩阵/向量计算库。 \\n[https://github.com/HongZhaoHua/jstarcraft-ai](https://github.com/HongZhaoHua/jstarcraft-ai)\\n3. JStarCraft RNS：包含了70多种排序预测与评分预测算法，支持多种脚本语言，是覆盖范围最全的Java推荐与搜索引擎。\\n[https://github.com/HongZhaoHua/jstarcraft-rns](https://github.com/HongZhaoHua/jstarcraft-rns)\\n4. JStarCraft Example：一个基于JStarCraft RNS引擎,Spring Boot框架和MovieLens 100K数据集搭建的电影演示项目，包括个性化推荐与个性化搜索两个部分。系统会根据用户的行为记录，自动调整用户的推荐内容和搜索内容。[https://github.com/HongZhaoHua/jstarcraft-example](https://github.com/HongZhaoHua/jstarcraft-example)\\n\\n| 作者 | 洪钊桦 |\\n| --- | --- |\\n| E-mail | 110399057@qq.com, jstarcraft@gmail.com |\\n\\n\\n ## baidudianshi 百度点石比赛 baseline demo\\n\\n 参考资料：\\n  1. 比赛地址：http://dianshi.baidu.com/dianshi/pc/competition/22/rule\\n  2. 防止比赛结束，数据寻回链接：https://pan.baidu.com/s/1_M0yPejFTvxDFOn4780OPA\\n  3. Baseline 0.83 得分模型：https://pan.baidu.com/s/1i-v02HnMPQwjtm32fPp67A （已经保存 Updater 信息，可用于增量训练）\\n  4. 内存管理官方文档：https://deeplearning4j.org/docs/latest/deeplearning4j-config-memory\\n  5. 迁移学习官方文档：https://deeplearning4j.org/docs/latest/deeplearning4j-nn-transfer-learning\\n  6. 迁移学习推荐阅读博客：https://blog.csdn.net/wangongxi/article/details/75127131\\n  7. 早停法训练模型官方文档：https://deeplearning4j.org/docs/latest/deeplearning4j-nn-early-stopping\\n  8. [百度点石-“探寻地球密码”天宫数据利用大赛.md](https://github.com/sjsdfg/deeplearning4j-issues/blob/master/markdown/%E7%99%BE%E5%BA%A6%E7%82%B9%E7%9F%B3-%E2%80%9C%E6%8E%A2%E5%AF%BB%E5%9C%B0%E7%90%83%E5%AF%86%E7%A0%81%E2%80%9D%E5%A4%A9%E5%AE%AB%E6%95%B0%E6%8D%AE%E5%88%A9%E7%94%A8%E5%A4%A7%E8%B5%9B.md)\\n  9. [百度点石-“探寻地球密码”天宫数据利用大赛.pdf](https://github.com/sjsdfg/deeplearning4j-issues/blob/master/%E7%99%BE%E5%BA%A6%E7%82%B9%E7%9F%B3-%E2%80%9C%E6%8E%A2%E5%AF%BB%E5%9C%B0%E7%90%83%E5%AF%86%E7%A0%81%E2%80%9D%E5%A4%A9%E5%AE%AB%E6%95%B0%E6%8D%AE%E5%88%A9%E7%94%A8%E5%A4%A7%E8%B5%9B.pdf)\\n\\n ## 模型训练早停法\\n\\n ### 1. 创建 ModelSaver\\n\\n 用于在模型训练过程中，指定最好模型保存的位置：\\n\\n 1. InMemoryModelSaver：用于保存到内存中\\n 2. LocalFileModelSaver：用于保存到本地目录中，只能保存 `MultiLayerNetwork` 类型的网络结果\\n 3. LocalFileGraphSaver：用于保存到本地目录中，只能保存 `ComputationGraph` 类型的网络结果\\n\\n ### 2. 配置早停法训练配置项\\n\\n 1. epochTerminationConditions：训练结束条件\\n 2. evaluateEveryNEpochs：训练多少个epoch 来进行一次模型评估\\n 3. scoreCalculator：模型评估分数的计算者\\n     - org.deeplearning4j.earlystopping.scorecalc.RegressionScoreCalculator 用于回归的分数计算\\n     - ClassificationScoreCalculator 用于分类任务的分数计算\\n 4. modelSaver：模型的存储位置\\n 5. iterationTerminationConditions：在每一次迭代的时候用于控制\\n\\n ### 3. 获取早停法信息\\n ```Java\\n //Conduct early stopping training:\\n EarlyStoppingResult result = trainer.fit();\\n System.out.println(\"Termination reason: \" + result.getTerminationReason());\\n System.out.println(\"Termination details: \" + result.getTerminationDetails());\\n System.out.println(\"Total epochs: \" + result.getTotalEpochs());\\n System.out.println(\"Best epoch number: \" + result.getBestModelEpoch());\\n System.out.println(\"Score at best epoch: \" + result.getBestModelScore());\\n \\n //Print score vs. epoch\\n Map<Integer,Double> scoreVsEpoch = result.getScoreVsEpoch();\\n List<Integer> list = new ArrayList<>(scoreVsEpoch.keySet());\\n Collections.sort(list);\\n System.out.println(\"Score vs. Epoch:\");\\n for( Integer i : list){\\n     System.out.println(i + \"\\\\t\" + scoreVsEpoch.get(i));\\n }\\n ```\\n\\n ## 迁移学习\\n\\n ### 1. 获取原有的网络结构\\n\\n ```Java\\n  // 构造数据模型\\n ZooModel zooModel = VGG16.builder().build();\\n ComputationGraph vgg16 = (ComputationGraph) zooModel.initPretrained();\\n ```\\n\\n\\n ### 2. 修改模型的训练部分超参数\\n\\n  1. updater\\n  2. 学习率\\n  3. 随机数种子：用于模型的复现\\n\\n ```java\\n  FineTuneConfiguration fineTuneConf = new FineTuneConfiguration.Builder()\\n                 .updater(new Nesterovs(0.1, 0.9))\\n                 .seed(123)\\n                 .build();\\n ```\\n\\n ### 3. 修改网络架构\\n\\n #### 3.1 setFeatureExtractor\\n\\n 用于指定那个层以下为非 frozen 层，非冻结层。\\n\\n\\n #### 3.2 结构更改\\n\\n 1. 一般只有不同网络层之间才会出现 shape 异常：需要根据异常信息调整我们的网络层结构和参数\\n 2. `removeVertexKeepConnections` 和 `addLayer` 或者是 `addVertex` 进行网络结构的更改\\n\\n## 自定义网络层实现GRU\\n\\n参考资料：\\n 1. https://github.com/Gerry-Pan/pan-dl4j\\n\\n根据GRU前向公式推导反向公式，并在dl4j中实现。\\n\\n## 整合DL4J训练模型与Web工程\\n参考资料：\\n 1. 博文地址：https://my.oschina.net/u/1778239/blog/1648854\\n 2. 源码地址：https://gitee.com/lxkm/dl4j-demo/tree/master/digitalrecognition\\n\\n## 【深度学习】图像矫正、dl4j yolo和tesseract ocr\\n\\n参考资料：\\n 1. 视频地址：https://tianchi.aliyun.com/forum/videoStream.html#postsId=5312\\n 2. 视频代码所在github：https://github.com/awaymeet/tesseract\\n\\n## 人脸识别 - FaceRecognition\\n\\n参考资料：\\n  1. https://github.com/fradino/FaceRecognition\\n  2. https://gitee.com/xshuai/FaceRecognition\\n  \\n\\n## 参数共享\\n\\n感谢群友 [@冷血狂魔]() [@Gerry]()\\n\\n- StackVertex，用它就能实现参数共享\\n\\n数学方法参数证明：\\n\\n- [DeepLearning4j 的 StackVertex 实现参数共享 ](https://blog.csdn.net/pangerry/article/details/90814970)\\n\\n与其类似的类为 `MergeVertex`，与其的主要区别在于：\\n\\n- Merge 的合并方式是沿着 dimension=1 合并，Stack 是沿着 dimension=0 合并\\n- 沿着 dimension=0 合并的好处是无论第 0 维的长度为多少，权重 w 的 shape 都不会变，而且在反向计算梯度恰好等于 n 个 input 的产生的梯度的和\\n- 所以只要使用 StackVertex 对 n 个输入进行合并，后面随意拼接神经网络层，诸如 DenseLayer, LSTM, ConvLayer 都能达到目的，最后用 UnstackVertex 拆分输出\\n\\n数学推导 [@Gerry]() 提供。其从矩阵运算角度说明 StackVertex 只做合并就能实现参数共享的原因\\n  \\n## 天池比赛-工业蒸汽量预测\\n\\n群友 [@冷血狂魔]()提供\\n\\n- [参赛地址](https://tianchi.aliyun.com/competition/entrance/231693/introduction?spm=5176.12281949.1003.9.493e4c2apzwdri)\\n\\n### 背景\\n  火力发电的基本原理是：燃料在燃烧时加热水生成蒸汽，蒸汽压力推动汽轮机旋转，然后汽轮机带动发电机旋转，产生电能。在这一系列的能量转化中，影响发电效率的核心是锅炉的燃烧效率，即燃料燃烧加热水产生高温高压蒸汽。锅炉的燃烧效率的影响因素很多，包括锅炉的可调参数，如燃烧给量，一二次风，引风，返料风，给水水量；以及锅炉的工况，比如锅炉床温、床压，炉膛温度、压力，过热器的温度等。\\n  \\n### 目标\\n\\n经脱敏后的锅炉传感器采集的数据（采集频率是分钟级别），根据锅炉的工况，预测产生的蒸汽量。\\n\\n- [deeplearning4j解蒸汽比赛的思路（0.1136）](https://tianchi.aliyun.com/notebook-ai/detail?spm=5176.12281897.0.0.a0792658MVZabi&postId=58269)\\n\\n## 推荐系统 - Recommend\\n\\n参考资料：\\n  1. [lib-rec](https://github.com/guoguibing/librec)：https://github.com/guoguibing/librec/tree/3.0.0/core/src/main/java/net/librec/recommender/nn\\n  2. [inception](https://github.com/inception-project/inception)：https://github.com/inception-project/inception/tree/master/inception-imls-dl4j/src/main/java/de/tudarmstadt/ukp/inception/recommendation/imls/dl4j/pos\\n\\n## Deeplearning4j 实现 Attention\\n\\n参考资料：\\n1. [直播实现视频 youtube(自备梯子)](https://www.youtube.com/watch?v=XrZ_Y4koV5A)\\n2. [Implementing NLP Attention Mechanisms with DeepLearning4j(搬运到国内bilibili)](https://www.bilibili.com/video/av37100054/)\\n3. [attention 实现源码](https://github.com/treo/dl4j_attention)\\n4. [Attention Mechanisms (Enterprise AI Virtual Meetup).pdf](https://pan.baidu.com/s/1BzrteMiqlvm_l7Cv54Yc4g)\\n\\n## GAN\\n\\n- [GAN 使用 MNIST 实例](https://github.com/sjsdfg/dl4j-tutorials/tree/master/src/main/java/gan)。群友 @城枫林 和 [@liweigu](https://github.com/liweigu) 提供\\n- [gan_deeplearning4j](https://github.com/hamaadshah/gan_deeplearning4j)\\n- [如何用Deeplearning4j实现GAN](https://my.oschina.net/u/1778239/blog/3090072)\\n\\n## 自制AI图像搜索引擎\\n\\n  群友 @射水鱼 攥写了一本使用 DeepLearning4j 实现的《自制AI图像搜索引擎》\\n\\n按章节详细讲述了图像搜索引擎各主要组成部分的原理和实现，并在最后一章带领大家使用DL4J从零开始逐步构建了一个基于深度学习的Web图像搜索引擎，使读者能够更透彻地理解图像检索的理论并具有独立地实现一个在线图像搜索引擎的实际能力。每章都在对相关理论和方法进行阐述的同时，使用基于Java语言的实现代码和详实的代码注释来对相关理论和方法进行复述。\\n\\n- 书籍地址：[https://www.epubit.com/book/detail/30316](qq://txfile/#)\\n- 源码地址：[https://box.lenovo.com/l/LHh2vR](qq://txfile/#) 密码: 1aaa  \\n\\n```xml\\n<dependency>\\n    <groupId>be.tarsos</groupId>\\n    <artifactId>TarsosLSH</artifactId>\\n    <version>${tarsosLSH.version}</version>\\n</dependency>\\n```\\n\\n如果导入项目中有依赖缺失，下载以下 jar 包：\\n\\n- [TarsosLSH-0.9 下载地址](https://pan.baidu.com/s/1sbmvbkab6K5tRF92U-ItHw) 提取码：88qv\\n- [TarsosLSH github地址，也可以自行编译](https://github.com/JorenSix/TarsosLSH)\\n\\n使用 `<scope> system </scope>`进行本地的 jar 包导入，或者使用以下命令安装在本地的 maven 仓库中：\\n\\n```bash\\nmvn install:install-file -Dfile=/path/to/jar -DgroupId=be.tarsos -DartifactId=TarsosLSH -Dversion=0.9 -Dpackaging=jar\\n```\\n\\n## 强化学习 RL4j\\n\\n参考资料：\\n 1. 简书文章：https://www.jianshu.com/p/4d7f23395e92\\n 2. gitee代码：https://gitee.com/re6g3y/DL4J-with-LIBGDX\\n\\n <div align=\"center\"> <img src=\"https://upload-images.jianshu.io/upload_images/2137832-9a808a77f1cab0b9.gif?imageMogr2/auto-orient/strip\"/> </div>\\n\\n## Deeplearning4j 经典开源项目\\n\\n 1. [ScalphaGoZero](https://github.com/maxpumperla/ScalphaGoZero):An independent implementation of DeepMind\\'s AlphaGoZero in Scala, using Deeplearning4J (DL4J 实现阿尔法狗)\\n 2. https://github.com/tahaemara/yolo-custom-object-detector : 使用 YOLO 检测实时检测自定义数据集 - 魔方\\n 3. https://github.com/mccorby/PhotoLabeller : 安卓客户端实现分布式训练。 使用 Kotlin 实现\\n 4. https://github.com/tahaemara/real-time-sudoku-solver : 使用 dl4j 解决数独\\n 5. https://github.com/kaiwaehner/kafka-streams-machine-learning-examples : kafka 流训练\\n 6. https://github.com/fra82/textdigester : dl4j 实现文档总结\\n\\n## 获取最新的Deeplearning4j(Snapshots And Daily Builds)\\n\\n参考资料：\\n  1. https://deeplearning4j.org/docs/latest/deeplearning4j-config-snapshots\\n\\n配置 `pom.xml` 文件\\n```XML\\n<repositories>\\n    <repository>\\n        <id>snapshots-repo</id>\\n        <url>https://oss.sonatype.org/content/repositories/snapshots</url>\\n        <releases>\\n            <enabled>false</enabled>\\n        </releases>\\n        <snapshots>\\n            <enabled>true</enabled>\\n            <updatePolicy>daily</updatePolicy>  <!-- Optional, update daily -->\\n        </snapshots>\\n    </repository>\\n</repositories>\\n```\\n自动获取 skymind 所提供的 jar 包编译更新\\n\\n# Spark 读取数据\\n\\n1. https://github.com/deeplearning4j/dl4j-examples/issues/689\\n2. [Spark Data Pipelines Guide](https://deeplearning4j.org/docs/latest/deeplearning4j-scaleout-data-howto)\\n3. [Spark 训练指南](https://deeplearning4j.org/docs/latest/deeplearning4j-scaleout-howto)\\n\\n```java\\nok, so there\\'s 2 ways\\n(a) use SparkContext.parallelize (that\\'s a standard spark op) - easy but bad performance (all preprocessing happens on master)\\n(b) write a better data pipeline that does the proper reading + conversion in parallel\\n```\\n\\n# 群友项目分享\\n\\n1. 基于dl4j的快三爬取和训练以及预测:https://github.com/awaymeet/lottery_kuai_san/tree/master\\n2. 开源的学习SpringCloud和Deeplaring4j和IBMWaston知识的项目: https://github.com/JamesZow/Spring-Cloud-Project\\n3. DL4J的CNN+RNN+CTC的文字识别的demo：https://github.com/WuWei1986/dl4j-ocr-demo\\n4. 全连接层的神经网络框架的雏形：https://github.com/woshiyigebing/my_dl4j  （只用了INDarray。给新手研究入门一些启发）\\n\\n# 额外资源\\n\\n 1. [机器学习高质量数据集大合辑](https://mp.weixin.qq.com/s?__biz=MjM5MTQzNzU2NA==&mid=2651663921&idx=2&sn=300429e518d159bb7654e1771672429e&chksm=bd4c09a28a3b80b4aa961577a7f59229d23bbd5f88b50bec6de21b0f94bd2fd2b348d1d4eb04&mpshare=1&scene=23&srcid=1023m8ifSIuylq6VcBQKRkt7#rd)\\n 2. [中文开放聊天语料整理](https://github.com/codemayq/chaotbot_corpus_Chinese)\\n 3. [gitxiv:只提供有复现开源代码的论文](http://www.gitxiv.com/)\\n 4. [papers with code](https://paperswithcode.com/)\\n 4. [hadoop-winutils](https://github.com/steveloughran/winutils)：提供 hadoop 工具在 windows 平台下的 hadoop.dll和winutils.exe。便于 windows 下运行 spark-local 模式\\n 5. [hadoop-winutils for 3.+](https://github.com/cdarlint/winutils)\\n 5. [深度学习理论与实战：提高篇](http://fancyerii.github.io/2019/03/14/dl-book/)\\n 6. [tablesaw](https://github.com/jtablesaw/tablesaw)：Java dataframe and visualization library https://jtablesaw.github.io/tablesaw/  提供类似于 Python 中的 Pandas 和 Matplot 的功能\\n 7. 让机器学习更具有可解释性：https://christophm.github.io/interpretable-ml-book/\\n\\n\\n\\n \\n\\n[2]: https://deeplearning4j.org/cn/neuralnet-overview\\n[3]: https://deeplearning4j.org/cn/etl-userguide\\n[4]: https://deeplearning4j.org/cn/mnist-for-beginners\\n[5]: http://blog.csdn.net/u011669700/article/details/79113789\\n[6]: https://deeplearning4j.org/quickref\\n[7]: https://deeplearning4j.org/examples-tour\\n[8]: https://blog.csdn.net/u011669700/article/details/80139619\\n[9]: https://blog.csdn.net/qq_41185868/article/details/79995732\\n'},\n",
       " {'repo': 'gisikw/ksprogramming',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': 'Kerbal Space Programming\\n========================\\n\\nKerbal Space Programming is a YouTube playthrough of [Kerbal Space\\nProgram](https://kerbalspaceprogram.com) using entirely automated technology.\\nThe video series can be enjoyed\\n[here](https://www.youtube.com/watch?v=fNlAME5eU3o&list=PLb6UbFXBdbCrvdXVgY_3jp5swtvW24fYv).\\n\\nThis repository is set up to provide the ships and scripts used for each\\nepisode, and can be found in the various directories.\\n\\n## List of Mods Used\\n\\nFor this playthrough the following mods were installed using the [Comprehensive\\nKerbal Archive Network (CKAN)](https://github.com/KSP-CKAN/CKAN):\\n\\n- KerbalEngineerRedux\\n- kOS\\n- RemoteTech\\n- SmartParts\\n- SurfaceLights\\n- TAC Life Support\\n\\nThere are several visual enhancement mods used in the playthrough, but the\\nabove list should be sufficient to run the crafts in this repository.\\n\\nThis playthrough has a particular emphasis on the [Kerbal Operating System\\n(kOS)](https://github.com/KSP-KOS/KOS), which allows players to script the\\nbehavior of their vessels like so:\\n\\n    PRINT \"Launching the ship!\".\\n    STAGE.\\n    WAIT 10.\\n    STAGE.\\n    WAIT 10.\\n    PRINT \"...are we there yet?\".\\n\\nFor more information about the KerboScript syntax, visit their [documentation\\npage](http://ksp-kos.github.io/KOS_DOC/).\\n\\n## Loading Ships and Scripts\\n\\nAll of the craft files and kOS scripts can be found in episode directories for\\nthis repository. To use them in your own game, locate your Kerbal Space Program\\n[root directory](http://wiki.kerbalspaceprogram.com/wiki/Root_directory).\\n\\nCraft files should be placed in either `./saves/[YOURSAVEGAME]/Ships/VAB` or\\n`./saves[YOURSAVEGAME]/Ships/SPH`, based on whether you wish to load them in\\nthe Vehicle Assembly Building or the Spaceplane Hanger, respectively.\\n\\nkOS script files should be placed into `./Ships/Script`, so that they can be\\naccessed on the Archive Volume inside kOS.\\n\\n### Boot Scripts, Update Files, and Libraries\\n\\nIn later episodes, much of the mission logic has been handled using libraries\\nof shared utilities. To ensure that mission scripts are able to load these\\nlibraries, the contents of the `library` folder of this repository should also\\nbe copied into `./Ships/Script`.\\n\\nMany of the craft in the episodes have instructions automatically uploaded\\nusing the `boot.ks` script in the library directory. Mission scripts can then\\nbe directly uploaded to the vessel by naming them\\n`./Ships/Script/[SHIPNAME].update.ks`\\n\\n## Questions and Contributions\\n\\n*Please note: many of the scripts here were valid with older versions of kOS\\nand may include syntax that is no longer accurate.*\\n\\nIf you have any questions about the project, please feel free to to submit an\\n[issue](https://github.com/gisikw/ksprogramming/issues).\\n'},\n",
       " {'repo': 'nasa-jpl-memex/topic_space',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Topic Space\\n\\nA restful web application to provide a topic models service.\\n\\n## License\\n\\n[Apache License, version 2](http://www.apache.org/licenses/LICENSE-2.0)\\n'},\n",
       " {'repo': 'forensic-architecture/timemap',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '<h1 align=\"center\">\\n  TimeMap v0\\n</h1>\\n\\n<p align=\"center\">\\n  <strong>TimeMap is a tool for exploration, monitoring and classification of incidents in time and space.<br>See a <a href=\"https://blmprotests.forensic-architecture.org\">live instance here</a>.</strong><br>\\n</p>\\n\\n![](docs/example-timemap.png)\\n\\n[![Build status](https://travis-ci.com/forensic-architecture/timemap.svg?branch=develop)](https://travis-ci.com/forensic-architecture/timemap)\\n\\n## Overview\\n\\nTimeMap is a standalone frontend application that allows to explore and monitor events in time and space. TimeMap uses OpenStreetMap satellite imagery as a backdrop by default, but can also be configured to use [mapbox](https://www.mapbox.com/). It uses Leaflet and d3 to visually map information.\\n\\nThe recommended way to run a backend for timemap is using [datasheet-server](https://github.com/forensic-architecture/datasheet-server). This allows you to work with a spreadsheet or Google Sheet as a dynamic database for for timemap.\\n\\nTimeMap has the following high-level features capabilites:\\n\\n- Visualize incidents of particular events on a map.\\n- Visualize and filter these incidents over time, on an adjustable timeline that allows to zoom in and out.\\n- Visualize types of incidents by tag and by category, which can be displayed using different styles.\\n\\nA fully-functioning live version can be found as a result of the Forensic Architecture investigation of the [Battle of Ilovaisk](https://ilovaisk.forensic-architecture.org).\\n\\n## Get up and running\\n\\nThese easiest way to get up and running with timemap and datasheet-server is to\\n[follow the in-depth tutorial here](https://forensic-architecture.org/investigation/timemap-for-cartographic-platforms).\\n\\nWe recommend using **Node v16.x.x** for its current compatibility.\\nTimeMap may not work with other versions of Node.\\n\\n### Quickstart\\n\\n1. Pull this repository.\\n\\n```shell\\ngit clone https://github.com/forensic-architecture/timemap\\n```\\n\\n2. Install dependencies via npm.\\n\\n```shell\\nnpm install\\n```\\n\\n3. Copy the example config\\n\\n```shell\\ncp example.config.js config.js\\n```\\n\\n4. Run the development server, which will be available at http://localhost:8080.\\n\\n```shell\\nCONFIG=config.js npm run dev\\n```\\n\\nTo run with a file that is not \\'config.js\\' in the root directory, set the `CONFIG` environment variable:\\n\\n```\\nCONFIG=\"myotherconfig.js\" npm run dev\\n```\\n\\nAt this stage, you\\'ll probably only see a basic map with several error modals. In order for TimeMap to be able to display interesting information, you\\'ll have to make sure to have the capacity to serve data, as well as adjusting some configuration parameters. See the [in-depth tutorial](https://forensic-architecture.org/investigation/timemap-for-cartographic-platforms) or [datasheet-server](https://github.com/forensic-architecture/datasheet-server).\\n\\n#### Running without datasheet-server\\n\\nTechnically, timemap is backend agnostic, but it requires a series of endpoints to provide data for it to visualize. The data is expected in JSON format. Some data elements are required and their format has some required fields. Other additional endpoints are optional, and if enabled, they simply add features to your taste.\\n\\nThe combination of all these data types is called the `domain` of the application in the context of TimeMap.\\n\\n#### Running tests\\n\\nWe are currently using [Jest](https://jestjs.io/) for front-end and component testing. These tests can be found inside `src/test`. The test suite can be invoked through `CONFIG=\"my-optional-config.js\" npm run test`.\\n\\nWe also include an [Ava](https://github.com/avajs/ava) test suite for smoke testing the Node server process responsible for instantiating the app. This test suite can be invoked using `CONFIG=\"my-optional-config.js\" npm run test:ava`\\n\\n### Contributing\\n\\nInterested in helping us improve timemap? See [our contributing guide](CONTRIBUTING.md) to learn how to contribute and make suggestions. Please also read our [code of conduct](CODE_OF_CONDUCT.md). We endeavour to cultivate a community around timemap and other OSS at Forensic Architecture that is inclusive and respectful. Please join us in this!\\n\\n## Community\\n\\nIf you have any questions or just want to chat, please [join our Discord server](https://discord.gg/PjHKHJD5KX). This is where you can ask questions, as well as track our internal development on timemap and other codebases at Forensic Architecture.\\n\\n## [License](LICENSE.md)\\n\\ntimemap is distributed under the [DoNoHarm license](https://github.com/raisely/NoHarm).\\n'},\n",
       " {'repo': 'ahmadpak/erpnext_quota',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '## Erpnext Quota\\n\\nApp to manage ERPNext User, Company and Space limitations\\n\\n#### How to Install\\n```\\nbench get-app https://github.com/ahmadpak/erpnext_quota\\nbench --site *site_name* install-app erpnext_quota\\n```\\n### Usage\\nInstall the app. It will add quota config in the site_config.json file\\nContents will look similar:\\n\\n```json\\n{\\n \"db_name\": \"_93245f986574151a\",\\n \"db_password\": \"NQaBCOxtDUDWNJiP\",\\n \"db_type\": \"mariadb\",\\n \"quota\": {\\n  \"active_users\": 2,\\n  \"backup_files_size\": 2,\\n  \"company\": 2,\\n  \"count_administrator_user\": 0,\\n  \"count_website_users\": 0,\\n  \"db_space\": 100,\\n  \"private_files_size\": 0,\\n  \"public_files_size\": 0,\\n  \"space\": 5120,\\n  \"used_company\": 1,\\n  \"used_db_space\": 47,\\n  \"used_space\": 2,\\n  \"users\": 5,\\n  \"valid_till\": \"2022-12-25\"\\n }\\n}\\n```\\n\\nManually change the default values to change the limits. \\nDefault is:\\n- 5 active users not including website users\\n- 2 companies\\n\\nquota.json file will automatically get updated for any \\n\\nTo view the Usage info, find it in Settings Module or search \\'Usage Info\\' in the awesome bar\\n![Database Limit Screenshot](images/database_limit.png)\\n![Files Limit Screenshot](images/files_space_limit.png)\\n![User Limit Screenshot](images/user_limit.png)\\n![Login Limit Screenshot](images/login_validity.gif)\\n![Usage Info Screenshot](images/usage_info_doc.png)\\n\\n#### License\\nMIT\\n'},\n",
       " {'repo': 'webcomponents/community',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': \"# webcomponents community\\n\\n[![Join the chat at https://gitter.im/webcomponents/community](https://badges.gitter.im/webcomponents/community.svg)](https://gitter.im/webcomponents/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\\n[![Travis](https://img.shields.io/travis/webcomponents/community.svg)](https://travis-ci.org/webcomponents/community)\\n\\n# Adding or modifying content\\n 1. Decide what the content type is:\\n  * Evergreen pages such as Polyfills, Specifications should be modified in [/static]\\n  * Posts such as articles/podcasts/presentations should be added to [/documents]\\n 1. Send a PR with the changes.\\n 1. Done! Once it's merged, the site will update with the latest content.\\n\\n# Development\\n## Installing\\n```bash\\n$ npm install\\n```\\n\\n## Running locally\\n```bash\\n$ npm start\\n$ npm run monitor\\n```\\n\\n## Tests & linting\\n```bash\\n$ npm run lint\\n$ npm test\\n```\\n\\n## API\\n\\n### Content\\n```\\nGET /content/:path\\n```\\nReturns blob of markdown/html content.\\n\\n### Static\\n```\\nGET /static/:file\\n```\\nReturns static file. Static files do not appear in `/resources` lists and do not contain any docpad metadata.\\n\\n### List\\n```\\nGET /resources/[:type]\\n```\\nReturns a list of `n` resource excerpts of specified type.\\n`type` is optional.\\nQuery params:\\n * `offset` - offset in results to return. Default: 0\\n * `limit` - number of results to return. Default: 10\\n\"},\n",
       " {'repo': 'actuallyakash/spacers',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': 'spacers\\n-------\\n\\n[1]: <https://github.com/actuallyakash/spacers>\\n\\n_when you need more space_\\n\\n<img src=\"https://res.cloudinary.com/dmz9bftyk/image/upload/v1625301678/spacers_giwb8b.gif\">\\n\\n#### Demo\\n\\n[http://actuallyakash.github.io/spacers](http://actuallyakash.github.io/spacers/)\\n\\n#### Package Managers\\n\\n```sh\\n# Bower\\nbower install --save spacersjs\\n\\n# NPM\\nnpm install spacersjs\\n```\\n\\n#### CDNs\\n\\n```html\\n# unpkg\\n<link rel=\"stylesheet\" href=\"https://unpkg.com/spacersjs/spacers/spacers.min.css\" />\\n<script src=\"https://unpkg.com/spacersjs/spacers/spacers.min.js\"></script>\\n\\n# jsDelivr\\n<link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/spacersjs/spacers/spacers.min.css\" />\\n<script src=\"https://cdn.jsdelivr.net/npm/spacersjs/spacers/spacers.min.js\"></script>\\n\\n# CDNjs\\n<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/spacersjs/1.0.6/spacers.min.css\" />\\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/spacersjs/1.0.6/spacers.min.js\"></script>\\n\\n```\\n\\n#### Bookmarklet\\nTry spacers quickly on any webpage through the bookmarklet.\\n\\n```javascript\\njavascript: (() => {var spacerScript=document.createElement(\"script\");spacerScript.type=\"text/javascript\",spacerScript.src=\"https://cdn.jsdelivr.net/npm/spacersjs/spacers/spacers.min.js\",document.getElementsByTagName(\"head\")[0].appendChild(spacerScript);var spacersStyles=document.createElement(\"link\");spacersStyles.rel=\"stylesheet\",spacersStyles.type=\"text/css\",spacersStyles.href=\"https://cdn.jsdelivr.net/npm/spacersjs/spacers/spacers.min.css\",document.head.appendChild(spacersStyles),spacerScript.onload=function(){spacers({element:\"*\",showOnHover:!0,enableLock:!0,onDragEnd:function(e){console.log(e)}}),alert(\"Spacers active!\")};})();\\n```\\n\\n> <small><b>Note:</b> May not work on some websites due to Content Security Policy.</small>\\n\\n### Settings\\n\\n**Option**|**Type**|**Default**|**Description**\\n-----|-----|-----|-----\\nelement|string|null|Selector on which the spacer has to be initialized\\nappendHtml|string|begin|To append spacer divs after or before the specified `element`. Use `begin` to append before the selector and `end` to append after the selector.\\npadding|boolean|true|To enable padding, which is default behavior\\nmargin|boolean|false|To enable margin\\nonDragEnd|boolean|null|Function for using the spacer values when drag is ended\\ncontainedArea|object (DOM node or jQuery object) or window.document|Use if you\\'re not able to find the element (ex- when using iframe)\\nspacerClass|object|null|For adding custom classes in the spacers\\ndefaultSpacing|string|8|Initial starting spacer height.\\ndefaultPadding|object|null|an object with initial padding spacer top, bottom, left, right values. Overwrites the `defaultSpacing` parameter\\ndefaultMargin|object|null|an object with initial margin spacer top, bottom, left, right values. Overwrites the `defaultSpacing` parameter\\nspacingUnit|string|px|Change default spacing unit of spacers like em, rem, in, cm ..etc\\nshowOnHover|string|false|Show spacers only on hover\\nhideSpacingValue|boolean|false|Hides the margin/padding values at the center of the spacer\\nshowLabel|string|null|Enable and set the label to specified string beside the spacing value\\nenableLock|boolean|false|Link opposite spacers\\nlockIcon|string|```<span class=\"lock\"></span>```|HTML string for the lock icon\\nunlockIcon|string|```<span class=\"unlock\"></span>```|HTML string for the unlock icon\\n\\n#### Example\\n\\nInitialize with:\\n\\n```javascript\\nspacers({\\n    element: \\'.element-unique-class\\'\\n});\\n ```\\n\\n#### Dependencies\\n\\nVoila! It works without any dependencies.\\n\\n#### License\\n\\nCopyright (c) 2021 Akash\\nLicensed under the MIT license.\\n'},\n",
       " {'repo': 'maxjiang93/space_time_pde',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '[![DOI](https://zenodo.org/badge/226422310.svg)](https://zenodo.org/badge/latestdoi/226422310)\\n# MeshfreeFlowNet\\n\\nBy: [Chiyu \"Max\" Jiang*](http://maxjiang.ml/), [Soheil Esmaeilzadeh*](https://soheilesm.github.io/), [Kamyar Azizzadenesheli](https://www.cs.purdue.edu/homes/kamyar/), [Karthik Kashinath](http://www.nersc.gov/about/nersc-staff/data-analytics-services/karthik-kashinath/), [Mustafa Mustafa](https://www.nersc.gov/about/nersc-staff/data-analytics-services/mustafa-mustafa/), [Hamdi Tchelepi](https://profiles.stanford.edu/hamdi-tchelepi), [Philip Marcus](http://www.me.berkeley.edu/people/faculty/philip-s-marcus), [Prabhat](http://www.nersc.gov/about/nersc-staff/data-analytics-services/prabhat/), [Anima Anandkumar](http://tensorlab.cms.caltech.edu/users/anima/) (* Denotes Equal Contributions)\\n\\nPublished at International Conference for High Performance Computing, Networking, Storage and Analysis (SC20). Best Student Paper Award Finalist.\\n\\n\\\\[[Project Website](http://www.maxjiang.ml/proj/meshfreeflownet)\\\\] \\\\[[Paper](https://arxiv.org/pdf/2005.01463.pdf)\\\\] \\\\[[Video](https://youtu.be/mjqwPch9gDo)\\\\] \\\\[[Addtional Video - APS DFD 2020 Presentation](https://www.youtube.com/watch?v=anZ_gLrvnYs&t=538s&ab_channel=SoheilEsmaeilzadeh)\\\\]\\n \\n![teaser](doc/meshfreeflownet_wide.png \"meshfreeflownet_teaser\")\\n\\nThis is the code repository for the MeshfreeFlowNet: physical constrained space time super-resolution. Code implemented in PyTorch.\\n\\n## Introduction\\nMeshfreeFlowNet is a novel deep learning-based super-resolution framework to generate continuous (grid-free) spatio-temporal solutions from the low-resolution inputs. While being computationally efficient, MeshfreeFlowNet accurately recovers the fine-scale quantities of interest. MeshfreeFlowNet allows for: (i) the output to be sampled at all spatio-temporal resolutions, (ii) a set of Partial Differential Equation (PDE) constraints to be imposed, and (iii) training on fixed-size inputs on arbitrarily sized spatio-temporal domains owing to its fully convolutional encoder.\\n\\n## Repo highlights\\nHere are a few reasons why you might be interested in using our code:\\n* We provide a general PyTorch-ready PDE layer that (i) allows evaluation of arbitrary combinations of partial differential equations (ii) provides a user-friendly interface that parses equations from human-readable string. (iii) computes gradient through any black-box function written using pytorch. Easy to plug-and-play into any physics informed ML projects. Find documentation and examples under [`src/`](src).\\n* We provide general layers for 3D U-Nets, continuous decoding network (using IM-NET backbone), and the interpolation layer.\\n* We provide scripts to reproduce the results in our paper.\\n\\n\\n---\\n\\n### In case of using the code or finding the paper impactful in your research please consider citing:\\n\\n    @article{Jiang2020,\\n    archivePrefix = {arXiv},\\n    arxivId = {2005.01463},\\n    author = {Jiang, Chiyu Max and Esmaeilzadeh, Soheil and Azizzadenesheli, Kamyar and Kashinath, \\n    Karthik and Mustafa, Mustafa and Tchelepi, Hamdi A. and Marcus, Philip and Prabhat and Anandkumar, Anima},\\n    eprint = {2005.01463},\\n    title = {{MeshfreeFlowNet: A Physics-Constrained Deep Continuous Space-Time Super-Resolution Framework}},\\n    url = {http://arxiv.org/abs/2005.01463},\\n    year = {2020}\\n    }\\n\\n'},\n",
       " {'repo': 'brihernandez/ArcadeSpaceFlightExample',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# Arcade Space Flight Physics Example\\nSimple example of arcade style space sim flight physics. Built with Unity 5.6.\\n\\n![screenshot](http://i.imgur.com/vQEt6Jd.png)\\n\\n## Download\\n\\nYou can either clone the repository, or [download either the demo or asset package from the releases page](https://github.com/brihernandez/ArcadeSpaceFlightExample/releases).\\n\\n## Controls\\n\\nThere are two control modes supported. The first is **mouse flight** and the second is **traditional**. They can be toggled by checking the \"Use Mouse Input\" property on the ShipInput component.\\n\\n### Mouse Controls\\n- W/S: Increase/Decrease throttle\\n- A/D: Strafe Left/Right\\n- Mouse scrollwheel: Increase/Decrease throttle\\n- Mouse motion: Pitch/Yaw\\n\\n### Traditional Controls\\n- W/S: Pitch Down/Up\\n- A/D: Yaw Left/Right\\n- R/T: Increase/Decrease throttle\\n\\nNote that with the traditional control setting, a gamepad or joystick\\'s X/Y axes can be used for flight as well.\\n\\n## Component Organization\\n\\nWhen possible, I like to break things off into very self-contained components. This isn\\'t always possible, but I feel it\\'s a good practice in general that keeps your source files small and easily understood. However, this does add some complexity in that it requires you to link components together in some way.\\n\\nPersonally, I prefer doing these associations in code, especially when the components are likely to be on the same GameObject. In the case of the ship, it\\'s made of three different components: Ship, Input, and Physics. The way I configured it here, the Ship component is meant to represent the entire ship. All external access to ship properties such as velocity or input are meant to go through the ship. In an effort to keep individual components as dumb as possible, and to minimize cross-component dependency, they communicate (only if necessary) to the ship, rather than other components directly. While there isn\\'t an example of this in the project, each component has a reference to the ship just in case. The ship itself also passes information between components as needed.\\n\\nThere are advantages and disadvantages to this method, but I think it\\'s worth showing as an example of one way to tackle complex component interaction.\\n\\n## Ship\\n\\nAs mentioned above, the ship component represents the ship as a whole and provides a communication mechanism between components.\\n\\n## Ship Input\\n\\nIn keeping with the mentality of separating out responsibilities, this component handles all input. In this example, only player input is considered, but it would be trivial to add a flag here to ignore player input if this isn\\'t the player (by checking ship.isPlayer). A great use case for this would be if you want an AI to instead drive the input. Having an input layer between the player/AI, and the ship itself, allows the ship to only care about ship input and not where it\\'s coming from.\\n\\n## Ship Physics\\n\\nThis component is taken almost verbatim from my [UnityCommon repository](https://github.com/brihernandez/UnityCommon). It\\'s a generic space physics component that can apply forces and torques to a rigidbody using inputs that a ship\\'s input would give. You give it the forces that it can apply in each axis, and then how much of those forces to apply.\\n\\n## Changelog\\n\\n### 1.0 08/24/2017\\n- Initial commit'},\n",
       " {'repo': 'drphilmarshall/SpaceWarps',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': 'Space Warps\\n-----------\\n\\nCrowd-sourced discovery of gravitational lenses. We are a Zooniverse project,\\nwhich launched in Spring 2013 at [http://spacewarps.org](http://spacewarps.org). This repository contains code and documentation\\nbeing developed by the science team.\\n\\nTwo papers on the first dataset we searched for lenses, the CFHTLS, have been published:\\n\\n* [Marshall et al. 2016: *\"Space Warps I: Crowd-sourcing the Discovery of Gravitational Lenses\"*, *MNRAS*, 455, 1171](https://github.com/drphilmarshall/SpaceWarps/raw/master/doc/sw-system-published.pdf)\\n* [More et al. 2016: *\"Space Warps II: New Gravitational Lens Candidates from the CFHTLS Discovered through Citizen Science\"*, *MNRAS*, 455, 1191](https://github.com/drphilmarshall/SpaceWarps/raw/master/doc/sw-cfhtls-published.pdf)\\n\\nThe web app itself is being developed [here](https://github.com/zooniverse/Lens-Zoo) as part of the [Zooniverse](http://zooniverse.org) project. The simulated lenses used in the training sample are created with [SIMCT](https://github.com/anupreeta27/SIMCT). If you are interested in building a site like Space Warps using Zooniverse techology, keep an eye on their website or drop them a line.\\n\\nSee also, for fun:\\n* [LensToy](http://github.com/slowe/LensToy/)\\n* [LensWrangler](http://github.com/drphilmarshall/LensWrangler/)\\n\\n\\n### Contact\\n\\n* Phil Marshall (KIPAC, SLAC National Accelerator Laboratory)\\n* Aprajita Verma (Physics Department, University of Oxford)\\n* Anupreeta More (Kavli IPMU, University of Tokyo)\\n\\nYou can email us at [spacewarpspi@googlegroups.com](mailto:spacewarpspi@googlegroups.com), or just [send us an issue!](https://github.com/drphilmarshall/SpaceWarps/issues)\\n\\n### License\\n\\nAll our code is free to re-use under the MIT license. If you make use of any of it in your research, please cite us at this website, and as *\"(Marshall et al. 2016, More et al. 2016)\"*. Please do get in touch though - it would be great to collaborate on improving the SWAP analysis, for example!\\n'},\n",
       " {'repo': 'ably-labs/realtime-multiplayer-space-invaders',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '![](./public/space-invaders-hero.svg)\\n\\n# Realtime multiplayer game of Space Invaders\\n\\nThis project runs a realtime multiplayer version of the classic retro game, Space Invaders.\\n\\n![Preview of the game](https://user-images.githubusercontent.com/5900152/84092843-7ea1ce80-a9f0-11ea-809d-41cd20fb8e59.gif)\\n\\n## Services/ libraries used in the game\\n\\n- [Phaser 3](https://phaser.io)\\n- [p2 NPM library](https://www.npmjs.com/package/p2)\\n- [Ably Realtime](https://www.ably.com) \\n\\nYou will require an Ably API Key, to run this demo, [sign-up for FREE account](https://ably.com/sign-up)\\n\\n# How to run this game\\n\\n1. Create a free account with [Ably Realtime](https://www.ably.com) and obtain an API Key\\n1. Clone this repo locally\\n1. Navigate to the project folder and run `npm install` to install the dependencies\\n1. Rename `.env-sample` to `.env`, then edit the file and add your Ably API key and prefered PORT (default 8080).\\n1. (Optional) You can update the `MIN_PLAYERS_TO_START_GAME` to enforce a minimum number of players. (see `server-worker.js`)\\n1. Run the server with `node server.js` and then open a brower to [localhost:8080](http://localhost:8080)\\n\\nRead the full blog post series on [dev.to](https://dev.to/ably/building-a-realtime-multiplayer-browser-game-in-less-than-a-day-part-1-4-14pm).\\n\\nPlease [reach out to me on Twitter](https://www.twitter.com/Srushtika) for any questions, \\nor follow us [@ablyrealtime](https://twitter.com/ablyrealtime)\\n'},\n",
       " {'repo': 'maryrosecook/retro-games',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Retro games\\n\\nSkeletal versions of Asteroids, Lunar Lander, Space Invaders and Snake in JavaScript.  Written as practice for my workshop at Strange Loop 2014.\\n\\n* By Mary Rose Cook\\n* http://maryrosecook.com\\n* maryrosecook@maryrosecook.com\\n\\n![Screenshot of Asteroids](asteroids/screenshot.png)\\n\\n![Screenshot of Lunar Lander](lunar-lander/screenshot.png)\\n\\n![Screenshot of Space Invaders](space-invaders/screenshot.png)\\n\\n![Screenshot of Snake](snake/screenshot.png)\\n'},\n",
       " {'repo': 'thewebbooth/KSP-X-Science',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': 'Z-Key Aerospace has rebuilt this KSP Mod for KSP V1.0 onwards\\n[X] Science! was by Brodrick and is released under \"Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International license\"\\n\\nAs of V4.1 it is rebuilt and fixed by Z-Key Aerospace.\\n\\n\\n\\nBrodrick\\'s readme follows...\\n\\n\\n\\n[x] Science!\\n============\\n\\nTired of wondering whether you need to go back to Minmus to get an EVA report while landed on the Greater Flats? Then **[x] Science!** is the mod for you!\\n\\n**[x] Science!** is a science checklist plugin for Kerbal Space Program. It allows you to see what science experiments you have left to perform - and how much science you\\'ll get for doing them.\\n\\nInstallation\\n------------\\n\\nSimply copy the GameData folder into your KSP directory.\\n\\nLicense\\n------\\n\\n**[x] Science!** is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International license][1].\\n\\nYou are free to:  \\n\\n * **Share** - copy and redistribute the material in any medium or format\\n\\n * **Adapt** - remix, transform, and build upon the material\\n\\nAs long as:\\n\\n * **Attribution** - You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.\\n\\n * **NonCommercial** - You may not use the material for commercial purposes.\\n\\n * **ShareAlike** - If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.\\n\\n\\nVersion Checking\\n---\\n\\nThis mod includes version checking using [MiniAVC][2]. If you opt-in, it will use the internet to check whether there is a new version available. Data is only read from the internet and no personal information is sent. For a more comprehensive version checking experience, please download the [KSP-AVC Plugin][2].\\n\\n[1]:http://creativecommons.org/licenses/by-nc-sa/4.0/\\n[2]:http://forum.kerbalspaceprogram.com/threads/79745'},\n",
       " {'repo': 'lyxok1/STM-Training',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Trainig Script for Space Time Memory Network\\r\\n\\r\\nThis codebase implemented training code for [Space Time Memory Network](http://openaccess.thecvf.com/content_ICCV_2019/html/Oh_Video_Object_Segmentation_Using_Space-Time_Memory_Networks_ICCV_2019_paper.html) with some [cyclic features](https://arxiv.org/abs/2010.12176).\\r\\n\\r\\n<img src=\"./demo/sample/demo.gif\" alt=\"sample results\" style=\"max-width:75%;\">\\r\\n\\r\\n## Update\\r\\n\\r\\n- We have post a journal version of our paper [here](https://arxiv.org/abs/2111.01323), the modified cycle version of [AOT model](https://github.com/13633491388/AOT_cycle) in our journal paper is also available now.\\r\\n\\r\\n## Requirement\\r\\n### python package\\r\\n- torch\\r\\n- python-opencv\\r\\n- pillow\\r\\n- yaml\\r\\n- imgaug\\r\\n- yacs\\r\\n- progress\\r\\n- [nvidia-dali](https://docs.nvidia.com/deeplearning/dali/user-guide/docs/index.html) (optional)\\r\\n\\r\\n### GPU support\\r\\n\\r\\n- GPU Memory >= 12GB\\r\\n- CUDA >= 10.0\\r\\n\\r\\n## Data\\r\\n\\r\\nSee the doc [DATASET.md](./DATASET.md) for more details on data organization of our prepared dataset.\\r\\n\\r\\n## Release\\r\\nWe provide pre-trained model with different backbone in our codebase, results are validated on DAVIS17-val with gradient correction.\\r\\n\\r\\n| model |backbone|data backend| J | F | J & F | link |FPS|\\r\\n|:-----:|:------:|:----------:|:-:|:-:|:-----:|:----:|:-:|\\r\\n| STM-Cycle | Resnet18 | DALI | 65.3 | 70.8 | 68.1 | [Google Drive](https://drive.google.com/file/d/1Lp9X2b0_som0WagT2MAovJ0NokjCfGUJ/view?usp=sharing) |14.8|\\r\\n| STM-Cycle | Resnet50 | PIL | 70.5 | 76.3 | 73.4 | [Google Drive](https://drive.google.com/file/d/1tSTNBeqa9hyKBPX6NzL1N7EgkWAg_2cv/view?usp=sharing)|9.3|\\r\\n\\r\\n## Runing\\r\\nAppending the root folder to the search path of python interpreter\\r\\n```bash\\r\\nexport PYTHONPATH=${PYTHONPATH}:./\\r\\n```\\r\\n\\r\\nTo train the STM network, run following command.\\r\\n```bash\\r\\npython3 train.py --cfg config.yaml OPTION_KEY OPTION_VAL\\r\\n```\\r\\n\\r\\nTo test the STM network, run following command\\r\\n```bash\\r\\npython3 test.py --cfg config.yaml initial ${PATH_TO_MODEL} OPTION_KEY OPTION_VAL\\r\\n```\\r\\nThe test results will be saved as indexed png file at `${ROOT}/${output_dir}/${valset}`.\\r\\n\\r\\nTo run a segmentation demo, run following command\\r\\n```bash\\r\\npython3 demo/demo.py --cfg demo/demo.yaml OPTION_KEY OPTION_VAL\\r\\n```\\r\\nThe segmentation results will be saved at `${output_dir}`.\\r\\n\\r\\n## Acknowledgement\\r\\nThis codebase borrows the code and structure from [official STM repository](https://github.com/seoungwugoh/STM)\\r\\n\\r\\n## Reference\\r\\nThe codebase is built based on following works\\r\\n```Bibtex\\r\\n@InProceedings{Oh_2019_ICCV,\\r\\nauthor = {Oh, Seoung Wug and Lee, Joon-Young and Xu, Ning and Kim, Seon Joo},\\r\\ntitle = {Video Object Segmentation Using Space-Time Memory Networks},\\r\\nbooktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},\\r\\nmonth = {October},\\r\\nyear = {2019}\\r\\n}\\r\\n\\r\\n@InProceedings{Li_2020_NeurIPS,\\r\\nauthor = {Li, Yuxi and Xu, Ning and Peng Jinlong and John See and Lin Weiyao},\\r\\ntitle = {Delving into the Cyclic Mechanism in Semi-supervised Video Object Segmentation},\\r\\nbooktitle = {Neural Information Processing System (NeurIPS)},\\r\\nyear = {2020}\\r\\n}\\r\\n\\r\\n@article{li2022exploring,\\r\\n  title={Exploring the Semi-Supervised Video Object Segmentation Problem from a Cyclic Perspective},\\r\\n  author={Li, Yuxi and Xu, Ning and Yang, Wenjie and See, John and Lin, Weiyao},\\r\\n  journal={International Journal of Computer Vision},\\r\\n  pages={1--17},\\r\\n  year={2022},\\r\\n  publisher={Springer}\\r\\n}\\r\\n```\\r\\n\\r\\n'},\n",
       " {'repo': 'owncloud/files_antivirus',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': \"# ownCloud Antivirus App   \\n\\nfiles_antivirus is an antivirus app for [ownCloud](https://github.com/owncloud) based on [ClamAV](http://www.clamav.net).\\n\\n## Details\\n\\nThe idea is to check for virus at upload-time, notifying the user (on screen and/or email) and\\nremove the file if it's infected.\\n\\n## QA metrics on master branch:\\n\\n[![Build Status](https://drone.owncloud.com/api/badges/owncloud/files_antivirus/status.svg?branch=master)](https://drone.owncloud.com/owncloud/files_antivirus)\\n[![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=owncloud_files_antivirus&metric=alert_status)](https://sonarcloud.io/dashboard?id=owncloud_files_antivirus)\\n[![Security Rating](https://sonarcloud.io/api/project_badges/measure?project=owncloud_files_antivirus&metric=security_rating)](https://sonarcloud.io/dashboard?id=owncloud_files_antivirus)\\n[![Coverage](https://sonarcloud.io/api/project_badges/measure?project=owncloud_files_antivirus&metric=coverage)](https://sonarcloud.io/dashboard?id=owncloud_files_antivirus)\\n\\n## Status\\n\\nThe App is not complete yet, the following works/is done:\\n* It can be configured to work with the executable or the daemon mode of ClamAV\\n* If used in daemon mode it can connect through network- or local file-socket\\n* In daemon mode, it sends files to a remote/local server using INSTREAM command\\n* When the user uploads a file, it's checked\\n* If an uploaded file is infected, it's deleted and a notification is shown to the user on screen and an email is sent with details.\\n* Tested in Linux only\\n* Background Job to scan all files\\n* Test uploading from clients\\n* File size limit\\n\\n## ToDo\\n\\n* Configurations Tuneups\\n* Other OS Testing\\n* Look for ideas :P\\n\\n## Requirements\\n\\n* ClamAV (Binaries or a server running ClamAV in daemon mode)\\n\\n\\n## Install\\n\\n* Install and enable the App\\n* Go to Admin Panel and [configure](https://doc.owncloud.org/server/10.0/admin_manual/configuration/server/antivirus_configuration.html) the App\\n\\n## Enterprise Feature: ICAP Antivirus integration\\n\\nThe Files Antivirus app can support the [ICAP](https://tools.ietf.org/html/rfc3507) protocol if you are using the ownCloud Enterprise Edition.\\n\\nUsing the ICAP mode requires a valid enterprise license. If no license key is present, it will trigger the grace period to obtain a valid key.\\nAfter the expiration of the grace period / license key, the files_antivirus app will be disabled.\\n\\n### Run with c-icap/clamav\\n\\nc-icap has a built-in clamav module see https://sourceforge.net/p/c-icap/wiki/ModulesConfiguration/\\n\\nAn out-of-the-box docker image  _for testing purpose_ is available at https://hub.docker.com/r/deepdiver/icap-clamav-service\\n\\nFor simple local testing run docker run -ti deepdiver/icap-clamav-service and get it's ip using docker inspect.\\nThe IP address needs to be setup in the configuration - see above\\n\\nThe request service for clamav has to be set to 'avscan' and the response header to 'X-Infection-Found'\\n\\n\\n### Run with Kaspersky\\n\\nKaspersky provides docker images as well (https://box.kaspersky.com/d/c8d8577dc2494256b45e/)\\nFollow the instructions in Kaspersky ScanEngine for Kubernetes.7z\\n\\nAdditional configuration: \\nEnable Allow204 - this is necessary to tell kav to not send back the file contents.\\nsee https://support.kaspersky.com/ScanEngine/1.0/en-US/201151.htm\\n\\nThe request service for clamav has to be set to 'req' and the response header to 'X-Virus-ID'\\n\\n\\nNOTE: The older versions of KAV did not send back the virus/infection name in an icap header.\\n\\nIn v2.0.0 the header to transport the virus can be configured. Default: No header is sent.\\nsee https://support.kaspersky.com/ScanEngine/1.0/en-US/201214.htm\\n\\n\\n### Run with FortiSandbox in ICAP Mode\\n\\nSelect 'Fortinet' from the dropdown.\\n\\nThe request service for FortiSandbox has to be set to 'respmod' and the response header to 'X-Virus-Name'.\\n\\nFortinet provides product trials of FortiSandbox, please have a look at [Fortinet](https://www.fortinet.com/de/products/sandbox/fortisandbox).\\n\\n\\n### Run with McAfee Web Gateway 10.x and higher in ICAP Mode\\n\\nSelect 'McAfee Web Gateway 10.x and higher' from the dropdown.\\n\\nThe request service for McAfee has to be set to 'respmod' and the response header to 'X-Virus-Name'.\\n\\nMcAfee provides product trial for evaluation purposes. Have a look at [the McAfee Webpage](https://www.skyhighsecurity.com/en-us/products/secure-web-gateway.html) for the Web Gateway.\\n\\nNote: Product is now called 'Skyhigh Secure Web Gateway'\\n\\nAuthors:\\n\\n[Manuel Delgado López](https://github.com/valarauco/) :: manuel.delgado at ucr.ac.cr  \\n[Bart Visscher](https://github.com/bartv2/)  \\n[Viktar Dubiniuk](https://github.com/vicdeo/)\\n\"},\n",
       " {'repo': 'lllyasviel/PaintingLight',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Project PaintingLight\\n\\nPaintingLight is a project conducted by the Style2Paints team, aimed at finding a method to manipulate the illumination in digital paintings. The project started at about 2019 January, and the core algorithm is accepted by ACM Transitions on Graphics at 2020. \\n\\nBecause digital painting illumination data is not easy to obtain, this algorithm does not use deep learning. The core idea is to make use of color geometry to build up a perceptually workable relighting system. Such relighting may not be physically accurate, but are good enough for artistic use cases.\\n\\nNote that the project is still in its technical researching stage. If you are a digital painting artist and you accidentally find this page, you may have to wait for our ongoing PhotoShop plug-in for practical usages.\\n\\n# Technical Paper\\n\\nPlease refer to our [project page](https://lllyasviel.github.io/PaintingLight/) for our TOG/SIGGRAPH paper. \\n\\n(c) artwork traced by ToS2P from an artwork by David Revoy under CC-BY license, www.peppercarrot.com\\n\\n# Video and Animated Demos\\n\\n[![video_link](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/vid_icon.png)](https://www.youtube.com/watch?v=X7li86oMBLA)\\n\\n(c) artwork traced by ToS2P from an artwork by David Revoy under CC-BY license, www.peppercarrot.com\\n\\n# Installation\\n\\nThe codes have been tested for python 3.6 in both Windows 10 and Ubuntu 16.04. (If you use windows then python 3.6 is a must.)\\n\\nTo download codes:\\n\\n    git clone https://github.com/lllyasviel/PaintingLight.git\\n    cd PaintingLight\\n    cd code\\n\\nTo install some environment:\\n\\n    pip install opencv-python\\n    pip install opencv-contrib-python\\n    pip install h5py\\n\\nTo install some super accurate environment:\\n\\n    pip install tensorflow==1.4.0\\n    pip install scipy==1.1.0\\n    pip install trimesh==2.37.1\\n\\nThen install the rtree package. The original rtree does not support windows, nevertheless here I provide a windows binary so that you can directly install it.\\n\\n    (linux only) sudo apt install libspatialindex-dev\\n    (linux only) pip install rtree==0.9.3\\n    \\n    (windows only) pip install Rtree-0.9.3-cp36-cp36m-win_amd64.whl\\n\\nTo install pyembree to enable GPU ray tracing:\\n\\n(Optional, you can skip this step if you do not care about speed.) \\n\\n[Linux Pyembree](https://github.com/scopatz/pyembree)\\n\\n[Windows Pyembree](https://github.com/scopatz/pyembree/issues/14)\\n\\n# Playing with Examples\\n\\nYou may directly play with our interactive examples! \\n\\n**The image noise artifacts in this webpage is caused by web GIF compression.**\\n\\n**These artifacts do not exist when you try on your own.**\\n\\n\\n![002](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/002.gif)  | * Example 002 <br> * Not need a mask <br> * Input image \"002.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example002.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![003](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/003.gif)  | * Example 003 <br> * Not need a mask <br> * Input image \"003.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example003.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![004](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/004.gif)  | * Example 004 <br> * Not need a mask <br> * Input image \"004.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example004.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![005](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/005.gif)  | * Example 005 <br> * Not need a mask <br> * Input image \"005.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example005.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![006](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/006.gif)  | * Example 006 <br> * Not need a mask <br> * Input image \"006.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example006.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![007](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/007.gif)  | * Example 007 <br> * Not need a mask <br> * Input image \"007.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example007.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![008](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/008.gif)  | * Example 008 <br> * Not need a mask <br> * Input image \"008.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example008.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![009](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/009.gif)  | * Example 009 <br> * Not need a mask <br> * Input image \"009.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example009.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![010](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/010.gif)  | * Example 010 <br> * Not need a mask <br> * Input image \"010.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example010.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![011](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/011.gif)  | * Example 011 <br> * Not need a mask <br> * Input image \"011.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example011.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![012](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/012.gif)  | * Example 012 <br> * Not need a mask <br> * Input image \"012.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example012.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![013](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/013.gif)  | * Example 013 <br> * Not need a mask <br> * Input image \"013.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example013.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![014](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/014.gif)  | * Example 014 <br> * Not need a mask <br> * Input image \"014.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example014.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![015](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/015.gif)  | * Example 015 <br> * Not need a mask <br> * Input image \"015.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example015.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![016](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/016.gif)  | * Example 016 <br> * Not need a mask <br> * Input image \"016.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example016.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![017](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/017.gif)  | * Example 017 <br> * Not need a mask <br> * Input image \"017.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example017.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![018](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/018.gif)  | * Example 018 <br> * Not need a mask <br> * Input image \"018.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example018.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![019](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/019.gif)  | * Example 019 <br> * Not need a mask <br> * Input image \"019.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example019.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![020](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/020.gif)  | * Example 020 <br> * Not need a mask <br> * Input image \"020.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example020.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![021](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/021.gif)  | * Example 021 <br> * Not need a mask <br> * Input image \"021.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example021.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![022](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/022.gif)  | * Example 022 <br> * Not need a mask <br> * Input image \"022.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example022.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![023](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/023.gif)  | * Example 023 <br> * Not need a mask <br> * Input image \"023.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example023.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![024](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/024.gif)  | * Example 024 <br> * Not need a mask <br> * Input image \"024.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example024.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![025](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/025.gif)  | * Example 025 <br> * Not need a mask <br> * Input image \"025.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example025.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![026](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/026.gif)  | * Example 026 <br> * Not need a mask <br> * Input image \"026.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example026.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![027](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/027.gif)  | * Example 027 <br> * Not need a mask <br> * Input image \"027.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example027.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![028](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/028.gif)  | * Example 028 <br> * Not need a mask <br> * Input image \"028.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example028.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![029](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/029.gif)  | * Example 029 <br> * Not need a mask <br> * Input image \"029.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example029.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![030](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/030.gif)  | * Example 030 <br> * Not need a mask <br> * Input image \"030.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example030.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![031](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/031.gif)  | * Example 031 <br> * Not need a mask <br> * Input image \"031.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example031.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![032](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/032.gif)  | * Example 032 <br> * Not need a mask <br> * Input image \"032.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example032.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![033](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/033.gif)  | * Example 033 <br> * Not need a mask <br> * Input image \"033.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example033.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![034](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/034.gif)  | * Example 034 <br> * Not need a mask <br> * Input image \"034.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example034.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![035](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/035.gif)  | * Example 035 <br> * Not need a mask <br> * Input image \"035.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example035.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![036](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/036.gif)  | * Example 036 <br> * Not need a mask <br> * Input image \"036.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example036.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![037](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/037.gif)  | * Example 037 <br> * Not need a mask <br> * Input image \"037.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example037.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![038](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/038.gif)  | * Example 038 <br> * Not need a mask <br> * Input image \"038.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example038.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![039](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/039.gif)  | * Example 039 <br> * Not need a mask <br> * Input image \"039.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example039.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![040](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/040.gif)  | * Example 040 <br> * Not need a mask <br> * Input image \"040.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example040.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![041](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/041.gif)  | * Example 041 <br> * Not need a mask <br> * Input image \"041.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example041.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![042](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/042.gif)  | * Example 042 <br> * Not need a mask <br> * Input image \"042.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example042.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![043](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/043.gif)  | * Example 043 <br> * Not need a mask <br> * Input image \"043.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example043.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n\\n![044](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/044.gif)  | * Example 044 <br> * Not need a mask <br> * Input image \"044.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example044.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :-----\\n\\n# Playing with Examples with Masks\\n\\n![045](https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/045.gif)  | <img src=\"https://raw.githubusercontent.com/lllyasviel/PaintingLight/master/code/imgs/045.mask.png\" width=\"256\" hegiht=\"256\" align=center />  | * Example 045 <br> * Need a mask <br> * Input image \"045.jpg\" <br>  <br>  <br>  <br> To try it: <br> <br> --- <br> >> python example045.py <br> --- <br>  <br> <br> Image has copyrights.\\n---- | :----- | :-----\\n\\n# Playing with Your Own Images\\n\\nJust try:\\n\\n    python default.py your_image.png\\n\\nIt is also possible to edit parameters in *default.py*. See codes for detals.\\n\\nNote that big images may cause a out-of-memory error. See [Here](https://github.com/lllyasviel/PaintingLight/issues/2). Please resize your images to about 512px before processing.\\n\\n# FAQs\\n\\n    Q: It is mentioned that this project does not using \\n       deep learning, then why it is still required to install tensorflow?\\n    \\n    A: This is because we use SRCNN, a tensorflow neural network, to \\n       pre-process input images in order to remove JPEG artifacts. Therefore \\n       you still need to install tensorflow with a proper version.\\n<br>\\n\\n    Q: I am trying with my own images. Can you explain \\n       the parameters so that I can get better results?\\n    \\n    A: Here we list all possible parameters:\\n    \\n    image:                               the input image.\\n    \\n    mask:                                a paired mask. you can set it to None as it is optional.\\n    \\n    ambient_intensity:                   the environment ambient light intensity. 0.45 recommended.\\n    \\n    light_intensity:                     the intensity of your light. 0.85 recommended.\\n    \\n    light_source_height:                 the height of your light source. It is the distance \\n                                         from the image to your light source. 1.0 recommended.\\n                                \\n    gamma_correction:                    the gamma correction parameter. It is a common parameter in \\n                                         many digital cameras or smartphone cameras, and we provide \\n                                         it if necessary. 1.0 recommended.\\n                                \\n    stroke_density_clipping:             a scalar to clip the stroke density. Bigger number results \\n                                         in sharper results. 1.2 recommended.\\n                                \\n    enabling_multiple_channel_effects:   whether to generate multiple-channel lighting \\n                                         effects. True recommended.\\n                                         \\n    light_color_red:                     color of your light. 1.0 recommended.\\n    \\n    light_color_green:                   color of your light. 1.0 recommended.\\n    \\n    light_color_blue:                    color of your light. 1.0 recommended.\\n<br>\\n\\n    Q: I am currently trying with flat cell illustrations or line drawings, but \\n       the results are bad. Is this method not suitable to line drawings and flat \\n       cell illustrations?\\n\\n    A: This method not suitable to line drawings and flat cell illustrations. This \\n       is because the main technique of this algorithm is called stroke density. \\n       The algorithm fails if the input image do not contain such strokes or \\n       similar patterns.\\n <br>\\n \\n    Q: I have tried many parameters but I am still not very satisfied. \\n        What can I do to realize the full potential of this algorithm?\\n\\n    A: If you really need you may manually annotate a mask and use the masked mode. \\n       You may see also the code for the “Playing with Examples with Masks” examples.\\n       \\n# Citation\\n\\n    @Article{ZhangTOG2020,\\n       author    = {Lvmin Zhang and Edgar Simo-Serra and Yi Ji and Chunping Liu},\\n       title     = {{Generating Digital Painting Lighting Effects via RGB-space Geometry}},\\n       journal   = \"Transactions on Graphics (Presented at SIGGRAPH)\",\\n       year      = 2020,\\n       volume    = 39,\\n       number    = 2,\\n    }\\n\\n# 中文社区\\n\\n我们有一个除了技术什么东西都聊的以技术交流为主的群。如果你一次加群失败，可以多次尝试: 816096787。\\n\\n'},\n",
       " {'repo': 'peachananr/label_better',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '#Label Better by Pete R.\\nLabel your form input like a boss with beautiful animation and without taking up space\\nCreated by [Pete R.](http://www.thepetedesign.com), Founder of [BucketListly](http://www.bucketlistly.com)\\n\\nLicense: [Attribution-ShareAlike 4.0 International](http://creativecommons.org/licenses/by-sa/4.0/deed.en_US)\\n\\n\\n## Demo\\n[View demo](http://peachananr.github.io/label_better/demo/demo.html)\\n\\n## Compatibility\\nModern browsers such as Chrome, Firefox, and Safari on both desktop and smartphones have been tested. I have not tested this on IE.\\n\\n## Basic Usage\\njQuery Label Better will let you create a beautiful unobtrusive label for your form\\'s input fields. The uniqueness about this plugin is that all you have to do is add a placeholder text, and we will show the label only when the user needs it. \\n\\nTo add this to your website, simply include the latest jQuery library together with `jquery.label_better.js` into your document\\'s `<head>`, and simply call the function like this:\\n\\n````javascript\\n  $(\"input.label_better\").label_better({\\n    position: \"top\", // This will let you define the position where the label will appear when the user clicked on the input fields. Acceptable options are \"top\", \"bottom\", \"left\" and \"right\". Default value is \"top\".\\n    animationTime: 500, // This will let you control the animation speed when the label appear. This option accepts value in milliseconds. The default value is 500.\\n    easing: \"ease-in-out\", // This option will let you define the CSS easing you would like to see animating the label. The option accepts all default CSS easing such as \"linear\", \"ease\" etc. Another extra option is you can use is \"bounce\". The default value is \"ease-in-out\".\\n    offset: 20, // You can add more spacing between the input and the label. This option accepts value in pixels (without the unit). The default value is 20.\\n    hidePlaceholderOnFocus: true // The default placeholder text will hide on focus\\n  });\\n````\\n## Markups\\n\\nWith this plugin, you can use a markup to override the global options defined in the function above. Here are all the markups you can use to customize your experience to your liking:\\n\\n### data-position\\nThis markup will let you define the position of each input field individually.\\n\\n````html\\n<input type=\"text\" class=\"label_better\" data-position=\"top\" placeholder=\"Username\">\\n<input type=\"text\" class=\"label_better\" data-position=\"right\" placeholder=\"Email Address\">\\n````\\n\\n### data-new-placeholder\\nThere may be times when you want your placeholder text to be different from the label text. You can do that by defining the new placeholder text as follows and this value will be shown as the label instead.\\n\\n````html\\n<input type=\"text\" class=\"label_better\" data-new-placeholder=\"Type your username\" placeholder=\"Username\">\\n<input type=\"text\" class=\"label_better\" data-new-placeholder=\"Type your email address\" placeholder=\"Email Address\">\\n````\\n\\nAnd that\\'s all for Label Better plugin. Stay tuned for more updates.\\n\\nIf you want to see more of my plugins, visit [The Pete Design](http://www.thepetedesign.com/#design), or follow me on [Twitter](http://www.twitter.com/peachananr) and [Github](http://www.github.com/peachananr).\\n\\n## Other Resources\\n- [Tutorial](http://www.onextrapixel.com/2014/01/07/label-your-input-fields-like-a-boss-with-label_better-js/)\\n'},\n",
       " {'repo': 'contentful/cf-graphql',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# cf-graphql\\n\\n[![travis build status](https://img.shields.io/travis/contentful-labs/cf-graphql.svg)](https://travis-ci.org/contentful-labs/cf-graphql)\\n[![npm version](https://img.shields.io/npm/v/cf-graphql.svg)](https://www.npmjs.com/package/cf-graphql)\\n[![npm downloads](https://img.shields.io/npm/dt/cf-graphql.svg)](https://www.npmjs.com/package/cf-graphql)\\n[![deps status](https://img.shields.io/david/contentful-labs/cf-graphql.svg)](https://david-dm.org/contentful-labs/cf-graphql)\\n[![dev deps status](https://img.shields.io/david/dev/contentful-labs/cf-graphql.svg)](https://david-dm.org/contentful-labs/cf-graphql?type=dev)\\n[![codecov coverage](https://img.shields.io/codecov/c/github/contentful-labs/cf-graphql.svg)](https://codecov.io/gh/contentful-labs/cf-graphql)\\n\\n`cf-graphql` is a library that allows you to query your data stored in [Contentful](https://www.contentful.com/) with [GraphQL](http://graphql.org/). A schema and value resolvers are automatically generated out of an existing space.\\n\\nGenerated artifacts can be used with any node-based GraphQL server. The outcome of the project\\'s main function call is an instance of the [`GraphQLSchema`](http://graphql.org/graphql-js/type/#graphqlschema) class.\\n\\n\\n## Table of contents\\n\\n- [Disclaimers](#disclaimers)\\n- [First steps](#first-steps)\\n- [Demo](#demo)\\n  - [Run it locally](#run-it-locally)\\n  - [Deploy to Zeit\\'s now](#deploy-to-zeits-now)\\n- [Programmatic usage](#programmatic-usage)\\n- [Querying](#querying)\\n- [Helpers](#helpers)\\n- [Contributing](#contributing)\\n\\n\\n## Disclaimers\\n\\nPlease note that `cf-graphql` library is released as an experiment:\\n\\n- we might introduce breaking changes into programmatic interfaces and space querying approach before v1.0 is released\\n- there’s no magic bullet: complex GraphQL queries can result in a large number of CDA calls, which will be counted against your quota\\n- we might discontinue development of the library and stop maintaining it\\n\\n\\n## First steps\\n\\nIf you just want to see how it works, please follow the [Demo](#demo) section. You can deploy the demo with your own credentials so it queries your own data.\\n\\nIn general `cf-graphql` is a library and it can be used as a part of your project. If you want to get your hands dirty coding, follow the [Programmatic usage](#programmatic-usage) section.\\n\\n\\n## Demo\\n\\nWe host an [online demo](https://cf-graphql-demo.now.sh/) for you. You can query Contentful\\'s \"Blog\" space template there. This how its graph looks like:\\n\\n![Demo space graph](./demo/demo-space-graph.png)\\n\\n\\n### Run it locally\\n\\nThis repository contains a demo project. The demo comes with a web server (with [CORS](https://developer.mozilla.org/en-US/docs/Web/HTTP/Access_control_CORS) enabled) providing the GraphQL, [an in-browser IDE (GraphiQL)](https://github.com/graphql/graphiql) and a React Frontend application using this endpoint.\\n\\nTo run it, clone the repository, install dependencies and start a server:\\n\\n```\\ngit clone git@github.com:contentful-labs/cf-graphql.git\\ncd cf-graphql/demo\\n# optionally change your node version with nvm, anything 6+ should work just fine\\n# we prefer node v6 matching the current AWS Lambda environment\\nnvm use\\nnpm install\\nnpm start\\n```\\n\\nUse <http://localhost:4000/graphql/> to query the data from within your application and navigate to <http://localhost:4000> to use the IDE (GraphiQL) for test-querying. Please refer to the [Querying](#querying) section for more details.\\n\\nIf you also want to see how to integrate GraphQL in a React technology stack the demo project also contains an application based on the [Apollo framework](https://www.apollodata.com/). To check it out use <http://localhost:4000/client/>.\\n\\nTo use your own Contentful space with the demo, you have to provide:\\n\\n- space ID\\n- CDA token\\n- CMA token\\n\\nPlease refer the [\"Authentication\" section](https://www.contentful.com/developers/docs/references/authentication/) of Contentful\\'s documentation.\\n\\nYou can provide listed values with env variables:\\n\\n```\\nSPACE_ID=some-space-id CDA_TOKEN=its-cda-token CMA_TOKEN=your-cma-token npm start\\n```\\n\\n\\n### Deploy to [Zeit\\'s `now`](https://zeit.co/now)\\n\\nTo be able to deploy to [Zeit\\'s `now`](https://zeit.co/now) you need to have an activated account. There is a free open source option available.\\n\\nYou can also deploy the demo with `now`. In your terminal, navigate to the `demo/` directory and run:\\n\\n```\\nnpm run deploy-demo-now\\n```\\n\\nAs soon as the deployment is done you\\'ll have a URL of your GraphQL server copied.\\n\\nYou can also create a deployment for your own space:\\n\\n```\\nSPACE_ID=some-space-id CDA_TOKEN=its-cda-token CMA_TOKEN=your-cma-token npm run deploy-now\\n```\\n\\nPlease note:\\n\\n- when deploying a server to consume Contentful\\'s \"Blog\" space template, the command to use is `npm run deploy-demo-now`; when the demo should be configured to use your own space, the command is `npm run deploy-now`\\n- if you\\'ve never used `now` before, you\\'ll be asked to provide your e-mail; just follow on-screen instructions\\n- if you use `now`\\'s OSS plan (the default one), the source code will be public; it\\'s completely fine: all credentials are passed as env variables and are not available publicly\\n\\n\\n## Programmatic usage\\n\\nThe library can be installed with `npm`:\\n\\n```\\nnpm install --save cf-graphql\\n```\\n\\nLet\\'s assume we\\'ve required this module with `const cfGraphql = require(\\'cf-graphql\\')`. To create a schema out of your space you need to call `cfGraphgl.createSchema(spaceGraph)`.\\n\\nWhat is `spaceGraph`? It is a graph-like data structure containing descriptions of content types of your space which additionally provide some extra pieces of information allowing the library to create a GraphQL schema.\\n\\nTo prepare this data structure you need to fetch raw content types data from the [CMA](https://www.contentful.com/developers/docs/references/content-management-api/). Let\\'s create a Contentful client first:\\n\\n```js\\nconst client = cfGraphql.createClient({\\n  spaceId: \\'some-space-id\\',\\n  cdaToken: \\'its-cda-token\\',\\n  cmaToken: \\'your-cma-token\\'\\n});\\n```\\n\\n`spaceId`, `cdaToken` and `cmaToken` options are required. You can also pass the following options:\\n\\n- `locale` - a locale code to use when fetching content. If not provided, the default locale of a space is used\\n- `preview` - if `true`, CPA will be used instead of CDA for fetching content\\n- `cpaToken` - if `preview` is `true` then this option has to hold a CPA token\\n\\nFetch content types with your `client` and then pass them to `cfGraphql.prepareSpaceGraph(rawCts)`:\\n\\n```js\\nclient.getContentTypes()\\n.then(cfGraphql.prepareSpaceGraph)\\n.then(spaceGraph => {\\n  // `spaceGraph` can be passed to `cfGraphql.createSchema`!\\n});\\n```\\n\\nThe last step is to use the schema with a server. A popular choice is [express-graphql](https://github.com/graphql/express-graphql). The only caveat is how the context is constructed. The library expects the `entryLoader` key of the context to be set to an instance created with `client.createEntryLoader()`:\\n\\n```js\\n// Skipped in snippet: `require` calls, Express app setup, `client` creation.\\n// `spaceGraph` was fetched and prepared in the previous snippet. In most cases\\n// you shouldn\\'t be doing it per request, once is fine.\\nconst schema = cfGraphql.createSchema(spaceGraph);\\n\\n// IMPORTANT: we\\'re passing a function to `graphqlHTTP`: this function will be\\n// called every time a GraphQL query arrives to create a fresh entry loader.\\n// You can also use `expressGraphqlExtension` described below.\\napp.use(\\'/graphql\\', graphqlHTTP(function () {\\n  return {\\n    schema,\\n    context: {entryLoader: client.createEntryLoader()}\\n  };\\n}));\\n```\\n\\n[You can see a fully-fledged example in the `demo/` directory](./demo/server.js).\\n\\n\\n## Querying\\n\\nFor each Contentful content type three root-level fields are created:\\n\\n- a singular field accepts a required `id` argument and resolves to a single entity\\n- a collection field accepts an optional `q`, `skip` and `limit` arguments and resolves to a list of entities\\n- a collection metadata field accepts an optional `q` argument and resolves to a metadata object (currently comprising only `count`)\\n\\n\\nPlease note that:\\n\\n- the `q` argument is a query string you could use with the [CDA](https://www.contentful.com/developers/docs/references/content-delivery-api/)\\n- both `skip` and `limit` arguments can be used to fetch desired page of results\\n  * `skip` defaults to `0`\\n  * `limit` defaults to `50` and cannot be greater than `1000`\\n* some query string parameters cannot be used:\\n  * `skip`, `limit` - use collection field arguments instead\\n  * `include`, `content_type` - no need for them, the library will determine and use appropriate values internally\\n  * `locale` - all the content is fetched for a single locale. By default the default locale is used; alternate locale can be selected with the `locale` configuration option of `cfGraphql.createClient`\\n\\nAssuming you\\'ve got two content types named `post` and `author` with listed fields, this query is valid:\\n\\n```graphql\\n{\\n  authors {\\n    name\\n  }\\n\\n  authors(skip: 10, limit: 10) {\\n    title\\n    rating\\n  }\\n\\n  _authorsMeta {\\n    count\\n  }\\n\\n  posts(q: \"fields.rating[gt]=5\") {\\n    title\\n    rating\\n  }\\n\\n  _postsMeta(q: \"fields.rating[gt]=5\") {\\n    count\\n  }\\n\\n  post(id: \"some-post-id\") {\\n    title\\n    author\\n    comments\\n  }\\n}\\n```\\n\\nReference fields will be resolved to:\\n\\n- a specific type, if there is a validation that allows only entries of some specific content type to be linked\\n- the `EntryType`, if there is no such constraint. The `EntryType` is an interface implemented by all the specific types\\n\\nExample where the `author` field links only entries of one content type and the `related` field links entries of multiple content types:\\n\\n```graphql\\n{\\n  posts {\\n    author {\\n      name\\n      website\\n    }\\n\\n    related {\\n      ... on Tag {\\n        tagName\\n      }\\n      ... on Place {\\n        location\\n        name\\n      }\\n    }\\n  }\\n}\\n```\\n\\nBackreferences (_backrefs_) are automatically created for links. Assume our `post` content type links to the `author` content type via a field named `author`. Getting an author of a post is easy, getting a list of posts by an author is not. `_backrefs` mitigate this problem:\\n\\n```graphql\\n{\\n  authors {\\n    _backrefs {\\n      posts__via__author {\\n        title\\n      }\\n    }\\n  }\\n}\\n```\\n\\nWhen using backreferences, there is a couple of things to keep in mind:\\n\\n- backrefs may be slow; always test with a dataset which is comparable with what you\\'ve got in production\\n- backrefs are generated only when a reference field specifies a single allowed link content type\\n- `_backrefs` is prefixed with a single underscore\\n- `__via__` is surrounded with two underscores; you can read this query out loud like this: _\"get posts that link to author via the author field\"_\\n\\n\\n## Helpers\\n\\n`cf-graphql` comes with helpers that help you with the `cf-graphql` integration. These are used inside of [the demo application](https://github.com/contentful-labs/cf-graphql/tree/master/demo).\\n\\n\\n### `expressGraphqlExtension`\\n\\n`expressGraphqlExtension` is a simple utility producing a function that can be passed directly to the [`express-graphql` middleware](https://github.com/graphql/express-graphql).\\n\\n```javascript\\n// Skipped in this snippet: client and space graph creation\\nconst schema = cfGraphql.createSchema(spaceGraph);\\n\\nconst opts = {\\n  // display the current cf-graphql version in responses\\n  version: true,\\n  // include list of the underlying Contentful CDA calls with their timing\\n  timeline: true,\\n  // display detailed error information\\n  detailedErrors: true\\n};\\n\\nconst ext = cfGraphql.helpers.expressGraphqlExtension(client, schema, opts);\\napp.use(\\'/graphql\\', graphqlHTTP(ext));\\n```\\n\\n**Important**: Most likely don\\'t want to enable `timeline` and `detailedErrors` in your production environment.\\n\\n\\n### `graphiql`\\n\\nIf you want to run your own GraphiQL and don\\'t want to rely on the one shipping with e.g. [express-graphql](https://github.com/graphql/express-graphql) then you could use the `graphiql` helper.\\n\\n```javascript\\nconst ui = cfGraphql.helpers.graphiql({title: \\'cf-graphql demo\\'});\\napp.get(\\'/\\', (_, res) => res.set(ui.headers).status(ui.statusCode).end(ui.body));\\n```\\n\\n\\n## Contributing\\n\\nIssue reports and PRs are more than welcomed.\\n\\n\\n## License\\n\\nMIT\\n'},\n",
       " {'repo': 'stojanovic/space-explorer-bot',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Space Explorer Bot\\nSpace Explorer is simple Messenger chat bot that uses NASA\\'s API to get the data and images about the space.  \\n\\nIt\\'s created for fun and also as a showcase for [Claudia Bot Builder](https://github.com/claudiajs/claudia-bot-builder), node.js library for creating chat bots for various platform and deploying them on AWS Lambda.\\n\\n## Watch the video\\n\\nShort video is available on [Vimeo](https://vimeo.com/172001135).\\n\\n## Try it live\\n\\nScan the code:\\n\\n[![Messenger code](assets/images/messenger_code.png)](https://m.me/space.explorer.bot)\\n\\nOr go to [https://m.me/space.explorer.bot](https://m.me/space.explorer.bot).\\n\\n## How to run and deploy it\\n\\n### Prerequests:\\n\\n- Create a new bot page in Facebook and a messenger app, as explained in the Facebook Messenger Getting Started Guide.\\n\\n### Steps:\\n\\n1. Clone the repository and run:\\n\\n   ```\\n   npm init\\n   ```\\n\\n2. Install `claudia` as a global utility, if you do not have it already:\\n\\n   ```\\n   npm install claudia -g\\n   ```\\n\\n3. Create a new bot in AWS. If you changed the name of bot.js file, change the `--api-module` argument below accordingly.\\n\\n   ```\\n   claudia create --region us-east-1 --api-module bot\\n   ```\\n\\n4. Configure Facebook Bot with a following command and follow the instructions:\\n\\n   ```\\n   claudia update --configure-fb-bot --configure-app\\n   ```\\n\\n   - `--configure-fb-bot` will prompt you for FB page access token and FB app secret (for message validation) and it\\'ll print out your webhook URL;  \\n   - `--configure-app` will prompt you for the Nasa API key (Check https://github.com/stojanovic/space-explorer-bot/blob/master/bot.js#L181);\\n\\nAnd that\\'s it :)\\n\\nFull instructions for Claudia Bot Builder are available in [Getting started with Claudia Bot Builder](https://github.com/claudiajs/claudia-bot-builder/blob/master/docs/GETTING_STARTED.md) guide.\\n\\n# License\\n\\nThe MIT License (MIT)\\n\\nCopyright (c) 2016 Slobodan Stojanović\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'},\n",
       " {'repo': 'stojanovic/space-explorer-bot',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Space Explorer Bot\\nSpace Explorer is simple Messenger chat bot that uses NASA\\'s API to get the data and images about the space.  \\n\\nIt\\'s created for fun and also as a showcase for [Claudia Bot Builder](https://github.com/claudiajs/claudia-bot-builder), node.js library for creating chat bots for various platform and deploying them on AWS Lambda.\\n\\n## Watch the video\\n\\nShort video is available on [Vimeo](https://vimeo.com/172001135).\\n\\n## Try it live\\n\\nScan the code:\\n\\n[![Messenger code](assets/images/messenger_code.png)](https://m.me/space.explorer.bot)\\n\\nOr go to [https://m.me/space.explorer.bot](https://m.me/space.explorer.bot).\\n\\n## How to run and deploy it\\n\\n### Prerequests:\\n\\n- Create a new bot page in Facebook and a messenger app, as explained in the Facebook Messenger Getting Started Guide.\\n\\n### Steps:\\n\\n1. Clone the repository and run:\\n\\n   ```\\n   npm init\\n   ```\\n\\n2. Install `claudia` as a global utility, if you do not have it already:\\n\\n   ```\\n   npm install claudia -g\\n   ```\\n\\n3. Create a new bot in AWS. If you changed the name of bot.js file, change the `--api-module` argument below accordingly.\\n\\n   ```\\n   claudia create --region us-east-1 --api-module bot\\n   ```\\n\\n4. Configure Facebook Bot with a following command and follow the instructions:\\n\\n   ```\\n   claudia update --configure-fb-bot --configure-app\\n   ```\\n\\n   - `--configure-fb-bot` will prompt you for FB page access token and FB app secret (for message validation) and it\\'ll print out your webhook URL;  \\n   - `--configure-app` will prompt you for the Nasa API key (Check https://github.com/stojanovic/space-explorer-bot/blob/master/bot.js#L181);\\n\\nAnd that\\'s it :)\\n\\nFull instructions for Claudia Bot Builder are available in [Getting started with Claudia Bot Builder](https://github.com/claudiajs/claudia-bot-builder/blob/master/docs/GETTING_STARTED.md) guide.\\n\\n# License\\n\\nThe MIT License (MIT)\\n\\nCopyright (c) 2016 Slobodan Stojanović\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'},\n",
       " {'repo': 'glotzerlab/signac-flow',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# <img src=\"https://raw.githubusercontent.com/glotzerlab/signac-flow/master/doc/images/palette-header.png\" width=\"75\" height=\"58\"> signac-flow - manage workflows with signac\\n\\n[![Affiliated with NumFOCUS](https://img.shields.io/badge/NumFOCUS-affiliated%20project-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://numfocus.org/sponsored-projects/affiliated-projects)\\n[![PyPI](https://img.shields.io/pypi/v/signac-flow.svg)](https://pypi.org/project/signac-flow/)\\n[![conda-forge](https://img.shields.io/conda/vn/conda-forge/signac-flow.svg?style=flat)](https://anaconda.org/conda-forge/signac-flow)\\n![CircleCI](https://img.shields.io/circleci/project/github/glotzerlab/signac-flow/master.svg)\\n[![RTD](https://img.shields.io/readthedocs/signac.svg?style=flat)](https://docs.signac.io)\\n[![License](https://img.shields.io/github/license/glotzerlab/signac-flow.svg)](https://github.com/glotzerlab/signac-flow/blob/master/LICENSE.txt)\\n[![PyPI-downloads](https://img.shields.io/pypi/dm/signac-flow.svg?style=flat)](https://pypistats.org/packages/signac-flow)\\n[![Slack](https://img.shields.io/badge/Slack-chat%20support-brightgreen.svg?style=flat&logo=slack)](https://signac.io/slack-invite/)\\n[![Twitter](https://img.shields.io/twitter/follow/signacdata?style=social)](https://twitter.com/signacdata)\\n[![GitHub Stars](https://img.shields.io/github/stars/glotzerlab/signac-flow?style=social)](https://github.com/glotzerlab/signac-flow/)\\n\\nThe [**signac** framework](https://signac.io) helps users manage and scale file-based workflows, facilitating data reuse, sharing, and reproducibility.\\n\\nThe **signac-flow** tool provides the basic components to set up simple to complex workflows for projects managed by the [**signac** framework](https://signac.io).\\nThat includes the definition of data pipelines, execution of data space operations and the submission of operations to high-performance super computers.\\n\\n## Resources\\n\\n- [Framework documentation](https://docs.signac.io/):\\n  Examples, tutorials, topic guides, and package Python APIs.\\n- [Package documentation](https://docs.signac.io/projects/flow/):\\n  API reference for the **signac-flow** package.\\n- [Slack Chat Support](https://signac.io/slack-invite/):\\n  Get help and ask questions on the **signac** Slack workspace.\\n- [**signac** website](https://signac.io/):\\n  Framework overview and news.\\n\\n\\n## Installation\\n\\nThe recommended installation method for **signac-flow** is through **conda** or **pip**.\\nThe software is tested for Python versions 3.6+ and is built for all major platforms.\\n\\nTo install **signac-flow** *via* the [conda-forge](https://conda-forge.github.io/) channel, execute:\\n\\n```bash\\nconda install -c conda-forge signac-flow\\n```\\n\\nTo install **signac-flow** *via* **pip**, execute:\\n\\n```bash\\npip install signac-flow\\n```\\n\\n**Detailed information about alternative installation methods can be found in the [documentation](https://docs.signac.io/en/latest/installation.html).**\\n\\n\\n## Testing\\n\\nYou can test this package by executing\\n\\n    $ python -m pytest tests/\\n\\nwithin the repository root directory.\\n\\n\\n## Acknowledgment\\n\\nWhen using **signac** as part of your work towards a publication, we would really appreciate that you acknowledge **signac** appropriately.\\nWe have prepared examples on how to do that [here](http://docs.signac.io/en/latest/acknowledge.html).\\n**Thank you very much!**\\n\\nThe signac framework is a [NumFOCUS Affiliated Project](https://numfocus.org/sponsored-projects/affiliated-projects).\\n'},\n",
       " {'repo': 'EthanRosenthal/spacecutter',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"# spacecutter\\n\\n`spacecutter` is a library for implementing ordinal regression models in PyTorch. The library consists of models and loss functions. It is recommended to use [skorch](http://skorch.readthedocs.io/) to wrap the models to make them compatible with scikit-learn.\\n\\n## Installation\\n\\n```bash\\npip install spacecutter\\n```\\n\\n## Usage\\n\\n### Models\\n\\nDefine any PyTorch model you want that generates a single, scalar prediction value. This will be our `predictor` model. This model can then be wrapped with `spacecutter.models.OrdinalLogisticModel` which will convert the output of the `predictor` from a single number to an array of ordinal class probabilities. The following example shows how to do this for a two layer neural network `predictor` for a problem with three ordinal classes.\\n\\n```python\\nimport numpy as np\\nimport torch\\nfrom torch import nn\\n\\nfrom spacecutter.models import OrdinalLogisticModel\\n\\n\\nX = np.array([[0.5, 0.1, -0.1],\\n              [1.0, 0.2, 0.6],\\n              [-2.0, 0.4, 0.8]],\\n             dtype=np.float32)\\n\\ny = np.array([0, 1, 2]).reshape(-1, 1)\\n\\nnum_features = X.shape[1]\\nnum_classes = len(np.unique(y))\\n\\npredictor = nn.Sequential(\\n    nn.Linear(num_features, num_features),\\n    nn.ReLU(),\\n    nn.Linear(num_features, 1)\\n)\\n\\nmodel = OrdinalLogisticModel(predictor, num_classes)\\n\\ny_pred = model(torch.as_tensor(X))\\n\\nprint(y_pred)\\n\\n# tensor([[0.2325, 0.2191, 0.5485],\\n#         [0.2324, 0.2191, 0.5485],\\n#         [0.2607, 0.2287, 0.5106]], grad_fn=<CatBackward>)\\n\\n```\\n\\n### Training\\n\\nIt is recommended to use [skorch](http://skorch.readthedocs.io/) to train `spacecutter` models. The following shows how to train the model from the previous section using cumulative link loss with `skorch`:\\n\\n```python\\nfrom skorch import NeuralNet\\n\\nfrom spacecutter.callbacks import AscensionCallback\\nfrom spacecutter.losses import CumulativeLinkLoss\\n\\nskorch_model = NeuralNet(\\n    module=OrdinalLogisticModel,\\n    module__predictor=predictor,\\n    module__num_classes=num_classes,\\n    criterion=CumulativeLinkLoss,\\n    train_split=None,\\n    callbacks=[\\n        ('ascension', AscensionCallback()),\\n    ],\\n)\\n\\nskorch_model.fit(X, y)\\n\\n```\\n\\nNote that we must add the `AscensionCallback`. This ensures that the ordinal cutpoints stay in ascending order. While ideally this constraint would be factored directly into the model optimization, `spacecutter` currently hacks an SGD-compatible solution by utilizing a post-backwards-pass callback to clip the cutpoint values.\\n\"},\n",
       " {'repo': 'theq629/fulgurate',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"ABOUT\\n-----\\n\\nSimple spaced repetition flashcard software. Flashcards are stored in a simple text format and the code is small enough to be easily modifiable. The spaced repetition algorithm is SM-2 (http://www.supermemo.com/english/ol/sm2.htm). See the python files or the man pages for usage of the programs. See the SM-2 website for more details on the algorithm and the scale of the user self-evaluation input.\\n\\nDATA\\n----\\n\\nFlashcards have two parts, corresponding to the top and bottom of a card. The input to fulgurate-import should contain tab-separated two columns for the top and bottom fields respectively. The flashcard files themselves (produced by fulgurate-import) are similar but include the card state.\\n\\nEXAMPLE\\n-------\\n\\nAn example of common usage is as follows:\\n  fulgurate-import example.tsv example.cards # Create a flashcard set\\n  fulgurate-run example.cards # Run the flashcards in the terminal\\n  fulgurate-show-schedule example.cards # Show the current scheduling for the cards\\n\\nThe included example.tsv file is sample card data consisting of a few Chinese-English dictionary entries from CC-CEDICT. The files example-filter.sh and example-finish.sh are for fulgurate-run's -f and -F options respectively, and are designed to work with the same data.\\n\"},\n",
       " {'repo': 'jasperjorna/ST-Spacefunk',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Spacefunk\\n\\nSpacefunk is a minimalistic theme for Sublime Text, based on [kkga\\'s](https://github.com/kkga/spacegray) Spacegray theme.\\n\\n## Install\\n\\n### Via Package Control\\n\\nSearch for `Theme - Spacefunk` in [Sublime Package Control](https://sublime.wbond.net).\\n\\n### Manual\\n\\n1. [Download the .zip](https://github.com/Twiebie/ST-Spacefunk/archive/master.zip)\\n2. Unzip and rename the folder to `Theme - Spacefunk`\\n3. Copy the folder to your `Packages` directory. Quickly find your directory via\\n`Preferences -> Browse Packages...`.\\n\\n## Setup\\n\\nYou can activate the theme and its colour scheme by editing your user preferences.\\nGo to: `Preferences -> Settings - User`.\\n\\n### Example settings\\n#### Blue Monday:\\n```\\n{\\n  \"theme\": \"Spacefunk (Blue Monday).sublime-theme\",\\n}\\n```\\n#### Grey Tuesday\\n```\\n{\\n  \"theme\": \"Spacefunk (Grey Tuesday).sublime-theme\",\\n}\\n```\\n\\n### Additional customization\\nIf you prefer a lighter version of the sidebar you can add the following line to your preferences file. This is available for both versions of Spacefunk.\\n\\n```\\n{\\n  \"spacefunk_sidebar_light\": true\\n}\\n```\\nRestart Sublime Text to make sure all changes are applied. Enjoy!\\n\\n#### Future Funk - Colour schemes\\nI\\'ve created 2 colour schemes that fit well with the theme.\\nThey are maintained separately from Spacefunk.\\n\\nFind them on: [Package Control](https://sublime.wbond.net/packages/Future%20Funk%20-%20Color%20Scheme) or [Github](https://github.com/Twiebie/ST-FutureFunk).\\n\\n### Screenshots\\n#### Blue Monday\\n![image](http://i.imgur.com/tkbsNlI.png)\\n\\n#### Grey Tuesday\\n![image](http://i.imgur.com/jCAd3Af.png)\\n'},\n",
       " {'repo': 'Mondego/spacetime',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Spacetime Framework\\n===========\\nA framework for developing time-stepped, multi-worker applications based on the tuplespace model. Workers compute within spacetimed frames -- a fixed portion of the shared data during a fixed period of time. The locally modified data may be pushed back to the shared store at the end of each step. Pulling and pushing data from/to the store is done declaratively using two small DSLs: (1) a DSL for\\npredicate collection classes ([PCC](https://github.com/Mondego/pcc)) used for specifying algebraic operations on data sets,\\nand (2) a DSL for controlling data flow in terms of direction (to/from store) and, eventually, permissions.\\n\\nThe first implementation of spacetime is in Python. Follow the link to it for specific instructions on how to\\nuse the Python implementation.\\n'},\n",
       " {'repo': 'satellogic/orbit-predictor',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': 'Orbit Predictor\\n===============\\n\\n.. image:: https://github.com/satellogic/orbit-predictor/workflows/Python%20package/badge.svg\\n    :target: https://github.com/satellogic/orbit-predictor/actions\\n.. image:: https://coveralls.io/repos/github/satellogic/orbit-predictor/badge.svg?branch=master\\n    :target: https://coveralls.io/github/satellogic/orbit-predictor?branch=master\\n\\n\\nOrbit Predictor is a Python library to propagate orbits of Earth-orbiting objects (satellites, ISS, \\nSanta Claus, etc) using `TLE (Two-Line Elements set) <https://en.wikipedia.org/wiki/Two-line_element_set>`_\\n\\nAll the hard work is done by Brandon Rhodes implementation of \\n`SGP4 <https://github.com/brandon-rhodes/python-sgp4>`_. \\n\\nWe can say *Orbit predictor* is kind of a \"wrapper\" for the python implementation of SGP4\\n\\nTo install it\\n-------------\\n\\nYou can install orbit-predictor from pypi::\\n\\n    pip install orbit-predictor\\n\\nUse example\\n-----------\\n\\nWhen will be the ISS over Argentina?\\n\\n:: \\n\\n    In [1]: from orbit_predictor.sources import EtcTLESource\\n\\n    In [2]: from orbit_predictor.locations import ARG\\n\\n    In [3]: source = EtcTLESource(filename=\"examples/iss.tle\")\\n\\n    In [4]: predictor = source.get_predictor(\"ISS\")\\n\\n    In [5]: predictor.get_next_pass(ARG)\\n    Out[5]: <PredictedPass ISS over ARG on 2017-11-10 22:48:10.607212>\\n\\n    In [6]: predicted_pass = _\\n\\n    In [7]: position = predictor.get_position(predicted_pass.aos)\\n\\n    In [8]: ARG.is_visible(position)  # Can I see the ISS from this location?\\n    Out[8]: True\\n\\n    In [9]: import datetime\\n\\n    In [10]: position_delta = predictor.get_position(predicted_pass.los + datetime.timedelta(minutes=20))\\n\\n    In [11]: ARG.is_visible(position_delta)\\n    Out[11]: False\\n\\n    In [12]: tomorrow = datetime.datetime.utcnow() + datetime.timedelta(days=1)\\n\\n    In [13]: predictor.get_next_pass(ARG, tomorrow, max_elevation_gt=20)\\n    Out[13]: <PredictedPass ISS over ARG on 2017-11-11 23:31:36.878827>\\n\\n\\nSimplified creation of predictor from TLE lines:\\n\\n::\\n\\n    In [1]: import datetime\\n\\n    In [2]: from orbit_predictor.sources import get_predictor_from_tle_lines\\n\\n    In [3]: TLE_LINES = (\\n                \"1 43204U 18015K   18339.11168986  .00000941  00000-0  42148-4 0  9999\",\\n                \"2 43204  97.3719 104.7825 0016180 271.1347 174.4597 15.23621941 46156\")\\n\\n    In [4]: predictor = get_predictor_from_tle_lines(TLE_LINES)\\n\\n    In [5]: predictor.get_position(datetime.datetime(2019, 1, 1))\\n    Out[5]: Position(when_utc=datetime.datetime(2019, 1, 1, 0, 0),\\n        position_ecef=(-5280.795613274576, -3977.487633239489, -2061.43227648734),\\n        velocity_ecef=(-2.4601788971676903, -0.47182217472755117, 7.167517631852518),\\n        error_estimate=None)\\n\\nCurrently you have available these sources\\n------------------------------------------\\n\\n- Memorytlesource: in memory storage.\\n- EtcTLESource: a uniq TLE is stored in `/etc/latest_tle`\\n- WSTLESource: It reads a REST API currently used inside Satellogic. We are are working to make it publicly available.\\n\\nHow to contribute\\n-----------------\\n\\n- Write pep8 complaint code. \\n- Wrap the code on 100 collumns.\\n- Always use a branch for each feature and Merge Proposals.\\n- Always run the tests before to push. (test implies pep8 validation)\\n'},\n",
       " {'repo': 'diwi/Space_Partitioning_Octree_BVH',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '--------------------------------------------------------------------------------\\n--------------------------------------------------------------------------------\\nAUTHOR: Thomas Diewald\\n\\nDate:        07.02.2013\\nlast edited: 07.02.2013\\n\\nauthor:     www.thomasdiewald.com\\nblog-post:  www.thomasdiewald.com/blog/?p=1488\\nsource:     www.github.com/diwi/Space_Partitioning_Octree_BVH\\n--------------------------------------------------------------------------------\\n--------------------------------------------------------------------------------\\n\\n\\n\\n################################################################################\\n--------------------------------------------------------------------------------\\nSpace Partitioning: Octree vs. BVH\\n--------------------------------------------------------------------------------\\n################################################################################\\n\\nsource code of my first experiments with space-partitioning techniques: \\n\\n - Octree\\n - BVH\\n\\n \\nit was a study to compare performance and usage for raytracing/pathtracing.\\nmost important for me was the traversing-performance for ray-triangle-intersection,\\nwhich i needed for the opencl-implementation.\\nin the end i decided to use the BVH for that, and so the octree functionality \\nmight be not that \"perfect\".\\n\\na lot of things should/could be improved during building the data-structure.\\nat the moment it is way to slow for dynamic changes in the scene.\\nif you notice any errors, feel free to contact me.\\n\\n\\nfor more information, i\\'d like to refer to my blog-post:\\nhttp://thomasdiewald.com/blog/?p=1488\\n\\nthomas\\n\\n'},\n",
       " {'repo': 'rojo-rbx/rojo.space',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# rojo.space\\nThis is the Rojo website.\\n\\nThis website is built using [Docusaurus 2](https://docusaurus.io/).\\n\\n### Installation\\n\\n```\\n$ npm install\\n```\\n\\n### Local Development\\n\\n```\\n$ npm start\\n```\\n\\nThis command starts a local development server and opens up a browser window. Most changes are reflected live without having to restart the server.\\n\\n### Build\\n\\n```\\n$ npm run build\\n```\\n\\nThis command generates static content into the `build` directory and can be served using any static contents hosting service.'},\n",
       " {'repo': 'maloyan/Space',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': 'Satellite Image Difference\\n==============================\\n## Description\\n\\nWith a given two images of the same place from the different dates we need to figure out binary mask with changes\\n\\n[Selim\\'s pretrained models](https://github.com/selimsef/xview2_solution/releases/tag/0.0.1)\\n\\n## Train\\n1. Put data as shown below in tree structure of folders\\n2. Put pretrained models from [Selim](https://github.com/selimsef/xview2_solution/releases/tag/0.0.1) in a /models/pretrained folder. We need those for siamese net.\\n3. Change paths to the proper one in the config/ files\\n4. Run the training proccess with train.py. The first argument should be path to the config file \\n```\\npython train.py config/config_unet++_resnext50.json\\npython train.py config/config_siamese_seresnext50.json\\n```\\n5. After that you\\'ll have:\\n   -  saved models in /models/saved folder, logs of training\\n   -  logs of training proccess in /logs\\n   -  predicted non binary masks in /predicted_masks\\n\\n6. In the /notebooks/final_submission.ipynb generated a final submission file via averaging outputs from those two models\\n\\n## Solution description\\n1. I splited initial large image into small ones applying after that augmentation\\n2. Trained Unet++ with resnext50 backbone using [Segmentation models](https://github.com/qubvel/segmentation_models.pytorch) on 1 channel image difference\\n![image](https://github.com/selimsef/xview2_solution/raw/master/assets/dpn_unet.png)\\n\\n3. Trained Siamese net with seresnext50 backbone using models architecture from [xview2_solution](https://github.com/selimsef/xview2_solution) on RGB channel images \\n\\n![image](https://github.com/selimsef/xview2_solution/raw/master/assets/siamese_dpn.png)\\n\\n## Result\\n![image](https://sun9-11.userapi.com/impg/LVvt5FAwaDmlQoZTcl8s_HlHtpraQ9xXWhA5Hw/F0gUAVLeh5Y.jpg?size=613x850&quality=96&sign=d83b79097f9eee8a809523bb3769c75f&type=album)\\n\\nProject Organization\\n------------\\n\\n    ├── LICENSE\\n    ├── Makefile           <- Makefile with commands like `make data` or `make train`\\n    ├── README.md\\n    ├── data\\n    │\\xa0\\xa0 ├── Images                 <- Original 4 channel images from first and second dates.\\n    │\\xa0\\xa0 ├── Images_composit        <- Composed 8 channel images from original images.\\n    │\\xa0\\xa0 ├── mask                   <- Binary masks of images differences.\\n    │\\xa0\\xa0 ├── Rucode.xls             <- Table to match images from the same location.\\n    │\\xa0\\xa0 └── sample_submission.csv  <- Sample submission file.\\n    │\\n    ├── models             <- Trained and serialized models\\n    │\\xa0\\xa0 ├── pretrained     <- Pretrained models for siamese models from Selim.\\n    │\\xa0\\xa0 └── saved          <- Trained on the competition data models.\\n    |\\n    ├── notebooks          <- Jupyter notebooks.\\n    │\\n    ├── logs               <- Logs of training process: loss, IoU\\n    |\\n    ├── config             <- Configuration files for each type of model\\n    |\\n    ├── predicted_masks    <- Predicted probability masks in pickle format \\n    │\\n    ├── requirements.txt   <- The requirements file for reproducing the analysis environment,\\n    │                         `pip install -r requirements.txt`\\n    │\\n    ├── setup.py           <- makes project pip installable (pip install -e .) so src can be imported\\n    ├── train.py           <- Training process with evaluation on the end.\\n    ├── eval.py            <- Mask evaluation using the trained model.\\n    ├── src                <- Source code for use in this project.\\n    │\\xa0\\xa0 ├── __init__.py    <- Makes src a Python module\\n    │   │\\n    │\\xa0\\xa0 ├── models.py      <- Siamese models from Selim\\n    │\\xa0\\xa0 ├── dataset.py     <- Dataset class for satellite images\\n    │\\xa0\\xa0 └── utils.py       <- Small preprocess functions like normalization and decoding\\n    │\\n    └── tox.ini            <- tox file with settings for running tox; see tox.readthedocs.io\\n\\n\\n--------\\n\\n<p><small>Project based on the <a target=\"_blank\" href=\"https://drivendata.github.io/cookiecutter-data-science/\">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>\\n'},\n",
       " {'repo': 'guardianproject/haven',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '[![Build Status](https://travis-ci.org/guardianproject/haven.svg)](https://travis-ci.org/guardianproject/haven)\\n\\n# About Haven\\n\\nHaven is for people who need a way to protect their personal areas and possessions without compromising their privacy. It is an Android application that leverages on-device sensors to provide monitoring and protection of physical areas. Haven turns any Android phone into a motion, sound, vibration and light detector, watching for unexpected guests and unwanted intruders. We designed Haven for investigative journalists, human rights defenders and people at risk of forced disappearance to create a new kind of herd immunity. By combining the array of sensors found in any smartphone, with the world\\'s most secure communications technologies, like Signal and Tor, Haven prevents the worst kind of people from silencing citizens without getting caught in the act.\\n\\n<img src=\"https://raw.githubusercontent.com/guardianproject/haven/master/fastlane/android/metadata/en-US/images/phoneScreenshots/havenob1.png\" width=\"25%\"> \\n<img src=\"https://raw.githubusercontent.com/guardianproject/haven/master/fastlane/android/metadata/en-US/images/phoneScreenshots/havenob2.png\" width=\"25%\"> \\n<img src=\"https://raw.githubusercontent.com/guardianproject/haven/master/fastlane/android/metadata/en-US/images/phoneScreenshots/havenob3.png\" width=\"25%\"> \\n\\nView our full [Haven App Overview](https://guardianproject.github.io/haven/docs/preso/) presentation for more about the origins and goals of the project.\\n\\n## Announcement and Public Beta\\n\\nWe are announcing Haven today, as an open-source project, along with a public beta release of the app. We are looking for contributors who understand that physical security is as important as digital, and who have an understanding and compassion for the kind of threats faced by the users and communities we want to support. We also think it is cool, cutting-edge and making use of encrypted messaging and onion routing in whole new ways. We believe Haven points the way to a more sophisticated approach to securing communication within networks of things and home automation system.\\n\\nLearn more about the story of this project at the links below:\\n\\n* [Haven: Building the Most Secure Baby Monitor Ever?](https://guardianproject.info/2017/12/22/haven-building-the-most-secure-baby-monitor-ever/)\\n* [Snowden’s New App Uses Your Smartphone To Physically Guard Your Laptop](https://theintercept.com/2017/12/22/snowdens-new-app-uses-your-smartphone-to-physically-guard-your-laptop/)\\n* [Snowden\\'s New App Turns Your Phone Into a Home Security System](https://www.wired.com/story/snowden-haven-app-turns-phone-into-home-security-system/)\\n\\n## Project Team\\n\\nHaven was developed through a collaboration between [Freedom of the Press Foundation](https://freedom.press) and [Guardian Project](https://guardianproject.info). Prototype funding was generously provided by FPF, and donations to support continuing work can be contributed through their site: https://freedom.press/donate-support-haven-open-source-project/\\n\\n![Freedom of the Press Foundation](https://raw.githubusercontent.com/guardianproject/haven/master/art/logos/fopflogo.png)\\n![Guardian Project](https://raw.githubusercontent.com/guardianproject/haven/master/art/logos/gplogo.png)\\n\\n## Safety through Sensors\\nHaven only records when triggered by sound and motion and stores everything locally on the device. You can position the device\\'s camera to capture visible motion or place your phone somewhere discreet to listen for noises. Receive secure notifications of intrusion events instantly or access logs remotely later.\\n\\n<img src=\"https://raw.githubusercontent.com/guardianproject/haven/master/fastlane/android/metadata/en-US/images/phoneScreenshots/haven-sound-config.png\" width=\"25%\"> \\n<img src=\"https://raw.githubusercontent.com/guardianproject/haven/master/fastlane/android/metadata/en-US/images/phoneScreenshots/haven-event-media.png\" width=\"25%\"> \\n<img src=\"https://raw.githubusercontent.com/guardianproject/haven/master/fastlane/android/metadata/en-US/images/phoneScreenshots/haven-event-list.png\" width=\"25%\"> \\n\\nThe following sensors are monitored for a measurable change, and then recorded to an event log on the device:\\n\\n-   **Accelerometer**: phone\\'s motion and vibration\\n-   **Camera**: motion in the phone\\'s visible surroundings from front or back camera\\n-   **Microphone**: noises in the environment\\n-   **Light**: change in light from ambient light sensor\\n-   **Power**: detect device being unplugged or power loss  \\n\\n## Building\\n\\nThe application can be built using Android Studio and Gradle. It relies on a number of third-party dependencies, all of which are free, open-source, and listed at the end of this document.\\n\\n## Install\\n\\nYou can currently get the Haven BETA release in one of three ways:\\n\\n* Download [Haven from Google Play](https://play.google.com/store/apps/details?id=org.havenapp.main)\\n* First, [install F-Droid](https://f-droid.org) the open-source app store, and second, add our Haven Nightly \"Bleeding Edge\" repository by scanning the QR Code below:\\n\\n<img src=\"https://guardianproject.github.io/haven-nightly/icon.png\" width=\"50%\"/> \\n\\nor add this repository manually in F-Droid\\'s Settings->Repositories: [https://guardianproject.github.io/haven-nightly/fdroid/repo/](https://guardianproject.github.io/haven-nightly/fdroid/repo/)\\n\\n* Grab the APK files from the [GitHub releases page](https://github.com/guardianproject/haven/releases)\\n\\nYou can, of course, build the app yourself, from source.\\n\\nIf you are an Android developer, you can learn more about how you can make use of F-Droid in your development workflow, for nightly builds, testing, reproducibility and more here: [F-Droid Documentation](https://f-droid.org/en/docs/)\\n\\n## Why no iPhone Support?\\n\\nWhile we hope to support a version of Haven that runs directly on iOS devices in the future, iPhone users can still benefit from Haven today. You can purchase an inexpensive Android phone for less than $100 and use it as your \"Haven Device\"; leaving it behind whilst you keep your iPhone on you. If you run Signal on your iPhone you can configure Haven on Android to send encrypted notifications, with photos and audio, directly to you. If you enable the \"Tor Onion Service\" feature in Haven (requires installation of \"Orbot\" app as well) you can remotely access all Haven log data from your iPhone using the Onion Browser app.\\n\\nSo, no, iPhone users we didn\\'t forget about you and we hope you will pick up an inexpensive Android burner today!\\n\\n## Usage\\n\\nHaven is meant to provide a smooth onboarding experience that walks users through configuring the sensors on their device to best detect intrusions into their environment. The current implementation has some of this implemented, but we are looking to improve this user experience dramatically.\\n\\n### Main view\\n\\nThe application\\'s main view allows the user to select which sensors to use along with their corresponding levels of sensitivity. A security code is required to disable monitoring, which must be provided by the user. A phone number can be set, to which a message will be sent if any of the sensors are triggered.\\n\\n### Notifications\\n\\nWhen one of the sensors is triggered (reaches the configured sensitivity threshold), notifications are sent through the following channels (if enabled):\\n\\n- SMS: a message is sent to the number specified when monitoring started\\n- Signal: if configured, can send end-to-end encryption notifications via Signal\\n\\nNote that it is not necessary to install the Signal app on the device that runs Haven. Doing so may invalidate the app\\'s previous Signal registration and safety numbers. Haven uses normal APIs to communicate via Signal.\\n\\nNotifications are sent through a service running in the background that is defined in class `MonitorService`.\\n\\n### Remote Access\\n\\nAll event logs and captured media can be remotely accessed through a [Tor Onion Service](https://www.torproject.org/docs/onion-services). Haven must be configured as an Onion Service and requires the device to also have [Orbot: Tor for Android](https://guardianproject.info/apps/orbot) installed and running. \\n\\n## ATTRIBUTIONS\\n\\nThis project contains source code or library dependencies from the following projects:\\n\\n* SecureIt project available at: https://github.com/mziccard/secureit Copyright (c) 2014 Marco Ziccardi (Modified BSD)\\n* libsignal-service-java from Open Whisper Systems: https://github.com/WhisperSystems/libsignal-service-java (GPLv3)\\n* Guardian Project fork of signal-cli from AsamK: https://github.com/AsamK/signal-cli , https://github.com/guardianproject/signal-cli-android (GPLv3)\\n* JayDeep\\'s AudioWife: https://github.com/jaydeepw/audio-wife (MIT)\\n* AppIntro: https://github.com/apl-devs/AppIntro (Apache 2)\\n* Guardian Project\\'s NetCipher: https://guardianproject.info/code/netcipher/ (Apache 2)\\n* NanoHttpd: https://github.com/NanoHttpd/nanohttpd (BSD)\\n* MaterialDateTimePicker from wdullaer: https://github.com/wdullaer/MaterialDateTimePicker (Apache 2)\\n* Fresco Image Viewer: https://github.com/stfalcon-studio/FrescoImageViewer (Apache 2)\\n* Facebook Fresco Image Library: https://github.com/facebook/fresco (BSD)\\n* Audio Waveform Viewer: https://github.com/derlio/audio-waveform (Apache 2)\\n* FireZenk\\'s AudioWaves: https://github.com/FireZenk/AudioWaves (MIT)\\n* MaxYou\\'s SimpleWaveform: https://github.com/maxyou/SimpleWaveform (MIT)\\n* Siralam\\'s fork of CameraViewPlus: https://github.com/siralam/CameraViewPlus (Apache License 2.0)\\n* Halil Ozercan\\'s BetterVideoPlayer: https://github.com/halilozercan/BetterVideoPlayer\\n* Reneto Silva\\'s easyrs: https://github.com/silvaren/easyrs (MIT)\\n* Google\\'s libphonenumber: https://github.com/googlei18n/libphonenumber (Apache License 2.0)\\n* Mike Penz\\'s AboutLibraries: https://github.com/mikepenz/AboutLibraries (Apache License 2.0)\\n'},\n",
       " {'repo': 'cosmonium/cosmonium',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"![Cosmonium](textures/cosmonium-name.png)\\n\\n[![Build Status](https://github.com/cosmonium/cosmonium/workflows/build/badge.svg)](https://github.com/cosmonium/cosmonium/actions)\\n[![Latest release](https://img.shields.io/github/v/release/cosmonium/cosmonium?label=Lastest%20release)](https://github.com/cosmonium/cosmonium/wiki/Download)\\n[![Latest build](https://img.shields.io/github/v/release/cosmonium/cosmonium?include_prereleases&label=Lastest%20build)](https://github.com/cosmonium/cosmonium/wiki/Download)\\n[![GitHub](https://img.shields.io/github/license/cosmonium/cosmonium)](https://github.com/cosmonium/cosmonium/blob/master/COPYING.md)\\n\\nCosmonium is a 3D astronomy and space exploration program. With Cosmonium you can navigate in our solar system and discover all the planets and their moons. You can also visit the neighboring stars and discover the true size of our galaxy and the Universe.\\n\\nCosmonium supports (or will support) the creation of fictional planets, stellar systems nebulaes, ... using procedural generation.\\n\\nCosmonium also already supports some Celestia addons (though CMOD and CelX are not yet supported).\\n\\n### Requirements\\n\\nCosmonium runs on Windows (Vista or above), Linux (CentOS 5, Ubuntu 14 or above) or macOS (mac0S 10.9 or above)\\nwith a graphic card supporting OpenGL 2.1 or better (OpenGL 4.5 is recommended) and at least 512MB of disk\\n(up to 4GB if the HD and UHD textures are installed).\\n\\n### Installation \\n\\nDownload the installer or package for your platform from the [download](https://github.com/cosmonium/cosmonium/wiki/Download) page and see the [[Installation]] page\\nThe package contains only low resolution textures, see [here](https://github.com/cosmonium/cosmonium/wiki/Download#extra-textures) to install extra HD and UHD textures.\\n\\n### Screenshots\\n\\nSee in the [Wiki](https://github.com/cosmonium/cosmonium/wiki/Screenshots) some screenshots of the application with views of\\n[Saturn](https://github.com/cosmonium/cosmonium/wiki/Screenshots#rings-of-saturn),\\n[Jupiter](https://github.com/cosmonium/cosmonium/wiki/Screenshots#io-casting-a-shadow-on-jupiter),\\n[Mars](https://github.com/cosmonium/cosmonium/wiki/Screenshots#phobos-over-mars),\\nthe [Moon](https://github.com/cosmonium/cosmonium/wiki/Screenshots#moon-crescent),\\n[procedural planets](https://github.com/cosmonium/cosmonium/wiki/Screenshots#procedural-planet), ...\\n\\n![Jupiter](https://github.com/cosmonium/cosmonium/wiki/screenshots/Io+Jupiter.png)\\n\\n### Launch\\n\\nSimply starts cosmonium from your application menu or from the cosmonium folder. See also the [installation](https://github.com/cosmonium/cosmonium/wiki/Installation) page for more options.\\n\\n### User interface\\n\\nCosmonium user interface is still heavily based on Celestia, most of the command and keyboard shortcuts work the same.\\nGo to [First steps](https://github.com/cosmonium/cosmonium/wiki/First-steps) to have an explanation of the basic command or see the [Control](https://github.com/cosmonium/cosmonium/wiki/Control) page for an exhaustive list.\\n\\n### Full documentation\\n\\nCosmonium is still in its infancy, but it is already usable to explore all the planets and the moons of our solar system, all the neighbor or visible stars and much more.\\nIt also support custom content and addons, either as Cosmonium or Celestia addons.\\n\\nThe full documentation is available in the [Wiki](https://github.com/cosmonium/cosmonium/wiki)\\n\\n### Bugs\\n\\nIf you encounter any problem to install or run Cosmonium, please don't hesitate to fill a bug report in the [issue tracker](https://github.com/cosmonium/cosmonium/issues) here on Github.\\n\\n## License \\n\\nCosmonium is (C) 2018-2022 Laurent Deru.\\n\\nThis program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 3 of the License, or (at your option) any later version.\\n\\nThis program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details, which you should have received along with this program. If not, request a copy from: Free Software Foundation, Inc. 59 Temple Place - Suite 330 Boston, MA 02111-1307 USA.\\n\\nCosmonium uses several third-party libraries which are subject to their own licenses,  see [Third-Party.md](Third-Party.md) for the complete list.\\n\\nCosmonium data (textures, models, orbital elements,..) come from many sources. Their respective copyright holder, license and reference are available in the info panel of the displayed object and in the related yaml file.\\n\\n## Powered by\\n\\n[![Python](https://github.com/cosmonium/cosmonium/wiki/images/python-powered-w-200x80.png)](http://www.python.org)\\n\\n[![Panda3D](https://github.com/cosmonium/cosmonium/wiki/images/panda3d_logo.png)](http://www.panda3d.org)\\n\"},\n",
       " {'repo': 'IBM/spacetech-kubesat',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# KubeSat\\n\\nKubeSat is an open-source project for building a cognitive, autonomous framework for satellite constellations and swarms. It provides the framework needed to develop and operate tasks to be performed on Satellite. Also, it allows for the simulation and optimization of multi-satellite communications.\\n\\n## How It Works \\n\\nKubeSat provides a framework to manage services for devices.\\n\\n### Architecture\\n\\n<div  align=\"center\">\\n<img src=\"./assets/kubesat-diagram.png\" width = \"85%\" align=\"center\">\\n</div>\\n\\n### KubeSat environment\\n\\n- __NATs server__ : NATs are messaging services that support a variety of network topologies. For KubeSat to communicate through NATs, a NATs cluster must be configured in advance. Refer to [NATs Cluster Configuration](https://docs.nats.io/nats-server/configuration/clustering) for NATs cluster configuration.\\n- __Kubernetes API__ : Kubernetes is a run-time environment to execute services. KubeSat uses Kubernetes API to manage Services in Kubernetes.\\n\\n### KubeSat library\\n\\n- __Resource Manager__: Resource Manager monitors jobs and system resources in the Kubernetes and performs a new task as Kubernetes Jobs upon request.\\n - __Service__: A service runs as a Kubernetes Job. It can be a general container for Kubernetes or a container developed using the KubeSat library.\\n - __Service Information__: Stores available service information that can be performed.\\n\\n### KubeSat Simulation\\n\\nKubeSat Simulation is developed using KubeSat library to simulate multi-satellite communications. It simulates accurate orbital mechanics for each object via OreKit; uses these calculations to place restrictions on communications between satellites, groundstation, and ground sensors; incorporates NATS.io messaging services; and publishes these communications for visualization on a web dashboard built using Cesium and Carbon. Refer to [KubeSat Simulation](/simulation/) to see the detail.\\n\\n## Getting Started\\n\\nTo develop a new service, start with [getting-started](/docs/getting-started.md).\\n\\nTo deploy the simulation, try [simulation quick start](/docs/simulation-quick-start.md).\\n\\nThere are examples in [examples](/examples).\\n\\n## License\\n\\nKubeSat is licensed under the Apache 2.0 license. Full license text is available at [LICENSE](LICENSE).\\n'},\n",
       " {'repo': 'sindresorhus/camelcase',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': \"# camelcase\\n\\n> Convert a dash/dot/underscore/space separated string to camelCase or PascalCase: `foo-bar` → `fooBar`\\n\\nCorrectly handles Unicode strings.\\n\\nIf you use this on untrusted user input, don't forget to limit the length to something reasonable.\\n\\n## Install\\n\\n```sh\\nnpm install camelcase\\n```\\n\\n## Usage\\n\\n```js\\nimport camelCase from 'camelcase';\\n\\ncamelCase('foo-bar');\\n//=> 'fooBar'\\n\\ncamelCase('foo_bar');\\n//=> 'fooBar'\\n\\ncamelCase('Foo-Bar');\\n//=> 'fooBar'\\n\\ncamelCase('розовый_пушистый_единорог');\\n//=> 'розовыйПушистыйЕдинорог'\\n\\ncamelCase('Foo-Bar', {pascalCase: true});\\n//=> 'FooBar'\\n\\ncamelCase('--foo.bar', {pascalCase: false});\\n//=> 'fooBar'\\n\\ncamelCase('Foo-BAR', {preserveConsecutiveUppercase: true});\\n//=> 'fooBAR'\\n\\ncamelCase('fooBAR', {pascalCase: true, preserveConsecutiveUppercase: true}));\\n//=> 'FooBAR'\\n\\ncamelCase('foo bar');\\n//=> 'fooBar'\\n\\nconsole.log(process.argv[3]);\\n//=> '--foo-bar'\\ncamelCase(process.argv[3]);\\n//=> 'fooBar'\\n\\ncamelCase(['foo', 'bar']);\\n//=> 'fooBar'\\n\\ncamelCase(['__foo__', '--bar'], {pascalCase: true});\\n//=> 'FooBar'\\n\\ncamelCase(['foo', 'BAR'], {pascalCase: true, preserveConsecutiveUppercase: true})\\n//=> 'FooBAR'\\n\\ncamelCase('lorem-ipsum', {locale: 'en-US'});\\n//=> 'loremIpsum'\\n```\\n\\n## API\\n\\n### camelCase(input, options?)\\n\\n#### input\\n\\nType: `string | string[]`\\n\\nString to convert to camel case.\\n\\n#### options\\n\\nType: `object`\\n\\n##### pascalCase\\n\\nType: `boolean`\\\\\\nDefault: `false`\\n\\nUppercase the first character: `foo-bar` → `FooBar`\\n\\n##### preserveConsecutiveUppercase\\n\\nType: `boolean`\\\\\\nDefault: `false`\\n\\nPreserve consecutive uppercase characters: `foo-BAR` → `FooBAR`.\\n\\n##### locale\\n\\nType: `false | string | string[]`\\\\\\nDefault: The host environment’s current locale.\\n\\nThe locale parameter indicates the locale to be used to convert to upper/lower case according to any locale-specific case mappings. If multiple locales are given in an array, the best available locale is used.\\n\\n```js\\nimport camelCase from 'camelcase';\\n\\ncamelCase('lorem-ipsum', {locale: 'en-US'});\\n//=> 'loremIpsum'\\n\\ncamelCase('lorem-ipsum', {locale: 'tr-TR'});\\n//=> 'loremİpsum'\\n\\ncamelCase('lorem-ipsum', {locale: ['en-US', 'en-GB']});\\n//=> 'loremIpsum'\\n\\ncamelCase('lorem-ipsum', {locale: ['tr', 'TR', 'tr-TR']});\\n//=> 'loremİpsum'\\n```\\n\\nSetting `locale: false` ignores the platform locale and uses the [Unicode Default Case Conversion](https://unicode-org.github.io/icu/userguide/transforms/casemappings.html#simple-single-character-case-mapping) algorithm:\\n\\n```js\\nimport camelCase from 'camelcase';\\n\\n// On a platform with 'tr-TR'\\n\\ncamelCase('lorem-ipsum');\\n//=> 'loremİpsum'\\n\\ncamelCase('lorem-ipsum', {locale: false});\\n//=> 'loremIpsum'\\n```\\n\\n## camelcase for enterprise\\n\\nAvailable as part of the Tidelift Subscription.\\n\\nThe maintainers of camelcase and thousands of other packages are working with Tidelift to deliver commercial support and maintenance for the open source dependencies you use to build your applications. Save time, reduce risk, and improve code health, while paying the maintainers of the exact dependencies you use. [Learn more.](https://tidelift.com/subscription/pkg/npm-camelcase?utm_source=npm-camelcase&utm_medium=referral&utm_campaign=enterprise&utm_term=repo)\\n\\n## Related\\n\\n- [decamelize](https://github.com/sindresorhus/decamelize) - The inverse of this module\\n- [uppercamelcase](https://github.com/SamVerschueren/uppercamelcase) - Like this module, but to PascalCase instead of camelCase\\n- [titleize](https://github.com/sindresorhus/titleize) - Capitalize every word in string\\n- [humanize-string](https://github.com/sindresorhus/humanize-string) - Convert a camelized/dasherized/underscored string into a humanized one\\n- [camelcase-keys](https://github.com/sindresorhus/camelcase-keys) - Convert object keys to camel case\\n\"},\n",
       " {'repo': 'futurice/space-tyckiting',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': \"# Space Tyckiting\\n\\n![Space Tyckiting banner](space-tyckiting-banner.jpg)\\n\\nWelcome to **Futurice Space Tyckiting**!\\n\\nThis repository contains the Space Tyckiting server and client skeletons.\\n\\nSpace Tyckiting server communicates via a *JSON-over-TCP* protocol, making it possible for the clients to be implemented in any language of your choice.\\n\\nReady-made client skeletons, which provide a convenient network communication layer, have been provided for a number of programming languages. You may also wish to create your very own client from scratch &mdash; no problem! Take a look at the example clients and documentation and code ahead! If you'd like to share your client skeleton, please issue a Pull Request.\\n\\nIf you discover any bugs or issues with the server or the provided client skeletons, please let us know, and if possible, issue a PR with a fix!\\n\\nPlease see the server [README](server/README.md), and the README's of the clients, for details on how to develop your AI and run Space Tyckiting.\\n\\nHappy Space Tyckiting!\\n\\n![Space Tyckiting](space-tyckiting.gif)\\n\"},\n",
       " {'repo': 'amymcgovern/spacesettlers',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '# spacesettlers\\nSpacewar/Spacesettlers game used by the AI class at the University of Oklahoma\\n\\nThis project stores the source code for the game Space Settlers, formerly known as Spacewar. This game is used to teach Artificial Intelligence at the University of Oklahoma. The game enables students to implement their own search, learning, multi-agent, and planning systems within a complex real-world type environment. They also learn how to use existing software packages. You are welcome to use this software in your class! Please cite one of the papers below.  Additional documentation is available on the github wiki pages and through the javadocs.\\n\\nMcGovern, Amy and Trytten, Deborah. (2013). Making In-Class Competitions Desirable For Marginalized Groups. Proceedings of the 2013 Frontiers in Education Conference, pages 704-706. \\n\\nMcGovern, Amy; Tidwell, Zachery and Rushing, Derek. (2011). Teaching Introductory Artificial Intelligence through Java-based Games. Proceedings of the symposium on Educational Advances in Artificial Intelligence. \\n\\nMcGovern, Amy and Fager, Jason. (2007) Creating Significant Learning Experiences in Introductory Artificial Intelligence. Proceedings of SIGCSE 2007, technical symposium on computer science education, pages 39-43. \\n'},\n",
       " {'repo': 'adelinolobao/Space-Invaders-HTML-5',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': 'Implementation of the classic game Space Invaders using HTML 5 and Javascript'},\n",
       " {'repo': 'danieljf24/hybrid_space',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"# Dual Encoding for Video Retrieval by Text\\n\\nSource code of our TPAMI'21  paper [Dual Encoding for Video Retrieval by Text](https://arxiv.org/abs/2009.05381) and CVPR'19 paper [Dual Encoding for Zero-Example Video Retrieval](https://openaccess.thecvf.com/content_CVPR_2019/html/Dong_Dual_Encoding_for_Zero-Example_Video_Retrieval_CVPR_2019_paper.html).\\n\\n![image](dual_encoding.jpg)\\n\\n## Table of Contents\\n* [Environments](#environments)\\n* [Dual Encoding on MSRVTT10K](#dual-encoding-on-msrvtt10k)\\n  * [Required Data](#required-data)\\n  * [Model Training and Evaluation](#model-training-and-evaluation)\\n  * [Evaluation using Provided Checkpoints](#evaluation-using-provided-checkpoints)\\n  * [Expected Performance](#expected-performance)\\n* [Dual Encoding on VATEX](#dual-encoding-on-vatex)\\n  * [Required Data](#required-data-1)\\n  * [Model Training and Evaluation](#model-training-and-evaluation-1)\\n  * [Expected Performance](#expected-performance-1)\\n* [Dual Encoding on Ad-hoc Video Search](#dual-encoding-on-ad-hoc-video-search-avs)\\n  * [Required Data](#required-data-2)\\n  * [Train Dual Encoding model from scratch](#train-dual-encoding-model-from-scratch)\\n* [How to run Dual Encoding on other datasets](#how-to-run-dual-encoding-on-other-datasets)\\n  * [Single-folder structure](#single-folder-structure)\\n  * [Multiple-folder structure](#multiple-folder-structure)\\n* [References](#references)\\n\\n\\n## Environments\\n\\n* **Ubuntu** 16.04\\n* **CUDA** 10.1\\n* **Python** 3.8\\n* **PyTorch** 1.5.1\\n\\nWe used Anaconda to setup a deep learning workspace that supports PyTorch.\\nRun the following script to install the required packages.\\n```shell\\nconda create --name ws_dual_py3 python=3.8\\nconda activate ws_dual_py3\\ngit clone https://github.com/danieljf24/hybrid_space.git\\ncd hybrid_space\\npip install -r requirements.txt\\nconda deactivate\\n```\\n\\n## Dual Encoding on MSRVTT10K\\n### Required Data\\nRun the following script to download and extract MSR-VTT ([msrvtt10k-resnext101_resnet152.tar.gz(4.3G)](http://8.210.46.84:8787/tpami2021/msrvtt10k-resnext101_resnet152.tar.gz)) dataset and a pre-trained word2vec ([vec500flickr30m.tar.gz(3.0G)](http://lixirong.net/data/w2vv-tmm2018/word2vec.tar.gz). The data can also be downloaded from Baidu pan ([url](https://pan.baidu.com/s/1lg23K93lVwgdYs5qnTuMFg), password:p3p0) or Google drive ([url](https://drive.google.com/drive/folders/1TEIjErztZNQAi6AyNu9cK5STwo74oI8I?usp=sharing)). For more information about the dataset, please refer to [here](dataset/README.md).\\nThe extracted data is placed in `$HOME/VisualSearch/`.\\n```shell\\nROOTPATH=$HOME/VisualSearch\\nmkdir -p $ROOTPATH && cd $ROOTPATH\\n\\n# download and extract dataset\\nwget http://8.210.46.84:8787/tpami2021/msrvtt10k-resnext101_resnet152.tar.gz\\ntar zxf msrvtt10k-resnext101_resnet152.tar.gz -C $ROOTPATH\\n\\n# download and extract pre-trained word2vec\\nwget http://lixirong.net/data/w2vv-tmm2018/word2vec.tar.gz\\ntar zxf word2vec.tar.gz -C $ROOTPATH\\n```\\n\\n### Model Training and Evaluation\\nRun the following script to train and evaluate `Dual Encoding` network with hybrid space on the `official` partition of MSR-VTT. The video features are the concatenation of ResNeXt-101 and ResNet-152 features. The code of video feature extraction we used in the paper is available at [here](https://github.com/xuchaoxi/video-cnn-feat). \\n```shell\\nconda activate ws_dual_py3\\n./do_all.sh msrvtt10k hybrid resnext101-resnet152\\n```\\nRunning the script will do the following things:\\n1. Train `Dual Encoding` network with hybrid space and select a checkpoint that performs best on the validation set as the final model. Notice that we only save the best-performing checkpoint on the validation set to save disk space.\\n2. Evaluate the final model on the test set.\\nNote that the dataset has already included vocabulary and concept annotations. If you would like to generate vocabulary and concepts by yourself, run the script `./do_vocab_concept.sh msrvtt10k 1 $ROOTPATH`.\\n\\n\\nIf you would like to train `Dual Encoding` network with the latent space learning (Conference Version), please run the following scrip:\\n```shell\\n./do_all.sh msrvtt10k latent resnext101-resnet152 $ROOTPATH\\n```\\n\\nTo train the model on the `Test1k-Miech` partition and `Test1k-Yu` partition of MSR-VTT, please run the following scrip:\\n```shell\\n./do_all.sh msrvtt10kmiech hybrid resnext101-resnet152 $ROOTPATH\\n./do_all.sh msrvtt10kyu hybrid resnext101-resnet152 $ROOTPATH\\n```\\n\\n### Evaluation using Provided Checkpoints\\n\\nThe overview of pre-trained checkpoints on MSR-VTT is as follows. \\n| Split         | Pre-trained Checkpoints |\\n| ----------    | ------------ |\\n| Official      | [msrvtt10k_model_best.pth.tar(264M)](http://8.210.46.84:8787/tpami2021/checkpoints/msrvtt10k_model_best.pth.tar) |\\n| Test1k-Miech  | [msrvtt10kmiech_model_best.pth.tar(267M)](http://8.210.46.84:8787/tpami2021/checkpoints/msrvtt10kmiech_model_best.pth.tar) |\\n| Test1k-Yu     | [msrvtt10kyu_model_best.pth.tar(267M)](http://8.210.46.84:8787/tpami2021/checkpoints/msrvtt10kyu_model_best.pth.tar) |\\n\\nNote that if you would like to evaluate using our trained checkpoints, please make sure to use the vocabulary and concept annotations that are provided in the `msrvtt10k-resnext101_resnet152.tar.gz`.\\n\\n\\n#### On the official split\\nRun the following script to download and evaluate our trained checkpoints on the official split of MSR-VTT. The trained checkpoints can also be downloaded from Baidu pan ([url](https://pan.baidu.com/s/1lg23K93lVwgdYs5qnTuMFg), password:p3p0). \\n\\n```shell\\nMODELDIR=$HOME/VisualSearch/checkpoints\\nmkdir -p $MODELDIR\\n\\n# download trained checkpoints\\nwegt -P $MODELDIR http://8.210.46.84:8787/tpami2021/checkpoints/msrvtt10k_model_best.pth.tar\\n\\n# evaluate on the official split of MSR-VTT\\nCUDA_VISIBLE_DEVICES=0 python tester.py --testCollection msrvtt10k --logger_name $MODELDIR  --checkpoint_name msrvtt10k_model_best.pth.tar\\n```\\n\\n#### On Test1k-Miech and Test1k-Yu splits\\nIn order to evaluate on `Test1k-Miech` and  `Test1k-Yu` splits, please run the following script.\\n\\n```shell\\nMODELDIR=$HOME/VisualSearch/checkpoints\\n\\n# download trained checkpoints on Test1k-Miech\\nwegt -P $MODELDIR http://8.210.46.84:8787/tpami2021/checkpoints/msrvtt10kmiech_model_best.pth.tar\\n\\n# evaluate on Test1k-Miech of MSR-VTT\\nCUDA_VISIBLE_DEVICES=0 python tester.py --testCollection msrvtt10kmiech --logger_name $MODELDIR  --checkpoint_name msrvtt10kmiech_model_best.pth.tar\\n```\\n\\n```shell\\nMODELDIR=$HOME/VisualSearch/checkpoints\\n\\n# download trained checkpoints on Test1k-Yu\\nwegt -P $MODELDIR http://8.210.46.84:8787/tpami2021/checkpoints/msrvtt10kyu_model_best.pth.tar\\n\\n# evaluate on Test1k-Yu of MSR-VTT\\nCUDA_VISIBLE_DEVICES=0 python tester.py --testCollection msrvtt10kyu --logger_name $MODELDIR  --checkpoint_name msrvtt10kyu_model_best.pth.tar\\n```\\n### Expected Performance\\nThe expected performance of Dual Encoding on MSR-VTT is as follows. Notice that due to random factors in SGD based training, the numbers differ slightly from those reported in the paper.\\n<table>\\n    <tr>\\n        <th rowspan='2'>Split</th><th colspan='5'>Text-to-Video Retrieval</th> <th colspan='5'>Video-to-Text Retrieval</th>  <th rowspan='2'>SumR</th>\\n    </tr>\\n    <tr>\\n        <th> R@1 </th> <th> R@5 </th> <th> R@10 </th> <th> MedR </th> <th>\\tmAP </th> <th> R@1 </th> <th> R@5 </th> <th> R@10 </th> <th> MedR </th> <th>\\tmAP </th>\\n    </tr>\\n    <tr>  \\n    \\t<td>Official</td>\\n\\t\\t<td>11.8</td><td>30.6</td><td>41.8</td><td>17</td><td>21.4</td> \\n    \\t<td>21.6</td><td>45.9</td><td>58.5</td><td>7</td><td>10.3</td> \\n    \\t<td>210.2</td> \\n    </tr>\\n\\t<tr>  \\n\\t\\t<td>Test1k-Miech</td>\\n\\t\\t<td>22.7</td><td>50.2</td><td>63.1</td><td>5</td><td>35.6</td> \\n    \\t<td>24.7</td><td>52.3</td><td>64.2</td><td>5</td><td>37.2</td> \\n    \\t<td>277.2</td> \\n    </tr>\\n\\t<tr>  \\n\\t\\t<td>Test1k-Yu</td>\\n\\t\\t<td>21.5</td><td>48.8</td><td>60.2</td><td>6</td><td>34.0</td> \\n    \\t<td>21.7</td><td>49.0</td><td>61.4</td><td>6</td><td>34.6</td> \\n    \\t<td>262.6</td>      \\n\\t</tr>\\n</table>\\n\\n\\n\\n## Dual Encoding on VATEX\\n\\n### Required Data\\nDownload VATEX dataset ([vatex-i3d.tar.gz(3.0G)](http://8.210.46.84:8787/tpami2021/vatex-i3d.tar.gz)) and a pre-trained word2vec ([vec500flickr30m.tar.gz(3.0G)](http://lixirong.net/data/w2vv-tmm2018/word2vec.tar.gz)). The data can also be downloaded from Baidu pan ([url](https://pan.baidu.com/s/1lg23K93lVwgdYs5qnTuMFg), password:p3p0) or Google drive ([url](https://drive.google.com/drive/folders/1TEIjErztZNQAi6AyNu9cK5STwo74oI8I?usp=sharing)). For more information about the dataset, please refer to [here](dataset/README.md). Please extract data into `$HOME/VisualSearch/`.\\n\\n### Model Training and Evaluation\\nRun the following script to train and evaluate `Dual Encoding` network with hybrid space on VATEX.\\n```shell\\n# download and extract dataset\\nwget http://8.210.46.84:8787/tpami2021/vatex-i3d.tar.gz\\ntar zxf vatex-i3d.tar.gz -C $ROOTPATH\\n\\n./do_all.sh vatex hybrid i3d_kinetics $ROOTPATH\\n```\\n\\n### Expected Performance\\nRun the following script to download and evaluate our trained model ([vatex_model_best.pth.tar(230M)](http://8.210.46.84:8787/tpami2021/checkpoints/vatex_model_best.pth.tar)) on VATEX.\\n```shell\\nMODELDIR=$HOME/VisualSearch/checkpoints\\n\\n# download trained checkpoints\\nwegt -P $MODELDIR http://8.210.46.84:8787/tpami2021/checkpoints/vatex_model_best.pth.tar\\n\\nCUDA_VISIBLE_DEVICES=0 python tester.py --testCollection vatex --logger_name $MODELDIR  --checkpoint_name vatex_model_best.pth.tar\\n```\\n\\nThe expected performance of Dual Encoding with hybrid space learning on VATEX is as follows. \\n<table>\\n    <tr>\\n        <th rowspan='2'>Split</th><th colspan='5'>Text-to-Video Retrieval</th> <th colspan='5'>Video-to-Text Retrieval</th>  <th rowspan='2'>SumR</th>\\n    </tr>\\n    <tr>\\n        <th> R@1 </th> <th> R@5 </th> <th> R@10 </th> <th> MedR </th> <th>\\tmAP </th> <th> R@1 </th> <th> R@5 </th> <th> R@10 </th> <th> MedR </th> <th>\\tmAP </th>\\n    </tr>\\n    <tr>  \\n    \\t<td>VATEX</td>\\n\\t\\t<td>35.8</td><td>72.8</td><td>82.9</td><td>2</td><td>52.0</td> \\n    \\t<td>47.5</td><td>76.0</td><td>85.3</td><td>2</td><td>39.1</td> \\n    \\t<td>400.3</td>      \\n    </tr>\\n</table>\\n\\n\\n## Dual Encoding on Ad-hoc Video Search (AVS)\\n\\n### Required Data\\nThe following datasets are used for training, validation and testing: the joint collection of MSR-VTT and TGIF, tv2016train and IACC.3. For more information about these datasets, please refer to [here](dataset/README.md).\\n\\n#### Frame-level feature data\\nPlease download the frame-level features from Baidu pan ([url](https://pan.baidu.com/s/1uk1PTptOgM-UgiS56M1Eqg), password:qwlc). The filename of feature data are summarized as follows.\\n|  Datasets     | 2048-dim ResNeXt-101 |  2048-dim ResNet-152 | \\n| ------------  | ------------ | ------------ |\\n| MSR-VTT       | msrvtt10k_ResNext-101.tar.gz   | msrvtt10k_ResNet-152.tar.gz   |\\n| TGIF          | tgif_ResNext-101.tar.gz        | tgif_ResNet-152.tar.gz        |\\n| tv2016train   | tv2016train_ResNext-101.tar.gz | tv2016train_ResNet-152.tar.gz |\\n| IACC.3        | iacc.3_ResNext-101.tar.gz      | iacc.3_ResNet-152.tar.gz      |\\n\\nNote if you have already download MSR-VTT data we provide above, you need not download `msrvtt10k_ResNext-101.tar.gz` and `msrvtt10k_ResNet-152.tar.gz`.\\n\\n#### Sentence data\\n* Sentences: [TGIF and MSR-VTT ](http://lixirong.net/data/mm2019/tgif-msrvtt10k-sent.tar.gz), [tv2016train](http://lixirong.net/data/mm2019/tv2016train-sent.tar.gz)\\n* TRECVID 2016 / 2017 / 2018 AVS topics and ground truth:  [iacc.3](http://lixirong.net/data/mm2019/iacc.3-avs-topics.tar.gz)\\n\\n\\nPlease download the above data, and run the following scripts to extract them into `$HOME/VisualSearch/`.\\n```shell\\nROOTPATH=$HOME/VisualSearch\\n\\n# extract ResNext-101\\ntar zxf tgif_ResNext-101.tar.gz -C $ROOTPATH\\ntar zxf msrvtt10k_ResNext-101.tar.gz -C $ROOTPATH\\ntar zxf tv2016train_ResNext-101.tar.gz -C $ROOTPATH\\ntar zxf iacc.3_ResNext-101.tar.gz -C $ROOTPATH\\n\\n# extract ResNet-152\\ntar zxf tgif_ResNet-152.tar.gz -C $ROOTPATH\\ntar zxf msrvtt10k_ResNet-152.tar -C $ROOTPATH\\ntar zxf tv2016train_ResNet-152.tar.gz -C $ROOTPATH\\ntar zxf iacc.3_ResNet-152.tar.gz -C $ROOTPATH\\n\\n# combine feature of tgif and msrvtt10k\\n./do_combine_features.sh\\n\\n```\\n\\n### Train Dual Encoding model from scratch\\n\\n```shell\\nROOTPATH=$HOME/VisualSearch\\ntrainCollection=tgif-msrvtt10k\\noverwrite=0\\n\\n# Generate a vocabulary on the training set\\n./util/do_get_vocab.sh $trainCollection $ROOTPATH $overwrite\\n\\n# Generate concepts according to video captions\\n./util/do_get_tags.sh $trainCollection $ROOTPATH $overwrite\\n\\n# Generate video frame info\\nvisual_feature=resnext101-resnet152\\n./util/do_get_frameInfo.sh $trainCollection $visual_feature $ROOTPATH $overwrite\\n\\n# training and testing\\n./do_all_avs.sh $ROOTPATH\\n\\n```\\n\\n## How to run Dual Encoding on other datasets?\\n\\nOur code supports dataset structure:\\n* `Single-folder structure`: train, validation and test subset are stored in a folder.\\n* `Multiple-folder structure`: train, validation and test subset are stored in three folders respectively.\\n\\n\\n### Single-folder structure\\nStore the train, validation and test subset into a folder in the following structure.\\n```shell\\n${collection}\\n├── FeatureData\\n│   └── ${feature_name}\\n│       ├── feature.bin\\n│       ├── shape.txt\\n│       └── id.txt\\n└── TextData\\n    └── ${collection}train.caption.txt\\n    └── ${collection}val.caption.txt\\n    └── ${collection}test.caption.txt\\n```\\n\\n* `FeatureData`: video frame features. Using [txt2bin.py](https://github.com/danieljf24/simpleknn/blob/master/txt2bin.py) to convert video frame feature in the required binary format.\\n* `${collection}train.caption.txt`: training caption data.\\n* `${collection}val.caption.txt`: validation caption data.\\n* `${collection}test.caption.txt`: test caption data. \\nThe file structure is as follows, in which the video and sent in the same line are relevant.\\n```\\nvideo_id_1#1 sentence_1\\nvideo_id_1#2 sentence_2\\n...\\nvideo_id_n#1 sentence_k\\n...\\n```\\n\\nPlease run the script to generate vocabulary and concepts:\\n```shell\\n./util/do_vocab_concept.sh $collection 0 $ROOTPATH\\n```\\n\\nRun the following script to train and evaluate Dual Encoding on your own dataset:\\n```shell\\n./do_all_singlefolder.sh ${collection} hybrid ${feature_name} ${rootpath}\\n```\\n\\n\\n### Multiple-folder structure\\nStore the training, validation and test subsets into three folders in the following structure respectively.\\n```shell\\n${subset_name}\\n├── FeatureData\\n│   └── ${feature_name}\\n│       ├── feature.bin\\n│       ├── shape.txt\\n│       └── id.txt\\n└── TextData\\n    └── ${subset_name}.caption.txt\\n```\\n\\n* `FeatureData`: video frame features.\\n* `${dsubset_name}.caption.txt`: caption data of corresponding subset.\\n\\nYou can run the following script to check whether the data is ready:\\n```shell\\n./do_format_check.sh ${train_set} ${val_set} ${test_set} ${rootpath} ${feature_name}\\n```\\nwhere `train_set`, `val_set` and `test_set` indicate the name of training, validation and test set, respectively, ${rootpath} denotes the path where datasets are saved and `feature_name` is the video frame feature name.\\n\\nPlease run the script to generate vocabulary and concepts:\\n```shell\\n./util/do_vocab_concept.sh ${train_set} 0 $ROOTPATH\\n```\\n\\nIf you pass the format check, use the following script to train and evaluate Dual Encoding on your own dataset:\\n```shell\\n./do_all_multifolder.sh ${train_set} ${val_set} ${test_set} hybrid ${feature_name} ${rootpath}\\n```\\n\\n\\n## References\\nIf you find the package useful, please consider citing our TPAMI'21 or CVPR'19 paper:\\n```\\n@article{dong2021dual,\\n  title={Dual Encoding for Video Retrieval by Text},\\n  author={Dong, Jianfeng and Li, Xirong and Xu, Chaoxi and Yang, Xun and Yang, Gang and Wang, Xun and Wang, Meng},\\n  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n  doi = {10.1109/TPAMI.2021.3059295},\\n  year={2021}\\n}\\n```\\n\\n\\n```\\n@inproceedings{cvpr2019-dual-dong,\\ntitle = {Dual Encoding for Zero-Example Video Retrieval},\\nauthor = {Jianfeng Dong and Xirong Li and Chaoxi Xu and Shouling Ji and Yuan He and Gang Yang and Xun Wang},\\nbooktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\\nyear = {2019},\\n}\\n```\\n\"},\n",
       " {'repo': 'MagicSmokeIndustries/InfernalRobotics',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': \"All credit to Damned Robotics goes to:\\nPlugin/lots of stuff : r4m0n\\n3dmodels/'textures' : DYJ\\n\\noriginal source location: http://svn.mumech.com/KSP/trunk/MuMechLib/\\n\\nAll new 3dmodels & textures : sirkut, Devo, ZodiusInfuser\"},\n",
       " {'repo': 'joseluis/huion-linux-drivers',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# User Space Linux Drivers for Huion Tablets\\n\\n---\\n\\n## PSA\\n\\nThis project is currently not actively maintained and its future is uncertain. I started it a long time ago in order to fulfill a specific need I had at the time,  deciding to make it open and more general in case more people found it useful. But I didn\\'t expect for it to become inmanageable as much for me.\\n\\nMy circumstances have changed and I no longer have the motivation to keep working on this project. For one, my tablet works better now without it that with it, in recent versions of Linux. Many bugs have arised related with the compatilibility between different Python versions and different distributions, which is very frustrating (and this has also contributed to my avoiding of Python AMAP). Plus obscure compatibility problems with many tablets which I can\\'t test nor desire to debug. It also doesn\\'t work with Wayland, and it doesn\\'t support mixing multiple resolutions in multi-monitor mode... The list goes on.\\n\\nIf you believe you want to take care of gthe project and try to fix its multiple issues and keep maintaing it, please contact me and we can arrange some relay.\\n\\nThank you, and I\\'m sorry for the broken expectatives\\n\\n---\\n\\n\\n## Table of Contents\\n\\n- [Features](#features-)\\n- [Usage](#usage-)\\n- [Requirements](#requirements-)\\n  - [Dependencies](#dependencies-)\\n  - [Xorg extra code](#xorg-extra-code-)\\n- [Troubleshooting](#troubleshooting-)\\n- [Config Examples](#config-examples-)\\n  - [Multi-Monitor](#multi-monitor-)\\n  - [Shortcuts](#shortcuts-)\\n- [Help Welcomed](#help-welcomed-)\\n\\n## Features [↑](#table-of-contents \"Back to TOC\")\\n\\n * Supports multiple tablet models\\n * Precise cursor positioning\\n * Full pressure sensitivity\\n * Both stylus buttons\\n * Compatible with multi-monitor setups\\n * Customizable buttons and scrollbar shortcuts\\n * Multiple sets of shortcuts\\n * Optional desktop notifications\\n * Versatile configuration file\\n\\n\\n## Usage [↑](#table-of-contents \"Back to TOC\")\\n\\n * Follow the requirements: Install the dependencies and the xorg extra code.\\n * Download this repository (You only need `huion-tablet-driver.py` and `config.ini`).\\n * Edit `config.ini` to match your tablet, multi-monitor setup and desired shortcuts.\\n * Run `sudo ./huion-tablet-driver.py` (needs superuser privileges)\\n\\n\\n## Requirements [↑](#table-of-contents \"Back to TOC\")\\n\\n### Dependencies [↑](#table-of-contents \"Back to TOC\")\\n\\n * [python](https://www.python.org/) version 3.5 or greater\\n * [uclogic-tools](https://github.com/DIGImend/uclogic-tools) ([read why][2])\\n\\n    ```\\n    # Installation from source: install dependencies, clone, compile & install\\n\\n    $ sudo apt install make automake gcc pkg-config libusb-1.0-0-dev  # For ubuntu\\n\\n    git clone https://github.com/DIGImend/uclogic-tools\\n    cd uclogic-tools\\n    autoreconf -i -f && ./configure --prefix=/usr/local/ && make\\n    sudo make install\\n    ```\\n\\n * [xinput](https://wiki.archlinux.org/index.php/Xinput)\\n * [evdev](https://wiki.gentoo.org/wiki/Evdev)\\n * [python-evdev](https://github.com/gvalkov/python-evdev)\\n * [pyusb](https://walac.github.io/pyusb/)\\n * [numexpr](https://github.com/pydata/numexpr)\\n * [xdotool][7] (optional, for button shorcuts)\\n * [notify-send][8] (optional, for desktop notifications)\\n * [xrandr][9] (optional, for monitor configuration) (and [arandr][10])\\n\\n[2]: https://github.com/benthor/HuionKamvasGT191LinuxDriver/issues/1#issuecomment-351207116\\n[7]: http://www.semicomplete.com/projects/xdotool/\\n[8]: https://wiki.archlinux.org/index.php/Desktop_notifications\\n[9]: https://wiki.archlinux.org/index.php/xrandr\\n[10]: https://christian.amsuess.com/tools/arandr/\\n\\n\\nInstall packages in Archlinux:\\n\\n```\\n$  pacman -S xorg-xinput xf86-input-evdev python-evdev python-pyusb xdotool \\\\\\nlibnotify xorg-xrandr arandr python-numexpr\\n```\\n\\nInstall packages in Ubuntu:\\n```\\n$ sudo apt install xinput xserver-xorg-input-evdev python3-evdev python3-usb \\\\\\nxdotool libnotify-bin arandr python3-numexpr\\n```\\n\\n### Xorg Extra Code [↑](#table-of-contents \"Back to TOC\")\\n\\nYou will likely also need to add some code to the Xorg server.\\nCreate a new file in` /etc/X11/xorg.conf.d/evdev-tablet.conf` with the following content:\\n\\n```\\nSection \"InputClass\"\\n\\tIdentifier \"evdev tablet catchall\"\\n\\tMatchIsTablet \"on\"\\n\\tMatchDevicePath \"/dev/input/event*\"\\n\\tDriver \"evdev\"\\nEndSection\\n```\\n\\n## Troubleshooting [↑](#table-of-contents \"Back to TOC\")\\n\\n### My tablet doesn\\'t provide any inputs despite all my debugging efforts\\n\\nMaybe you\\'re connecting the table through a USV hub, or USB docking station? This is known to have caused problems in the past. Try plugging the tablet directly to the computer.\\n\\n### /usr/local/bin/uclogic-probe: not found\\n\\nYou eiher need to compile uclogic binaries (see [Dependencies](#dependencies-)), or they are installed in a different location. For example Debian 10 automatically installs them under `/usr/bin/`. Try updating the path in the `config.ini` file, which by default is: `uclogic_bins = /usr/local/bin`.\\n\\n\\n## Config Examples [↑](#table-of-contents \"Back to TOC\")\\n\\n### Multi-Monitor [↑](#table-of-contents \"Back to TOC\")\\n\\nIf you have a multi-monitor setup, edit your copy of `config.ini`\\nwith the correct values for your particular setup.\\n\\n```\\n# Multi Monitor Configuration\\nenable_multi_monitor  = true\\nenable_xrandr         = false\\ncurrent_monitor_setup = [monitor_3]\\n```\\n\\nYou\\'ll have to customize your current monitor setup, by modifying one of the\\nexisting examples in the section 3 of the `config.ini` file.\\n\\n[More information about multiple monitors in the wiki](https://github.com/joseluis/huion-linux-drivers/wiki/Multi-Monitor)\\n\\n\\n### Shortcuts [↑](#table-of-contents \"Back to TOC\")\\n\\nTo customize the shortcuts associated with the buttons and the scrollbar,\\nedit the file `config.ini`, and use the xdotool syntax for the buttons actions.\\n\\nFirst, assign the menu you\\'re going to use as the starting menu.\\n\\n### Example with a Single Buttons Menu\\n\\n```\\nstart_menu = [menu_simple_10b]\\n\\n[menu_simple_10b]\\n\\n# upper buttons\\nb0 = key Tab           # hide interface\\nb1 = key r             # rect select (gimp) & pick layer (krita)\\nb2 = key ctrl+x        # cut\\nb3 = key ctrl+c        # copy\\nb4 = key ctrl+v        # paste\\n\\n# scrollbar\\nsu = click 4           # mouse wheel up\\nsd = click 5           # mouse wheel down\\n\\n# lower buttons\\nb5 = key ctrl+z        # undo\\nb6 = key ctrl+y        # redo (gimp)\\nb7 = key ctrl+shift+z  # redo (krita)\\nb8 = key 4             # turn left (krita)\\nb9 = key 6             # turn right (krita)\\n```\\n\\n[See an example with multiple menus in the wiki](https://github.com/joseluis/huion-linux-drivers/wiki/Buttons-Shortcuts#12-example-with-multiple-menus)\\n\\n\\n## Help Welcomed [↑](#table-of-contents \"Back to TOC\")\\n\\nI originally started this project for my own simple needs and my single Huion tablet.\\nNow it has grown more than I imagined, and I don\\'t have much time or energy to support all use-cases and fix all the issues.\\n\\nI\\'m very open to add additional mantainers, so tell me if you are interested.\\nOr you can take a look to the open issues if you want to help with something.\\n\\nThank you!\\n'},\n",
       " {'repo': 'Xerxes1138/ScreenSpaceSubsurfaceScattering',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# ScreenSpaceSubsurfaceScattering\\n\\n![SSSS](https://raw.githubusercontent.com/Xerxes1138/ScreenSpaceSubsurfaceScattering/master/SSS.png)\\n\\n# Features\\n- SubSurface Scattering (SSS) blur is performed in screen space\\n- SSS attenuation can be defined by a color or a texture\\n- Transmittance\\n- Not limited to skin\\n\\n# Limitations\\n- Deferred shading only\\n- Terrain is not supported\\n- Ambient occlusion and screen space reflection from post processing stack are not supported\\n\\n# Requirements\\n\\nUnity 2017.2.0f3 and a shader model 3.0 ( dx9 ) graphic card.\\n\\n# How to use\\n\\nSet project in Linear Color space and make sure that under ProjectSettings/Graphics it is setup like the image below\\n\\n![GraphicsSettings](https://raw.githubusercontent.com/Xerxes1138/ScreenSpaceSubsurfaceScattering/master/GraphicsSettings.png)\\n\\nSee pref_CameraRig for reference on how to setup ScreenSpaceSubSurfaceScattering.cs on your camera (should always be the first effect to be used)\\n\\nExisting surface shader need to be updated, see SurfaceShaderSSSTemplate.shader and  SurfaceShaderTemplate.shader as guide\\n\\n# References\\n- [Jimenez et al. 2011] \"https://github.com/iryoku/separable-sss\"\\n- [Mavridis 2012] \"The Compact YCoCg Frame Buffer\"\\n- [Sousa  2013] \"The Rendering Technologies of Crysis 3\"\\n- [Jimenez et al. 2013] \"Next-Generation Character Rendering\"\\n- [Jimenez 2014] \"Next Generation Post Processing In Call Of Duty Advanced Warfare\" \\n- [Senua 2016] \"Digital Humans\"\\n- [Mikkel Gjøl & Mikkel Svendsen] \"THE RENDERING OF INSIDE\"'},\n",
       " {'repo': 'Ezriilc/HyperEdit',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': \"HyperEdit\\n=========\\n\\nA plugin for Kerbal Space Program, proudly maintained by Kerbaltek.\\n\\nTo use this mod, DO NOT DOWNLOAD FROM THIS REPO, as it only contains the source code and not the actual binary .dll file.  All releases will be on our own site: http://www.Kerbaltek.com/hyperedit\\n\\nPlease talk to us on our site: http://www.Kerbaltek.com/contact\\nOr, on the KSP forum at the link on our site.\\n\\nPull requests are always welcome, as is any other input.  In fact, we rely on contributions to keep our projects going.\\n\\nPlease leave our .version file as it is.  That's only here for CKAN, and it will be changed manually by the owner when we package the mod for release.\\n\\nTo build:\\nChange the .csproj file where notated to suit your own setup, but please keep it out of your commits when pushing or making a pull request.\\n\\nVS2015+ is required, or other C#6 compliant C# compiler (khyperia uses mono 4.2.2, Ezriilc uses VS2019 Community)\\n\"},\n",
       " {'repo': 'tompazourek/Colourful',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# ![Colourful logo](https://raw.githubusercontent.com/tompazourek/Colourful/master/assets/logo_32.png) Colourful .NET\\n\\n[![Build status](https://img.shields.io/appveyor/ci/tompazourek/colourful/master.svg)](https://ci.appveyor.com/project/tompazourek/colourful)\\n[![Tests](https://img.shields.io/appveyor/tests/tompazourek/colourful/master.svg)](https://ci.appveyor.com/project/tompazourek/colourful/build/tests)\\n[![codecov](https://codecov.io/gh/tompazourek/Colourful/branch/master/graph/badge.svg?token=gSGKtsdmw3)](https://codecov.io/gh/tompazourek/Colourful)\\n[![NuGet version](https://img.shields.io/nuget/v/Colourful.svg)](https://www.nuget.org/packages/Colourful/)\\n[![NuGet downloads](https://img.shields.io/nuget/dt/Colourful.svg)](https://www.nuget.org/packages/Colourful/)\\n\\n*Open source .NET library for working with color spaces.*\\n\\nThe library is written in C# and released with an [MIT license](https://raw.githubusercontent.com/tompazourek/Colourful/LICENSE), so feel **free to fork** or **use commercially**.\\n\\n**Any feedback is appreciated, please visit the [issues](https://github.com/tompazourek/Colourful/issues?state=open) page or send me an [e-mail](mailto:tom.pazourek@gmail.com).**\\n\\n\\n## Download\\n\\nBinaries of the last build can be downloaded on the [AppVeyor CI page of the project](https://ci.appveyor.com/project/tompazourek/colourful/build/artifacts).\\n\\nThe library is also [published on NuGet.org](https://www.nuget.org/packages/Colourful/), install using:\\n\\n```\\nPM> Install-Package Colourful\\n```\\n\\nColourful is CLS Compliant (to allow use in VB.NET etc.) and is built for these target frameworks:\\n\\n- .NET 6\\n- .NET Framework 4.5\\n- .NET Standard 2.0\\n- .NET Standard 1.1\\n- *For older .NET Framework 4.0 see [version 1 of the library](https://github.com/tompazourek/Colourful/releases/tag/1.2.2).*\\n\\n\\n## Usage\\n\\nExample \"hello world\" usage that converts a color from sRGB to XYZ (keeping the D65 white point):\\n\\n```csharp\\nIColorConverter<RGBColor, XYZColor> converter = new ConverterBuilder()\\n    .FromRGB(RGBWorkingSpaces.sRGB)\\n    .ToXYZ(Illuminants.D65)\\n    .Build();\\n\\nRGBColor rgbColor = new RGBColor(1, 0, 0.5);\\nXYZColor xyzColor = converter.Convert(rgbColor); // XYZ [X=0.45, Y=0.23, Z=0.22]\\n```\\n\\n\\n## Documentation\\n\\nPlease see the docs pages below for various topics:\\n\\n- [Conversion between color spaces](docs/topic-conversion.md)\\n  - also handles chromatic adaptation with multiple possible LMS transformation matrices:\\n    - Bradford (default)\\n    - Von Kries (Hunt-Pointer-Estevez adjusted for D65)\\n    - Von Kries (Hunt-Pointer-Estevez for equal energy)\\n    - XYZ scaling\\n    - Spectral-sharpened Bradford \\n    - CMCCAT2000\\n    - CAT02\\n    - *(user-defined chromatic adaptation matrix)*\\n- [Correlated color temperature (CCT)](docs/topic-cct.md)\\n  - Planckian locus approximation method\\n- [Ranges of channel values and clamping](docs/topic-clamp.md)\\n- [Computing color difference](docs/topic-color-difference.md)\\n  - multiple algorithms supported:\\n    - CIE Delta-E 1976\\n    - CMC l:c (1984)\\n    - CIE Delta-E 1994\\n    - CIE Delta-E 2000\\n    - J<sub>z</sub>C<sub>z</sub>h<sub>z</sub> Delta-E<sub>z</sub>\\n    - Euclidean distance\\n- [Cylindrical color spaces](docs/topic-cylindrical-spaces.md)\\n- [Illuminants and white points](docs/topic-illuminants.md)\\n  - white points are handled correctly throughout the conversions\\n  - multiple illuminant are built-in:\\n    - A *(Incandescent / Tungsten)*\\n    - B *(Direct sunlight at noon (obsolete))*\\n    - C *(Average / North sky Daylight (obsolete))*\\n    - D50 *(Horizon Light. ICC profile PCS)*\\n    - D55 *(Mid-morning / Mid-afternoon Daylight)*\\n    - D65 *(Noon Daylight: Television, sRGB color space)*\\n    - D75 *(North sky Daylight)*\\n    - E *(Equal energy)*\\n    - F2 *(Cool White Fluorescent)*\\n    - F7 *(D65 simulator, Daylight simulator)*\\n    - F11 *(Philips TL84, Ultralume 40)*\\n    - *(user-defined white points)*\\n- [Macbeth ColorChecker chart](docs/topic-macbeth-color-checker.md)\\n- [Changes between v2 and v3](docs/topic-changes-v2-v3.md)\\n\\nFor information about specific color spaces, see the following docs pages:\\n\\n- [RGB color spaces](docs/spaces-rgb.md)\\n  - support for both ordinary RGB and [linear RGB](http://stackoverflow.com/questions/12524623/what-are-the-practical-differences-when-working-with-colors-in-a-linear-vs-a-no)\\n  - multiple working spaces supported:\\n    - sRGB\\n    - Simplified sRGB\\n    - ECI RGB v2\\n    - Adobe RGB (1998)\\n    - Apple sRGB\\n    - Best RGB\\n    - Beta RGB\\n    - Bruce RGB\\n    - CIE RGB\\n    - ColorMatch RGB\\n    - Don RGB 4\\n    - Ekta Space PS5\\n    - NTSC RGB\\n    - PAL/SECAM RGB\\n    - ProPhoto RGB\\n    - SMPTE-C RGB\\n    - Wide Gamut RGB\\n    - Rec. 709 *(ITU-R Recommendation BT.709 &ndash; HDTV)*\\n    - Rec. 2020 *(ITU-R Recommendation BT.2020 &ndash; UHDTV)*\\n    - *(user-defined RGB working spaces)*\\n- [Lab color spaces](docs/spaces-lab.md)\\n  - CIE L\\\\*a\\\\*b\\\\* (1976) *(CIELAB)*\\n  - CIE L\\\\*C\\\\*h°<sub>ab</sub> *(CIELCH)*\\n  - Hunter Lab\\n- [Luv color spaces](docs/spaces-luv.md)\\n  - CIE L\\\\*u\\\\*v\\\\* (1976) *(CIELUV)*\\n  - CIE L\\\\*C\\\\*h°<sub>uv</sub> *(CIELCH)*\\n- [XYZ color space](docs/spaces-xyz.md)\\n  - CIE XYZ (1931)\\n  - CIE xyY *(derived from XYZ)*\\n- [J<sub>z</sub>a<sub>z</sub>b<sub>z</sub> color spaces](docs/spaces-jzazbz.md)\\n  - J<sub>z</sub>a<sub>z</sub>b<sub>z</sub> *(Safdar & al., 2017)*\\n  - J<sub>z</sub>C<sub>z</sub>h<sub>z</sub> *(polar of J<sub>z</sub>a<sub>z</sub>b<sub>z</sub>)*\\n- [LMS color space](docs/spaces-lms.md)\\n- [xy chromaticity](docs/spaces-xy.md)\\n'},\n",
       " {'repo': 'maloyan/Space',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': 'Satellite Image Difference\\n==============================\\n## Description\\n\\nWith a given two images of the same place from the different dates we need to figure out binary mask with changes\\n\\n[Selim\\'s pretrained models](https://github.com/selimsef/xview2_solution/releases/tag/0.0.1)\\n\\n## Train\\n1. Put data as shown below in tree structure of folders\\n2. Put pretrained models from [Selim](https://github.com/selimsef/xview2_solution/releases/tag/0.0.1) in a /models/pretrained folder. We need those for siamese net.\\n3. Change paths to the proper one in the config/ files\\n4. Run the training proccess with train.py. The first argument should be path to the config file \\n```\\npython train.py config/config_unet++_resnext50.json\\npython train.py config/config_siamese_seresnext50.json\\n```\\n5. After that you\\'ll have:\\n   -  saved models in /models/saved folder, logs of training\\n   -  logs of training proccess in /logs\\n   -  predicted non binary masks in /predicted_masks\\n\\n6. In the /notebooks/final_submission.ipynb generated a final submission file via averaging outputs from those two models\\n\\n## Solution description\\n1. I splited initial large image into small ones applying after that augmentation\\n2. Trained Unet++ with resnext50 backbone using [Segmentation models](https://github.com/qubvel/segmentation_models.pytorch) on 1 channel image difference\\n![image](https://github.com/selimsef/xview2_solution/raw/master/assets/dpn_unet.png)\\n\\n3. Trained Siamese net with seresnext50 backbone using models architecture from [xview2_solution](https://github.com/selimsef/xview2_solution) on RGB channel images \\n\\n![image](https://github.com/selimsef/xview2_solution/raw/master/assets/siamese_dpn.png)\\n\\n## Result\\n![image](https://sun9-11.userapi.com/impg/LVvt5FAwaDmlQoZTcl8s_HlHtpraQ9xXWhA5Hw/F0gUAVLeh5Y.jpg?size=613x850&quality=96&sign=d83b79097f9eee8a809523bb3769c75f&type=album)\\n\\nProject Organization\\n------------\\n\\n    ├── LICENSE\\n    ├── Makefile           <- Makefile with commands like `make data` or `make train`\\n    ├── README.md\\n    ├── data\\n    │\\xa0\\xa0 ├── Images                 <- Original 4 channel images from first and second dates.\\n    │\\xa0\\xa0 ├── Images_composit        <- Composed 8 channel images from original images.\\n    │\\xa0\\xa0 ├── mask                   <- Binary masks of images differences.\\n    │\\xa0\\xa0 ├── Rucode.xls             <- Table to match images from the same location.\\n    │\\xa0\\xa0 └── sample_submission.csv  <- Sample submission file.\\n    │\\n    ├── models             <- Trained and serialized models\\n    │\\xa0\\xa0 ├── pretrained     <- Pretrained models for siamese models from Selim.\\n    │\\xa0\\xa0 └── saved          <- Trained on the competition data models.\\n    |\\n    ├── notebooks          <- Jupyter notebooks.\\n    │\\n    ├── logs               <- Logs of training process: loss, IoU\\n    |\\n    ├── config             <- Configuration files for each type of model\\n    |\\n    ├── predicted_masks    <- Predicted probability masks in pickle format \\n    │\\n    ├── requirements.txt   <- The requirements file for reproducing the analysis environment,\\n    │                         `pip install -r requirements.txt`\\n    │\\n    ├── setup.py           <- makes project pip installable (pip install -e .) so src can be imported\\n    ├── train.py           <- Training process with evaluation on the end.\\n    ├── eval.py            <- Mask evaluation using the trained model.\\n    ├── src                <- Source code for use in this project.\\n    │\\xa0\\xa0 ├── __init__.py    <- Makes src a Python module\\n    │   │\\n    │\\xa0\\xa0 ├── models.py      <- Siamese models from Selim\\n    │\\xa0\\xa0 ├── dataset.py     <- Dataset class for satellite images\\n    │\\xa0\\xa0 └── utils.py       <- Small preprocess functions like normalization and decoding\\n    │\\n    └── tox.ini            <- tox file with settings for running tox; see tox.readthedocs.io\\n\\n\\n--------\\n\\n<p><small>Project based on the <a target=\"_blank\" href=\"https://drivendata.github.io/cookiecutter-data-science/\">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>\\n'},\n",
       " {'repo': 'rojo-rbx/rojo.space',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# rojo.space\\nThis is the Rojo website.\\n\\nThis website is built using [Docusaurus 2](https://docusaurus.io/).\\n\\n### Installation\\n\\n```\\n$ npm install\\n```\\n\\n### Local Development\\n\\n```\\n$ npm start\\n```\\n\\nThis command starts a local development server and opens up a browser window. Most changes are reflected live without having to restart the server.\\n\\n### Build\\n\\n```\\n$ npm run build\\n```\\n\\nThis command generates static content into the `build` directory and can be served using any static contents hosting service.'},\n",
       " {'repo': 'guardianproject/haven',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '[![Build Status](https://travis-ci.org/guardianproject/haven.svg)](https://travis-ci.org/guardianproject/haven)\\n\\n# About Haven\\n\\nHaven is for people who need a way to protect their personal areas and possessions without compromising their privacy. It is an Android application that leverages on-device sensors to provide monitoring and protection of physical areas. Haven turns any Android phone into a motion, sound, vibration and light detector, watching for unexpected guests and unwanted intruders. We designed Haven for investigative journalists, human rights defenders and people at risk of forced disappearance to create a new kind of herd immunity. By combining the array of sensors found in any smartphone, with the world\\'s most secure communications technologies, like Signal and Tor, Haven prevents the worst kind of people from silencing citizens without getting caught in the act.\\n\\n<img src=\"https://raw.githubusercontent.com/guardianproject/haven/master/fastlane/android/metadata/en-US/images/phoneScreenshots/havenob1.png\" width=\"25%\"> \\n<img src=\"https://raw.githubusercontent.com/guardianproject/haven/master/fastlane/android/metadata/en-US/images/phoneScreenshots/havenob2.png\" width=\"25%\"> \\n<img src=\"https://raw.githubusercontent.com/guardianproject/haven/master/fastlane/android/metadata/en-US/images/phoneScreenshots/havenob3.png\" width=\"25%\"> \\n\\nView our full [Haven App Overview](https://guardianproject.github.io/haven/docs/preso/) presentation for more about the origins and goals of the project.\\n\\n## Announcement and Public Beta\\n\\nWe are announcing Haven today, as an open-source project, along with a public beta release of the app. We are looking for contributors who understand that physical security is as important as digital, and who have an understanding and compassion for the kind of threats faced by the users and communities we want to support. We also think it is cool, cutting-edge and making use of encrypted messaging and onion routing in whole new ways. We believe Haven points the way to a more sophisticated approach to securing communication within networks of things and home automation system.\\n\\nLearn more about the story of this project at the links below:\\n\\n* [Haven: Building the Most Secure Baby Monitor Ever?](https://guardianproject.info/2017/12/22/haven-building-the-most-secure-baby-monitor-ever/)\\n* [Snowden’s New App Uses Your Smartphone To Physically Guard Your Laptop](https://theintercept.com/2017/12/22/snowdens-new-app-uses-your-smartphone-to-physically-guard-your-laptop/)\\n* [Snowden\\'s New App Turns Your Phone Into a Home Security System](https://www.wired.com/story/snowden-haven-app-turns-phone-into-home-security-system/)\\n\\n## Project Team\\n\\nHaven was developed through a collaboration between [Freedom of the Press Foundation](https://freedom.press) and [Guardian Project](https://guardianproject.info). Prototype funding was generously provided by FPF, and donations to support continuing work can be contributed through their site: https://freedom.press/donate-support-haven-open-source-project/\\n\\n![Freedom of the Press Foundation](https://raw.githubusercontent.com/guardianproject/haven/master/art/logos/fopflogo.png)\\n![Guardian Project](https://raw.githubusercontent.com/guardianproject/haven/master/art/logos/gplogo.png)\\n\\n## Safety through Sensors\\nHaven only records when triggered by sound and motion and stores everything locally on the device. You can position the device\\'s camera to capture visible motion or place your phone somewhere discreet to listen for noises. Receive secure notifications of intrusion events instantly or access logs remotely later.\\n\\n<img src=\"https://raw.githubusercontent.com/guardianproject/haven/master/fastlane/android/metadata/en-US/images/phoneScreenshots/haven-sound-config.png\" width=\"25%\"> \\n<img src=\"https://raw.githubusercontent.com/guardianproject/haven/master/fastlane/android/metadata/en-US/images/phoneScreenshots/haven-event-media.png\" width=\"25%\"> \\n<img src=\"https://raw.githubusercontent.com/guardianproject/haven/master/fastlane/android/metadata/en-US/images/phoneScreenshots/haven-event-list.png\" width=\"25%\"> \\n\\nThe following sensors are monitored for a measurable change, and then recorded to an event log on the device:\\n\\n-   **Accelerometer**: phone\\'s motion and vibration\\n-   **Camera**: motion in the phone\\'s visible surroundings from front or back camera\\n-   **Microphone**: noises in the environment\\n-   **Light**: change in light from ambient light sensor\\n-   **Power**: detect device being unplugged or power loss  \\n\\n## Building\\n\\nThe application can be built using Android Studio and Gradle. It relies on a number of third-party dependencies, all of which are free, open-source, and listed at the end of this document.\\n\\n## Install\\n\\nYou can currently get the Haven BETA release in one of three ways:\\n\\n* Download [Haven from Google Play](https://play.google.com/store/apps/details?id=org.havenapp.main)\\n* First, [install F-Droid](https://f-droid.org) the open-source app store, and second, add our Haven Nightly \"Bleeding Edge\" repository by scanning the QR Code below:\\n\\n<img src=\"https://guardianproject.github.io/haven-nightly/icon.png\" width=\"50%\"/> \\n\\nor add this repository manually in F-Droid\\'s Settings->Repositories: [https://guardianproject.github.io/haven-nightly/fdroid/repo/](https://guardianproject.github.io/haven-nightly/fdroid/repo/)\\n\\n* Grab the APK files from the [GitHub releases page](https://github.com/guardianproject/haven/releases)\\n\\nYou can, of course, build the app yourself, from source.\\n\\nIf you are an Android developer, you can learn more about how you can make use of F-Droid in your development workflow, for nightly builds, testing, reproducibility and more here: [F-Droid Documentation](https://f-droid.org/en/docs/)\\n\\n## Why no iPhone Support?\\n\\nWhile we hope to support a version of Haven that runs directly on iOS devices in the future, iPhone users can still benefit from Haven today. You can purchase an inexpensive Android phone for less than $100 and use it as your \"Haven Device\"; leaving it behind whilst you keep your iPhone on you. If you run Signal on your iPhone you can configure Haven on Android to send encrypted notifications, with photos and audio, directly to you. If you enable the \"Tor Onion Service\" feature in Haven (requires installation of \"Orbot\" app as well) you can remotely access all Haven log data from your iPhone using the Onion Browser app.\\n\\nSo, no, iPhone users we didn\\'t forget about you and we hope you will pick up an inexpensive Android burner today!\\n\\n## Usage\\n\\nHaven is meant to provide a smooth onboarding experience that walks users through configuring the sensors on their device to best detect intrusions into their environment. The current implementation has some of this implemented, but we are looking to improve this user experience dramatically.\\n\\n### Main view\\n\\nThe application\\'s main view allows the user to select which sensors to use along with their corresponding levels of sensitivity. A security code is required to disable monitoring, which must be provided by the user. A phone number can be set, to which a message will be sent if any of the sensors are triggered.\\n\\n### Notifications\\n\\nWhen one of the sensors is triggered (reaches the configured sensitivity threshold), notifications are sent through the following channels (if enabled):\\n\\n- SMS: a message is sent to the number specified when monitoring started\\n- Signal: if configured, can send end-to-end encryption notifications via Signal\\n\\nNote that it is not necessary to install the Signal app on the device that runs Haven. Doing so may invalidate the app\\'s previous Signal registration and safety numbers. Haven uses normal APIs to communicate via Signal.\\n\\nNotifications are sent through a service running in the background that is defined in class `MonitorService`.\\n\\n### Remote Access\\n\\nAll event logs and captured media can be remotely accessed through a [Tor Onion Service](https://www.torproject.org/docs/onion-services). Haven must be configured as an Onion Service and requires the device to also have [Orbot: Tor for Android](https://guardianproject.info/apps/orbot) installed and running. \\n\\n## ATTRIBUTIONS\\n\\nThis project contains source code or library dependencies from the following projects:\\n\\n* SecureIt project available at: https://github.com/mziccard/secureit Copyright (c) 2014 Marco Ziccardi (Modified BSD)\\n* libsignal-service-java from Open Whisper Systems: https://github.com/WhisperSystems/libsignal-service-java (GPLv3)\\n* Guardian Project fork of signal-cli from AsamK: https://github.com/AsamK/signal-cli , https://github.com/guardianproject/signal-cli-android (GPLv3)\\n* JayDeep\\'s AudioWife: https://github.com/jaydeepw/audio-wife (MIT)\\n* AppIntro: https://github.com/apl-devs/AppIntro (Apache 2)\\n* Guardian Project\\'s NetCipher: https://guardianproject.info/code/netcipher/ (Apache 2)\\n* NanoHttpd: https://github.com/NanoHttpd/nanohttpd (BSD)\\n* MaterialDateTimePicker from wdullaer: https://github.com/wdullaer/MaterialDateTimePicker (Apache 2)\\n* Fresco Image Viewer: https://github.com/stfalcon-studio/FrescoImageViewer (Apache 2)\\n* Facebook Fresco Image Library: https://github.com/facebook/fresco (BSD)\\n* Audio Waveform Viewer: https://github.com/derlio/audio-waveform (Apache 2)\\n* FireZenk\\'s AudioWaves: https://github.com/FireZenk/AudioWaves (MIT)\\n* MaxYou\\'s SimpleWaveform: https://github.com/maxyou/SimpleWaveform (MIT)\\n* Siralam\\'s fork of CameraViewPlus: https://github.com/siralam/CameraViewPlus (Apache License 2.0)\\n* Halil Ozercan\\'s BetterVideoPlayer: https://github.com/halilozercan/BetterVideoPlayer\\n* Reneto Silva\\'s easyrs: https://github.com/silvaren/easyrs (MIT)\\n* Google\\'s libphonenumber: https://github.com/googlei18n/libphonenumber (Apache License 2.0)\\n* Mike Penz\\'s AboutLibraries: https://github.com/mikepenz/AboutLibraries (Apache License 2.0)\\n'},\n",
       " {'repo': 'cosmonium/cosmonium',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"![Cosmonium](textures/cosmonium-name.png)\\n\\n[![Build Status](https://github.com/cosmonium/cosmonium/workflows/build/badge.svg)](https://github.com/cosmonium/cosmonium/actions)\\n[![Latest release](https://img.shields.io/github/v/release/cosmonium/cosmonium?label=Lastest%20release)](https://github.com/cosmonium/cosmonium/wiki/Download)\\n[![Latest build](https://img.shields.io/github/v/release/cosmonium/cosmonium?include_prereleases&label=Lastest%20build)](https://github.com/cosmonium/cosmonium/wiki/Download)\\n[![GitHub](https://img.shields.io/github/license/cosmonium/cosmonium)](https://github.com/cosmonium/cosmonium/blob/master/COPYING.md)\\n\\nCosmonium is a 3D astronomy and space exploration program. With Cosmonium you can navigate in our solar system and discover all the planets and their moons. You can also visit the neighboring stars and discover the true size of our galaxy and the Universe.\\n\\nCosmonium supports (or will support) the creation of fictional planets, stellar systems nebulaes, ... using procedural generation.\\n\\nCosmonium also already supports some Celestia addons (though CMOD and CelX are not yet supported).\\n\\n### Requirements\\n\\nCosmonium runs on Windows (Vista or above), Linux (CentOS 5, Ubuntu 14 or above) or macOS (mac0S 10.9 or above)\\nwith a graphic card supporting OpenGL 2.1 or better (OpenGL 4.5 is recommended) and at least 512MB of disk\\n(up to 4GB if the HD and UHD textures are installed).\\n\\n### Installation \\n\\nDownload the installer or package for your platform from the [download](https://github.com/cosmonium/cosmonium/wiki/Download) page and see the [[Installation]] page\\nThe package contains only low resolution textures, see [here](https://github.com/cosmonium/cosmonium/wiki/Download#extra-textures) to install extra HD and UHD textures.\\n\\n### Screenshots\\n\\nSee in the [Wiki](https://github.com/cosmonium/cosmonium/wiki/Screenshots) some screenshots of the application with views of\\n[Saturn](https://github.com/cosmonium/cosmonium/wiki/Screenshots#rings-of-saturn),\\n[Jupiter](https://github.com/cosmonium/cosmonium/wiki/Screenshots#io-casting-a-shadow-on-jupiter),\\n[Mars](https://github.com/cosmonium/cosmonium/wiki/Screenshots#phobos-over-mars),\\nthe [Moon](https://github.com/cosmonium/cosmonium/wiki/Screenshots#moon-crescent),\\n[procedural planets](https://github.com/cosmonium/cosmonium/wiki/Screenshots#procedural-planet), ...\\n\\n![Jupiter](https://github.com/cosmonium/cosmonium/wiki/screenshots/Io+Jupiter.png)\\n\\n### Launch\\n\\nSimply starts cosmonium from your application menu or from the cosmonium folder. See also the [installation](https://github.com/cosmonium/cosmonium/wiki/Installation) page for more options.\\n\\n### User interface\\n\\nCosmonium user interface is still heavily based on Celestia, most of the command and keyboard shortcuts work the same.\\nGo to [First steps](https://github.com/cosmonium/cosmonium/wiki/First-steps) to have an explanation of the basic command or see the [Control](https://github.com/cosmonium/cosmonium/wiki/Control) page for an exhaustive list.\\n\\n### Full documentation\\n\\nCosmonium is still in its infancy, but it is already usable to explore all the planets and the moons of our solar system, all the neighbor or visible stars and much more.\\nIt also support custom content and addons, either as Cosmonium or Celestia addons.\\n\\nThe full documentation is available in the [Wiki](https://github.com/cosmonium/cosmonium/wiki)\\n\\n### Bugs\\n\\nIf you encounter any problem to install or run Cosmonium, please don't hesitate to fill a bug report in the [issue tracker](https://github.com/cosmonium/cosmonium/issues) here on Github.\\n\\n## License \\n\\nCosmonium is (C) 2018-2022 Laurent Deru.\\n\\nThis program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 3 of the License, or (at your option) any later version.\\n\\nThis program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details, which you should have received along with this program. If not, request a copy from: Free Software Foundation, Inc. 59 Temple Place - Suite 330 Boston, MA 02111-1307 USA.\\n\\nCosmonium uses several third-party libraries which are subject to their own licenses,  see [Third-Party.md](Third-Party.md) for the complete list.\\n\\nCosmonium data (textures, models, orbital elements,..) come from many sources. Their respective copyright holder, license and reference are available in the info panel of the displayed object and in the related yaml file.\\n\\n## Powered by\\n\\n[![Python](https://github.com/cosmonium/cosmonium/wiki/images/python-powered-w-200x80.png)](http://www.python.org)\\n\\n[![Panda3D](https://github.com/cosmonium/cosmonium/wiki/images/panda3d_logo.png)](http://www.panda3d.org)\\n\"},\n",
       " {'repo': 'IBM/spacetech-kubesat',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# KubeSat\\n\\nKubeSat is an open-source project for building a cognitive, autonomous framework for satellite constellations and swarms. It provides the framework needed to develop and operate tasks to be performed on Satellite. Also, it allows for the simulation and optimization of multi-satellite communications.\\n\\n## How It Works \\n\\nKubeSat provides a framework to manage services for devices.\\n\\n### Architecture\\n\\n<div  align=\"center\">\\n<img src=\"./assets/kubesat-diagram.png\" width = \"85%\" align=\"center\">\\n</div>\\n\\n### KubeSat environment\\n\\n- __NATs server__ : NATs are messaging services that support a variety of network topologies. For KubeSat to communicate through NATs, a NATs cluster must be configured in advance. Refer to [NATs Cluster Configuration](https://docs.nats.io/nats-server/configuration/clustering) for NATs cluster configuration.\\n- __Kubernetes API__ : Kubernetes is a run-time environment to execute services. KubeSat uses Kubernetes API to manage Services in Kubernetes.\\n\\n### KubeSat library\\n\\n- __Resource Manager__: Resource Manager monitors jobs and system resources in the Kubernetes and performs a new task as Kubernetes Jobs upon request.\\n - __Service__: A service runs as a Kubernetes Job. It can be a general container for Kubernetes or a container developed using the KubeSat library.\\n - __Service Information__: Stores available service information that can be performed.\\n\\n### KubeSat Simulation\\n\\nKubeSat Simulation is developed using KubeSat library to simulate multi-satellite communications. It simulates accurate orbital mechanics for each object via OreKit; uses these calculations to place restrictions on communications between satellites, groundstation, and ground sensors; incorporates NATS.io messaging services; and publishes these communications for visualization on a web dashboard built using Cesium and Carbon. Refer to [KubeSat Simulation](/simulation/) to see the detail.\\n\\n## Getting Started\\n\\nTo develop a new service, start with [getting-started](/docs/getting-started.md).\\n\\nTo deploy the simulation, try [simulation quick start](/docs/simulation-quick-start.md).\\n\\nThere are examples in [examples](/examples).\\n\\n## License\\n\\nKubeSat is licensed under the Apache 2.0 license. Full license text is available at [LICENSE](LICENSE).\\n'},\n",
       " {'repo': 'futurice/space-tyckiting',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': \"# Space Tyckiting\\n\\n![Space Tyckiting banner](space-tyckiting-banner.jpg)\\n\\nWelcome to **Futurice Space Tyckiting**!\\n\\nThis repository contains the Space Tyckiting server and client skeletons.\\n\\nSpace Tyckiting server communicates via a *JSON-over-TCP* protocol, making it possible for the clients to be implemented in any language of your choice.\\n\\nReady-made client skeletons, which provide a convenient network communication layer, have been provided for a number of programming languages. You may also wish to create your very own client from scratch &mdash; no problem! Take a look at the example clients and documentation and code ahead! If you'd like to share your client skeleton, please issue a Pull Request.\\n\\nIf you discover any bugs or issues with the server or the provided client skeletons, please let us know, and if possible, issue a PR with a fix!\\n\\nPlease see the server [README](server/README.md), and the README's of the clients, for details on how to develop your AI and run Space Tyckiting.\\n\\nHappy Space Tyckiting!\\n\\n![Space Tyckiting](space-tyckiting.gif)\\n\"},\n",
       " {'repo': 'amymcgovern/spacesettlers',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '# spacesettlers\\nSpacewar/Spacesettlers game used by the AI class at the University of Oklahoma\\n\\nThis project stores the source code for the game Space Settlers, formerly known as Spacewar. This game is used to teach Artificial Intelligence at the University of Oklahoma. The game enables students to implement their own search, learning, multi-agent, and planning systems within a complex real-world type environment. They also learn how to use existing software packages. You are welcome to use this software in your class! Please cite one of the papers below.  Additional documentation is available on the github wiki pages and through the javadocs.\\n\\nMcGovern, Amy and Trytten, Deborah. (2013). Making In-Class Competitions Desirable For Marginalized Groups. Proceedings of the 2013 Frontiers in Education Conference, pages 704-706. \\n\\nMcGovern, Amy; Tidwell, Zachery and Rushing, Derek. (2011). Teaching Introductory Artificial Intelligence through Java-based Games. Proceedings of the symposium on Educational Advances in Artificial Intelligence. \\n\\nMcGovern, Amy and Fager, Jason. (2007) Creating Significant Learning Experiences in Introductory Artificial Intelligence. Proceedings of SIGCSE 2007, technical symposium on computer science education, pages 39-43. \\n'},\n",
       " {'repo': 'sindresorhus/camelcase',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': \"# camelcase\\n\\n> Convert a dash/dot/underscore/space separated string to camelCase or PascalCase: `foo-bar` → `fooBar`\\n\\nCorrectly handles Unicode strings.\\n\\nIf you use this on untrusted user input, don't forget to limit the length to something reasonable.\\n\\n## Install\\n\\n```sh\\nnpm install camelcase\\n```\\n\\n## Usage\\n\\n```js\\nimport camelCase from 'camelcase';\\n\\ncamelCase('foo-bar');\\n//=> 'fooBar'\\n\\ncamelCase('foo_bar');\\n//=> 'fooBar'\\n\\ncamelCase('Foo-Bar');\\n//=> 'fooBar'\\n\\ncamelCase('розовый_пушистый_единорог');\\n//=> 'розовыйПушистыйЕдинорог'\\n\\ncamelCase('Foo-Bar', {pascalCase: true});\\n//=> 'FooBar'\\n\\ncamelCase('--foo.bar', {pascalCase: false});\\n//=> 'fooBar'\\n\\ncamelCase('Foo-BAR', {preserveConsecutiveUppercase: true});\\n//=> 'fooBAR'\\n\\ncamelCase('fooBAR', {pascalCase: true, preserveConsecutiveUppercase: true}));\\n//=> 'FooBAR'\\n\\ncamelCase('foo bar');\\n//=> 'fooBar'\\n\\nconsole.log(process.argv[3]);\\n//=> '--foo-bar'\\ncamelCase(process.argv[3]);\\n//=> 'fooBar'\\n\\ncamelCase(['foo', 'bar']);\\n//=> 'fooBar'\\n\\ncamelCase(['__foo__', '--bar'], {pascalCase: true});\\n//=> 'FooBar'\\n\\ncamelCase(['foo', 'BAR'], {pascalCase: true, preserveConsecutiveUppercase: true})\\n//=> 'FooBAR'\\n\\ncamelCase('lorem-ipsum', {locale: 'en-US'});\\n//=> 'loremIpsum'\\n```\\n\\n## API\\n\\n### camelCase(input, options?)\\n\\n#### input\\n\\nType: `string | string[]`\\n\\nString to convert to camel case.\\n\\n#### options\\n\\nType: `object`\\n\\n##### pascalCase\\n\\nType: `boolean`\\\\\\nDefault: `false`\\n\\nUppercase the first character: `foo-bar` → `FooBar`\\n\\n##### preserveConsecutiveUppercase\\n\\nType: `boolean`\\\\\\nDefault: `false`\\n\\nPreserve consecutive uppercase characters: `foo-BAR` → `FooBAR`.\\n\\n##### locale\\n\\nType: `false | string | string[]`\\\\\\nDefault: The host environment’s current locale.\\n\\nThe locale parameter indicates the locale to be used to convert to upper/lower case according to any locale-specific case mappings. If multiple locales are given in an array, the best available locale is used.\\n\\n```js\\nimport camelCase from 'camelcase';\\n\\ncamelCase('lorem-ipsum', {locale: 'en-US'});\\n//=> 'loremIpsum'\\n\\ncamelCase('lorem-ipsum', {locale: 'tr-TR'});\\n//=> 'loremİpsum'\\n\\ncamelCase('lorem-ipsum', {locale: ['en-US', 'en-GB']});\\n//=> 'loremIpsum'\\n\\ncamelCase('lorem-ipsum', {locale: ['tr', 'TR', 'tr-TR']});\\n//=> 'loremİpsum'\\n```\\n\\nSetting `locale: false` ignores the platform locale and uses the [Unicode Default Case Conversion](https://unicode-org.github.io/icu/userguide/transforms/casemappings.html#simple-single-character-case-mapping) algorithm:\\n\\n```js\\nimport camelCase from 'camelcase';\\n\\n// On a platform with 'tr-TR'\\n\\ncamelCase('lorem-ipsum');\\n//=> 'loremİpsum'\\n\\ncamelCase('lorem-ipsum', {locale: false});\\n//=> 'loremIpsum'\\n```\\n\\n## camelcase for enterprise\\n\\nAvailable as part of the Tidelift Subscription.\\n\\nThe maintainers of camelcase and thousands of other packages are working with Tidelift to deliver commercial support and maintenance for the open source dependencies you use to build your applications. Save time, reduce risk, and improve code health, while paying the maintainers of the exact dependencies you use. [Learn more.](https://tidelift.com/subscription/pkg/npm-camelcase?utm_source=npm-camelcase&utm_medium=referral&utm_campaign=enterprise&utm_term=repo)\\n\\n## Related\\n\\n- [decamelize](https://github.com/sindresorhus/decamelize) - The inverse of this module\\n- [uppercamelcase](https://github.com/SamVerschueren/uppercamelcase) - Like this module, but to PascalCase instead of camelCase\\n- [titleize](https://github.com/sindresorhus/titleize) - Capitalize every word in string\\n- [humanize-string](https://github.com/sindresorhus/humanize-string) - Convert a camelized/dasherized/underscored string into a humanized one\\n- [camelcase-keys](https://github.com/sindresorhus/camelcase-keys) - Convert object keys to camel case\\n\"},\n",
       " {'repo': 'Adivise/SpaceSocial',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '<p align=\"center\">\\n<img src=\"https://capsule-render.vercel.app/api?type=waving&color=gradient&height=200&section=header&text=SpaceSocial&fontSize=80&fontAlignY=35&animation=twinkling&fontColor=gradient\"/> </a> \\n</p>\\n\\n<p align=\"center\"> \\n  <a href=\"https://discord.gg/SNG3dh3MbR\" target=\"_blank\"> <img src=\"https://discordapp.com/api/guilds/903043706410643496/widget.png?style=banner2\"/> </a> \\n</p>\\n\\n<p align=\"center\"> \\n  <a href=\"https://ko-fi.com/nanotect\" target=\"_blank\"> <img src=\"https://ko-fi.com/img/githubbutton_sm.svg\"/> </a> \\n</p>\\n\\n## ⁉ WARNING\\n\\n> This project is not complete have a bug sure!\\n\\n## 📑 Feature\\n- [x] SlashCommands\\n- [x] Economy System\\n- [x] Roulette (inspired **UnbelievaBoat** bot)\\n- [x] Clan System\\n- [x] Marry System\\n- [x] Gacha System (inspired **Genshin Impact**)\\n- [x] Auction System\\n- [x] Dark Auction System (inspired **Hypixel Skyblock**)\\n- [x] Shop System\\n- [x] Can config all System\\n- [x] Easy to use!\\n\\n## 📎 Requirements\\n\\n1. Node.js v16+ **[Download](https://nodejs.org/en/download/)**\\n2. Discord Bot Token **[Guide](https://discordjs.guide/preparations/setting-up-a-bot-application.html#creating-your-bot)**\\n3. MongaDB **[Download](https://www.mongodb.com/try/download/community)** (Download & install = Finish!)\\n\\n## 📚 Installation\\n\\n```\\ngit clone https://github.com/Adivise/SpaceSocial\\ncd SpaceSocial\\nnpm install\\n```\\n\\nAfter installation finishes you can use `node .` to start the bot. or `Run Start.bat`\\n\\n## 📄 Configuration\\n\\nCopy or Rename `.env.example` to `.env` and fill out the values:\\n\\n```.env\\nTOKEN=REPLACE_HERE\\nEMBED_COLOR=#000001\\nOWNER_ID=REPLACE_HERE\\nMONGO_URI=mongodb://127.0.0.1:27017/spacesocial\\n```\\n\\n## 🔩 Features & Commands\\n\\n> Note: The default prefix is \\'/\\' (SlashCommands)\\n\\n> Your can settings all in Folder `Settings`\\n\\n> Optional: [] | Required: ()\\n\\n💫 **General Commands!** \\n- `/leaderboard ([money] - [ticket] - [reputation]) [page]` - Check the leaderboard\\n- `/marry (user)` - Marry someone\\n- `/marry Divorce` - Divorce partner\\n- `/profile [user]` - Check your profile\\n- `/vote (user)` - Vote reputation\\n- `/ticket [user]` - Check your ticket\\n\\n💌 **Economy Commands!** \\n- `/money [user]` - Check your balance\\n- `/pay (user) (amount)` - Pay someone\\n- `/rob (user)` - Rob someone (In Progress, But Already Add!) **(Beta)**\\n- `/roulette (bet) (place)` - Play Roulette\\n- `/withdraw ([number] - [all])` - Withdraw money\\n- `/deposit ([number] - [all])` - Deposit money\\n- `/work` - Work and earn money\\n- `/crime` - Crime and earn money\\n\\n🚻 **Clan Commands!** (Beta)\\n- `/clan create (name)` - Create a clan (Use money to create)\\n- `/clan invite (user)` - Invite a user to your clan \\n- `/clan kick (user)` - Kick a member\\n- `/clan leave` - Leave your clan\\n- `/clan disband` - Disband your clan\\n- `/clan info (clan-tag)` - Check clan info\\n- `/clan list [page]` - Check clan list\\n- `/clan tranfer (user)` - Transfer clan ownership\\n- `/clan leaderboard ([level] - [money] - [member]) [page]` - Check clan leaderboard\\n- `/clan setting (icon)` - Change clan icon\\n- `/clan alliance add (clan-tag)` - Add a clan to your alliance\\n- `/clan alliance remove (clan-tag)` - Remove a clan from your alliance\\n- `/clan buy chat` - Buy a clan chat (Need Clan Role)\\n- `/clan buy voice` - Buy a clan voice (Need Clan Role)\\n- `/clan buy role` - Buy a clan role\\n- `/clan buy levelup` - Buy a clan levelup\\n- `/clan buy rename` - Buy a clan rename\\n- `/clan buy update` - Buy a clan update (Need Clan Role)\\n\\n🟥 **Special Commands!** (Beta)\\n- `/gacha (x1 & x10)` - Gacha and get a random ticket rarity (Use money to gacha)\\n- `/auction sell (role) (price)` - Sell a role\\n- `/auction buy (user) (id)` - Buy a role\\n- `/auction view ([global] - [user]) [page]` - View the auction\\n- `/darkaction start (role) (price)` - Start a darkaction (Access Only for Admins)\\n- `/shop (item)` - Buy item for you self!\\n\\n## 📝 Credits\\nDeveloped by [Adivise](https://github.com/Adivise)\\n'},\n",
       " {'repo': 'jimkon/Deep-Reinforcement-Learning-in-Large-Discrete-Action-Spaces',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"# Deep-Reinforcement-Learning-in-Large-Discrete-Action-Spaces\\nLink to [paper](https://arxiv.org/abs/1512.07679)\\n\\nImplementation of the algorithm in Python 3, TensorFlow and OpenAI Gym.\\n\\n\\n\\nThis paper introduces Wolpertinger training algorithm that extends the Deep Deterministic Policy Gradient training algorithm introduced in [this](https://arxiv.org/abs/1509.02971) paper.\\n\\nI used and extended  **stevenpjg**'s implementation of **DDPG** algorithm found [here](https://github.com/stevenpjg/ddpg-aigym) licensed under the MIT license.\\n\\nMaster is currently **only for continuous action spaces**.\\n\\nThe branch discrete-and-continuous provides the ability to use the discrete environments of the gym. \\n\"},\n",
       " {'repo': 'reime005/react-native-spaceviewer',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': \"# Space Viewer - Rocket Infos (Monorepo)\\n\\nNOTE: This project (API) is deprecated. Please see [Space Seek](https://github.com/reime005/spaceseek/) for the follow up.\\n\\n![iOS build](https://github.com/reime005/react-native-spaceviewer/workflows/iOS/badge.svg)\\n\\n![Android build](https://github.com/reime005/react-native-spaceviewer/workflows/Android/badge.svg)\\n\\nThis is the source code for the iOS App ['Space Viewer - Rocket Infos'](https://itunes.apple.com/us/app/space-viewer-rocket-infos/id1434055829?ls=1&mt=8) and Android App ['Space Viewer - Information about Rocket Launches'](https://play.google.com/store/apps/details?id=com.mariusreimer.spaceviewer). You can see lots of information about rocket space launches from all over the world! Want to know at which location it will launch? Or do you want to see its live stream? All the information are bundled in this app.\\n\\nThis includes rocket launches from SpaceX, NASA, ROSCOSMOS, ISRO, ULA and many more!\\n\\n![space-viewer](packages/mobile/mockup.png)\\n\\n## Tech Stack\\n\\n* Monorepo (yarn workspace)\\n* React Native (without Expo)\\n* Code Push by Microsoft App Center\\n* Redux-Saga for asynchronous actions\\n* Moment.Js for date operations\\n\\nReact Native project can be found at [packages/mobile](packages/mobile)\\n\\n## Build & Run\\nShould be as easy as:\\n\\n      yarn; yarn mobile/start\\n      yarn run-android\\n      yarn run-ios\\n\\n## Credits / Special Thanks\\n\\n* All the data that is publicly available via the [LaunchLibrary.net](https://launchlibrary.net) API.\\n* Icon made by [Freepik](https://www.freepik.com) from [www.flaticon.com](https://www.flaticon.com)\\n\\n## License\\nCopyright © Marius Reimer\\n\\nDistributed under the [Apache 2 License](http://www.apache.org/licenses/LICENSE-2.0.html).\\n\"},\n",
       " {'repo': 'dylang/space-hogs',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': \"# Space Hogs [![Build Status](https://travis-ci.org/dylang/space-hogs.svg?branch=master)](https://travis-ci.org/dylang/space-hogs)\\n\\n> Discover surprisingly large directories from the command line.\\n\\n\\n```\\n~/projects/npm $ space-hogs\\n151 MB ~/projects/npm\\nLargest children directories, each larger than 9 MB\\n├──  31 MB [▒   ] /.git\\n├──   9 MB [▒   ] /node_modules/npm-registry-couchapp/node_modules\\n├──  12 MB [▒   ] /node_modules/tap/node_modules/nyc/node_modules\\n├──  20 MB [▒   ] /node_modules/standard/node_modules/standard-engine/node_modules/eslint/node_modules\\n├──  17 MB [▒   ] /node_modules/standard/node_modules/standard-format/node_modules/esformatter-jsx/node_modules/babel-core/node_modules\\n└──  62 MB [▒▒  ] (everything else)\\n    151 MB Total\\n```\\n\\n```\\n~/projects/npm $ space-hogs node_modules 5 --depth=0\\n114 MB ~/projects/npm/node_modules\\nLargest children directories, each larger than 5 MB\\n├──   6 MB [▒   ] /node-gyp\\n├──  11 MB [▒   ] /npm-registry-couchapp\\n├──  27 MB [▒   ] /tap\\n├──  56 MB [▒▒  ] /standard\\n└──  13 MB [▒   ] (everything else)\\n    114 MB Total\\n```\\n\\n\\n## Install\\n\\n```\\n$ npm i -g space-hogs\\n```\\n\\n\\n## Usage\\n\\n```\\n$ space-hogs --help\\n\\n    Usage\\n      space-hogs [path] [size] [--depth=number]\\n\\n    Options\\n      directory        Directory to scan. Defaults to the current directory.\\n      size             Minimum size in MB. Defaults to 6% of the total MB.\\n      --depth=number   Number of sub-directories to dive into. 0 = none. Defaults to all.\\n\\n    Examples\\n\\n      $ space-hogs\\n      $ space-hogs node_modules 5 --depth=0\\n      $ space-hogs 1000\\n```\\n\\n### Tips\\n\\n* I don't recommend using this on the root of your drive, there are [better tools](#similar-tools) for exploring your entire disk.\\n* There isn't a real API yet, it will always output to the console, but I hope to have a promise-based API.\\n* Test coverage isn't really there yet.\\n\\n## Contributions\\n\\nI'm happy to take contributions.\\n\\nHere's some ideas:\\n\\n### Colors\\n\\n* Show me what it should look like, or make a PR using [chalk](https://github.com/chalk/chalk).\\n* Even though all terminals should have a black background, some people use white, and we should be attentive to that when picking colors.\\n\\n### API + Refactoring\\n\\n* I feel that my recursive promise implementation could be done better with [observables/RxJS](https://github.com/Reactive-Extensions/RxJS).\\n* I think this will make it possible to have a good API.\\n* If this doesn't make sense it's probably because I don't know enough about observables/RxJS.\\n\\n### Cross-platform (aka Windows support)\\n\\n* I currently use `du` for calculating disk usage. This won't work in Windows.\\n* If you would like to help make it work in Windows let me know and start working on a pull request.\\n\\n### Performance\\n\\n* I wonder if there are faster ways to get disk usage than `du` using native code. Ideas?\\n\\n### Test Coverage\\n\\n* This project uses the [AVA test runner](https://github.com/sindresorhus/ava), I recommend trying it!\\n* Help me improve the coverage.\\n\\n### Troubleshooting\\n\\n* This is meant for projects, *not for checking your entire disk, or even all of `/usr/`*.\\n* Add `--debug` and to get some debug info that will be helpful for creating tickets.\\n* Windows is not yet supported\\n* Versions of node before 4 are not supported.\\n\\n## Inspiration\\n\\nSuper awesome and prolific node module creator Sindre Sorhus has a [repo for sharing ideas for new modules](https://github.com/sindresorhus/module-requests/issues).\\nOn Feb 10, 2016, I submitted a proposal for [space-hogs: cli for discovering surprisingly large directories](https://github.com/sindresorhus/module-requests/issues/59).\\nThere was a lot of interest, but nobody else created it, so I decided to see if I could.\\n\\n## Similar Tools\\n\\n* [ndu](https://github.com/groupon/ndu) - Generates a web page with an ordinal graph showing how much space node modules take up.\\n* [WinDirStat](https://windirstat.info/) - Windows only, GUI only, shows every directory, not just the largest offenders.\\n* [Grand Perspective](http://grandperspectiv.sourceforge.net/) - Mac only, GUI only, shows every directory, not just the largest offenders.\\n\\n### About the Author\\n\\nHi! My name is **Dylan Greene**. When not overwhelmed with my two young kids I enjoy contributing\\nto the open source community. I'm also a tech lead at [Opower](http://opower.com). [![@dylang](https://img.shields.io/badge/twitter-dylang-blue.svg)](https://twitter.com/dylang)\\n\\n## License\\n\\nMIT © [Dylan Greene](https://github.com/dylang)\\n\"},\n",
       " {'repo': 'spedas/pyspedas',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '\\n# PySPEDAS\\n[![build](https://github.com/spedas/pyspedas/workflows/build/badge.svg)](https://github.com/spedas/pyspedas/actions)\\n[![Coverage Status](https://coveralls.io/repos/github/spedas/pyspedas/badge.svg)](https://coveralls.io/github/spedas/pyspedas)\\n[![Version](https://img.shields.io/pypi/v/pyspedas.svg)](https://pypi.org/project/pyspedas/)\\n![Status](https://img.shields.io/pypi/status/pyspedas.svg)\\n![License](https://img.shields.io/pypi/l/pyspedas.svg)\\n\\nThe Python-based Space Physics Environment Data Analysis Software (PySPEDAS) framework supports multi-mission, multi-instrument retrieval, analysis, and visualization of heliophysics time series data.\\n\\n## Projects Supported\\n- [Advanced Composition Explorer (ACE)](https://pyspedas.readthedocs.io/en/latest/ace.html)\\n- [Akebono](https://pyspedas.readthedocs.io/en/latest/akebono.html)\\n- [Arase (ERG)](https://pyspedas.readthedocs.io/en/latest/erg.html)\\n- [Cluster](https://pyspedas.readthedocs.io/en/latest/cluster.html)\\n- [Colorado Student Space Weather Experiment (CSSWE)](https://pyspedas.readthedocs.io/en/latest/csswe.html)\\n- [Communications/Navigation Outage Forecasting System (C/NOFS)](https://pyspedas.readthedocs.io/en/latest/cnofs.html)\\n- [Deep Space Climate Observatory (DSCOVR)](https://pyspedas.readthedocs.io/en/latest/dscovr.html)\\n- [Dynamics Explorer 2 (DE2)](https://pyspedas.readthedocs.io/en/latest/de2.html)\\n- [Equator-S](https://pyspedas.readthedocs.io/en/latest/equator-s.html)\\n- [Fast Auroral Snapshot Explorer (FAST)](https://pyspedas.readthedocs.io/en/latest/fast.html)\\n- [Geotail](https://pyspedas.readthedocs.io/en/latest/geotail.html)\\n- [Geostationary Operational Environmental Satellite (GOES)](https://pyspedas.readthedocs.io/en/latest/goes.html)\\n- [Imager for Magnetopause-to-Aurora Global Exploration (IMAGE)](https://pyspedas.readthedocs.io/en/latest/image.html)\\n- [Kyoto Dst Index](https://pyspedas.readthedocs.io/en/latest/kyoto.html)\\n- [LANL](https://pyspedas.readthedocs.io/en/latest/lanl.html)\\n- [Mars Atmosphere and Volatile Evolution (MAVEN)](https://pyspedas.readthedocs.io/en/latest/maven.html)\\n- [Magnetic Induction Coil Array (MICA)](https://pyspedas.readthedocs.io/en/latest/mica.html)\\n- [Magnetospheric Multiscale (MMS)](https://pyspedas.readthedocs.io/en/latest/mms.html)\\n- [OMNI](https://pyspedas.readthedocs.io/en/latest/omni.html)\\n- [Polar Orbiting Environmental Satellites (POES)](https://pyspedas.readthedocs.io/en/latest/poes.html)\\n- [Polar](https://pyspedas.readthedocs.io/en/latest/polar.html)\\n- [Parker Solar Probe (PSP)](https://pyspedas.readthedocs.io/en/latest/psp.html)\\n- [Solar Orbiter (SOLO)](https://pyspedas.readthedocs.io/en/latest/solo.html)\\n- [Solar Terrestrial Relations Observatory (STEREO)](https://pyspedas.readthedocs.io/en/latest/stereo.html)\\n- [Space Technology 5 (ST5)](https://pyspedas.readthedocs.io/en/latest/st5.html)\\n- [Spherical Elementary Currents (SECS)](https://github.com/spedas/pyspedas/blob/master/pyspedas/secs/README.md)\\n- [Swarm](https://github.com/spedas/pyspedas/blob/master/pyspedas/swarm/README.md)\\n- [Time History of Events and Macroscale Interactions during Substorms (THEMIS)](https://pyspedas.readthedocs.io/en/latest/themis.html)\\n- [Two Wide-Angle Imaging Neutral-Atom Spectrometers (TWINS)](https://pyspedas.readthedocs.io/en/latest/twins.html)\\n- [Ulysses](https://pyspedas.readthedocs.io/en/latest/ulysses.html)\\n- [Van Allen Probes (RBSP)](https://pyspedas.readthedocs.io/en/latest/rbsp.html)\\n- [Wind](https://pyspedas.readthedocs.io/en/latest/wind.html)\\n\\n## Requirements\\n\\nPython 3.7+ is required.  \\n\\nWe recommend [Anaconda](https://www.continuum.io/downloads/) which comes with a suite of packages useful for scientific data analysis. Step-by-step instructions for installing Anaconda can be found at: [Windows](https://docs.anaconda.com/anaconda/install/windows/), [macOS](https://docs.anaconda.com/anaconda/install/mac-os/), [Linux](https://docs.anaconda.com/anaconda/install/linux/)\\n\\n## Installation\\n\\n### Virtual Environment\\nTo avoid potential dependency issues with other Python packages, we suggest creating a virtual environment for PySPEDAS; you can create a virtual environment in your terminal with:\\n\\n```bash\\npython -m venv pyspedas\\n```\\n\\nTo enter your virtual environment, run the \\'activate\\' script:\\n\\n#### Windows\\n\\n```bash\\n.\\\\pyspedas\\\\Scripts\\\\activate\\n```\\n\\n#### macOS and Linux\\n\\n```bash\\nsource pyspedas/bin/activate\\n```\\n\\n#### Using Jupyter notebooks with your virtual environment\\n\\nTo get virtual environments working with Jupyter, in the virtual environment, type:\\n\\n```bash\\npip install ipykernel\\npython -m ipykernel install --user --name pyspedas --display-name \"Python (pySPEDAS)\"\\n```\\n\\n(note: \"pyspedas\" is the name of your virtual environment)\\n\\nThen once you open the notebook, go to \"Kernel\" then \"Change kernel\" and select the one named \"Python (PySPEDAS)\"\\n\\n### Install\\nPySPEDAS supports Windows, macOS and Linux. To get started, install the `pyspedas` package using PyPI:\\n\\n```bash\\npip install pyspedas\\n```\\n\\n### Upgrade\\n\\nTo upgrade to the latest version of PySPEDAS:\\n\\n```bash\\npip install pyspedas --upgrade\\n```\\n\\n## Local Data Directories\\n\\nThe recommended way of setting your local data directory is to set the `SPEDAS_DATA_DIR` environment variable. `SPEDAS_DATA_DIR` acts as a root data directory for all missions, and will also be used by IDL (if you’re running a recent copy of the bleeding edge).\\n\\nMission specific data directories (e.g., `MMS_DATA_DIR` for MMS, `THM_DATA_DIR` for THEMIS) can also be set, and these will override `SPEDAS_DATA_DIR`\\n\\n## Usage\\n\\nTo get started, import pyspedas and pytplot:\\n\\n```python\\nimport pyspedas\\nfrom pytplot import tplot\\n```\\n\\nYou can load data into tplot variables by calling `pyspedas.mission.instrument()`, e.g., \\n\\nTo load and plot 1 day of THEMIS FGM data for probe \\'d\\':\\n```python\\nthm_fgm = pyspedas.themis.fgm(trange=[\\'2015-10-16\\', \\'2015-10-17\\'], probe=\\'d\\')\\n\\ntplot([\\'thd_fgs_gse\\', \\'thd_fgs_gsm\\'])\\n```\\n\\nTo load and plot 2 minutes of MMS burst mode FGM data:\\n```python\\nmms_fgm = pyspedas.mms.fgm(trange=[\\'2015-10-16/13:05:30\\', \\'2015-10-16/13:07:30\\'], data_rate=\\'brst\\')\\n\\ntplot([\\'mms1_fgm_b_gse_brst_l2\\', \\'mms1_fgm_b_gsm_brst_l2\\'])\\n```\\n\\nNote: by default, PySPEDAS loads all data contained in CDFs found within the requested time range; this can potentially load data outside of your requested trange. To remove the data outside of your requested trange, set the `time_clip` keyword to `True`\\n\\nTo load and plot 6 hours of PSP SWEAP/SPAN-i data:\\n```python\\nspi_vars = pyspedas.psp.spi(trange=[\\'2018-11-5\\', \\'2018-11-5/06:00\\'], time_clip=True)\\n\\ntplot([\\'DENS\\', \\'VEL\\', \\'T_TENSOR\\', \\'TEMP\\'])\\n```\\n\\nTo download 5 days of STEREO magnetometer data (but not load them into tplot variables):\\n```python\\nstereo_files = pyspedas.stereo.mag(trange=[\\'2013-11-1\\', \\'2013-11-6\\'], downloadonly=True)\\n```\\n\\n### Standard Options\\n- `trange`: two-element list specifying the time range of interest. This keyword accepts a wide range of formats\\n- `time_clip`: if set, clip the variables to the exact time range specified by the `trange` keyword \\n- `suffix`: string specifying a suffix to append to the loaded variables\\n- `varformat`: string specifying which CDF variables to load; accepts the wild cards * and ?\\n- `varnames`: string specifying which CDF variables to load (exact names)\\n- `get_support_data`: if set, load the support variables from the CDFs\\n- `downloadonly`: if set, download the files but do not load them into tplot\\n- `no_update`: if set, only load the data from the local cache\\n- `notplot`: if set, load the variables into dictionaries containing numpy arrays (instead of creating the tplot variables)\\n\\n## Examples\\nPlease see the following notebooks for examples of using PySPEDAS\\n\\n### Plotting\\n- [Annotations](https://github.com/spedas/pyspedas_examples/blob/master/pyspedas_examples/notebooks/PyTplot_annotations.ipynb)\\n- [Range options](https://github.com/spedas/pyspedas_examples/blob/master/pyspedas_examples/notebooks/PyTplot_range_options.ipynb)\\n- [Spectrogram options](https://github.com/spedas/pyspedas_examples/blob/master/pyspedas_examples/notebooks/PyTplot_spectrogram_options.ipynb)\\n- [Legend options](https://github.com/spedas/pyspedas_examples/blob/master/pyspedas_examples/notebooks/PyTplot_legend_options.ipynb)\\n- [Markers and symbols](https://github.com/spedas/pyspedas_examples/blob/master/pyspedas_examples/notebooks/PyTplot_markers_and_symbols.ipynb)\\n- [Error bars](https://github.com/spedas/pyspedas_examples/blob/master/pyspedas_examples/notebooks/PyTplot_error_bars.ipynb)\\n- [Pseudo variables](https://github.com/spedas/pyspedas_examples/blob/master/pyspedas_examples/notebooks/PyTplot_pseudo_variables.ipynb)\\n- [Highlight intervals and vertical bars](https://github.com/spedas/pyspedas_examples/blob/master/pyspedas_examples/notebooks/PyTplot_highlight_intervals_and_vertical_bars.ipynb)\\n\\n### Loading Data\\n- [MMS examples](https://github.com/spedas/mms-examples/tree/master/basic)\\n- [THEMIS examples](https://github.com/spedas/themis-examples/tree/main/basic)\\n- [Load data from HAPI servers](https://github.com/spedas/pyspedas_examples/blob/master/pyspedas_examples/notebooks/PySPEDAS_loading_data_from_HAPI_servers.ipynb)\\n- [Exploring the Heliosphere with Python](https://github.com/spedas/pyspedas_examples/blob/master/pyspedas_examples/notebooks/Exploring_the_Heliosphere_with_Python.ipynb)\\n\\nAdditional examples of loading and plotting data can be found in the documentation for the project you\\'re interested in ([PySPEDAS projects](https://pyspedas.readthedocs.io/en/latest/projects.html)), as well as the project\\'s README file.\\n\\n### Dates and Times\\n- [Working with dates and times](https://github.com/spedas/pyspedas_examples/blob/master/pyspedas_examples/notebooks/Working_with_dates_and_times_with_PySPEDAS_PyTplot.ipynb)\\n\\n### Coordinate Transformations\\n- [Coordinate transformations](https://github.com/spedas/pyspedas_examples/blob/master/pyspedas_examples/notebooks/Coordinate_transformations_with_OMNI_data.ipynb)\\n- [Transforming MMS data to boundary normal (LMN) coordinates](https://github.com/spedas/mms-examples/blob/master/advanced/MMS_LMN_coordinate_transformation.ipynb)\\n- [Quaternion transformations with SpacePy](https://github.com/spedas/mms-examples/blob/master/basic/MMS_quaternion_coordinate_transformations.ipynb)\\n\\n### Analysis\\n- [Plasma calculations with PlasmaPy](https://github.com/spedas/mms-examples/blob/master/advanced/Plasma%20calculations%20with%20PlasmaPy.ipynb)\\n- [Calculating Poynting flux with MMS data](https://github.com/spedas/mms-examples/blob/master/advanced/Poynting_flux_with_MMS_data.ipynb)\\n- [Plasma beta with MMS data](https://github.com/spedas/mms-examples/blob/master/basic/Plasma%20Beta%20with%20FGM%20and%20FPI%20data.ipynb) (note: the PlasmaPy notebook above shows a much easier method)\\n- [MMS curlometer calculations](https://github.com/spedas/mms-examples/blob/master/basic/Curlometer%20Technique.ipynb)\\n- [Wave polarization calculations](https://github.com/spedas/mms-examples/blob/master/advanced/Wave_polarization_using_SCM_data.ipynb)\\n- [Dynamic power spectra calculations](https://github.com/spedas/mms-examples/blob/master/basic/Search-coil%20Magnetometer%20(SCM).ipynb)\\n- [2D slices of MMS distribution functions](https://github.com/spedas/mms-examples/blob/master/advanced/Generate_2D_slices_of_FPI_and_HPCA_data.ipynb)\\n- [Generating spectrograms and moments from MMS distribution functions](https://github.com/spedas/mms-examples/blob/master/advanced/Generate%20spectrograms%20and%20moments%20with%20mms_part_getspec.ipynb)\\n\\n\\n## Documentation\\nFor more information, please see our HTML documentation at: \\n\\nhttps://pyspedas.readthedocs.io/\\n\\n## Getting Help\\nTo find the options supported, call `help` on the instrument function you\\'re interested in:\\n```python\\nhelp(pyspedas.themis.fgm)\\n```\\n\\nYou can ask questions by creating an issue or by joining the [SPEDAS mailing list](http://spedas.org/mailman/listinfo/spedas-list_spedas.org).\\n\\n## Contributing\\nWe welcome contributions to PySPEDAS; to learn how you can contribute, please see our [Contributing Guide](https://github.com/spedas/pyspedas/blob/master/CONTRIBUTING.md)\\n\\n## Code of Conduct\\nIn the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. To learn more, please see our [Code of Conduct](https://github.com/spedas/pyspedas/blob/master/CODE_OF_CONDUCT.md).\\n\\n## Additional Information\\n\\nFor examples of pyspedas, see: https://github.com/spedas/pyspedas_examples\\n\\nFor MMS examples, see: https://github.com/spedas/mms-examples\\n\\nFor pytplot, see: https://github.com/MAVENSDC/PyTplot\\n\\nFor cdflib, see: https://github.com/MAVENSDC/cdflib\\n\\nFor SPEDAS, see http://spedas.org/\\n'},\n",
       " {'repo': 'fanfoujs/space-fanfou',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '<img src=\"https://raw.githubusercontent.com/fanfoujs/space-fanfou/master/static/icons/icon-256.png\" alt=\"太空饭否 logo\" align=\"right\" width=\"82px\" />\\n\\n# 太空饭否\\n\\n[![](https://github.com/fanfoujs/space-fanfou/workflows/Node%20CI/badge.svg)](https://github.com/fanfoujs/space-fanfou/actions)\\n[![](https://img.shields.io/github/release/fanfoujs/space-fanfou.svg)](https://github.com/fanfoujs/space-fanfou/releases)\\n[![](https://img.shields.io/github/license/fanfoujs/space-fanfou.svg)](https://github.com/fanfoujs/space-fanfou/blob/master/LICENSE)\\n\\n### 简介\\n\\n太空饭否是一个免费、用心的开源项目，是目前最强大最好用的饭否浏览器扩展。可以给饭否添加回复和转发展开、桌面通知、浮动输入框、多用户切换、消息批量管理、自动翻页等功能，并且使饭否页面变得更美更舒心，符合您的使用习惯。\\n\\n### 团队\\n\\n<!-- Generated by https://jakebathman.github.io/Markdown-Table-Generator/ -->\\n<!-- [@太空小孩](https://fanfou.com/anegie),[@锐风](https://fanfou.com/ruif),[@饭小默](https://fanfou.com/lito),[@Xidorn](https://fanfou.com/xidorn),[@.rex](https://fanfou.com/zhasm) -->\\n**[@太空小孩](https://fanfou.com/anegie)**|**[@锐风](https://fanfou.com/ruif)**|**[@饭小默](https://fanfou.com/lito)**|**[@Xidorn](https://fanfou.com/xidorn)**|**[@.rex](https://fanfou.com/zhasm)**\\n:-----:|:-----:|:-----:|:-----:|:-----:\\n\\n### 下载安装\\n\\n请使用 Chrome 浏览器访问网上应用店获取插件下载。\\n\\n<a href=\"https://chrome.google.com/webstore/detail/mfofmcdbaeajgdeihmcjjohmhepcdcol\">\\n  <img src=\"https://raw.githubusercontent.com/fanfoujs/space-fanfou/master/media/chrome-web-store-badge.png\" alt=\"Download on the Chrome Web Store\" width=\"248px\" height=\"75px\" />\\n</a>\\n\\n### 参阅\\n\\n<!-- Generated by https://jakebathman.github.io/Markdown-Table-Generator/ -->\\n<!-- → [官方饭否](https://fanfou.com/spacefanfou),→ [入门手册](https://spacekid.me/spacefanfou/) -->\\n**→ [官方饭否](https://fanfou.com/spacefanfou)**|**→ [入门手册](https://spacekid.me/spacefanfou/)**\\n:-----:|:-----:\\n\\n### 协议\\n\\n基于 [GPL v3](COPYING) 协议发布。版权所有 © 2011-2020 太空饭否开发组。\\n'},\n",
       " {'repo': 'TBartl/space-station-13-idle',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Space Station Idle\\n\\nSpace Station Idle is a web-based idle game based on Melvor Idle, and set in the Space Station 13 universe.\\n\\nThere\\'s very little public documentation on implementation right now.\\n\\nIf you want to contribute anyway, pull requests are open!\\n\\n___\\n\\n## Setup and Deployment\\n\\n### Prerequsites\\n\\nYou will need Git Bash as well as Node.js installed.\\n\\nIf you do not already have Vue CLI installed, continue reading, otherwise skip to the next header.\\n\\nTo install Vue CLI, type the following into Git Bash:\\n\\n```\\nnpm install -g @vue/cli\\n```\\n\\nIf it appears frozen, let it run. It can take from just a minute to tens of minutes, depending on your system.\\n\\n\\n### Project setup\\n\\nTo prepare a local version of the game for testing, type the following:\\n\\n```\\nnpm install\\n```\\n\\nAgain, if it appears frozen, let it run. It can take from just a minute to tens of minutes, depending on your system.\\n\\nTo launch a hot-reloading copy of your repo, type the following: (You will also only need to run this command on subsequent launches of the live edit page.)\\n\\n```\\nnpm run serve\\n```\\n\\n\\n### Deploy To Github\\n\\nFirst, edit `deployGit.sh` as well as `vue.config.js` with your repo address (You will only need to edit `vue.config.js` if you modify the name of the repo, such as cloning vs forking.)\\n\\nThen, run the following to deploy to the `gh-pages` branch of your repo.\\n\\n```\\n./deployGit.sh\\n```\\n! This requires admin access to the repository in your `deployGit.sh` file.\\n\\n### Deploy to AWS\\n```\\n./deployAws.sh\\n```\\n! Requires a signed in AWS account, `aws configure`\\n___\\n\\n## Implementation Information\\n\\nTODO: Fill out with item items, actions, enemies, etc...\\n\\n### How to add a new job\\n1. Create and populate job data and actions in a new file under the `src/data/` folder\\n2. Add the job in the `src/data/jobs.js` file\\n3. Create and populate new item data in a new file under the `src/data/items/` folder\\n4. Add the items in the `src/data/items.js` file\\n5. Create a module for the job in a new file under the `src/state/`\\n6. Add the job module in the `src/state/store.js` file. Note that it must be added as both a module and in the initial state\\n7. Add a new content file for the job under `src/components/content/`\\n8. Add the new file in the `src/components/content/ContentWrapper.vue` file\\n\\n### Understanding drop tables\\nThere are multiple valid ways to make drop tables:\\n\\n**A) As a string with just the itemId :** a single item of that type will always be dropped\\n```\\nitem: \\'iron\\'\\n```\\n\\n**B) As an object with a static count:** a static number of that type will be dropped \\n```\\nitems: {\\n\\tid: \\'iron\\',\\n\\tcount: 2\\n}\\n```\\n\\n**C) As an object with a range count:** any number of items between that range (inclusive) will be dropped\\n```\\nitems: {\\n\\tid: \\'iron\\',\\n\\tcount: [0, 5]\\n}\\n```\\n**D) As multiple weighted objects in a list:** the higher the weight of the item, the more likely it is to drop. The weights do not need to add up to 100. Previous count syntax is also applicable, though not required. \\n```\\nitemTable: [\\n\\t{\\n\\t\\tid: \"iron\",\\n\\t\\tcount: [1, 3],\\n\\t\\tweight: 6\\n\\t}, {\\n\\t\\tid: \"silver\",\\n\\t\\tcount: 2,\\n\\t\\tweight: 2\\n\\t}, {\\n\\t\\tid: \"gold\",\\n\\t\\tweight: 1\\n\\t},\\n\\t{\\n\\t\\tid: null,\\n\\t\\tweight: 1\\n\\t}\\n]\\n```\\n\\n**E) As multiple lists of other combinations:** You can have more than one type of item drop by defining multiple item tables, each with their own chance.\\n```\\nitemTables: [\\n\\t{\\n\\t\\tchance: 1,\\n\\t\\titem: \\'iron\\'\\n\\t},\\n\\t{\\n\\t\\tchance: 0.1,\\n\\t\\titems: {\\n\\t\\t\\tid: \\'silver\\',\\n\\t\\t\\tcount: [0, 2]\\n\\t\\t}\\n\\t},\\n\\t{\\n\\t\\tchance: 0.01,\\n\\t\\titemTable: [\\n\\t\\t\\t{\\n\\t\\t\\t\\tid: \\'gold\\',\\n\\t\\t\\t\\tweight: 3\\n\\t\\t\\t},\\n\\t\\t\\t{\\n\\t\\t\\t\\tid: \\'titanium\\',\\n\\t\\t\\t\\tweight: 1\\n\\t\\t\\t}\\n\\t\\t]\\n\\t}\\n]\\n```\\n'},\n",
       " {'repo': 'alessandrokonrad/spacebudz',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '<p align=\"center\">\\n  <a href=\"https://www.gatsbyjs.com/?utm_source=starter&utm_medium=readme&utm_campaign=minimal-starter\">\\n    <img alt=\"Gatsby\" src=\"https://www.gatsbyjs.com/Gatsby-Monogram.svg\" width=\"60\" />\\n  </a>\\n</p>\\n<h1 align=\"center\">\\n  Gatsby minimal starter\\n</h1>\\n\\n## 🚀 Quick start\\n\\n1.  **Create a Gatsby site.**\\n\\n    Use the Gatsby CLI to create a new site, specifying the minimal starter.\\n\\n    ```shell\\n    # create a new Gatsby site using the minimal starter\\n    npm init gatsby\\n    ```\\n\\n2.  **Start developing.**\\n\\n    Navigate into your new site’s directory and start it up.\\n\\n    ```shell\\n    cd my-gatsby-site/\\n    npm run develop\\n    ```\\n\\n3.  **Open the code and start customizing!**\\n\\n    Your site is now running at http://localhost:8000!\\n\\n    Edit `src/pages/index.js` to see your site update in real-time!\\n\\n4.  **Learn more**\\n\\n    - [Documentation](https://www.gatsbyjs.com/docs/?utm_source=starter&utm_medium=readme&utm_campaign=minimal-starter)\\n\\n    - [Tutorials](https://www.gatsbyjs.com/tutorial/?utm_source=starter&utm_medium=readme&utm_campaign=minimal-starter)\\n\\n    - [Guides](https://www.gatsbyjs.com/tutorial/?utm_source=starter&utm_medium=readme&utm_campaign=minimal-starter)\\n\\n    - [API Reference](https://www.gatsbyjs.com/docs/api-reference/?utm_source=starter&utm_medium=readme&utm_campaign=minimal-starter)\\n\\n    - [Plugin Library](https://www.gatsbyjs.com/plugins?utm_source=starter&utm_medium=readme&utm_campaign=minimal-starter)\\n\\n    - [Cheat Sheet](https://www.gatsbyjs.com/docs/cheat-sheet/?utm_source=starter&utm_medium=readme&utm_campaign=minimal-starter)\\n'},\n",
       " {'repo': 'michellab/BioSimSpace',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"`BioSimSpace <http://biosimspace.org>`__\\n========================================\\n\\n.. image:: https://github.com/michellab/BioSimSpace/workflows/Build/badge.svg\\n   :target: https://github.com/michellab/BioSimSpace/actions?query=workflow%3ABuild)\\n   :alt: Build status\\n\\n.. image:: https://anaconda.org/michellab/biosimspace/badges/downloads.svg\\n   :target: https://anaconda.org/michellab/biosimspace\\n   :alt: Conda Downloads\\n\\n.. image:: https://img.shields.io/badge/License-GPL%20v2-blue.svg\\n   :target: https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html\\n   :alt: License\\n\\n.. image:: https://joss.theoj.org/papers/4ba84ad443693b5dded90e35bf5f8225/status.svg\\n   :target: https://joss.theoj.org/papers/4ba84ad443693b5dded90e35bf5f8225\\n   :alt: Paper\\n\\nAbout\\n-----\\n\\n`BioSimSpace <https://biosimspace.org>`__ is an interoperable Python framework\\nfor biomolecular simulation. With it you can:\\n\\n* Write robust and portable biomolecular workflow components that work on\\n  different hardware, with different software packages, and that can be\\n  run in different ways, e.g. command-line, `Jupyter <https://jupyter.org>`__.\\n* Interact with molecular-simulation processes in real time.\\n\\nCitation |DOI for Citing BioSimSpace|\\n=====================================\\n\\nIf you use BioSimSpace in any scientific software, please cite the following paper: ::\\n\\n    @article{Hedges2019,\\n      doi = {10.21105/joss.01831},\\n      url = {https://doi.org/10.21105/joss.01831},\\n      year = {2019},\\n      publisher = {The Open Journal},\\n      volume = {4},\\n      number = {43},\\n      pages = {1831},\\n      author = {Lester Hedges and Antonia Mey and Charles Laughton and Francesco Gervasio and Adrian Mulholland and Christopher Woods and Julien Michel},\\n      title = {BioSimSpace: An interoperable Python framework for biomolecular simulation},\\n      journal = {Journal of Open Source Software}\\n    }\\n\\n.. |DOI for Citing BioSimSpace| image:: https://joss.theoj.org/papers/4ba84ad443693b5dded90e35bf5f8225/status.svg\\n   :target: https://joss.theoj.org/papers/4ba84ad443693b5dded90e35bf5f8225\\n\\nDocumentation\\n-------------\\n\\nFull documentation can be found `here <https://biosimspace.org>`__.\\n\\nInstallation\\n------------\\n\\nConda package\\n^^^^^^^^^^^^^\\n\\nThe easiest way to install BioSimSpace is using our `conda channel <https://anaconda.org/michellab/repo>`__.\\nBioSimSpace is built using dependencies from `conda-forge <https://conda-forge.org/>`__,\\nso please ensure that the channel takes strict priority. We recommend using\\n`Miniforge <https://github.com/conda-forge/miniforge>`__.\\n\\nTo create a new environment:\\n\\n.. code-block:: bash\\n\\n    conda create -n biosimspace -c conda-forge -c michellab biosimspace\\n    conda activate biosimspace\\n\\nTo install the latest development version you can use:\\n\\n.. code-block:: bash\\n\\n    conda create -n biosimspace-dev -c conda-forge -c michellab/label/dev biosimspace\\n    conda activate biosimspace-dev\\n\\nWhen updating the development version it is generally advised to update `Sire <https://github.com/michellab/Sire>`_\\nat the same time:\\n\\n.. code-block:: bash\\n\\n    conda update -c conda-forge -c michellab/label/dev biosimspace sire\\n\\nIf you plan on using BioSimSpace interactively via Jupyter, then you might also\\nneed to enable the required notebook extensions within your Conda environment:\\n\\n.. code-block:: bash\\n\\n    jupyter-nbextension enable nglview --py --sys-prefix\\n\\nUnless you add the required channels to your Conda configuration, then you'll\\nneed to add them when updating, e.g., for the development package:\\n\\n.. code-block:: bash\\n\\n    conda update -c conda-forge -c michellab/label/dev biosimspace\\n\\nIf you find that Conda is particularly slow to install or upgrade BioSimSpace,\\nthen we advise using `mamba <https://github.com/TheSnakePit/mamba>`__:\\n\\n.. code-block:: bash\\n\\n    conda install -c conda-forge mamba\\n\\nYou can then replace all ``conda`` commands with ``mamba``, e.g.:\\n\\n.. code-block:: bash\\n\\n    mamba create -n biosimspace -c conda-forge -c michellab biosimspace\\n\\nInstalling from source\\n^^^^^^^^^^^^^^^^^^^^^^\\n\\nAlternatively, to install BioSimSpace from source:\\n\\n(Before starting, you'll need a working `Git <https://git-scm.com>`__ installation.)\\n\\nBioSimSpace is built on top of the `Sire <https://github.com/michellab/Sire>`__\\nmolecular simulation framework. To download and install Sire, follow the\\ninstructions `here <https://github.com/michellab/Sire#installation>`__, making\\nsure that BioSimSpace's dependencies are installed into the Sire conda\\nenvironment at the point at which Sire is installed.\\n\\nNext you will need to download BioSimSpace and install it into your Sire\\nConda environment.\\n\\n.. code-block:: bash\\n\\n   git clone https://github.com/michellab/BioSimSpace\\n   cd BioSimSpace/python\\n   python setup.py install\\n\\nOnce finished, you can test the installation by running:\\n\\n.. code-block:: bash\\n\\n   python\\n\\nThen try importing the BioSimSpace package:\\n\\n.. code-block:: python\\n\\n   import BioSimSpace as BSS\\n\\nDevelopers\\n----------\\n\\nPlease follow the `developer's guide <https://biosimspace.org/development.html>`__.\\n\\nIssues\\n------\\n\\nPlease report bugs and other issues using the GitHub `issue tracker <https://github.com/michellab/BioSimSpace/issues>`__.\\nWhen reporting issues please try to include a minimal code snippet that reproduces\\nthe problem. Additional files can be also be uploaded as an archive, e.g. a zip\\nfile. Please also report the branch on which you are experiencing the issue,\\nalong with the BioSimSpace version number. This can be found by running:\\n\\n.. code-block:: python\\n\\n   import BioSimSpace as BSS\\n   print(BSS.__version__)\\n\"},\n",
       " {'repo': 'post-kerbin-mining-corporation/NearFutureSpacecraft',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# Near Future Spacecraft\\n\\nA mod pack for Kerbal Space Program, delivering an expansion to the spacecraft construction offerings in the game\\n\\n* [Features](#features)\\n* [Dependencies](#dependencies)\\n* [Installation](#installation)\\n* [Optional Patches](#optional-patches)\\n* [External Compatibility](#features)\\n* [Contributing](#contributing)\\n\\n## Features\\n\\nA number of parts designed to help build better and cooler orbital (and not so orbital) spacecraft, including:\\n\\n* **Orbital Command Pods:**  New command pods, focused on orbital operations with better comms and control\\n* **Advanced Command Pods:** Command pods that are similar to stock\\'s offerings, but uprated\\n* **Lander Command Pods:** Command pods with integral slots for landing engines\\n* **Monopropellant Tanks:** A full set of monopropellant tanks in 1.25m, 2.5m and 3.75m sizes\\n* **Service Fuel Tanks:** A few service bay/fuel tank combinations\\n* **Advanced RCS Thrusters:** High efficiency orbital RCS thrusters\\n* **Orbital Monopropellant Engines:** Specialized, efficient engines for orbital use fuelled by monopropellant\\n* **Landing Monopropellant Engines:** Specialized and high thrust engines for lander and abort system use\\n* **IVAs:** All the pods have fully featured IVAs, and some have more complex RPM IVAs\\n\\n## Dependencies\\n\\n### Required\\nThese components are required for the mod to function and are bundled as part of any download:\\n* [ModuleManager (4.2.1)](https://github.com/sarbian/ModuleManager)\\n* [B9PartSwitch (2.18.0)](https://github.com/blowfishpro/B9PartSwitch)\\n* [Near Future Props (0.6.5)](https://github.com/post-kerbin-mining-corporation/NearFutureProps)\\n\\n## Installation\\n\\nTo install, place the GameData folder inside your Kerbal Space Program folder. If asked to overwrite files, please do so.\\n\\nNOTE: Do NOT rename or move folders within the GameData folder - this mod uses absolute paths to assets and will break if this happens.\\n\\n## Optional Patches\\n\\nSome extra patches are bundled that you can use to tweak your installation. To install them, drop the correct folder from the **Extras** folder into your KSP GameData Folder\\n\\n* **OrbitalLFOEngines**: Converts the orbital monoprop engines to use LF/O\\n\\n## External Mod Compatibility\\n\\nThis mod includes compatibility patches for the following mods:\\n* [KSP-AVC](https://github.com/CYBUTEK/KSPAddonVersionChecker): Provides version checking\\n* [Community Tech Tree](https://github.com/ChrisAdderley/CommunityTechTree): Provides an expanded, community-sourced technology tree for modders to use\\n* ASET Props: If ASET Props and RPM are installed, certain command pods will have RPM layouts\\n* Connected Living Spaces: Correctly marks parts as passable or impassable\\n* TAC-LS: sets up relevant life support for parts\\n* USI-LS:  sets up relevant life support for parts\\n\\nNote that the vast majority of this compatibility is community-sourced and not maintained by me.\\n\\n## Contributing\\n\\nI certainly accept pull requests. Please target all such things to the `dev` branch though!\\n\\n\\n## Translations\\n\\nFor translation instructions please see [Localization Instructions](https://github.com/ChrisAdderley/NearFutureSpacecraft/blob/master/GameData/NearFutureSpacecraft/Localization/Localization.md)\\n\\n* **Spanish**: fitiales\\n* **Russian**: Dr. Jet and Sooll3\\n* **German**: Three_Pounds and LeLeon\\n* **Simplified Chinese**: Sonkin\\n* **French**: don-vip\\n\\n## Licensing\\n\\nThe art assets in this pack (all .dds, .png and .mu files) are distributed under an All Rights Reserved license. You may not redistribute or re-use these assets without express permission from the author.\\n\\nAny bundled mods are distributed under their own licenses:\\n* ModuleManager by ialdabaoth and sarbian is distributed under a Creative Commons Sharealike license. More details, including source code, can be found [here](http://forum.kerbalspaceprogram.com/threads/31342-0-20-ModuleManager-1-3-for-all-your-stock-modding-needs?p=528607&viewfull=1#post528607)\\n* B9PartSwitch by blowfish is also distributed under its own license. Please find source and more details [here](https://github.com/blowfishpro/B9PartSwitch)\\n\\nEverything else is distributed under the MIT license.\\n\\nCopyright (c) 2019 Chris Adderley\\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n'},\n",
       " {'repo': 'CentennialCollege/SpaceShooter',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# SpaceShooter\\n\\nThis repo is for students. Use this to start your Assignment 1.\\n'},\n",
       " {'repo': 'googlecreativelab/sounds-in-space',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# Sounds in Space (SiS)\\n\\nBy Google\\'s Creative Lab, Sydney\\n\\nThis is not an officially supported Google product\\n\\nContents:\\n\\n- [What is the Sounds in Space Experiment?](#intro)\\n- [How does it work?](#how)\\n- [Known Issues](#issues)\\n- [Developer Setup](#setup)\\n- [Using a Flic Button](#flic-button)\\n- [Kiosk Mode](#kiosk-mode)\\n- [How to create your own experience](#how-to)\\n- [General Notes](#general-notes)\\n- [Contributors](#contributors)\\n\\n<a name=\"intro\"></a>\\n\\n## What is the Sounds in Space Experiment?\\n\\nSounds in Space is an augmented reality audio experiment by Google Creative Lab in Sydney. It allows you to make your own locational audio experience in a physical space.\\n\\nThis is very much an experiment. We are still learning lots and would love you to help us share in this experimentation and learning!\\n\\n<a name=\"how\"></a>\\n\\n## How does it work?\\n\\nSounds in Space is a mobile app that was built with Unity and [Google\\'s ARCore tech](https://developers.google.com/ar/). \\n\\nARCore uses the phone\\'s camera to track its movement through space. As the phone approaches a predetermined location an audio track is triggered and plays through the person’s headphones or speakers.\\n\\nYou can upload your own sound files to a phone (or group of phones), and use the app\\'s UI to place the sounds around a physical space. The experience can be saved and later experienced by an audience.\\n\\nTo the audience, this is an audio only experience. The phone screen is only required for initial set-up. In fact, we found the experienced worked best when the phone and its screen were mostly invisible. We wanted our audience to focus on the audio rather than the phone or what\\'s on it screen. So we experimented with attaching it to the body with various props similar to a lanyard. The phone does not need to be visible as long as the camera is unobstructed and facing outward.\\n\\n<img src=\"img/2019-08-23-13-25-19.png\" width=\"30%\"/>\\n<img src=\"img/2019-08-23-13-25-08.png\" width=\"30%\"/>\\n\\n------------\\n\\n<a name=\"issues\"></a>\\n\\n## Known Issues\\n\\n- This project has mainly been tested on Pixel XL mobile devices running **Android 9 Pie**. There is no support for running on any other (non-Android) operating system at this time.\\n- Connecting & disconnecting bluetooth audio devices can cause audio issues. If this occurs, you may need to restart the app.\\n- At this stage, the app only recognises .WAV audio filetypes.\\n- When loading your own audio files onto the device, be wary that the app will load all audio into memory on startup. (This can be optimised in the future)\\n- When switching between apps or opening the app again, AR core may experience some tracking issues. If this happens to you, try restarting the app!\\n- The \\'On-demand audio\\' feature in settings is experimental and still in beta. It is off by default, if you have more sounds than the app can handle, try turning this on.\\n\\n------------\\n\\n<a name=\"setup\"></a>\\n\\n## Developer Setup\\n\\nThe app has not been released to the public on a mobile app store, but can be downloaded and built in Unity. So far the experience has only been tested on fairly recent Android phones, such as a Pixel or Samsung Galaxy 7 or higher.\\n\\n1. Download Unity version [2018.4.13f1](https://unity3d.com/unity/qa/lts-releases?version=2018.4) for compatibility.\\n\\n2. Clone this repo to your machine\\n\\n3. Import the project into Unity.\\n\\n4. Deploy to an [ARCore compatible device](https://developers.google.com/ar/discover/supported-devices).\\n\\n5. Have a fun.\\n\\n------------\\n\\n<a name=\"flic-button\"></a>\\n\\n## Using a FLIC button\\n\\nIn the FLIC app, when a button is clicked, double clicked, or held, send an \\'Intent\\' action.\\nIn the Intent edit menu, set the \\'Action (Optional)\\' field to \"com.google.cl.syd.soundsinspace.flic.click\" and press SAVE ACTION.\\n\\nIf you want to use other FLIC events (like \\'double click\\' and \\'click and hold\\'), set an extra key-value pair in the Intent editor.\\n- Click: (default)\\n- Double click event: [\"type\" : \"double-clicked\"]\\n- Click and Hold event: [\"type\" : \"click-hold\"]\\n\\n<a name=\"kiosk-mode\"></a>\\n\\n## Kiosk Mode\\n\\nKiosk mode can be accessed from the Main Menu. It will lock out any user interaction, until someone unlocks the mode using the access code.\\nThe access code is \\'3000\\'.\\n\\nNote: The Flic button can still be used while in Kiosk mode.\\n\\n<a name=\"how-to\"></a>\\n\\n## How to create your own experience\\n\\nSummary:\\n\\n1. Create sound files.\\n2. Copy the sound files onto the phone’s local storage.\\n3. Use the app to position the sound files around a room.\\n4. Optionally copy the sound files to the other phones.\\n\\n------------\\n\\n### Step 1: Create sound files\\n\\nImportant: You will need to use your own sound equipment. This experiment does not record sound.\\n\\nCreate your sounds and save them in .wav format. They can be whatever you like -- narrative, soundscapes, dialogue, etc.\\n\\nTechnically the app supports up to 255 sounds (depending on the size and quality of each sound file). However, in our experience the kit works best with 20-40.\\n\\n------------\\n\\n### Step 2. Copy sound files to the phone\\n\\n<img src=\"img/2019-08-23-13-31-49.png\" width=\"50%\"/>\\n\\n#### If your sound files are on a Mac computer\\n\\n1. Plug the phone into your Mac (use an adapter if necessary).\\n2. Download and install Android File Transfer.\\n3. Android File Transfer should open automatically, but if it doesn’t, open it manually by going to Finder -> Applications.\\n    - If Android File Transfer does not detect your device, go to the end of this document for assistance.\\n    - In Android File Transfer go to Android/data/com.google.cl.syd.soundsinspace/files.\\n    - There are 2 important folders inside ‘files’, ‘layouts’ & ‘sounds’.\\n4. Copy all of your wav sound files into the folder called ‘sounds’ as shown below (Android/data/com.google.cl.syd.soundsinspace/files/sounds).\\n\\n#### If your sound files are on a Windows computer\\n\\n1. Plug the phone into your Windows machine using the supplied USB cable (use an adapter if necessary).\\n2. A file transfer window will pop up as soon as you connect the phone.\\n3. In the file window go to Android/data/com.google.cl.syd.soundsinspace/files.\\n    - There are 2 important folders inside ‘files’, ‘layouts’ & ‘sounds’.\\n4. Copy all of your wav sound files into the folder called ‘sounds’ as shown below (Android/data/com.google.cl.syd.soundsinspace/files/sounds).\\n\\n------------\\n\\n### Step 3. Place your sound files\\n\\n#### A - Open the ‘Sounds In Space’ app, and set your ‘Start Position’\\n\\n1. The ‘Start Position’, is an important reference point when beginning the experience. The sounds will be positioned relative to this starting position, so people must start the experience at exactly that location and orientation.\\n2. Stand on your starting reference position, point the phone forward, and press the ‘Set Start Position’ button.\\n3. A virtual ‘feet icon’ will appear, positioned roughly where you were standing.\\n4. It may help to indicate the starting point to your audience using sinage, or by placing a mat on the floor.\\n\\n<img src=\"img/Screenshot_20190630-225647.jpeg\" width=\"30%\"/>\\n<img src=\"img/Screenshot_20190630-225710.jpeg\" width=\"30%\"/>\\n<img src=\"img/Screenshot_20190630-225839.jpeg\" width=\"30%\"/>\\n\\n#### B - Place Sound Markers in the room/space\\n\\n1. Open the menu at the bottom right of the screen.\\n2. Tap on ‘Sound Markers’\\n3. This is where your Sound Markers will be listed.\\n4. Tap on the green ‘Place Markers’ button to create markers.\\n5. A 3D cursor will appear, you can the ‘+’ icon, or anywhere on the screen to create a new Sound Marker at the cursor position.\\n6. You can also place the Marker at the position of the phone you are holding.\\n\\n<img src=\"img/Screenshot_20190630-230102.jpeg\" width=\"30%\"/>\\n<img src=\"img/Screenshot_20190630-230107.jpeg\" width=\"30%\"/>\\n<img src=\"img/Screenshot_20190630-230124.jpeg\" width=\"30%\"/>\\n\\n#### C - Edit the Sound Markers you have placed (WAV file, Sound Radius, Name, etc...)\\n\\nNow that you have created some Sound Markers, you can tap on any of them to edit their properties.\\n\\n- Once a Sound Marker is selected, you can change:\\n  - The audio file being played\\n  - The distance away from the sound, for a user to hear it.\\n  - The position of the marker\\n  - The name of the marker\\n  - The colour or shape\\n- You can also tap ‘More...’ for advanced options. Like deleting the marker.\\n- You can also edit and delete all the markers in your scene.\\n  - By tapping ‘Sound Markers’ from the main menu.\\n  - Select the marker you want to edit, then tap ‘Edit Marker’\\n\\n<img src=\"img/Screenshot_20190630-230223.jpeg\" width=\"30%\"/>\\n<img src=\"img/Screenshot_20190630-230243.jpeg\" width=\"30%\"/>\\n<img src=\"img/Screenshot_20190701-003609.jpeg\" width=\"30%\"/>\\n\\n- Change Sound File\\n- Edit Sound Distance\\n  - Set the min/max radius.\\n  - Set max all the way to the top and it will ALWAYS be heard\\n- Delete Sound Marker\\n  - In ‘More…’ menu\\n\\n<img src=\"img/Screenshot_20190630-230617.jpeg\" width=\"30%\"/>\\n<img src=\"img/Screenshot_20190630-230633.jpeg\" width=\"30%\"/>\\n<img src=\"img/Screenshot_20190630-230641.jpeg\" width=\"30%\"/>\\n\\n#### D - Load, duplicate and create new Layouts (a layout is a collection of Sound Markers designed for a room/space)\\n\\n- You can access the ‘Layouts’ list from the Main Menu.\\n- Tap a layout from the list to highlight it…\\n  - Tap ‘Open Layout’ to load those Sound Markers into the app.\\n  - Tap the duplicate button at the top right of the window to copy and modify a layout that already exists.\\n- Tap ‘New Layout’ to create an empty room/space\\n\\n<img src=\"img/Screenshot_20190630-230102.jpeg\" width=\"30%\"/>\\n<img src=\"img/Screenshot_20190630-230659.jpeg\" width=\"30%\"/>\\n\\nNote: You can move or reset your starting reference position at any time.\\n\\n- You can move to a different position at any time and press the ‘Reset Start Pos.’ button.\\n- If you do this, any sounds you have positioned will move to a different position. So they are in the same relative position to your starting location (the feet icon).\\n\\n<img src=\"img/ResetPos.jpg\" width=\"30%\"/>\\n\\n### Step 4. Copy the experience to the other phones\\n\\nTo avoid repeating the above steps and placing sounds around the room with every phone, you can copy the sound files and their location in the room to the other phones by:\\n\\n1. Using Android File Transfer (on Mac) or the file transfer window (on Windows), and copying the two important folders inside the ‘files’ folder from your phone.\\n    - The important folders are named ‘sounds’ and ‘layouts’.\\n2. Next, copy those folders to each phone you want the experience to run on by repeating Step 2 - ‘Copy sound files to the first phone’.\\n\\n<a name=\"general-notes\"></a>\\n\\n## General Notes\\n\\n### How to “play” your Sounds In Space experience\\n\\nOnce all your sound files have been placed around the room and loaded onto all phones you can playback your Sounds In Space experience with just a little more prep on each phone:\\n\\n*We advise completing these steps before your audience arrives.\\n\\n1. Go to the starting point (for example a mat on the floor), facing the right direction.\\n2. Hold the phone at around the same height it would sit in the prop.\\n3. Open the “solokit” app.\\n4. Tap the button ‘Set Start Position’ button, on startup (or ‘Reset Start Pos.’).\\n5. The last layout that was open will be loaded in by default. You can change layouts by going to the ‘Layouts’ list and loading the appropriate layout.\\n6. Repeat for each phone in the kit.\\n\\n### Notes\\n\\n- You might want to run a test on each phone before giving it to other people to try. You can check the position of your sounds visually by looking through the phone screen.\\n- It’s a good idea to run this test (on each device) once a day, or any time the phones have been off or not in use for several hours.\\n\\n### Tips for a great experience\\n\\n- The experiment uses the phone’s camera to understand its position in a physical space. Make sure the phone’s back camera is not covered and that the room is not too dark. Some experimentation may be needed.\\n- Ensure the room/space contains a few distinguishing physical features. For example, furniture, wallpaper, plants. A room that is completely or almost completely empty with monochrome coloured walls may not work well.\\n- If sounds are not positioning correctly try moving the phone around so the camera can see and learn about your surroundings.\\n- For longer experiences ensure the phones and headphones are charging when not in use.\\n- For longer experiences, it’s a good idea to occasionally check that the sounds are still positioned correctly. The sounds may lose their positions if the phone is turned off or the camera is covered for an extended period of time.\\n- The experience has not been tested with more than a few dozen sounds. It’s likely it will work fine with more, but this has not been confirmed.\\n- The experience has been tested in smaller rooms, around two to three times the size of a typical bedroom. It’s likely it will work well in much larger rooms, but confirmation and testing is needed.\\n\\n## Android File Transfer isn’t working\\n\\n[Click here for troubleshooting help](https://www.androidphonesoft.com/resources/fix-android-file-transfer-not-working-mac.html)\\n\\n\\n<a name=\"contributors\"></a>\\n\\n## Contributors\\n\\n - [Jude Osborn](https://github.com/JudeOsborn)\\n - [Nick Cellini](http://ncellini.com) | [Github](https://github.com/cellininicholas) | [Twitter](https://twitter.com/nfcellini)\\n - Byron Hallett\\n - [Rupert Parry](http://rparry.me) | [Github](https://github.com/rupertparry) | [Twitter](https://twitter.com/rupert_parry)\\n\\n## License & Notes\\n[Apache 2.0 license.](https://www.apache.org/licenses/LICENSE-2.0)\\n\\nThis is not an official Google product, but an AR Experiment developed at the Google Creative Lab. We’ll do our best to support and maintain this experiment but your mileage may vary.\\n\\nWe encourage open sourcing projects as a way of learning from each other. Please respect our and other creators’ rights, including copyright and trademark rights when present, when sharing these works and creating derivative work.\\n\\n------------\\n'},\n",
       " {'repo': 'flrngel/TagSpace-tensorflow',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"# TagSpace-tensorflow\\n\\n![model image of TagSpace](https://raw.githubusercontent.com/flrngel/TagSpace-tensorflow/master/resources/tagspace-model.png)\\n\\nTensorflow implementation of Facebook **#TagSpace**\\n\\nYou can read more about #TagSpace from [here](https://research.fb.com/publications/tagspace-semantic-embeddings-from-hashtags/)\\n\\nSpecial thanks to Facebook research team's [Starspace](https://github.com/facebookresearch/Starspace) project, it was really good reference.\\n\\n## Key Concept\\n\\nBeside choosing 1000 random negative tag (for performance reason I guess), I choosed worst positive tag, best negative tag for calculating WARP loss. It's not good for performance but since we don't have much tags(labels) as Facebook, it seems okay.\\n\\n## Usage\\n\\nDownload [ag news dataset](https://github.com/mhjabreel/CharCNN/tree/36791268d7eec96dc3330cf7eedbfb427524b604/data/ag_news_csv) as below\\n\\n```\\n$ tree ./data\\n./data\\n└── ag_news_csv\\n    ├── classes.txt\\n    ├── readme.txt\\n    ├── test.csv\\n    ├── train.csv\\n    └── train_mini.csv\\n```\\n\\nand then\\n\\n```\\n$ python train.py\\n```\\n\\n## Result\\n\\nAccuracy 0.89 (ag test data, compare 0.91 from StarSpace with same condition [5 epoch, 10 dim])\\n\\n## To-do list\\n\\n- support multiple dataset\\n- improve performance\\n- adopt WARP sampling (now is just a WARP loss)\\n- add Tensorboard metrics\\n\"},\n",
       " {'repo': 'zhengqili/Neural-Scene-Flow-Fields',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Neural Scene Flow Fields\\nPyTorch implementation of paper \"Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes\", CVPR 2021\\n\\n[[Project Website]](https://www.cs.cornell.edu/~zl548/NSFF/) [[Paper]](https://arxiv.org/abs/2011.13084) [[Video]](https://www.youtube.com/watch?v=qsMIH7gYRCc&feature=emb_title)\\n\\n\\n## Dependency\\nThe code is tested with Python3, Pytorch >= 1.6 and CUDA >= 10.2, the dependencies includes \\n* configargparse\\n* matplotlib\\n* opencv\\n* scikit-image\\n* scipy\\n* cupy\\n* imageio.\\n* tqdm\\n* kornia\\n\\nThe current version in this github include some improvement for monocular videos in the wild. For reference code matched paper\\'s description, please check out [this branch](https://github.com/zhengqili/Neural-Scene-Flow-Fields/tree/5bfedc477bab845d539e7b70d114ba39c1644b0e)\\n\\n## Video preprocessing \\n1. Download nerf_data.zip from [link](https://drive.google.com/drive/folders/1G-NFZKEA8KSWojUKecpJPVoq5XCjBLOV?usp=sharing), an example input video with SfM camera poses and intrinsics estimated from [COLMAP](https://colmap.github.io/) (Note you need to use COLMAP \"colmap image_undistorter\" command to undistort input images to get \"dense\" folder as shown in the example, this dense folder should include \"images\" and \"sparse\" folders).\\n\\n2. Download single view depth prediction model \"model.pt\" from [link](https://drive.google.com/drive/folders/1G-NFZKEA8KSWojUKecpJPVoq5XCjBLOV?usp=sharing), and put it on the folder \"nsff_scripts\".\\n\\n3. Run the following commands to generate required inputs for training/inference:\\n```bash\\n    # Usage\\n    cd nsff_scripts\\n    # create camera intrinsics/extrinsic format for NSFF, same as original NeRF where it uses imgs2poses.py script from the LLFF code: https://github.com/Fyusion/LLFF/blob/master/imgs2poses.py\\n    python save_poses_nerf.py --data_path \"/home/xxx/Neural-Scene-Flow-Fields/kid-running/dense/\"\\n    # Resize input images and run single view model, \\n    # argument resize_height: resized image height for model training, width will be resized based on original aspect ratio\\n    python run_midas.py --data_path \"/home/xxx/Neural-Scene-Flow-Fields/kid-running/dense/\" --resize_height 288\\n    # Run optical flow model\\n    ./download_models.sh\\n    python run_flows_video.py --model models/raft-things.pth --data_path /home/xxx/Neural-Scene-Flow-Fields/kid-running/dense/ \\n```\\n\\n## Rendering from an example pretrained model\\n1. Download pretraind model \"kid-running_ndc_5f_sv_of_sm_unify3_F00-30.zip\" from [link](https://drive.google.com/drive/folders/1G-NFZKEA8KSWojUKecpJPVoq5XCjBLOV?usp=sharing). Unzipping and putting it in the folder \"nsff_exp/logs/kid-running_ndc_5f_sv_of_sm_unify3_F00-30/360000.tar\". \\n\\nSet datadir in config/config_kid-running.txt to the root directory of input video. Then go to directory \"nsff_exp\":\\n```bash\\n   cd nsff_exp\\n   mkdir logs\\n```\\n\\n2. Rendering of fixed time, viewpoint interpolation\\n```bash\\n   python run_nerf.py --config configs/config_kid-running.txt --render_bt --target_idx 10\\n```\\n\\nBy running the example command, you should get the following result:\\n![Alt Text](https://github.com/zhengqili/Neural-Scene-Flow-Fields/blob/main/demo/vi.gif)\\n\\n3. Rendering of fixed viewpoint, time interpolation\\n```bash\\n   python run_nerf.py --config configs/config_kid-running.txt --render_lockcam_slowmo --target_idx 8\\n```\\n\\nBy running the example command, you should get the following result:\\n![Alt Text](https://github.com/zhengqili/Neural-Scene-Flow-Fields/blob/main/demo/ti.gif)\\n\\n4. Rendering of space-time interpolation\\n```bash\\n   python run_nerf.py --config configs/config_kid-running.txt --render_slowmo_bt  --target_idx 10\\n```\\n\\nBy running the example command, you should get the following result:\\n![Alt Text](https://github.com/zhengqili/Neural-Scene-Flow-Fields/blob/main/demo/sti.gif)\\n\\n## Training\\n1. In configs/config_kid-running.txt, modifying expname to any name you like (different from the original one), and running the following command to train the model:\\n```bash\\n    python run_nerf.py --config configs/config_kid-running.txt\\n```\\nThe per-scene training takes ~2 days using 4 Nvidia GTX2080TI GPUs.\\n\\n2. Several parameters in config files you might need to know for training a good model on in-the-wild video\\n* final_height: this must be same as --resize_height argument in run_midas.py, in kid-running case, it should be 288.\\n* N_samples: in order to render images with higher resolution, you have to increase number sampled points such as 256 or 512\\n* chain_sf: model will perform local 5 frame consistency if set True, and perform 3 frame consistency if set False. For faster training, setting to False.\\n* start_frame,  end_frame: indicate training frame range. The default model usually works for video of 1~2s and 30-60 frames work the best for default hyperparameters. Training on longer frames can cause oversmooth rendering. To mitigate the effect, you can increase the capacity of the network by increasing netwidth to 512.\\n* decay_iteration: number of iteartion in initialization stage. Data-driven losses will decay every 1000 * decay_iteration steps. We have updated code to automatically calculate number of decay iterations.\\n* no_ndc: our current implementation only supports reconstruction in NDC space, meaning it only works for forward-facing scene, same as original NeRF.\\n* use_motion_mask, num_extra_sample: whether to use estimated coarse motion segmentation mask to perform hard-mining sampling during initialization stage, and how many extra samples during initialization stage.\\n* w_depth, w_optical_flow: weight of losses for single-view depth and geometry consistency priors described in the paper. Weights of (0.4, 0.2) or (0.2, 0.1) usually work the best for most of the videos. \\n* If you see signifacnt ghosting result in the final rendering, you might try the suggestion from [link](https://github.com/zhengqili/Neural-Scene-Flow-Fields/issues/18)\\n\\n## Evaluation on the Dynamic Scene Dataset\\n1. Download Dynamic Scene dataset \"dynamic_scene_data_full.zip\" from [link](https://drive.google.com/drive/folders/1G-NFZKEA8KSWojUKecpJPVoq5XCjBLOV?usp=sharing)\\n\\n2. Download pretrained model \"dynamic_scene_pretrained_models.zip\" from [link](https://drive.google.com/drive/folders/1G-NFZKEA8KSWojUKecpJPVoq5XCjBLOV?usp=sharing), unzip and put them in the folder \"nsff_exp/logs/\"\\n\\n3. Run the following command for each scene to get quantitative results reported in the paper:\\n```bash\\n   # Usage: configs/config_xxx.txt indicates each scene name such as config_balloon1-2.txt in nsff/configs\\n   python evaluation.py --config configs/config_xxx.txt\\n```\\n\\n* Note: you have to use modified LPIPS implementation included in this branch in order to measure LIPIS error for dynamic region only as described in the paper.\\n\\n## Acknowledgment\\nThe code is based on implementation of several prior work:\\n\\n* https://github.com/sniklaus/softmax-splatting\\n* https://github.com/yenchenlin/nerf-pytorch\\n* https://github.com/JKOK005/dVRK-Linear-Interpolator-\\n* https://github.com/richzhang/PerceptualSimilarity\\n* https://github.com/intel-isl/MiDaS\\n* https://github.com/princeton-vl/RAFT\\n* https://github.com/NVIDIA/flownet2-pytorch\\n\\n## License\\nThis repository is released under the [MIT license](hhttps://opensource.org/licenses/MIT).\\n\\n## Citation\\nIf you find our code/models useful, please consider citing our paper:\\n```bash\\n@InProceedings{li2020neural,\\n  title={Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes},\\n  author={Li, Zhengqi and Niklaus, Simon and Snavely, Noah and Wang, Oliver},\\n  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n  year={2021}\\n}\\n'},\n",
       " {'repo': 'SilenceDut/ExpandableLayout',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '# ExpandableLayout\\n\\nA expandable Layout to save space and  reduce jump between Activity and Fragment\\n####[![](https://jitpack.io/v/SilenceDut/ExpandableLayout.svg)](https://jitpack.io/#SilenceDut/ExpandableLayout)\\n\\n\\nUSE\\n----------------------\\n\\n**Project**\\n\\nIt had been used in project [KnowWeather](https://github.com/SilenceDut/KnowWeather),you can learn more.\\n\\n**SimpleUse**\\n\\n![intro](media/simple_use.gif)\\n\\n**Use in RecyclerView**\\n\\nparent view  scroll automatically if the view expand  out of device screen,\\n\\n(当展开时如果超过屏幕的高度时父控件自动上移)\\n\\n![intro](media/recyclerview_withParentScroll_together.gif)\\n\\n**Use in ListView**\\n\\nparent view  not scroll automatically if the view expand  out of device screen\\n\\n(当展开时如果超过屏幕的高度时父控件不自动上移)\\n\\n![intro](media/listview_withoutParentScroll.gif)\\n\\n(It runs smoothly, but gif is not appear well)\\n[**sample.apk**](https://github.com/SilenceDut/DayNightToggleButton/blob/master/apk/expandable.apk?raw=true) \\n\\n\\nAttention\\n---------\\n**`scroll automatically `** function  not perform well in listview so far.but \\nit perform well in RecyclerView\\n\\n\\nAdding to your project\\n----------------------\\nThis library is available through JitPack.\\n\\nStep 1. Add the JitPack repository to your build file\\n\\n**gradle**\\n```groovy\\nallprojects {\\n    repositories {\\n        ...\\n        maven { url \"https://jitpack.io\" }\\n    }\\n}\\n```\\n**maven**\\n```xml\\n<repositories>\\n    <repository>\\n        <id>jitpack.io</id>\\n        <url>https://jitpack.io</url>\\n    </repository>\\n</repositories>\\n```\\n\\nStep 2. Add the dependency\\n\\n**gradle**\\n\\n```groovy\\ncompile \\'com.github.SilenceDut:ExpandableLayout:1.2.0\\'\\n```\\n**maven**\\n\\n```xml\\n<dependency>\\n    <groupId>com.github.SilenceDut</groupId>\\n    <artifactId>ExpandableLayout</artifactId>\\n    <version>{latest-version}</version>\\n</dependency>\\n```\\nBasic Usage\\n-----------\\n**Supported Attributes**\\n\\n|           attr        \\t|     default      |                         mean                          \\t |\\n|:------------------------- |:---------------- |:------------------------------------------------------- |\\n| expDuration      \\t| 300            | expand duration.  |\\n| expWithParentScroll   | false  | parent view should scroll automatically if the view expand  out of device screen（当展开时如果超过屏幕的高度是父控件否自动上移).   |\\n| expExpandScrollTogether| false          | parent view should scroll together with view expanding. |\\n**ExpandableLayout inherited from LinearLayout,and the default Orientation is VERTICAL**\\n\\n```xml\\n<com.silencedut.expandablelayout.ExpandableLayout\\n    xmlns:android=\"http://schemas.android.com/apk/res/android\"\\n    xmlns:app=\"http://schemas.android.com/apk/res-auto\"\\n    app:expWithParentScroll=\"true\"\\n    app:expDuration = \"300\"\\n    app:expExpandScrollTogether = \"false\"\\n    android:layout_width=\"match_parent\"\\n    android:layout_height=\"wrap_content\">\\n\\n    <layout1\\n    ...\\n    />\\n    \\n    <layout2\\n    ...\\n    />\\n\\n</com.silencedut.expandablelayout.ExpandableLayout>\\n```\\n\\n\\nLicense\\n-------\\n\\n    Copyright 2015-2016 SilenceDut\\n\\n    Licensed under the Apache License, Version 2.0 (the \"License\");\\n    you may not use this file except in compliance with the License.\\n    You may obtain a copy of the License at\\n\\n       http://www.apache.org/licenses/LICENSE-2.0\\n\\n    Unless required by applicable law or agreed to in writing, software\\n    distributed under the License is distributed on an \"AS IS\" BASIS,\\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n    See the License for the specific language governing permissions and\\n    limitations under the License.\\n'},\n",
       " {'repo': 'space-wizards/SS14.Launcher',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# SS14.Launcher\\n\\nThis is the launcher you should be using to connect to SS14 servers. Server browser, content downloads, account management. It\\'s got it all!\\n\\n# Development\\n\\nUseful environment variables for development:\\n* `SS14_LAUNCHER_APPDATA_NAME=launcherTest` to change the user data directories the launcher stores its data in. This can be useful to avoid breaking your \"normal\" SS14 launcher data while developing something.\\n* `SS14_LAUNCHER_OVERRIDE_AUTH=https://.../` to change the auth API URL to test against a local dev version of the API.\\n'},\n",
       " {'repo': 'shaidavis/spacebook-c2',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': ''},\n",
       " {'repo': 'XereoNet/SpaceModule',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': \"# SpaceModule - the awesome way!\\nA powerful yet simple web panel for administering your Minecraft servers with ease.\\n\\n## Description\\nSpaceModule is a project developed over the last 6 months aiming to bring an advanced and powerful web administration framework to Bukkit. What makes it unique is, on one hand, it's graphical user interface, and on the other hand some nifty features enlisted just below.\\n\\n## Features\\n- General\\n    * Attractive Interface\\n    * Non lethal doses of awesomeness\\n    * Multiple Servers\\n    * Multi-user access with role setting\\n    * Theme Support for extra commiseration\\n    * Per server - per user role settings\\n    * Console access\\n- Dashboard\\n    * Pretty and quick statistics about your server\\n    * Activity feed - who did what, and when\\n    * Chat - talk with your players\\n- Players\\n    * Player management - kick, kill, feed, heal, ban, op and more!\\n    * Whitelist\\n    * Bans\\n- Plugins\\n    * Plugin management\\n    * Config editor\\n    * Installing and Deinstalling\\n    * Updating and disabling\\n    * Bukget integration\\n- Worlds\\n    * World management\\n    * Multiworld support\\n    * Backups\\n    * Chunkster\\n    * MapAutoTrim\\n- Servers\\n    * Craftbukkit one-click installing and updating\\n    * Server Properties saving\\n    * Schedules\\n    \\n## Credits\\n * [Antariano](https://github.com/Antariano/) - SpaceCP.\\n * [JamyDev](https://github.com/JamyDev/) - SpaceCP.\\n * [NeatMonster](https://github.com/NeatMonster/) - SpaceModule, SpaceRTK and SpaceBukkit.\\n * [Drdanick](https://github.com/Drdanick/) - RemoteToolkit, SpaceModule, SpaceRTK and Spacebukkit.\\n\\n## Links\\n- Website: [http://spacebukkit.xereo.net/](http://spacebukkit.xereo.net/).\\n- Forums: [http://forums.xereo.net/](http://forums.xereo.net/).\\n- Wiki: [http://spacebukkit.xereo.net/wiki](http://spacebukkit.xereo.net/wiki).\\n \\n## License\\nSpaceModule is free software: you can redistribute it and/or modify it under\\nthe terms of the Attribution-NonCommercial-ShareAlike Unported (CC BY-NC-SA)\\nlicense as published by the Creative Common organization, either version 3.0 of\\nthe license, or (at your option) any later version.\\n\\nSpaceModule is distributed in the hope that it will be useful, but WITHOUT ANY\\nWARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A\\nPARTICULAR PURPOSE. See the Attribution-NonCommercial-ShareAlike Unported (CC \\nBY-NC-SA) license for more details.\\n\\nYou should have received a copy of the Attribution-NonCommercial-ShareAlike \\nUnported (CC BY-NC-SA) license along with this program. If not, see \\n[http://creativecommons.org/licenses/by-nc-sa/3.0/](http://creativecommons.org/licenses/by-nc-sa/3.0/).\"},\n",
       " {'repo': 'contentful/gallery-app-android',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '# Gallery Showcase\\n\\nThis is an Android application example for the [Contentful][1] gallery space template.\\n\\n## Intro:\\n\\n> [Contentful][1] is a content management platform for web applications, mobile apps and connected devices. It allows you to create, edit & manage content in the cloud and publish it anywhere via powerful API. Contentful offers tools for managing editorial teams and enabling cooperation between organizations.\\n\\n## Screenshots\\n\\n![Screenshots](screenshots/sc1.png) ![Screenshots](screenshots/sc2.png)\\n\\n## Usage\\n\\n- Create a space with the \"Gallery\" space template on [Contentful][1]\\n- Clone this repo\\n- Enter your Delivery API credentials in the [Const.java][const] file\\n- \\\\o/\\n\\n## License\\n\\nCopyright (c) 2015 Contentful GmbH. See [LICENSE][2] for further details.\\n\\n\\n[1]: https://www.contentful.com\\n[2]: LICENSE\\n[const]: app/src/main/java/gallery/templates/contentful/lib/Const.java'},\n",
       " {'repo': 'fengjixuchui/SpaceCow',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"# SpaceCow - Python Rootkit\\n\\nFollow me on [Twitter](https://twitter.com/__SPX__) \\n\\nIn the past days i spent a lot of time watching some RedTeam ops and I saw all these little tools making some awsome stuff... and in the 90% of the cases RedTeams don't share their tricks and softwares with others.\\n\\nSo i thought I could create something open source. And after some days i crawled up with something...\\n![SpaceCow-c2c](http://zetabay.net/wp-content/uploads/2019/06/Cattura-1024x556.png)\\n\\n## Why do you need SpaceCow?\\n\\nThis software is a minimal Rootkit written in pure __Python3__ and does some little tricks to make itself stealthy, so let's listing all its capabilities :\\n\\n- __Socket Server MultiPort__ : I saw that a lot of reverse shells listen only on one port so i thought 'Why i can't listen on 100?'.\\n- __Socket Server MultiClient__ : Yep, botnet ... are you happy SKs?\\n- __CrossPlatform__: theoretically it can be run only both Windows and Linux platform, just require Python3.\\n- __Encrypted Communication__ : The network comm is completly encrypted using RSA encryption.\\n- __Custom TCP Protocol__: You can have multiple theories on what does it mean, but in my opinion sending packets following a specific set of rules is a custom protocol already, infact the sending and the receiving methods between client and server is optimized. Each packet is being splitted in more piecies in order to allow the RSA algorithm to encrypt everything and then each packet is sent following a ACK - SYNC kind of style, so both the client and server are sure that the other received the last packet correctly.\\n- __Runtime Payload Execution__ : this is cool, so id order to make the stub less FUD i decided to execute the '_critical payloads_' such as command shell execution, ... in runtime. So in the python script is not written a backdoor functionality for command exec. But the client once triggered will download from remote the custom payload to execute : __SHELL__ commands and __POWERSHELL__ commands. \\n\\n    (P.S. The commands sent using this payloads should be encoded in base64 to avoid F* unicode decoding errors but I didn't done this. )\\n\\n- __Traffic Obfuscation__ : So this is fun because WireShark uses a specific Windows API to intercept the traffic so there's a Library created for C++ that allows the same kind of manipulation at the same level. Cool and someone decided to make it for python (PyDivert). So using this lib you can take each packet on a specific port, modifing it by your need and re-inject it into the network (Pit-Stop style). So thanks to this i managed to modify the source IP address of each packet incoming from the C&C with the destination one, so basically if you intercept the traffic you'll se in the incoming packets that the infected ones are coming from the loopback or from the LAN ... so less noise.\\n\\n- __Sandbox Aggressive Detection__ : I've taken some scripts around the network to perform an aggressive Sandbox detection to try to avoid analysis. This is not tested yet ! You can implement it if you want.\\n\\n- __Persistence via Windows Services__ : What is the best way to gain persistence without using the same REGKEYs? Windows Services... In the repo you have a file called '_ServiceCreator.py_' using that you can create a custom service that will execute the file at the startup. Be sure to install the service setting it : \\n\\n    ```--startup=auto install```\\n\\n    This is tested but for some reasons the service is set on startup auto but is not being executed. Don't know why it needs some work but you can create a service using the ```sc.exe``` native Windows program to create a new one.\\n\\n\\n## How to use it and stuff\\n\\nBasically install Python3 and install the requirements using pip:\\n\\n\\n```pip.exe -r install requirements.txt```\\n\\nInside you have a custom library that i have written called '__TrueColors__' (color.py) you can grab it and using it in other projects, is based on Colorama.\\n\\nOnce you're ready start the file 'spacecow.py':\\n\\n```python spacecow.py -h\\n\\n           __n__n__\\n    .------`-\\\\00/-'\\n   /  ##  ## (oo)\\n  / \\\\## __   ./ SpaceCow\\n     |//YY \\\\|/ Windows Rootkit\\n     |||   |||\\n\\nusage: spacecow.py [-h] [-p PORTS] [--version]\\n\\noptional arguments:\\n  -h, --help  show this help message and exit\\n  -p PORTS    Define the ports for the socket server (ex. 2000,2001,...).\\n  --version   show program's version number and exit\\n\\n```\\n\\nTo listen on ports just enter the following syntax :\\n\\n```python spacecow.py -p 2000,2001```\\n\\nEnter the ports separated just with a comma.\\n\\nTo enable the persistence you have to run the file '_ServiceCreator.py_', you can add it as module in the client.py. Remember to change '_exepath_' in the file with the final path of the .exe malware and you can modify the 3 class init variables defining the Service Name, Description and DisplayName. In the end to create a new service you can run the following syntax:\\n\\n```python ServiceCreator.py --startup=auto install```\\n\\nTo start the service :\\n\\n```python ServiceCreator.py start```\\n\\nTo uninstall the service :\\n\\n```python ServiceCreator.py remove```\\n\\nThen reboot the system.\\n\\n## C2 Commands\\n\\nI didn't set a help menu so the commands are the following :\\n\\n### Command Line\\n- ```list```: list all the implants\\n- ```notify connection true/false```: this will inform you each time a new implant gets connected but this will break the current input and you need to press enter.\\n- ```drop */1,2,3,...```: you can broadcast a close connection to all implants using * or sending to specific indexes separating them with comma.\\n- ```jump (index)``` : select the index and you can spawn a interactive shell with the selected implant.\\n\\n### Interactive shell\\n- ```exit/background``` : to close the shell and drop the connection (yep need work to handle it).\\n- ```EXEC::command``` : execute a cmd command.\\n- ```PSEXEC::command``` : execute a powershell command.\\n\\n## Conclusions\\n\\nThis is a PoC it needs some work ( download, upload, broadcast, handling, ...) but it's all optional the basic functions are full working. So feel free to implement it and if you want to help me creating stuff pm me.\\n\\n## Disclaimer\\n\\nMeant for study only purposes not for illegal.\\n\\n## License\\n\\nYou can use this code even for commercial purposes but please give credit, I've spent hours on it.\\n\"},\n",
       " {'repo': 'madduccino/coding.space',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': ''},\n",
       " {'repo': 'JonRivera/cs-guided-project-time-and-space-complexity',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Computer Science Fundamentals - Time and Space Complexity\\n\\nThis is the starter code for Computer Science - Sprint 1: Computer Science Fundamentals - Module 3: Time and Space Complexity.\\n\\nPlease fork and clone this repo to your computer by the start of class.\\n'},\n",
       " {'repo': 'SVijayB/JavaSpace',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '# JavaSpace\\n\\n<p align=\"center\">\\n    <a href=\"https://github.com/SVijayB/JavaSpace\"><img src=\"assets/Java_logo_icon.png\" alt=\"Logo\" border=\"0\"></a>\\n    <br>A collection of all the Java programs I\\'ve worked on so far.\\n</p>\\n\\n<div align=\"center\">\\n    \\n![Issues](https://img.shields.io/github/issues/SVijayB/JavaSpace?color=brightgreen)\\n![Pull requests](https://img.shields.io/github/issues-pr/SVijayB/JavaSpace?color=brigthgreen)\\n![forks](https://img.shields.io/github/forks/SVijayB/JavaSpace)\\n![stars](https://img.shields.io/github/stars/SVijayB/JavaSpace)\\n![license](https://img.shields.io/github/license/SVijayB/JavaSpace?color=orange)\\n\\n</div>\\n\\n---\\n\\n<details>\\n\\n<summary>Table of contents</summary>\\n\\n## Table of Contents\\n\\n-   [Basics](#Basics)\\n-   [Data Conversion](#Data-Conversion)\\n-   [Data Structures](#Data-Structures)\\n-   [Searching And Sorting Algorithms](#Searching-And-Sorting-Algorithms)\\n-   [OOPS](#OOPS)\\n-   [Exception Handling](#Exception-Handling)\\n-   [Threads](#threads)\\n-   [Miscellaneous](#Miscellaneous)\\n-   [Contributing](#Contributing)\\n-   [License](#License)\\n\\n</details>\\n\\n## Basics\\n\\nBasics of Java are covered within these programs.\\n\\n<pre>\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/GP.java\">GP.java</a>                       | Calculates Grade Pay.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/Pyramid.java\">Pyramid.java</a>                  | Number Pyramid.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/average.java\">Average.java</a>                  | Average of N numbers.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/cutoff.java\">Cutoff.java</a>                   | Calculate PCM Cut-off.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/denomination.java\">Denomination.java</a>             | Money Denomination in India.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/evenandodd.java\">EvenAndOdd.java</a>               | Finds Odd and Even numbers from given numbers.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/flag.java\">Flag.java</a>                     | A Star pattern.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/fun1.java\">Fun1.java</a>                     | A Simple turn-based game.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/leapyear.java\">LeapYear.java</a>                 | Checks if leap year.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/merge.java\">Merge.java</a>                    | Merges two Arrays.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/palindrome.java\">Palindrome.java</a>               | Checks if Palindrome or not.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/power.java\">Power.java</a>                    | Calculates A raised to B.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/prime.java\">Prime.java</a>                    | Prints prime numbers from 0-100.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/recursion.java\">Recursion.java</a>                | Prints N Fibonacci numbers.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/reverse.java\">Reverse.java</a>                  | Reversing a Number or String.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/sorting.java\">Sorting.java</a>                  | Sorting N numbers.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/squares.java\">Squares.java</a>                  | Print squares until N\\'th turn.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/tables.java\">Tables.java</a>                   | Print Multiplication table.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/factorail.java\">Factorial.java</a>                | To find Factorial of a number.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/strong number.java\">Strong number.java</a>            | Check for Strong number.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/Fibonacci.java\">Fibonacci.java</a>                | To find Fibonacci series up to a given number.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/FizzBuzz.java\">FizzBuzz.java</a>                 | The FizzBuzz problem is a classic test given in coding interviews.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/DuplicateChars.java\">DuplicateChars.java</a>           | A simple program that checks a string for duplicate characters.\\n</pre>\\n\\n---\\n\\n## Data Conversion\\n\\nContains programs that revolve around various Number Systems.\\n\\n<pre>\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Data_Conversion/BinaryCheck.java\">BinaryCheck.java</a>              | Checks if number has even number 1\\'s in it\\'s binary format or not.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Data_Conversion/Bintodecimal.java\">Bintodecimal.java</a>             | Binary to Decimal.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Data_Conversion/binary.java\">Binary.java</a>                   | Check if Binary or not.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Data_Conversion/decitobinary.java\">DeciToBinary.java</a>             | Decimal to Binary.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Data_Conversion/decitooctal.java\">DeciToOctal.java</a>              | Decimal to Octal.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Data_Conversion/DeciToHexa.java\">DeciToHexa.java </a>              | Decimal to Hexa\\n</pre>\\n\\n---\\n\\n## Data Structures\\n\\nPrograms Relating to the implementation of various Data Structures are present in this folder.\\n\\n<pre>\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Data_Structures/Queue.java\">Queue.java</a>                    | Queue Implementation.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Data_Structures/BST.java\">BST.java</a>                      | Binary Search Tree.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Data_Structures/circularqueue.java\">CircularQueue.java</a>            | Circular Queue Implementation.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Data_Structures/DLL.java\">DLL.java</a>                      | Double Linked List.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Data_Structures/linkedlist.java\">Linkedlist.java</a>               | Linked List.\\n• <a href=https://github.com/SVijayB/JavaSpace/blob/master/Data_Structures/DetectCycleInList.java\">DetectCycleInList.java</a>               | Detect Cycle in an Undirected Graph (using DFS)\\n <a href=https://github.com/SVijayB/JavaSpace/blob/master/Data_Structures/Topological_sort_BFS.java\">Topological_sort_BFS.java</a>               | Topological_sort_BFS\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Data_Structures/stackpr1.java\">Stackpr1.java</a>                 | Stack Implementation.\\n• <a href=\"https://github.com/SVijayB/Java/blob/master/Data_Structures/Kruskal%E2%80%99s%20algorithm.java\">Kruskal\\'s algorithm.java</a>      | Kruskal\\'s algorithm to find shortest path.\\n• <a href=\"https://github.com/SVijayB/Java/blob/master/Data_Structures/Linear%20Probing.java\">Linear Probing.java</a>           | Creating a Hash Table using Linear Probing method.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Data_Structures/InorderTraversal.java\">InorderTraversal.java</a>         | Inorder traversal in java.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Data_Structures/KthMinAndMaxInArray.java\">KthMinAndMaxInArray.java</a>      | Finding Kth smallest and largest element in an array.\\n</pre>\\n\\n---\\n\\n## Searching And Sorting Algorithms\\n\\nPrograms used for searching and sorting given elements.\\n\\n<pre>\\n• <a href=\"https://github.com/SVijayB/Java/blob/master/Searching%20%26%20Sorting/Binary%20Search.java\">Binary Search.java</a>            | Iterative Binary Search.\\n• <a href=\"https://github.com/SVijayB/Java/blob/master/Searching%20%26%20Sorting/Linear%20Search.java\">Linear Search.java</a>            | Linear Searching Algorithm.\\n• <a href=\"https://github.com/SVijayB/Java/blob/master/Searching%20%26%20Sorting/Heap%20Sort.java\">Heap Sort.java</a>                | Sorting entires using Heap Sort.\\n• <a href=\"https://github.com/SVijayB/Java/blob/master/Searching%20%26%20Sorting/Merge%20Sort.java\">Merge Sort.java</a>               | Sorting using Merge Sort.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Searching%20%26%20Sorting/BubbleSort.java\">Bubble Sort.java</a>              | Sorting using Bubble Sort.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Searching%20%26%20Sorting/QuickSort.java\">QuickSort.java</a>                | Quick Sort.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Searching%20%26%20Sorting/Selection%20Sort.java\">Selection Sort.java</a>           | Selection Sort.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Searching%20%26%20Sorting/InsertionSort.java\">Insertion Sort.java</a>           | Insertion Sort.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Searching%20%26%20Sorting/Radix Sort.java\">Radix Sort.java</a>           | Radix Sort.\\n</pre>\\n\\n---\\n\\n## OOPS\\n\\nImplementing Object Oriented Programming System(OOPs) in Java.\\n\\n<pre>\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/OOPS/Classes.java\">Classes.java</a>                  | Understanding classes in java.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/OOPS/Initializing_objects.java\">Initializing_objects.java</a>     | Creating/initializing data using reference variables.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/OOPS/Static.java\">Static.java</a>                   | Understanding uses of static variables.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/OOPS/Polymorphism.java\">Polymorphism.java</a>             | Exploring Polymorphism in OOPS.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/OOPS/Constructors.java\">Constructors.java</a>             | Initializing variables using constructors in Java.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/OOPS/Object.java\">Object.java</a>                   | Objects in java.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/OOPS/Variables.java\">Variables.java</a>                | Different types of variables in java.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/OOPS/ArrayOfObjects.java\">ArrayOfObjects.java</a>           | Creating an array of objects.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/OOPS/Encapsulation/Encapsulation.java\">Encapsulation.java</a>            | Encapsulation of data.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/OOPS/Encapsulation/Accessing.java\">Accessing.java</a>                | Accessing private variables using public methods.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/OOPS/AbstractClass.java\">AbstractClass.java</a>            | Understanding abstract classes.\\n</pre>\\n\\n### Examples\\n\\nVarious examples to help implement OOPS in Java.\\n\\n<pre>\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/OOPS/Examples/Holiday.java\">Holiday.java</a>                  | Class Objects and Methods.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/OOPS/Examples/Bookstore.java\">Bookstore.java</a>                | Static, final and Constructors.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/OOPS/Examples/SemesterMarks.java\">SemesterMarks.java</a>            | Understanding Inheritance.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/OOPS/Examples/Shape.java\">Shape.java</a>                    | Example for Polymorphism.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/OOPS/Examples/GenericMethods.java\">GenericMethods.java</a>           | Using generic classes and methods.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/OOPS/Examples/Distribution.java\">Distribution.java</a>             | Parameterised constructor and exception handling.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/OOPS/Examples/Shapes.java\">Shapes.java</a>                   | Example for Inheritance.\\n</pre>\\n\\n---\\n\\n## Exception Handling\\n\\nHandling various exceptions using the try, catch, finally and throw keywords in Java.\\n\\n<pre>\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Exception_Handling/Try_and_catch.java\">Try_and_catch.java</a>            | Using the try and catch keywords.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Exception_Handling/Try_and_finally.java\">Try_and_finally.java</a>          | Replacing try, catch and finally keywords.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Exception_Handling/Multicatch_block.java\">Multicatch_block.java</a>         | Implementing multiple catch blocks.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Exception_Handling/Nested_try1.java\">Nested_try1.java</a>              | Example - 1 for nested try blocks.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Exception_Handling/Nested_try2.java\">Nested_try2.java</a>              | Example - 2 for nested try blocks.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Exception_Handling/Throw.java\">Throw.java</a>                    | Using the throw keyword.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Exception_Handling/Throw2.java\">Throw2.java</a>                   | Execution of lines after the throw keyword.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Exception_Handling/Balance_addition.java\">Balance_addition.java</a>         | Exception handling example.\\n</pre>\\n\\n---\\n\\n## Threads\\n\\n<pre>\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Threads/threads.java\">Threads.java</a>                  | Threads implementation in java.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Threads/multi_threads.java\">multi_threads.java</a>            | Implementing multi-threads.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Threads/synchronization.java\">synchronization.java</a>          | Synchronizing threads.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Threads/Matrix.java\">Matrix.java</a>                   | Finding the sum of matrix elements using threads.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Threads/Test1.java\">Test1.java</a>                    | Test case example for multi-threads.\\n</pre>\\n\\n---\\n\\n\\n## Miscellaneous\\n\\n<pre>\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/Binaryconcat.java\">BinaryConcat.java</a>             | Decimal value of Binary concatenation of given number.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/Dungeon%20quest.java\">DungeonQuest.java</a>             | A turn-based RPG text game.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/EvenPerfectSquare.java\">EvenPerfectSquare.java</a>        | Checks if sum of numbers at even position is perfect square or not.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/FibonacciPrime.java\">FibonacciPrime.java</a>           | Prints Prime Numbers in Fibonacci Series.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/GameOfLife.java\">GameOfLife.java</a>               | A short choice based game.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/Game_of_Coordinates.java\">GameOfCoordinates.java</a>        | A game to check if you can move to a point based on preset rules.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/Narcissisticno.java\">NarcissticNo.java</a>             | Prints Narcisstic and Armstrong Numbers.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/PythagoreanPrimes.java\">PythagoreanPrimes.java</a>        | Checks if given number is a Pythagorean Prime or not.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/RLD.java\">RLD.java</a>                      | Checks if given number is Reverse Length Divisible or not.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/amicable_no.java\">AmicableNo.java</a>               | Prints all the pairs of Amicable Numbers until N.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/decimal.java\">Decimal.java</a>                  | Finds the sum of numbers before and after decimal point.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/encryption.java\">Encryption.java</a>               | Encrypts and Decrypts Information Provided.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/flames.java\">Flames.java</a>                   | A simulation of the game, flames.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/nonloop.java\">NonLoop.java</a>                  | A simple game to understand recursion.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/permstring.java\">Permstring.java</a>               | Prints permutations of a given String.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/project.java\">Project.java</a>                  | A school project on university counselling\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/RandomNo.java\">RandomNo.java</a>                 | Prints N random numbers within a given range\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/resort.java\">Resort.java</a>                   | A Simple program to understand concepts of OOP in Java.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/append.java\">Append.java</a>                   | Helps in creating lists in python.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/UrlConcat.java\">UrlConcat.java</a>                | Program to create a link tag for HTML and saving output to clipboard.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/TagContentExtractor.java\">TagContentExtractor.java</a>      | Obtaining string between html tags.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/SportsCatalogue.java\">SportsCatalogue.java</a>          | Creating a sports catalogue with OOPS.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/DutchNationalFlagAlgo.java\">DutchNationalFlagAlgo.java</a>    | Algorithm to sort 0\\'s 1\\'s and 2\\'s in a array\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/InterchangeAndAdd.java\">InterchangeAndAdd.java</a>        | Interchange matrices and add them.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/PalindromeCheck.java\">PalindromeCheck.java</a>          | Checks if given string is a palindrome.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/TwoPointersAlgo.java\">TwoPointersAlgo.java</a>          | Searches for pairs in a sorted array.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/matrix.java\">matrix.java</a>                   | Finding the sum of two matrices.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/Pizza.java\">Pizza.java</a>                    | GUI calculator to bill products.\\n</pre>\\n\\n---\\n\\n## Contributing\\n\\nTo contribute to the program collection, fork the repository, create a new branch and send us a pull request. Make sure you read [CONTRIBUTING.md](https://github.com/SVijayB/JavaSpace/blob/master/.github/CONTRIBUTING.md) before sending us Pull requests.\\n\\n### Special thanks to all the contributors✨\\n\\n[![Contributors](https://contrib.rocks/image?repo=SVijayB/JavaSpace)](https://github.com/SVijayB/JavaSpace/graphs/contributors) \\n\\n## License\\n\\nThis repository is under The MIT License. Read the [LICENSE](https://github.com/SVijayB/JavaSpace/blob/master/LICENSE) file for more information.\\n'},\n",
       " {'repo': 'SVijayB/JavaSpace',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '# JavaSpace\\n\\n<p align=\"center\">\\n    <a href=\"https://github.com/SVijayB/JavaSpace\"><img src=\"assets/Java_logo_icon.png\" alt=\"Logo\" border=\"0\"></a>\\n    <br>A collection of all the Java programs I\\'ve worked on so far.\\n</p>\\n\\n<div align=\"center\">\\n    \\n![Issues](https://img.shields.io/github/issues/SVijayB/JavaSpace?color=brightgreen)\\n![Pull requests](https://img.shields.io/github/issues-pr/SVijayB/JavaSpace?color=brigthgreen)\\n![forks](https://img.shields.io/github/forks/SVijayB/JavaSpace)\\n![stars](https://img.shields.io/github/stars/SVijayB/JavaSpace)\\n![license](https://img.shields.io/github/license/SVijayB/JavaSpace?color=orange)\\n\\n</div>\\n\\n---\\n\\n<details>\\n\\n<summary>Table of contents</summary>\\n\\n## Table of Contents\\n\\n-   [Basics](#Basics)\\n-   [Data Conversion](#Data-Conversion)\\n-   [Data Structures](#Data-Structures)\\n-   [Searching And Sorting Algorithms](#Searching-And-Sorting-Algorithms)\\n-   [OOPS](#OOPS)\\n-   [Exception Handling](#Exception-Handling)\\n-   [Threads](#threads)\\n-   [Miscellaneous](#Miscellaneous)\\n-   [Contributing](#Contributing)\\n-   [License](#License)\\n\\n</details>\\n\\n## Basics\\n\\nBasics of Java are covered within these programs.\\n\\n<pre>\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/GP.java\">GP.java</a>                       | Calculates Grade Pay.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/Pyramid.java\">Pyramid.java</a>                  | Number Pyramid.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/average.java\">Average.java</a>                  | Average of N numbers.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/cutoff.java\">Cutoff.java</a>                   | Calculate PCM Cut-off.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/denomination.java\">Denomination.java</a>             | Money Denomination in India.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/evenandodd.java\">EvenAndOdd.java</a>               | Finds Odd and Even numbers from given numbers.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/flag.java\">Flag.java</a>                     | A Star pattern.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/fun1.java\">Fun1.java</a>                     | A Simple turn-based game.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/leapyear.java\">LeapYear.java</a>                 | Checks if leap year.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/merge.java\">Merge.java</a>                    | Merges two Arrays.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/palindrome.java\">Palindrome.java</a>               | Checks if Palindrome or not.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/power.java\">Power.java</a>                    | Calculates A raised to B.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/prime.java\">Prime.java</a>                    | Prints prime numbers from 0-100.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/recursion.java\">Recursion.java</a>                | Prints N Fibonacci numbers.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/reverse.java\">Reverse.java</a>                  | Reversing a Number or String.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/sorting.java\">Sorting.java</a>                  | Sorting N numbers.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/squares.java\">Squares.java</a>                  | Print squares until N\\'th turn.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/tables.java\">Tables.java</a>                   | Print Multiplication table.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/factorail.java\">Factorial.java</a>                | To find Factorial of a number.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/strong number.java\">Strong number.java</a>            | Check for Strong number.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/Fibonacci.java\">Fibonacci.java</a>                | To find Fibonacci series up to a given number.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/FizzBuzz.java\">FizzBuzz.java</a>                 | The FizzBuzz problem is a classic test given in coding interviews.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Basics/DuplicateChars.java\">DuplicateChars.java</a>           | A simple program that checks a string for duplicate characters.\\n</pre>\\n\\n---\\n\\n## Data Conversion\\n\\nContains programs that revolve around various Number Systems.\\n\\n<pre>\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Data_Conversion/BinaryCheck.java\">BinaryCheck.java</a>              | Checks if number has even number 1\\'s in it\\'s binary format or not.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Data_Conversion/Bintodecimal.java\">Bintodecimal.java</a>             | Binary to Decimal.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Data_Conversion/binary.java\">Binary.java</a>                   | Check if Binary or not.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Data_Conversion/decitobinary.java\">DeciToBinary.java</a>             | Decimal to Binary.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Data_Conversion/decitooctal.java\">DeciToOctal.java</a>              | Decimal to Octal.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Data_Conversion/DeciToHexa.java\">DeciToHexa.java </a>              | Decimal to Hexa\\n</pre>\\n\\n---\\n\\n## Data Structures\\n\\nPrograms Relating to the implementation of various Data Structures are present in this folder.\\n\\n<pre>\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Data_Structures/Queue.java\">Queue.java</a>                    | Queue Implementation.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Data_Structures/BST.java\">BST.java</a>                      | Binary Search Tree.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Data_Structures/circularqueue.java\">CircularQueue.java</a>            | Circular Queue Implementation.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Data_Structures/DLL.java\">DLL.java</a>                      | Double Linked List.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Data_Structures/linkedlist.java\">Linkedlist.java</a>               | Linked List.\\n• <a href=https://github.com/SVijayB/JavaSpace/blob/master/Data_Structures/DetectCycleInList.java\">DetectCycleInList.java</a>               | Detect Cycle in an Undirected Graph (using DFS)\\n <a href=https://github.com/SVijayB/JavaSpace/blob/master/Data_Structures/Topological_sort_BFS.java\">Topological_sort_BFS.java</a>               | Topological_sort_BFS\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Data_Structures/stackpr1.java\">Stackpr1.java</a>                 | Stack Implementation.\\n• <a href=\"https://github.com/SVijayB/Java/blob/master/Data_Structures/Kruskal%E2%80%99s%20algorithm.java\">Kruskal\\'s algorithm.java</a>      | Kruskal\\'s algorithm to find shortest path.\\n• <a href=\"https://github.com/SVijayB/Java/blob/master/Data_Structures/Linear%20Probing.java\">Linear Probing.java</a>           | Creating a Hash Table using Linear Probing method.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Data_Structures/InorderTraversal.java\">InorderTraversal.java</a>         | Inorder traversal in java.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Data_Structures/KthMinAndMaxInArray.java\">KthMinAndMaxInArray.java</a>      | Finding Kth smallest and largest element in an array.\\n</pre>\\n\\n---\\n\\n## Searching And Sorting Algorithms\\n\\nPrograms used for searching and sorting given elements.\\n\\n<pre>\\n• <a href=\"https://github.com/SVijayB/Java/blob/master/Searching%20%26%20Sorting/Binary%20Search.java\">Binary Search.java</a>            | Iterative Binary Search.\\n• <a href=\"https://github.com/SVijayB/Java/blob/master/Searching%20%26%20Sorting/Linear%20Search.java\">Linear Search.java</a>            | Linear Searching Algorithm.\\n• <a href=\"https://github.com/SVijayB/Java/blob/master/Searching%20%26%20Sorting/Heap%20Sort.java\">Heap Sort.java</a>                | Sorting entires using Heap Sort.\\n• <a href=\"https://github.com/SVijayB/Java/blob/master/Searching%20%26%20Sorting/Merge%20Sort.java\">Merge Sort.java</a>               | Sorting using Merge Sort.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Searching%20%26%20Sorting/BubbleSort.java\">Bubble Sort.java</a>              | Sorting using Bubble Sort.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Searching%20%26%20Sorting/QuickSort.java\">QuickSort.java</a>                | Quick Sort.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Searching%20%26%20Sorting/Selection%20Sort.java\">Selection Sort.java</a>           | Selection Sort.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Searching%20%26%20Sorting/InsertionSort.java\">Insertion Sort.java</a>           | Insertion Sort.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Searching%20%26%20Sorting/Radix Sort.java\">Radix Sort.java</a>           | Radix Sort.\\n</pre>\\n\\n---\\n\\n## OOPS\\n\\nImplementing Object Oriented Programming System(OOPs) in Java.\\n\\n<pre>\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/OOPS/Classes.java\">Classes.java</a>                  | Understanding classes in java.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/OOPS/Initializing_objects.java\">Initializing_objects.java</a>     | Creating/initializing data using reference variables.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/OOPS/Static.java\">Static.java</a>                   | Understanding uses of static variables.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/OOPS/Polymorphism.java\">Polymorphism.java</a>             | Exploring Polymorphism in OOPS.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/OOPS/Constructors.java\">Constructors.java</a>             | Initializing variables using constructors in Java.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/OOPS/Object.java\">Object.java</a>                   | Objects in java.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/OOPS/Variables.java\">Variables.java</a>                | Different types of variables in java.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/OOPS/ArrayOfObjects.java\">ArrayOfObjects.java</a>           | Creating an array of objects.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/OOPS/Encapsulation/Encapsulation.java\">Encapsulation.java</a>            | Encapsulation of data.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/OOPS/Encapsulation/Accessing.java\">Accessing.java</a>                | Accessing private variables using public methods.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/OOPS/AbstractClass.java\">AbstractClass.java</a>            | Understanding abstract classes.\\n</pre>\\n\\n### Examples\\n\\nVarious examples to help implement OOPS in Java.\\n\\n<pre>\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/OOPS/Examples/Holiday.java\">Holiday.java</a>                  | Class Objects and Methods.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/OOPS/Examples/Bookstore.java\">Bookstore.java</a>                | Static, final and Constructors.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/OOPS/Examples/SemesterMarks.java\">SemesterMarks.java</a>            | Understanding Inheritance.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/OOPS/Examples/Shape.java\">Shape.java</a>                    | Example for Polymorphism.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/OOPS/Examples/GenericMethods.java\">GenericMethods.java</a>           | Using generic classes and methods.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/OOPS/Examples/Distribution.java\">Distribution.java</a>             | Parameterised constructor and exception handling.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/OOPS/Examples/Shapes.java\">Shapes.java</a>                   | Example for Inheritance.\\n</pre>\\n\\n---\\n\\n## Exception Handling\\n\\nHandling various exceptions using the try, catch, finally and throw keywords in Java.\\n\\n<pre>\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Exception_Handling/Try_and_catch.java\">Try_and_catch.java</a>            | Using the try and catch keywords.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Exception_Handling/Try_and_finally.java\">Try_and_finally.java</a>          | Replacing try, catch and finally keywords.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Exception_Handling/Multicatch_block.java\">Multicatch_block.java</a>         | Implementing multiple catch blocks.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Exception_Handling/Nested_try1.java\">Nested_try1.java</a>              | Example - 1 for nested try blocks.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Exception_Handling/Nested_try2.java\">Nested_try2.java</a>              | Example - 2 for nested try blocks.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Exception_Handling/Throw.java\">Throw.java</a>                    | Using the throw keyword.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Exception_Handling/Throw2.java\">Throw2.java</a>                   | Execution of lines after the throw keyword.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Exception_Handling/Balance_addition.java\">Balance_addition.java</a>         | Exception handling example.\\n</pre>\\n\\n---\\n\\n## Threads\\n\\n<pre>\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Threads/threads.java\">Threads.java</a>                  | Threads implementation in java.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Threads/multi_threads.java\">multi_threads.java</a>            | Implementing multi-threads.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Threads/synchronization.java\">synchronization.java</a>          | Synchronizing threads.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Threads/Matrix.java\">Matrix.java</a>                   | Finding the sum of matrix elements using threads.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Threads/Test1.java\">Test1.java</a>                    | Test case example for multi-threads.\\n</pre>\\n\\n---\\n\\n\\n## Miscellaneous\\n\\n<pre>\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/Binaryconcat.java\">BinaryConcat.java</a>             | Decimal value of Binary concatenation of given number.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/Dungeon%20quest.java\">DungeonQuest.java</a>             | A turn-based RPG text game.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/EvenPerfectSquare.java\">EvenPerfectSquare.java</a>        | Checks if sum of numbers at even position is perfect square or not.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/FibonacciPrime.java\">FibonacciPrime.java</a>           | Prints Prime Numbers in Fibonacci Series.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/GameOfLife.java\">GameOfLife.java</a>               | A short choice based game.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/Game_of_Coordinates.java\">GameOfCoordinates.java</a>        | A game to check if you can move to a point based on preset rules.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/Narcissisticno.java\">NarcissticNo.java</a>             | Prints Narcisstic and Armstrong Numbers.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/PythagoreanPrimes.java\">PythagoreanPrimes.java</a>        | Checks if given number is a Pythagorean Prime or not.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/RLD.java\">RLD.java</a>                      | Checks if given number is Reverse Length Divisible or not.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/amicable_no.java\">AmicableNo.java</a>               | Prints all the pairs of Amicable Numbers until N.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/decimal.java\">Decimal.java</a>                  | Finds the sum of numbers before and after decimal point.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/encryption.java\">Encryption.java</a>               | Encrypts and Decrypts Information Provided.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/flames.java\">Flames.java</a>                   | A simulation of the game, flames.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/nonloop.java\">NonLoop.java</a>                  | A simple game to understand recursion.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/permstring.java\">Permstring.java</a>               | Prints permutations of a given String.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/project.java\">Project.java</a>                  | A school project on university counselling\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/RandomNo.java\">RandomNo.java</a>                 | Prints N random numbers within a given range\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/resort.java\">Resort.java</a>                   | A Simple program to understand concepts of OOP in Java.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/append.java\">Append.java</a>                   | Helps in creating lists in python.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/UrlConcat.java\">UrlConcat.java</a>                | Program to create a link tag for HTML and saving output to clipboard.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/TagContentExtractor.java\">TagContentExtractor.java</a>      | Obtaining string between html tags.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/SportsCatalogue.java\">SportsCatalogue.java</a>          | Creating a sports catalogue with OOPS.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/DutchNationalFlagAlgo.java\">DutchNationalFlagAlgo.java</a>    | Algorithm to sort 0\\'s 1\\'s and 2\\'s in a array\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/InterchangeAndAdd.java\">InterchangeAndAdd.java</a>        | Interchange matrices and add them.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/PalindromeCheck.java\">PalindromeCheck.java</a>          | Checks if given string is a palindrome.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/TwoPointersAlgo.java\">TwoPointersAlgo.java</a>          | Searches for pairs in a sorted array.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/matrix.java\">matrix.java</a>                   | Finding the sum of two matrices.\\n• <a href=\"https://github.com/SVijayB/JavaSpace/blob/master/Miscellaneous/Pizza.java\">Pizza.java</a>                    | GUI calculator to bill products.\\n</pre>\\n\\n---\\n\\n## Contributing\\n\\nTo contribute to the program collection, fork the repository, create a new branch and send us a pull request. Make sure you read [CONTRIBUTING.md](https://github.com/SVijayB/JavaSpace/blob/master/.github/CONTRIBUTING.md) before sending us Pull requests.\\n\\n### Special thanks to all the contributors✨\\n\\n[![Contributors](https://contrib.rocks/image?repo=SVijayB/JavaSpace)](https://github.com/SVijayB/JavaSpace/graphs/contributors) \\n\\n## License\\n\\nThis repository is under The MIT License. Read the [LICENSE](https://github.com/SVijayB/JavaSpace/blob/master/LICENSE) file for more information.\\n'},\n",
       " {'repo': 'yodaiken/dolphinsr',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# DolphinSR: Spaced Repetition in JavaScript\\n\\nDolphinSR implements [spaced repetition](https://en.wikipedia.org/wiki/Spaced_repetition) in\\nJavaScript. Specifically, it uses [Anki\\'s modifications](https://apps.ankiweb.net/docs/manual.html#what-algorithm)\\nto the SM2 algorithm including:\\n\\n- an initial mode for learning new cards\\n- a mode for re-learning cards after forgetting them\\n- reducing the number of self-assigned ratings from 6 to 4\\n- factoring lateness into card scheduling\\n- Anki\\'s default configuration options\\n\\nWhile DolphinSR is intentionally very similar to Anki\\'s algorithm, it does deviate in a few ways:\\n\\n- improved support for adding reviews out of order (for example, due to network latency)\\n- very different internal data structures (DolphinSR is largely written in a functional style to\\n  make testing and debugging easier, and does not rely on storing computed data or any SQL database)\\n- only one kind of card\\n\\n## Installation\\n\\nDolphinSR is an `npm` package. Install it with either `yarn add dolphinsr` or\\n`npm install --save dolphinsr`.\\n\\n**It\\'s strongly recommended that you use Flow to statically check your code when using DolphinSR.**\\nWe rely on it exclusively for type-checking, and don\\'t do any runtime validation of type arguments.\\nFor more information, [visit the Flow webpage](https://flow.org);\\n\\n## Quick Start\\n\\n```{js}\\n\\nimport { DolphinSR, generateId } from \\'dolphinsr\\';\\nimport type { Master, Review } from \\'dolphinsr\\';\\n\\n// Specify the combinations DolphinSR should make out of your master cards.\\n// Numbers refer to indexes on the card. (Don\\'t worry and keep reading if you don\\'t understand)\\nconst chineseCombinations = [\\n  {front: [0], back: [1, 2]},\\n  {front: [1], back: [0, 2]},\\n  {front: [2], back: [0, 3]}\\n];\\nconst frenchCombinations = [\\n  {front: [0], back: [1]},\\n  {front: [1], back: [0]}\\n];\\n\\n// Create the master cards that DolphinSR will use spaced repetition to teach.\\n// Note: in a real program, you\\'d want to persist these somewhere (a database, localStorage, etc)\\nconst vocab: Array<Master> = [\\n  {\\n    id: generateId(),\\n    combinations: chineseCombinations,\\n    fields: [\\'你好\\', \\'nǐ hǎo\\', \\'hello\\']\\n  },\\n  {\\n    id: generateId(),\\n    combinations: chineseCombinations,\\n    fields: [\\'世界\\', \\'shìjiè\\', \\'world\\']\\n  },\\n  {\\n    id: generateId(),\\n    combinations: frenchCombinations,\\n    fields: [\\'le monde\\', \\'the world\\']\\n  },\\n  {\\n    id: generateId(),\\n    combinations: frenchCombinations,\\n    fields: [\\'bonjour\\', \\'hello (good day)\\']\\n  }\\n];\\n\\n// Create the datastore used to house reviews.\\n// Again, in a real app you\\'d want to persist this somewhere.\\nconst reviews: Array<Review> = [];\\n\\n// Create a new DolphinSR instance\\nconst d = new DolphinSR();\\n\\n// Add all of your vocab to the DolphinSR instance\\nd.addMasters(...vocab);\\n\\n// Add any existing reviews to the DolphinSR instance\\n// (In this example, this doesn\\'t do anything since reviews is empty.)\\nd.addReviews(...reviews);\\n\\n// Now, DolphinSR can tell us what card to review next.\\n// Since generateId() generates a random ID, it could be any of the cards we added.\\n// For example, it could be:\\n//     {\\n//       master: <Id>,\\n//       combination: {front: [0], back: [1, 2]},\\n//       front: [\\'你好\\'],\\n//       back: [\\'nǐ hǎo\\', \\'hello\\']\\n//     }\\nconst card = d.nextCard();\\n\\n// It will also give us statistics on the cards we have:\\n// Since we added 2 masters with 3 combinations (the Chinese vocab) and 2 masters with 2\\n// combinations (the French vocab), we will have 10 cards. Since we haven\\'t reviewed any of them\\n// yet, they will all be in a \"learning\" state.\\nconst stats = d.summary(); // => { due: 0, later: 0, learning: 10, overdue: 0 }\\n\\n// Now, we can review the current card (probably triggered by a real app\\'s UI)\\n// If we already knew the answer, we would create a review saying that it was \"easy\" to recall:\\nconst review: Review = {\\n  // identify which card we\\'re reviewing\\n  master: d.nextCard().master,\\n  combination: d.nextCard().combination,\\n\\n  // store when we reviewed it\\n  ts: new Date(),\\n\\n  // store how easy it was to remember\\n  rating: \\'easy\\'\\n};\\nreviews.push(review); // in a real app, we\\'d store this persistently\\nd.addReviews(review);\\n\\n// Since we reviewed the current card, and marked it easy to remember, DolphinSR will move it into\\n// \\'review\\' mode, which resembles classic SM2 spaced repetition. So everything else will still be in\\n// \\'learn\\' mode, and it will be scheduled to be reviewed later.\\nd.summary(); // => { due: 0, later: 1, learning: 9, overdue: 0 }\\n\\n// This will show the next card to review.\\nd.nextCard();\\n```\\n\\n## API\\n\\n### generateId(): Id\\nThis generates a new ID for a master card. It uses the `uuid` package under the hood. Always use\\n`generateId()` to generate IDs for your masters.\\n\\n### new DolphinSR()\\nCreate a new DolphinSR instance, `d`.\\n\\n### (new DolphinSR()).addMasters(...masters: Master[]): void\\nAdd masters to the DolphinSR instance. Masters with duplicate IDs will cause a runtime exception.\\n\\n### (new DolphinSR()).addReviews(...reviews: Review[]): boolean\\nAdd reviews to the DolphinSR instance.\\n\\n`addReviews()` is significantly more efficient if `reviews` are sorted in ascending order by `ts`,\\nand all chronologically come after the previous latest review for any card. If this condition is\\nmet, `addReviews()` will return `false`. Otherwise, it returns `true`.\\n\\n### (new DolphinSR()).summary(): { due: number, later: number, learning: number, overdue: number }\\nReturns summary statistics for cards. Each category (due, later, learning, overdue) is a count.\\n\\n- Due cards are cards that the algorithm has determined should be reviewed on the day of the call. \\n  (That is, the current date as reflected by `new Date()`.)\\n- Overdue cards are cards that the algorithm has determined should be reviewed earlier than the\\n  day of the call.\\n- Learning cards are cards that are either new and don\\'t have any reviews, or cards that were\\n  forgotten (reviewed with an `again` rating) and not yet re-learned.\\n- Later cards are cards that will be due in the future.\\n\\n### (new DolphinSR()).nextCard(): ?Card\\nReturns the next card to be reviewed, or `null` if there are no cards that need to be reviewed. Any\\ncard classified as due, overdue or learning by `.summary()` could appear in `.nextCard()`.\\n\\n*Note:* `.nextCard()` is deterministic--there is a defined order for prioritizing cards that are in\\ndifferent classifications and within classifications. That means that for any given set of masters\\nand reviews added to a DolphinSR instance, `nextCard()` will return the same result.\\n\\n\\n## Types\\n\\n### Master\\n\\nA `Master` is an object conforming to the following Flow signature:\\n\\n```{js}\\n{\\n  id: Id, // from generateId()\\n  fields: Array<Field>, // see Field\\n  combinations: Array<Combination>, // see Combination\\n}\\n```\\n\\n### Field\\n\\nA `Field` is a unit of data in a master. It is type alias for `string`.\\n\\n### Combination\\n\\nA `Combination` is an object representing how `Fields` in a master should be combined to fit on a\\ncard with a front and a back. It looks like this:\\n\\n```{js}\\n{front: number[], back: number[]}\\n```\\n\\n### Rating\\n\\nA `Rating` is an enum describing how well a user knows a specific combination of a master card. It\\ncan be (in descending order of ease):\\n\\n- `\\'easy\\'`\\n- `\\'good\\'`\\n- `\\'hard\\'`\\n- `\\'again\\'`\\n\\n### Review\\n\\nA `Review` is an object describing a review of a card by a user. It should look like this:\\n\\n```{js}\\n{\\n  master: Id,\\n  combination: Combination,\\n  ts: Date,\\n  rating: Rating,\\n}\\n```\\n\\n### Card\\n\\nA `Card` is an object returned by `.nextCard()` which describes a part of a master suitable for\\ndisplaying to a user. It should look like this:\\n\\n```{js}\\n{\\n  master: Id,\\n  combination: Combination,\\n  front: Array<Field>,\\n  back: Array<Field>\\n}\\n```\\n'},\n",
       " {'repo': 'choderalab/perses',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '<!--- [![Travis Build Status](https://travis-ci.org/choderalab/perses.svg?branch=master)](https://travis-ci.org/choderalab/perses/branches) --->\\n[![GH Actions Status](https://github.com/choderalab/perses/workflows/CI/badge.svg)](https://github.com/choderalab/perses/actions?query=branch%3Amaster)\\n[![codecov](https://codecov.io/gh/choderalab/perses/branch/main/graph/badge.svg)](https://codecov.io/gh/choderalab/perses/branch/main)\\n[![Documentation Status](https://readthedocs.org/projects/perses/badge/?version=latest)](http://perses.readthedocs.io/en/latest/?badge=latest)\\n[![DOI](https://zenodo.org/badge/27087846.svg)](https://zenodo.org/badge/latestdoi/27087846)\\n\\n# Perses\\n\\nExperiments with expanded ensemble simulation to explore chemical and mutational space.\\n\\n## License\\nThis software is licensed under the [MIT license](https://opensource.org/licenses/MIT), a permissive open source license.\\n\\n## Notice\\n\\nPlease be aware that this code is made available in the spirit of open science, but is currently pre-alpha--that is,\\n**it is not guaranteed to be completely tested or provide the correct results**, and the API can change at any time\\nwithout warning. If you do use this code, do so at your own risk. We appreciate your input, including raising issues\\nabout potential problems with the code, but may not be able to address your issue until other development activities\\nhave concluded.\\n\\n## Install\\n\\nSee our installation instructions [here](https://perses.readthedocs.io/en/latest/installation.html).\\n\\n### Quick Start\\n\\nIn a fresh conda environment:\\n\\n```\\n$ conda config --add channels conda-forge openeye\\n$ conda install perses openeye-toolkits\\n```\\n\\n## Manifest\\n\\n* `perses/` - Package containing code for performing expanded ensemble simulations\\n* `examples/` - Contains examples for various systems and methods of simulation\\n* `attic/` - some old code that may be useful as part of the new setup\\n* `devtools/` - Continuous integration and packaging utilities\\n* `notes/` - LaTeX notes deriving acceptance criteria and stochastic approximation methods\\n\\n## Contributors\\n\\nA complete list of contributors can be found at [GitHub Insights](https://github.com/choderalab/perses/graphs/contributors).\\n\\nMajor contributors include:\\n* Julie M. Behr\\n* Hannah E. Bruce Macdonald\\n* John D. Chodera\\n* Patrick B. Grinaway\\n* Mike M. Henry\\n* Iván J. Pulido\\n* Jaime Rodríguez-Guerra\\n* Dominic A. Rufa\\n* Ivy Zhang\\n'},\n",
       " {'repo': 'sthewissen/KickassUI.InSpace',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# KickassUI.InSpace\\n\\nA simple but good looking Xamarin.Forms UI screen as talked about in my blogpost on: https://www.thewissen.io/xamarin-forms-in-space/\\n\\nTools used\\nLiveXAML – Live simulator updates for your XAML code – http://www.livexaml.com\\n\\n![](https://github.com/sthewissen/Posy/blob/master/app.gif)\\n'},\n",
       " {'repo': 'yandex-shri/introtask-space',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Задача из анкеты для поступления в Школу Разработки Интерфейсов Яндекса 2013\\n\\nВы — пилот грузового межгалактического корабля. Вашей работой является перевозка грузов с одной планету на другую. Грузоподъемность вашего корабля ограничена, поэтому за один рейс вы можете перевезти не более N кг полезного груза. Ваш корабль умеет сообщать свое состояние (местоположение и степень загруженности), а также летать в любую точку пространства или на любую планету. Каждая планета может содержать на себе груз, который может быть погружен на корабль или выгружен обратно на планету.\\n\\n## Задание\\n\\nВ файле [task.js](task.js) дан интерфейс корабля и планеты. Эти интерфейсы не являются завершенными и скорее всего потребуют доработки. Напишите недостающий код.\\n\\n## Пример использования\\n\\nПеревоз 1000т груза с планеты B на планету A.\\n\\n```js\\nvar vessel = new Vessel(\\'Яндекс\\', [0,0], 1000);\\nvar planetA = new Planet(\\'A\\', [0,0], 0);\\nvar planetB = new Planet(\\'B\\', [100, 100], 5000);\\n\\n// Проверка текущего состояния\\nvessel.report(); // Грузовой корабль. \"Яндекс\". Местоположение: 0,0. Товаров нет.\\nplanetA.report(); // Планета \"A\". Местоположение: 0,0. Грузов нет.\\nplanetB.report(); // Планета \"B\". Местоположение: 100,100. Доступно груза: 5000т.\\n\\nvessel.flyTo(planetB);\\nplanetB.loadCargoTo(vessel, 1000);\\nvessel.report(); // Грузовой корабль. \"Яндекс\". Местоположение: \\'B\\'. Занято: 1000 из 1000т.\\n\\nvessel.flyTo(planetA);\\nplanetA.unloadCargoFrom(vessel, 500);\\nvessel.report(); // Грузовой корабль. \"Яндекс\". Местоположение: \\'A\\'. Занято: 500 из 1000т.\\nplanetA.report(); // Планета \"A\". Местоположение: 0,0. Доступно груза: 500т.\\nplanetB.report(); // Планета \"B\". Местоположение: 100,100. Доступно груза: 4000т.\\n```\\n\\n'},\n",
       " {'repo': 'turingschool/spaceport',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': \"# Spaceport\\n\\n## Overview\\n\\nIn front end web development, the programming language you will encounter most\\noften is JavaScript. Soon, we'll also use HTML and CSS to help a user interact\\nwith our applications, however before we do that, we need to understand how to\\nuse JavaScript to handle the logic of our applications.\\n\\nIn this project, you'll be gaining experience working with variables, primitive\\ndata types, looping, arrays, objects and classes. As you work through the\\niterations, be sure to take time to stop and refactor you solutions. There is\\nrarely one right way to solve a problem in programming, and part of your job\\nwill be evaulating the trade offs between different approaches to solving a\\nproblem.\\n\\n## Learning goals\\n\\n  - Understand what JavaScript primatives are, and how/when to use them\\n  - Understand how to declare variables and assign data to them\\n  - Practice using objects\\n  - Practice defining and creating classes\\n\\n## Setup\\n\\n  - Fork this project to your own Github account\\n  - clone the repository to your local machine\\n  - `cd` into the project\\n  - run `npm install` to install the necessary dependencies\\n\\n## Iterations\\n\\n### 0: Practice Variables, Primitives, Functions, Arrays, and Objects\\n  - In the `src/` directory, you'll find a file called 'warm-up.js'. Read\\n    through the instructions in the file carefully. The exercises in this file\\n    will help you to complete the rest of the iterations  \\n\\n### 1: Create Being & Part Classes\\n  - For the rest of the iterations, you will be working to build out some\\n    classes, using a test suite as your guide.  \\n  - Start with the Being class.  \\n    - Unskip the first test in `test/being.js`  \\n    - Run `npm test test/being.js`  \\n    - Read the error messages CAREFULLY!  \\n  - Once you've fully implemented the Being class, move on to the tests for the\\n    Part class    \\n    - Run `npm test test/part.js`  \\n  - Before moving on to the next iteration, take time to refactor your\\n    solutions. Is this the best approach to solving the problem? Is there a\\n    different way you could make the tests pass?  \\n\\n### 2: Create Ship Class\\n  - Unskip the first test in `test/ship.js`, and get to work implementing  \\n  - Run `npm test test/ship.js`  \\n  - Before moving on to the next iteration, take time to refactor your\\n    solutions. Is this the best approach to solving the problem? Is there a\\n    different way you could make the tests pass?  \\n\\n### 3: Create Shop Class\\n  - Unskip the first test in `test/shop.js`, and get to work implementing  \\n  - Run `npm test test/shop.js`  \\n  - Before moving on to the next iteration, take time to refactor your\\n    solutions. Is this the best approach to solving the problem? Is there a\\n    different way you could make the tests pass?  \\n\\n### 4: Create Planet Class\\n  - Unskip the first test in `test/planet.js`, and get to work implementing  \\n  - Run `npm test test/planet.js`  \\n\"},\n",
       " {'repo': 'bixbydevelopers/capsule-sample-space-resorts',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '<p align=\"Center\">\\n  <img src=\"https://bixbydevelopers.com/dev/docs-assets/resources/dev-guide/bixby_logo_github-11221940070278028369.png\">\\n  <br/>\\n  <h1 align=\"Center\">Bixby Space Resorts Sample Capsule</h1>\\n</p>\\n\\n## Overview\\n\\nThis capsule is the the companion code to the Bixby [Space Resorts Sample Capsule](https://bixbydevelopers.com/dev/docs/sample-capsules/walkthroughs/space-resorts) guide. Space Resorts is a fun capsule that allows you to book a space vacation! You can find a space resort, book a reservation, look up a reservation, and change or cancel a reservation. This advanced capsule demonstrates development and design of [searches](https://bixbydevelopers.com/dev/docs/sample-capsules/walkthroughs/simple-search), [transactions](https://bixbydevelopers.com/dev/docs/dev-guide/developers/modeling.modeling-actions.transactional-workflows) and [UI](https://bixbydevelopers.com/dev/docs/dev-guide/developers/creating-ui.building-views).\\n\\n---\\n\\n## Use cases\\n\\n### Finding Space Resorts\\n\\n#### Outer \"find\" Queries\\n\\n![Outer \"find\" Query](./assets/images/OuterFindQuery.png)\\n\\nYou can see all trained utterances and plans by entering this query in the\\ntraining tab search bar: `goal:SpaceResort#all -has:continue`. Examples:\\n\\n- Find space resorts\\n\\n- Show me space hotels with crater canyoneering\\n\\n- Search for hotels around The Red Planet\\n\\n- Look for space hotels with quantum bungee jumping around Saturn\\n\\nWe train these to have the goal `SpaceResort#all` and annotate any resort names,\\nplanets and search criteria as Values. Here `SpaceResort#all` is a property\\nprojection to the `all` property of the `SpaceResort`. This property is a\\nboolean of type `ViewAll`, which is always true and is used as a proxy to\\nsignify that we want to display the full space resort, instead of focusing on a\\nsingle property like `gravity` or `planet` (see property projections below). A\\nmatch-pattern ties the views and dialogs for this property projection to\\ndescribe the whole hotel.\\n\\n```\\nmatch {\\n  ViewAll(all) {\\n    from-property {\\n      SpaceResort (result)\\n    }\\n  }\\n}\\n```\\n\\nWhy treat this as a property projection instead of setting the goal to the\\n`SpaceResort` Structure?\\n\\nWhen the user wants to book a space resort and there are many possible\\ncandidates in context, the user picks a single one using a selection prompt, in\\norder to continue with the booking (see SpaceResort Selection Prompt selection\\nbelow for full details). The context for that prompt is `SpaceResort`, and\\n**selection prompt training must always have the same goal as its context**.\\nAccordingly, selection prompt training for `SpaceResort` needs to use\\n`SpaceResort` as its goal.\\n\\nThe selection prompt training also uses a special flagged signal to route the\\nplan through the `SelectResort` action that filters the hotels currently in\\ncontext based on the newly provided inputs. It is crucial to add the\\n`SelectResort` flagged signal to the selection prompt training annotations to\\nachieve this behavior.  However, we do not want to add the `SelectResort`\\nflagged signal to the \"find\" queries because these should issue a new search via\\nthe `FindSpaceResorts` action instead of filtering existing results via the\\n`SelectResort` action. Since the selection prompt requires different annotations\\npatterns compared to the \"find\" queries, and **annotation patterns must be\\nconsistent for the same goal**, this means that they must use a different goal.\\n\\nThis example demonstrates two points.  First, the selection prompt\\ntraining must use `SpaceResort` as a goal. Second, that the \"find\" queries\\nmust use a different goal than the selection prompts. By putting these together,\\nwe deduce that the \"find\" queries cannot use `SpaceResort` as a goal. Therefore,\\nwe use a distinct goal (`SpaceResort#all`) for the \"find\" queries in order to\\nprovide consistent annotation patterns per goal.\\n\\nWhy don\\'t we set the goal to the `FindSpaceResorts` action?\\n\\nAnother alternative would be to set the goal to the specific `FindSpaceResorts`\\naction, making it very clear how to fulfill the request. This approach would\\nsimplify the match patterns for views, so we would not need to use the\\n`from-property` key. However, we use the property projection approach\\nso that our final resting context for \"find\" queries is the same as for property\\nprojection queries.  This means that we can pivot between all these states\\nseamlessly, or launch the \"book\" flow from any of these. Example conversation:\\n\\n1. Find space resorts\\n\\n2. The second one\\n\\n3. What planet is it on?\\n\\n4. What\\'s the gravity there?\\n\\n5. Book it\\n\\n#### Inner \"find\" Queries (Continuations)\\n\\n![Inner \"find\" Query](./assets/images/InnerFindQuery.png)\\n\\nYou can see all trained utterances and plans by entering this query in the\\ntraining tab search bar: `goal:SpaceResort#all has:continue`. Examples:\\n\\n- On Jupiter\\n\\n- With low gravity\\n\\n- Only the ones that are kid-friendly\\n\\nThese are continuations of the outer \"find\" queries that allow users to refine\\ntheir space resorts search by providing additional inputs. Since the goal for\\nouter \"find\" queries is `SpaceResort#all`, we annotate both the goal and the\\n\"Continuation of\" to also be `SpaceResort#all`.  Any resort names, planets and\\nsearch criteria are annotated as Values.  This reissues a search with the new\\ninputs being added to those already in context.\\n\\n### Property Projection Flows (Planet, Gravity)\\n\\n#### Outer Property Projections\\n\\n![Outer Property Projection](./assets/images/OuterPropertyProjection.png)\\n\\nYou can see all trained utterances and plans by entering this query in the\\ntraining tab search bar: `goal:SpaceResort#* -goal:SpaceResort#all\\n-has:continue`. For example:\\n\\n- What\\'s the gravity at The Mercurial?\\n\\n- Where is Io-Tel?\\n\\nHere, the user is asking to know about a specific property of a space resort,\\nsuch as the gravity or the planet.  We train the goal to be that property\\nprojection (ex: `SpaceResort#gravity`), and we annotate any resort names,\\nplanets and search criteria as Values.  We also add a special flagged signal\\nroute to `ProjectResort`.  This is in case there were multiple space resorts\\nthat matched the search inputs.  Then the `ProjectResort` action will ask the\\nuser to select a single space resort before providing the answer.\\n\\n#### Inner Property Projections (Continuations)\\n\\n![Inner Property Projection](./assets/images/InnerPropertyProjection.png)\\n\\nYou can see all trained utterances and plans by entering this query in the\\ntraining tab search bar: `goal:SpaceResort#* -goal:SpaceResort#all\\nhas:continue`. For example:\\n\\n- What\\'s the gravity there?\\n\\n- What planet is it on?\\n\\nWe train these just like the outer property projections, with the addition of a\\n\"Continuation of\" `SpaceResort`.  This allows pivoting between inner/outer \"find\"\\nqueries and inner/outer property projections.\\n\\n### Booking Space Resorts\\n\\n#### Outer \"book\" Queries\\n\\nYou can see all trained utterances and plans by entering this query in the\\ntraining tab search bar: `goal:MakeReservation -has:continue`. For example:\\n\\n- Make a reservation for a space resort on Mars the third weekend in December\\nfor 2 astronauts\\n\\nWe train these to the goal `MakeReservation`, which is the Action to finalize the\\ntransaction. We also add two flagged signal routes: `CreateItem` and\\n`CreateOrder`.  We annotate as Values any present inputs for either \"find\" or\\n\"book\", such as resort name, planet, search criteria, number of astronauts, etc.\\nThis creates a plan to first find a space resort that matches the search inputs,\\nthen prepare an `Order` and pass it to the `MakeReservation` action, which will\\npresent the user with a Confirmation screen to review and agree to the\\nreservation.\\n\\n#### Inner \"book\" Queries (Continuations of SpaceResort)\\n\\nYou can see all trained utterances and plans by entering this query in the\\ntraining tab search bar: `goal:MakeReservation continuation:SpaceResort`. For\\nexample:\\n\\n- Make reservation\\n\\n- Book a pod for 2 astronauts\\n\\nWe train these just like the outer \"book\" queries, with the addition of a\\n\"Continuation of\" `SpaceResort`.  This is to cover cases where the users are\\nalready browsing space resorts and want to initiate a booking for one of the\\nresults in context.\\n\\n#### Inner \"Change Order\" Queries (Continuations to change the Order)\\n\\nYou can see all trained utterances and plans by entering this query in the\\ntraining tab search bar: `goal:MakeReservation\\ncontinuation:MakeReservation`. For example:\\n\\n- Pick a different habitat pod\\n\\n- Change that to 2 astronauts\\n\\n- Select a different date\\n\\nWe train these as \"Continuation of\" `MakeReservation` for cases where the\\nusers are at the Confirmation screen to review their order and decide that they\\nwant to make some changes.  The goal remains `MakeReservation`, and any Values\\nare annotated as such (ex: number of astronauts, pod name).  This time, the\\nflagged signal route is to `ChangeOrder`.  This is to re-route the request to\\nupdate the Order with the newly provided information.  For generic requests that\\ndo not contain a new input Value (ex: Change the number of astronauts), we\\nadd an extra flagged signal route to the action for that request (ex:\\nGetNumberOfAstronauts).\\n\\n### Prompting Flows\\n\\n#### Confirmation Prompt\\n\\n![Confirmation Prompt](./assets/images/ConfirmationPrompt.png)\\n\\nYou can see all trained utterances and plans by entering this query in the\\ntraining tab search bar: `prompt:Confirmation`. For example:\\n\\n- Yes\\n\\n- Do it\\n\\nWhen the user is done reviewing their Order at the Confirmation Prompt, they can\\nuse these utterances to move forward and proceed with the reservation. This is\\nan \"At prompt for\" `Confirmation` with goal `Confirmation`.  The Confirmation\\nitself is annotated with a boolean Value \"true\" or \"false\".\\n\\n#### SpaceResort Selection Prompt\\n\\n![SpaceResort Selection Prompt](./assets/images/SpaceResortSelectionPrompt.png)\\n\\nYou can see all trained utterances and plans by entering this query in the\\ntraining tab search bar: `prompt:SpaceResort`. For example:\\n\\n- The one with a refueling station\\n\\n- The Mercurial\\n\\n- The one on Venus\\n\\nThe booking flow only allows a single SpaceResort at a time, so when there are\\nmultiple candidates, the user will be presented with a `SpaceResort` Selection\\nPrompt.  For prompt training, the goal must always match the prompt context, so\\nwe train these as \"At prompt for\" `SpaceResort` with goal `SpaceResort`. There\\nare many ways the user can answer, so we annotate any provided Value (space\\nresort name, planet, search criteria) and add a special flagged signal route to\\nthe `SelectResort` action.  This action will take the hotels currently in\\ncontext and attempt to filter them based on the newly provided inputs.  For\\nexample:\\n\\n- User: Book a hotel near Jupiter\\n\\n- Bixby: Here are some hotels around Jupiter. Which one would you like?\\n\\n- User: The one with a refueling station.\\n\\n- Bixby: There are 3 hotels around Jupiter with a refueling station. Which one?\\n(Where the 3 options are a subset of the previous options, not a new search)\\n\\n- User: The Ganymede Moon Motel\\n\\n- Bixby: (Proceeds with the booking flow)\\n\\n#### Other Selection Prompts (NumberOfAstronauts, HabitatPod, DateInterval...)\\n\\n![HabitatPod Selection Prompt](./assets/images/HabitatPodSelectionPrompt.png)\\n\\nYou can see all trained utterances and plans by entering this query in the\\ntraining tab search bar: ` has:prompt -goal:SpaceResort -goal:Confirmation`. For\\nexample:\\n\\n- 3 astronauts\\n\\n- The Honey Moon Suite\\n\\n- Next weekend\\n\\nFor prompt training, the goal must always match the prompt context, so we train\\nthese as \"At prompt for\" `<Concept>` and goal `<Concept>`.  Then we annotate the\\nValue in the utterance for that `<Concept>`. For example, \"At prompt for\"\\n`HabitatPod` has goal `HabitatPod` and the `PodName` is annotated as a value.\\n\\n---\\n\\n## Additional Resources\\n\\n### Your Source for Everything Bixby\\n* [Bixby Developer Center](http://bixbydevelopers.com) - Everything you need to get started with Bixby Development!\\n\\n### Guides & Best Practices\\n* [Quick Start Guide](https://bixbydevelopers.com/dev/docs/get-started/quick-start) - Build your first capsule\\n* [Design Guides](https://bixbydevelopers.com/dev/docs/dev-guide/design-guides) - Best practices for designing your capsules\\n* [Developer Guides](https://bixbydevelopers.com/dev/docs/dev-guide/developers) - Guides that take you from design and modeling all the way through deployment of your capsules\\n\\n### Bixby Videos\\n* [Bixby Developers YouTube Channel](https://www.youtube.com/c/bixbydevelopers) - Tutorial videos, Presentations, Capsule Demos and more\\n\\n### Bixby Podcast\\n* [Bixby Developers Chat](http://bixbydev.buzzsprout.com/) - Voice, Conversational AI and Bixby discussions \\n\\n### Bixby on Social Media\\n* [@BixbyDevelopers](https://twitter.com/bixbydevelopers) - Twitter\\n* [Facebook](https://facebook.com/BixbyDevelopers)\\n* [Instagram](https://www.instagram.com/bixbydevelopers/)\\n\\n### Need Support?\\n* Have a feature request? Please suggest it in our [Support Community](https://support.bixbydevelopers.com/hc/en-us/community/topics/360000183273-Feature-Requests) to help us prioritize.\\n* Have a technical question? Ask on [Stack Overflow](https://stackoverflow.com/questions/tagged/bixby) with tag “bixby”\\n'},\n",
       " {'repo': 'repeat-space/anki-apkg-export',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# anki-apkg-export\\n\\n[![Build Status](https://travis-ci.org/repeat-space/anki-apkg-export.svg?branch=master)](https://travis-ci.org/repeat-space/anki-apkg-export)\\n\\nUniversal module for generating decks for Anki.\\n\\nPort of the Ruby gem https://github.com/albertzak/anki2\\n\\n## Install\\n\\n```\\n$ npm install anki-apkg-export --save\\n```\\n\\n## Usage\\n\\n### server\\n\\n```js\\nconst fs = require(\\'fs\\');\\nconst AnkiExport = require(\\'anki-apkg-export\\').default;\\n\\nconst apkg = new AnkiExport(\\'deck-name\\');\\n\\napkg.addMedia(\\'anki.png\\', fs.readFileSync(\\'anki.png\\'));\\n\\napkg.addCard(\\'card #1 front\\', \\'card #1 back\\');\\napkg.addCard(\\'card #2 front\\', \\'card #2 back\\', { tags: [\\'nice\\', \\'better card\\'] });\\napkg.addCard(\\'card #3 with image <img src=\"anki.png\" />\\', \\'card #3 back\\');\\n\\napkg\\n  .save()\\n  .then(zip => {\\n    fs.writeFileSync(\\'./output.apkg\\', zip, \\'binary\\');\\n    console.log(`Package has been generated: output.pkg`);\\n  })\\n  .catch(err => console.log(err.stack || err));\\n```\\n\\n### browser\\n\\nIntended to be used with [`webpack`](https://github.com/webpack/webpack)\\n\\n```js\\nconst webpack = require(\\'webpack\\');\\n\\nmodule.exports = {\\n  entry: \\'./index.js\\',\\n  module: {\\n    loaders: [\\n      {\\n        test: /\\\\.js$/,\\n        exclude: /node_modules/,\\n        loader: \\'babel\\'\\n      },\\n    ]\\n  },\\n  plugins: [\\n    new webpack.DefinePlugin({\\n      \\'process.env\\': {\\n        NODE_ENV: JSON.stringify(process.env.NODE_ENV || \\'development\\')\\n      },\\n    })\\n  ],\\n  output: {\\n    path: __dirname,\\n    filename: \\'bundle.js\\'\\n  }\\n};\\n```\\n\\nRequired loaders:\\n\\n- [`script-loader`](https://github.com/webpack/script-loader)\\n\\n```js\\nimport { saveAs } from \\'file-saver\\';\\nimport AnkiExport from \\'anki-apkg-export\\';\\n\\nconst apkg = new AnkiExport(\\'deck-name\\');\\n\\n// could be a File from <input /> or a Blob from fetch\\n// take a look at the example folder for a complete overview\\napkg.addMedia(\\'anki.png\\', file);\\n\\napkg.addCard(\\'card #1 front\\', \\'card #1 back\\');\\napkg.addCard(\\'card #2 front\\', \\'card #2 back\\', { tags: [\\'nice\\', \\'better card\\'] });\\napkg.addCard(\\'card #3 with image <img src=\"anki.png\" />\\', \\'card #3 back\\');\\n\\napkg\\n  .save()\\n  .then(zip => {\\n    saveAs(zip, \\'output.apkg\\');\\n  })\\n  .catch(err => console.log(err.stack || err));\\n```\\n\\n## Examples\\n\\n- [server from above](examples/server)\\n- [browser from above](examples/browser)\\n- [browser usage with media attachments via ajax](examples/browser-media-ajax)\\n- [browser usage with media attachments via <form />](examples/browser-media-file-input)\\n\\n## Changelog\\n\\n- `v4.0.0` - expose template variables (frontside, backside and css)\\n- `v3.1.0` - make setting APP_ENV optional\\n- `v3.0.0` - add tags, ES6 refactor (breaking)\\n- `v2.0.0` - add media support, update jszip dependency (breaking)\\n- `v1.0.0` - initial rewrite\\n\\n## Tips\\n\\n- [issue#25](https://github.com/ewnd9/anki-apkg-export/issues/25) - Dealing with `sql.js` memory limits\\n\\n## Related\\n\\n- [apkg format documentation](http://decks.wikia.com/wiki/Anki_APKG_format_documentation)\\n- [anki-apkg-export-cli](https://github.com/ewnd9/anki-apkg-export-cli) - CLI for this module\\n- [anki-apkg-export-app](https://github.com/ewnd9/anki-apkg-export-app) - Simple web app to generate cards online\\n\\n## License\\n\\nMIT © [ewnd9](http://ewnd9.com)\\n'},\n",
       " {'repo': 'sromku/android-storage',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': 'android-storage\\n======================\\n\\nLibrary to create, read, delete, append, encrypt files and more, on internal or external disk spaces with a really simple API.\\n\\n## Latest Release\\n\\n[ ![Download](https://api.bintray.com/packages/sromku/maven/storage/images/download.svg) ](https://bintray.com/sromku/maven/storage/_latestVersion)\\n\\n``` groovy\\ndependencies {\\n    compile \\'com.snatik:storage:2.1.0\\'\\n}\\n```\\n\\nDon\\'t forget to update `AndroidManifest.xml` and add next line:\\n\\n``` xml\\n<uses-permission android:name=\"android.permission.WRITE_EXTERNAL_STORAGE\" />\\n```\\n\\n## Usage\\n\\n``` java\\n\\n// init\\nStorage storage = new Storage(getApplicationContext());\\n\\n// get external storage\\nString path = storage.getExternalStorageDirectory();\\n\\n// new dir\\nString newDir = path + File.separator + \"My Sample Directory\";\\nstorage.createDirectory(newDir);\\n```\\n\\nCheck all options, scroll down ;)\\n\\n\\n## Sample app\\n\\nThe app has some simple UI screens and uses the storage library. This is just an example of what can be done with this lib.\\n\\n<img src=\"assets/sample_app.png\"/>\\n\\n## Options\\n* [Easy define Internal or External storage](#initialize)\\n* [Create directory](#create-directory)\\n* [Create file](#create-file)\\n* [Read file content](#read-file)\\n* [Append content to file](#append-content-to-file)\\n* [Copy](#copy)\\n* [Move](#move)\\n* [Delete directory](#delete-directory)\\n* [Delete file](#delete-file)\\n* [Get files](#get-files)\\n* [More options](#more)\\n* [Encrypt the file content](#security-configuration)\\n\\n### Initialize\\n\\n```\\nStorage storage = new Storage(getApplicationContext());\\n```\\n\\nWork on **External Storage**.\\n\\n- Check if external writable\\n\\n\\t``` java\\n\\tboolean isWritable = storage.isExternalWritable();\\n\\t```\\n\\n- Root external storage path\\n\\n\\t``` java\\n\\tString path = storage.getExternalStorageDirectory();\\n\\t```\\n\\n- If you want to use a particular public directory\\n\\n    ``` java\\n    Storage storage = SimpleStorage.getExternalStorage(Environment.DIRECTORY_PICTURES);\\n    ```\\n\\nWork on **Internal Storage**. \\n\\n- Directory for storing app internal files ([documentation](https://developer.android.com/training/basics/data-storage/files.html#WriteInternalStorage)):\\n\\n\\t``` java\\n\\tString path = SimpleStorage.getInternalFilesDirectory();\\n\\t```\\n\\t\\n- Cache dir\\n\\n\\t``` java\\n\\tString path = SimpleStorage.getInternalCacheDirectory();\\n\\t```\\n\\t\\n- Root internal storage dir\\n\\n\\t``` java\\n\\tString path = SimpleStorage.getInternalRootDirectory();\\n\\t```\\n\\n### Create directory\\n\\n- Create directory\\n\\n\\t``` java\\n\\tstorage.createDirectory(path);\\n\\t```\\n\\n- Create directory and **override** the existing one. \\n\\n\\t``` java\\n\\tstorage.createDirectory(path, true);\\n\\t```\\n\\n### Create file\\n\\nCreate a new file with the content in it.\\n\\n``` java\\nstorage.createFile(path, \"some content of the file\");\\n```\\n\\nThe `content` of the file can be one of the next types:\\n- `String`\\n- `byte[]`\\n- `Bitmap`\\n- `Storable`\\n\\n### Read file\\n\\nRead the content of any file to byte array.\\n\\n``` java\\nbyte[] bytes = storage.readFile(path);\\n```\\n\\nRead the content of the file to String.\\n``` java\\nString content = storage.readTextFile(path);\\n```\\n\\n### Append content to file\\n``` java\\nstorage.appendFile(path, \"more new data\");\\n```\\n\\nYou can append:\\n- `String`\\n- `byte[]`\\n\\n### Copy\\n``` java\\nstorage.copy(fromPath, toPath);\\n```\\n\\n### Move\\n``` java\\nstorage.move(fromPath, toPath);\\n```\\n\\n### Delete directory\\n``` java\\nstorage.deleteDirectory(path);\\n```\\n\\n### Delete file\\n``` java\\nstorage.deleteFile(path);\\n```\\n\\n### Get files\\n- Get files in ordered way by: `name`, `date`, `size`\\n\\t``` java\\n\\tList<File> files = storage.getFiles(path, OrderType.DATE);\\n\\t```\\n\\n- Get files and filter by regular expression:\\n\\t``` java\\n\\tString regex = ...;\\n\\tList<File> files = storage.getFiles(path, regex);\\n\\t```\\n\\n* Get all nested files (without the directories)\\n\\t``` java\\n\\tList<File> files = storage.getNestedFiles(path);\\n\\t```\\n\\n### More...\\n\\n* Is directory exists\\n\\t``` java\\n\\tboolean dirExists = storage.isDirectoryExists(path);\\n\\t```\\n\\n* Is file exists\\n\\t``` java\\n\\tboolean fileExists = storage.isFileExist(path);\\n\\t```\\n\\n\\n## Security configuration\\nYou can write and read files while the content is **encrypted**. It means, that no one can read the data of your files from external or internal storage.\\n\\nYou will continue using the same api as before. The only thing you need to do is to configure the Simple Storage library before the you want to create/read encrypted data.\\n\\n``` java\\n// set encryption\\nString IVX = \"abcdefghijklmnop\"; // 16 lenght - not secret\\nString SECRET_KEY = \"secret1234567890\"; // 16 lenght - secret\\nbyte[] SALT = \"0000111100001111\".getBytes(); // random 16 bytes array\\n\\n// build configuratio\\nEncryptConfiguration configuration = new EncryptConfiguration.Builder()\\n\\t.setEncryptContent(IVX, SECRET_KEY, SALT)\\n\\t.build();\\n\\t\\n// configure the simple storage\\nstorage.setEncryptConfiguration(configuration);\\n```\\n\\nNow, you can create a new file with content and the content will be automatically encrypted.<br>\\nYou can read the file and the content will be decrypted.\\n\\n**Example**\\n\\nCreate file with next content `\"this is the secret data\"`:\\n``` java\\nstorage.setEncryptConfiguration(configuration);\\nstorage.createFile(path, \"this is the secret data\");\\n```\\n\\nIf we open the file to see it\\'s content then it we will something like this: `„f°α�ΤG†_i\\x03ΐp` . It looks good :)\\n\\nAnd now, read the file data with the same api:\\n``` java\\nstorage.setEncryptConfiguration(configuration);\\nString content = storage.readTextFile(path);\\n```\\nYou will see that the content will be: `\"this is the secret data\"`.\\n\\n## Tests\\n\\nJust play and check the sample app ;)\\n'},\n",
       " {'repo': 'contentful/contentful-space-sync',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': \"# Contentful Space Sync [![deprecated](http://badges.github.io/stability-badges/dist/deprecated.svg)](http://github.com/badges/stability-badges)\\n\\n\\n[![npm](https://img.shields.io/npm/v/contentful-space-sync.svg)](https://www.npmjs.com/package/contentful-space-sync)\\n[![Build Status](https://travis-ci.org/contentful/contentful-space-sync.svg?branch=master)](https://travis-ci.org/contentful/contentful-space-sync)\\n[![Coverage Status](https://coveralls.io/repos/github/contentful/contentful-space-sync/badge.svg?branch=master)](https://coveralls.io/github/contentful/contentful-space-sync?branch=master)\\n[![Dependency Status](https://david-dm.org/contentful/contentful-space-sync.svg)](https://david-dm.org/contentful/contentful-space-sync)\\n[![devDependency Status](https://david-dm.org/contentful/contentful-space-sync/dev-status.svg)](https://david-dm.org/contentful/contentful-space-sync#info=devDependencies)\\n\\n[![semantic-release](https://img.shields.io/badge/%20%20%F0%9F%93%A6%F0%9F%9A%80-semantic--release-e10079.svg)](https://github.com/semantic-release/semantic-release)\\n[![js-standard-style](https://img.shields.io/badge/code%20style-standard-brightgreen.svg)](http://standardjs.com/)\\n\\n## Deprecation Notice\\n\\nWe have replaced this tool with the [contentful-export](https://github.com/contentful/contentful-export/) and [contentful-import](https://github.com/contentful/contentful-import/) tools and it is now considered deprecated. We won't offer support regarding the usage of this tool.\\n\\nThe new export tool allows you to export all content, including content types, assets and webhooks from a space. The new import tool enables you to import external content into a new space.\\n\\nRead more about the steps involved in [our guide for managing synchronization between multiple spaces](https://www.contentful.com/developers/docs/concepts/multiple-environments/).\\n\\n## What this tool is for\\n\\nThis tool allows you to perform a **one way** synchronization of **published** content from one Contentful space to another.\\n\\nThe tool makes use of Contentful's [Synchronization API](https://www.contentful.com/developers/docs/concepts/sync/) which means that if you run the tool in the future with the provided token, you will only synchronize new and updated Entries and Assets, as well as remove any that have been deleted.\\n\\n### Development environments\\n\\n-   You have a Production space where your content editors create and publish content\\n-   You want your developers to work on new features without touching the production content\\n-   Use the tool to create copies of your Production space where your Developers can try things out at will\\n\\n### Field deletion for published Content Types / Entries\\n\\n-   See the [Deleting Fields](#deleting-fields) section\\n\\n### Creating new spaces with a similar content model\\n\\n-   If you want to start out with a content model similar to what you have on another space, you can use the `--content-model-only` option\\n\\n## What this tool can be used for (but isn't advised)\\n\\n### Published content backups\\n\\nWhile this is possible, we do not advise that you use this tool for backups for the following reasons:\\n\\n-   This tool only synchronizes **published** content. Anything in Draft mode or any unpublished changes to a published Entry will not be synchronized.\\n-   Your content might have broken links (see [contentful-link-cleaner](https://github.com/contentful/contentful-link-cleaner))\\n-   The tool attempts to create every Content Type, Entry and Asset separately, so if failures such as network failures occur your copy might not be complete\\n-   Contentful already [backups your content](https://www.contentful.com/faq/backup-security-and-hosting/) and provides extra offsite backup capabilities\\n\\n## What this tool shouldn't be used for\\n\\n### Workflow management\\n\\n-   Initially, this tool was born as a replacement for [contentful-publication](https://github.com/jsebfranck/contentful-publication), a tool built to manage publication workflows, in which editors would work in a Source space, and content approved and meant for publishing would be synchronized to a Destination space\\n-   However, Contentful now has an improved [Roles and Permissions](https://www.contentful.com/r/knowledgebase/roles-and-permissions/) system, which allows for an easier content approval process\\n\\n# How does it work?\\n\\nEach time you run the tool it stores a [synchronization token](https://www.contentful.com/developers/docs/concepts/sync/) so only new Entries and Assets get copied, and so that deleted items can also be deleted on the destination space. See the [Synchronization](https://www.contentful.com/developers/docs/concepts/sync/) documentation for more details.\\n\\nContent Types will always be updated as they are not retrieved by the synchronization API, and Content Types which don't exist anymore in the source space will be deleted in the destination space as well.\\n\\nIf you make any manual changes in the destination space, be aware that **this tool will overwrite any changes** you've made to entities with the same ids as those existent on the source space.\\n\\nAlso, avoid creating new Content Types and Entries in the destination space. This tool is intended to be used with a workflow where you create content on one space and then regularly copy it somewhere else in an automated way.\\n\\n# Changelog\\n\\nCheck out the [releases](https://github.com/contentful/contentful-space-sync/releases) page.\\n\\n# Install\\n\\n`npm install -g contentful-space-sync`\\n\\n# Usage\\n\\n    Usage: contentful-space-sync [options]\\n\\n    Options:\\n      --version                       Show version number\\n\\n      --source-space                  ID of Space with source data\\n                                      [string] [required]\\n\\n      --destination-space             ID of Space data will be copied to\\n                                      [string] [required]\\n\\n      --source-delivery-token         Delivery API token for source space\\n                                      [string] [required]\\n\\n      --management-token              Management API token for both spaces.\\n                                      [string]\\n\\n      --source-management-token       Management API token for source space, if\\n                                      different from --management-token.\\n                                      [string]\\n\\n      --destination-management-token  Management API token for destination space if\\n                                      different from --management-token.\\n                                      [string]\\n\\n      --pre-publish-delay             Delay in milliseconds to account for delay\\n                                      after creating entities, due to internal\\n                                      database indexing\\n                                      [default: 5000]\\n\\n      --sync-token-dir                Defines the path for storing sync token files\\n                                      (default path is the current directory)\\n                                      [string]\\n\\n      --content-model-only            Copies only content types and locales\\n                                      [boolean]\\n\\n      --skip-content-model            Skips content types and locales. Copies only entries and assets\\n                                      [boolean]\\n\\n      --skip-locales                  Skips locales. Must be used with --content-model-only.\\n                                      Copies only content types.\\n                                      [boolean]\\n\\n      --delivery-host                 Host for the Delivery API.\\n                                      [string]\\n\\n      --delivery-port                 Port for the Delivery API.\\n                                      [string]\\n\\n      --delivery-insecure             If the Delivery API should use http instead of the default https.\\n                                      [boolean]\\n\\n      --management-host               Host for the Management API.\\n                                      [string]\\n\\n      --management-port               Port for the Management API.\\n                                      [string]\\n\\n      --management-insecure           If the Management API should use http instead of the default https.\\n                                      [boolean]\\n\\n      --proxy-host                    hostname of the proxy server. [string]\\n\\n      --proxy-port                    port of the proxy server. [string]\\n\\n      --rate-limit                    How many request per period of time, default 6 [number]\\n\\n      --rate-limit-period             How much time to wait before retry in ms, default 1000 [number]     \\n\\n      --config                        Configuration file with required values\\n\\nThe `--management-token` parameter allows you to specify a token which will be used for both spaces. If you get a token from <https://www.contentful.com/developers/docs/references/authentication/> and your user account has access to both spaces, this should be enough.\\n\\nIn case you actually need a different management token for any of the spaces, you can use the `--source-management-token` and `--destination-management-token` options to override it.\\n\\nCheck the `example-config.json` file for an example of what a configuration file would look like. If you use the config file, you don't need to specify the other options for tokens and space ids.\\n\\n# Example usage\\n\\n    contentful-space-sync \\\\\\n      --source-space sourceSpaceId \\\\\\n      --source-delivery-token sourceSpaceDeliveryToken \\\\\\n      --destination-space destinationSpaceId \\\\\\n      --destination-management-token destinationSpaceManagementToken\\n\\nor\\n\\n    contentful-space-sync --config example-config.json\\n\\nYou can create your own config file based on the [`example-config.json`](example-config.json) file.\\n\\n# Usage as a library\\n\\nWhile this tool is mostly intended to be used as a command line tool, it can also be used as a Node library:\\n\\n```js\\nvar spaceSync = require('contentful-space-sync')\\n\\nspaceSync(options)\\n.then((output) => {\\n  console.log('sync token', output.nextSyncToken)\\n})\\n.catch((err) => {\\n  console.log('oh no! errors occurred!', err)\\n})\\n```\\n\\nThe options object can contain any of the CLI options but written with a camelCase pattern instead, and no dashes. So `--source-space` would become `sourceSpace`.\\n\\nApart from those options, there are two additional ones that can be passed to it:\\n\\n-   `errorLogFile` - File to where any errors will be written.\\n-   `syncTokenFile` - File to where the sync token will be written.\\n\\nThe method returns a promise, where, if successful, you'll have an object which contains the `nextSyncToken` (only thing there at the moment). If not successful, it will contain an object with errors.\\n\\nYou can look at [`bin/space-sync`](bin/space-sync) to check how the CLI tool uses the library.\\n\\n# Synchronizing a space over time\\n\\nThis tool uses the Contentful [Synchronization](https://www.contentful.com/developers/docs/concepts/sync/) endpoint to keep content synchronized over repeated runs of the script.\\n\\nBehind the scenes, when you use the sync endpoint, apart from the content it also returns a sync token in its response. The sync token encodes information about the last synchronized content, so that when you request a new synchronization, you can supply it this content and you'll only get new and updated content, as well a list of what content has been deleted.\\n\\nWhen you run this tool, it will create a file in the current directory named `contentful-space-sync-token-sourceSpaceId-destinationSpaceId`. If you run the tool again in the directory where this file resides, with the same source and destination space IDs, it will read the token from the file. If you have a token from somewhere else you can just create the file manually.\\n\\n# The error log\\n\\nIf any errors occur during synchronization, the tool will also create a time stamped log file (`contentful-space-sync-timestamp.log`) with a list of any errors which occurred, and links to the entities in the source space which might have problems that need to be fixed.\\n\\nThe most common problem will probably be an `UnresolvedLinks` error, which means a published entry A links to another entry B or asset C which has been deleted since publishing of the entry A.\\n\\nIf you come across this problem, you can use [contentful-link-cleaner](https://github.com/contentful/contentful-link-cleaner) to clean all of those unresolved references.\\n\\n# Copying only the content model\\n\\nBy using the `--content-model-only` option, you can copy only Content Types and Locales. This means you'll get a space with the same content structure, but with no content at all.\\n\\nThis might be useful if you have been trying things out but want to start fresh with your content, or if you have a need to [delete fields](#deleting-fields) from your existing Content Types.\\n\\n# Copying only content\\n\\nBy using the `--skip-content-model` option, you can copy only Entries and Assets. This assumes you have used this script before with the `--content-model-only` option or created the exact same content structure by hand.\\n\\nEvery time you run the script without any of these options, it will attempt to update the content model as well, so on subsequent syncs it might be desirable to use this option to make things a bit faster.\\n\\n# Deleting fields\\n\\nWhile we used to recommend this tool as a way to do field deletion through a specific workflow, this feature is now available on the Contentful UI.\\n\\n# What happened to --force-overwrite and --fresh ?\\n\\nThese options were very problematic and caused more problems than they solved, so they were removed on version 4. You can see more details [here](https://github.com/contentful/contentful-space-sync/commit/066c629fec0e4c41b2094cbf6f5b01697c0b525f) and [here](https://github.com/contentful/contentful-space-sync/commit/44f1ac81ec12850c4342d91d53e0386dff68de32).\\n\\nIf you think you need these options, we advise you create a new, empty space, and restart the synchronization process from scratch.\\n\\nIf you really really really REALLY need those options and you are aware of how much trouble they can cause, you can always use an older version of the tool.\\n\"},\n",
       " {'repo': 'YotamNitzan/ID-disentanglement',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Face Identity Disentanglement via Latent Space Mapping\\n\\n<p align=\"center\">\\n<img src=\"docs/imgs/teaser.png\" width=\"400px\"/>\\n</p>\\n\\n\\n## Description   \\nOfficial Implementation of the paper *Face Identity Disentanglement via Latent Space Mapping*\\nfor both training and evaluation.\\n\\n> **Face Identity Disentanglement via Latent Space Mapping**<br>\\n> Yotam Nitzan<sup>1</sup>, Amit Bermano<sup>1</sup>, Yangyan Li<sup>2</sup>, Daniel Cohen-Or<sup>1</sup><br>\\n> <sup>1</sup>Tel-Aviv University, <sup>2</sup>Alibaba <br>\\n> https://arxiv.org/abs/2005.07728\\n>\\n> <p align=\"justify\"><b>Abstract:</b> <i>Learning disentangled representations of data is a fundamental problem in artificial intelligence. Specifically, disentangled latent representations allow generative models to control and compose the disentangled factors in the synthesis process. Current methods, however, require extensive supervision and training, or instead, noticeably compromise quality. In this paper, we present a method that learns how to represent data in a disentangled way, with minimal supervision, manifested solely using available pre-trained networks. Our key insight is to decouple the processes of disentanglement and synthesis, by employing a leading pre-trained unconditional image generator, such as StyleGAN. By learning to map into its latent space, we leverage both its state-of-the-art quality, and its rich and expressive latent space, without the burden of training it. We demonstrate our approach on the complex and high dimensional domain of human heads. We evaluate our method qualitatively and quantitatively, and exhibit its success with de-identification operations and with temporal identity coherency in image sequences. Through extensive experimentation, we show that our method successfully disentangles identity from other facial attributes, surpassing existing methods, even though they require more training and supervision.</i></p>\\n\\n## Setup\\n\\nTo setup everything you need check out the [setup instructions](docs/setup.md).\\n\\n## Training\\n\\n### Preparing the Dataset\\n\\nThe dataset is comprised of StyleGAN-generated images and W latent codes, both are generated from a single\\nStyleGAN model.\\n\\nWe also use real images from FFHQ to evaluate quality at test time.\\n\\nThe dataset is assumed to be in the following structure:\\n\\n| Path | Description\\n| :--- | :---\\n| base directory | Directory for all datasets\\n| &boxvr;&nbsp; real | FFHQ image dataset\\n| &boxvr;&nbsp; dataset_N | dataset for resolution NxN\\n| &boxv;&nbsp; &boxvr;&nbsp; images | images generated by StyleGAN\\n| &boxv;&nbsp; &boxur;&nbsp; ws | W latent codes generated by StyleGAN\\n\\nTo generate the `dataset_N` directory, run:\\n\\n```\\ncd utils\\\\\\npython generate_fake_data.py \\\\ \\n    --resolution N \\\\\\n    --batch_size BATCH_SIZE \\\\\\n    --output_path OUTPUT_PATH \\\\\\n    --pretrained_models_path PRETRAINED_MODELS_PATH \\\\\\n    --num_images NUM_IMAGES \\\\\\n    --gpu GPU\\n```\\n\\nIt will generate an image dataset in similar format to FFHQ.\\n\\n### Start training\\n\\nTo train the model as done in the paper\\n\\n```\\npython main.py\\n    NAME\\n    --resolution N\\n    --pretrained_models_path PRETRAINED_MODELS_PATH\\n    --dataset BASE_DATASET_DIR\\n    --batch_size BATCH_SIZE\\n    --cross_frequency 3\\n    --train_data_size 70000\\n    --results_dir RESULTS_DIR        \\n```\\n\\nPlease run `python main.py -h` for more details.\\n\\n## Inference\\n\\nFor convenience, there are a few inference functions - each serving a different use case.\\nThe functions are resolved using the name of the function.\\n\\n### All possible combinations in dirs\\n\\n<p align=\"center\">\\n<img src=\"docs/imgs/table_results.jpg\"/>\\n</p>\\n\\n**Input data: Two directories, one identity inputs and another for attribute inputs.** <br>\\nRuns over all N*M combinations in two directories.\\n\\n```\\npython test.py \\n    Name\\n    --pretrained_models_path PRETRAINED_MODELS_PATH \\\\\\n    --load_checkpoint PATH_TO_WEIGHTS \\\\\\n    --id_dir DIR_OF_IMAGES_FOR_ID \\\\\\n    --attr_dir DIR_OF_IMAGES_FOR_ATTR \\\\\\n    --output_dir DIR_FOR_OUTPUTS \\\\\\n    --test_func infer_on_dirs\\n```\\n\\n\\n### Paired data\\n\\n**Input data: Two directories, one identity inputs and another for attribute inputs**. <br>\\nThe two directories are assumed to be paired. Inference runs on images with the same names.\\n\\n```\\npython test.py \\n    Name\\n    --pretrained_models_path PRETRAINED_MODELS_PATH \\\\\\n    --load_checkpoint PATH_TO_WEIGHTS \\\\\\n    --id_dir DIR_OF_IMAGES_FOR_ID \\\\\\n    --attr_dir DIR_OF_IMAGES_FOR_ATTR \\\\\\n    --output_dir DIR_FOR_OUTPUTS \\\\\\n    --test_func infer_pairs\\n```\\n\\n### Disentangled interpolation\\n\\n#### Interpolating attributes\\n\\n<p align=\"center\">\\n<img src=\"docs/imgs/interpolate_attr.jpg\"/>\\n</p>\\n\\n#### Interpolating identity\\n\\n<p align=\"center\">\\n<img src=\"docs/imgs/interpolate_id.jpg\"/>\\n</p>\\n\\n**Input data: A directory with any number of subdirectories. In each subdir, there are three images.**\\nAll images should have exactly one of *attr* or *id* in their name.\\nIf there are two *attr* images and one *id* image, it will interpolate attribute.\\nIf there is one *attr* images and two *id* images, it will interpolate identity.\\n\\n\\n```\\npython test.py \\n    Name\\n    --pretrained_models_path PRETRAINED_MODELS_PATH \\\\\\n    --load_checkpoint PATH_TO_WEIGHTS \\\\\\n    --input_dir PARENT_DIR \\\\\\n    --output_dir DIR_FOR_OUTPUTS \\\\\\n    --test_func interpolate\\n```\\n\\n## Checkpoints\\n\\nOur pretrained 256x256 [checkpoint](https://drive.google.com/drive/folders/1lVizq4hCq-zTf8Q3fDqqfSnV6jIYEgY_?usp=sharing) is also available.\\n\\n## Citation\\nIf you use this code for your research, please cite our paper using:\\n\\n```\\n@article{Nitzan2020FaceID,\\n  title={Face identity disentanglement via latent space mapping},\\n  author={Yotam Nitzan and A. Bermano and Yangyan Li and D. Cohen-Or},\\n  journal={ACM Transactions on Graphics (TOG)},\\n  year={2020},\\n  volume={39},\\n  pages={1 - 14}\\n}\\n```'},\n",
       " {'repo': 'robhawkes/rawkets',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': 'Copyright (C) 2010 Rob Hawkes\\n\\nThis program is free software: you can redistribute it and/or modify\\nit under the terms of the GNU General Public License as published by\\nthe Free Software Foundation, either version 3 of the License, or\\n(at your option) any later version.\\n\\nThis program is distributed in the hope that it will be useful,\\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\nGNU General Public License for more details.\\n\\nYou should have received a copy of the GNU General Public License\\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.'},\n",
       " {'repo': 'avie-dev/spaceslounge',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': \"# SpacesLounge\\n\\nTwitter Spaces Host and Speaker's Lounge\\n\\n## Tech stack\\n\\n![Next.js](https://img.shields.io/badge/Next.js-305FCB?style=for-the-badge&logo=next.js&logoColor=white)\\n![tailwind css](https://img.shields.io/badge/tailwind_css-305FCB?style=for-the-badge&logo=tailwindcss&logoColor=white)\\n![Figma](https://img.shields.io/badge/Figma-305FCB?style=for-the-badge&logo=figma&logoColor=white)\\n\\n- Frontend - **Next.js**\\n- CSS Framework - **Tailwind CSS**\\n- Design & Prototype - **Figma**\\n\\n## Running the project\\n\\n1. [Fork](https://github.com/avie-dev/spaceslounge/fork) the project\\n\\n2. Clone the repo:\\n\\n   ```console\\n   $ git clone https://github.com/<your-github-username>/spaceslounge.git\\n   ```\\n\\n3. Navigate to the cloned directory:\\n\\n   ```console\\n   $ cd spaceslounge\\n   ```\\n\\n4. Install dependencies:\\n\\n   ```console\\n   $ yarn\\n   ```\\n   \\n   There is also [next-themes](https://www.npmjs.com/package/next-themes) being used and it might give error, in that case:\\n   \\n   ```console\\n   $ yarn add next-themes\\n   ```\\n\\n5. Run the project\\n\\n   ```console\\n   $ yarn dev\\n   ```\\n\\n## Contributing\\n\\nTo make a contribution to documentation, code or design please follow the [contributing guidelines](https://github.com/avie-dev/spaceslounge/blob/main/CONTRIBUTING.md)\\n\"},\n",
       " {'repo': 'space-voyager-21/space-voyager',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '<h2>Hey Developers! Presenting you all the \\'Space-Voyager\\'. </h2>\\n\\n\\n![image](https://user-images.githubusercontent.com/79041510/136692364-1a0c11db-dcc9-4168-9fd1-4e948933a353.png)\\n![image](https://user-images.githubusercontent.com/79041510/136077311-746ac1b6-99d1-4d12-b781-e72a2f2fb7af.png)\\n\\n\\n\\n\\n\\n## 🏅💻 Hacktoberfest21\\n![](https://hacktoberfest.digitalocean.com/_nuxt/img/logo-hacktoberfest-full.f42e3b1.svg)\\n\\nHacktoberfest is back this year! \\n\\nHacktoberfest is an annual month-long celebration of open source organised every year by DIgital Ocean. Every participant who follows their set of rules and get 4 Pull Requests (PRs) merged will receive swags. For the past years, learners have been hacking throughout October in the open-source community, and this 8th year you can avail yourself the opportunity to partake and contribute to this ever-growing platform.\\n\\n\\n## 💻Tech Stack\\n<h4> <i> This project is under construction. </i> </h4>\\n<h3>Project Link: <code><a href=\"https://space-voyager.netlify.app\"> https://space-voyager.netlify.app </a> </code> </h3>\\n\\n \\n \\n![REACT](https://img.shields.io/badge/React-20232A?style=for-the-badge&logo=react&logoColor=61DAFB)\\n![CSS](https://img.shields.io/badge/css3%20-%231572B6.svg?&style=for-the-badge&logo=css3&logoColor=white)\\n![JS](https://img.shields.io/badge/javascript%20-%23323330.svg?&style=for-the-badge&logo=javascript&logoColor=%23F7DF1E)\\n  \\n\\n\\n## 📌Contributing Guidelines :\\n**1.**  Fork [this](https://github.com/space-voyager-21/space-voyager.git) repository.\\n\\n**2.**  Clone your forked copy of the project.\\n```\\ngit clone --depth 1 https://github.com/<your_name>/space-voyager.git\\n```\\n**3.** Navigate to the project directory :file_folder: .\\n```\\ncd space-voyager\\n```\\n**4.** Add a reference(remote) to the original repository.\\n```\\ngit remote add upstream https://github.com/space-voyager-21/space-voyager.git\\n```\\n**5.** Check the remotes for this repository.\\n```\\ngit remote -v\\n```\\n**6.** Always take a pull from the upstream repository to your master branch to keep it at par with the main project(updated repository).\\n```\\ngit pull upstream main\\n```\\n**7.** Create a new branch.\\n```\\ngit checkout -b <your_branch_name>\\n```\\n**8.** Perfom your desired changes to the code base.\\n\\n\\n**9.** Track your changes:heavy_check_mark: \\n```\\ngit add . \\n```\\n**10.** Commit your changes .\\n```\\ngit commit -m \"Relevant message\"\\n```\\n**11.** Push the committed changes in your feature branch to your remote repo.\\n```\\ngit push -u origin <your_branch_name>\\n```\\n**12.** To create a pull request, click on `compare and pull requests`. Please ensure you compare your feature branch to the desired branch of the repo you are suppose to make a PR to.\\n\\n**13.** Add appropriate title and description to your pull request explaining your changes and efforts done. Always make sure you have pulled the latest code from the master branch before making a PR.\\n\\n**14.** Click on `Create Pull Request`.\\n\\n**15.** Hurray ❗ You have created a PR to the Space Voyager Website 💥 . Sit back patiently and relax till then the project maintainers will review your PR. Please understand, there will be some time taken to review a PR and can vary from a few hours to a few days too so be Patient and keep contributing.\\n\\nHappy Hacktoberfest!\\n\\n'},\n",
       " {'repo': 'space-voyager-21/space-voyager',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '<h2>Hey Developers! Presenting you all the \\'Space-Voyager\\'. </h2>\\n\\n\\n![image](https://user-images.githubusercontent.com/79041510/136692364-1a0c11db-dcc9-4168-9fd1-4e948933a353.png)\\n![image](https://user-images.githubusercontent.com/79041510/136077311-746ac1b6-99d1-4d12-b781-e72a2f2fb7af.png)\\n\\n\\n\\n\\n\\n## 🏅💻 Hacktoberfest21\\n![](https://hacktoberfest.digitalocean.com/_nuxt/img/logo-hacktoberfest-full.f42e3b1.svg)\\n\\nHacktoberfest is back this year! \\n\\nHacktoberfest is an annual month-long celebration of open source organised every year by DIgital Ocean. Every participant who follows their set of rules and get 4 Pull Requests (PRs) merged will receive swags. For the past years, learners have been hacking throughout October in the open-source community, and this 8th year you can avail yourself the opportunity to partake and contribute to this ever-growing platform.\\n\\n\\n## 💻Tech Stack\\n<h4> <i> This project is under construction. </i> </h4>\\n<h3>Project Link: <code><a href=\"https://space-voyager.netlify.app\"> https://space-voyager.netlify.app </a> </code> </h3>\\n\\n \\n \\n![REACT](https://img.shields.io/badge/React-20232A?style=for-the-badge&logo=react&logoColor=61DAFB)\\n![CSS](https://img.shields.io/badge/css3%20-%231572B6.svg?&style=for-the-badge&logo=css3&logoColor=white)\\n![JS](https://img.shields.io/badge/javascript%20-%23323330.svg?&style=for-the-badge&logo=javascript&logoColor=%23F7DF1E)\\n  \\n\\n\\n## 📌Contributing Guidelines :\\n**1.**  Fork [this](https://github.com/space-voyager-21/space-voyager.git) repository.\\n\\n**2.**  Clone your forked copy of the project.\\n```\\ngit clone --depth 1 https://github.com/<your_name>/space-voyager.git\\n```\\n**3.** Navigate to the project directory :file_folder: .\\n```\\ncd space-voyager\\n```\\n**4.** Add a reference(remote) to the original repository.\\n```\\ngit remote add upstream https://github.com/space-voyager-21/space-voyager.git\\n```\\n**5.** Check the remotes for this repository.\\n```\\ngit remote -v\\n```\\n**6.** Always take a pull from the upstream repository to your master branch to keep it at par with the main project(updated repository).\\n```\\ngit pull upstream main\\n```\\n**7.** Create a new branch.\\n```\\ngit checkout -b <your_branch_name>\\n```\\n**8.** Perfom your desired changes to the code base.\\n\\n\\n**9.** Track your changes:heavy_check_mark: \\n```\\ngit add . \\n```\\n**10.** Commit your changes .\\n```\\ngit commit -m \"Relevant message\"\\n```\\n**11.** Push the committed changes in your feature branch to your remote repo.\\n```\\ngit push -u origin <your_branch_name>\\n```\\n**12.** To create a pull request, click on `compare and pull requests`. Please ensure you compare your feature branch to the desired branch of the repo you are suppose to make a PR to.\\n\\n**13.** Add appropriate title and description to your pull request explaining your changes and efforts done. Always make sure you have pulled the latest code from the master branch before making a PR.\\n\\n**14.** Click on `Create Pull Request`.\\n\\n**15.** Hurray ❗ You have created a PR to the Space Voyager Website 💥 . Sit back patiently and relax till then the project maintainers will review your PR. Please understand, there will be some time taken to review a PR and can vary from a few hours to a few days too so be Patient and keep contributing.\\n\\nHappy Hacktoberfest!\\n\\n'},\n",
       " {'repo': 'Dhravya/Spacebot',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '<h1 align=\"center\">\\n  <br>\\n  <a href=\"https://github.com/dhravya/spacebot-discord\"><img src=\"https://i.imgur.com/hOZXyje.jpg\" alt=\"Spacebot - Discord Bot\"></a>\\n  <br>\\n  Spacebot -  Discord Bot\\n  <br>\\n</h1>\\n\\n## Spacebot is getting costlier to host! \\n[![ko-fi](https://ko-fi.com/img/githubbutton_sm.svg)](https://ko-fi.com/R6R782RBF)\\n\\n<h4 align=\"center\">Music, Moderation, Fun, Utilities, Games and Fully Configurable.</h4>\\n\\n<p align=\"center\">\\n  <a href=\"https://discord.gg/rqhgqTqFbp\">\\n    <img src=\"https://discordapp.com/api/guilds/905904010760966164/widget.png?style=shield\" alt=\"Discord Server\">\\n  </a>\\n  <a href=\"https://www.python.org/downloads/\">\\n    <img alt=\"PyPI - Python Version\" src=\"https://img.shields.io/pypi/pyversions/Red-Discordbot\">\\n  </a>\\n  <a href=\"https://github.com/pycord/pycord-development/\">\\n     <img src=\"https://img.shields.io/badge/discord-py-blue.svg\" alt=\"pycord\">\\n  </a>\\n</p>\\n\\n<p align=\"center\">\\n  <a href=\"https://github.com/psf/black\">\\n    <img src=\"https://img.shields.io/badge/code%20style-black-000000.svg\" alt=\"Code Style: Black\">\\n  </a>\\n  <a href=\"http://makeapullrequest.com\">\\n    <img src=\"https://img.shields.io/badge/PRs-welcome-brightgreen.svg\">\\n  </a>\\n  <br>\\n    <a href=\"https://top.gg/bot/881862674051391499\">\\n    <img src=\"https://top.gg/api/widget/servers/881862674051391499.svg\">\\n    </a>\\n    <a href=\"https://top.gg/bot/881862674051391499\">\\n    <img src=\"https://top.gg/api/widget/upvotes/881862674051391499.svg\">\\n    </a>\\n    <a href=\"https://top.gg/bot/881862674051391499\">\\n    <img src=\"https://top.gg/api/widget/owner/881862674051391499.svg\">\\n    </a>\\n</p>\\n\\n\\n<p align=\"center\">\\n  <a href=\"#overview\">Overview</a>\\n  •\\n  <a href=\"#contributing\">Contributing</a>\\n  •\\n  <a href=\"#self-hosting\">Self hosting</a>\\n  •\\n  <a>Documentation (not ready- send a PR!)</a>\\n  •\\n  <a href=\"#join-the-community\">Community</a>\\n  •\\n  <a href=\"#license\">License</a>\\n</p>\\n\\n# Overview\\nSpacebot is an open source discord bot that is designed to be fun, easy to use, and replace every other discord bot out there!!\\nFeel free to add a star ⭐ to the repository to promote the project!\\n\\n## Features\\n- ALL IN ONE !\\n- ✉️ Support for commands in direct messages\\n- ⚙️ Guild configuration (prefix, roles, etc.)\\n- 😀 Commands made pleasant thanks to the many emojis\\n- 🗳️ Rewards for voting on the bot on [top.gg](https://top.gg/bot/881862674051391499)\\n- Self hostable - a little programming knowledge required though\\n  \\n\\n## Commands\\n> Spacebot has 200+ commands! Slash and non-slash included.\\n- 🎵 Music - Play music from youtube, spotify, soundcloud, http links, vimeo, and a lot more! - NOW WITH FILTERS!\\n- 😊 Fun - Random, joke, AI, and more to engage communities!\\n- 🔧 Utilities - Helpful commands to make your life easier!\\n-  🎮 Games - Play games with the bot!\\n-  ⚒️ Moderation - Moderation commands to help keep the server safe!\\n-  ⚙️ Configurable - Configure the bot to your liking!\\n-  📷 Image manipulation - Make memes, filter images, convert file types and more!\\n\\n## Contributing\\nSpacebot is open source on github. Feel free to make a PR!\\nMake sure to read the [guidelines](CONTRIBUTING.md) but dont stress it!\\n\\n## Self hosting\\nYou can self host the bot for your own server, but you will need to have a lavalink server, discord bot account and the API keys for a ton of stuff. its just better to [invite spacebot](https://dsc.gg/spacebt) to your own server and use it!\\n\\n- Read this [guide](https://github.com/reactiflux/discord-irc/wiki/Creating-a-discord-bot-&-getting-a-token) to make a bot account.\\n- Make a [Lavalink server](https://dsharpplus.github.io/articles/audio/lavalink/setup.html) you will need this for the music to work.\\n- create a `dotenv` in the `main directory` file with the following contents:\\n```\\nREDDIT_CLIENT=  \\nREDDIT_SECRET=\\nBOT_TOKEN = \\nGOOGLE_KEY =\\nGOOGLE_CX=\\nWOLFRAM_API_KEY=\\nIMGUR_API_KEY=\\nAFP_KEY=\\nTENOR_API_KEY=\\nAI_HOST =\\nAI_KEY = \\nSPOTIPY_ID = \\nSPOTIPY_SECRET = \\nLAVALINK_HOST =\\nLAVALINK_PASSWORD= \\n```\\n> **Note:** You can choose not to put in certain API keys if you dont want the corresponding features.\\n\\n## Community\\nSpacebot has a [discord server!](https://discord.gg/rqhgqTqFbp) Stop by to say hello! \\n  <a href=\"https://discord.gg/rqhgqTqFbp\">\\n    <img src=\"https://discordapp.com/api/guilds/905904010760966164/widget.png?style=shield\" alt=\"Discord Server\">\\n  </a>\\n\\n## Future plans\\nThese are some planned features for Spacebot:\\n- 📃 Documentation\\n- Modmail\\n- Ticket system\\n- Automod\\n- Moderation logging\\n\\n> If you can contribute and help me do any of these features, feel free to comment on the corresponding [issues](github.com/dhravya/spacebot-discord/issues) that you are working on it! After you are done, you can submit a PR and i\\'ll add it to spacebot!\\n\\n## License\\nSpacebot is licensed under the [MIT license](LICENSE)\\n\\nAlso check : [ABLETON - by INFINIX](https://dsc.gg/ableton)\\n\\n### Thanks for showing your interest in Spacebot!! While you are here, why not give it a ⭐?\\n'},\n",
       " {'repo': 'SpruceGabriela/space-shooter-dio',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': \"# Oi, tudo bem? Chegou aqui através do curso, certo? 🙃\\n\\nEsse é o repositório da nossa aula de Javascript, na qual vamos fazer um joguinho de space shooter super legal! \\n\\n### Os requisitos são:\\n\\n* [HTML básico](https://www.w3schools.com/html/)\\n* [CSS básico](https://developer.mozilla.org/pt-BR/docs/Web/CSS)\\n* [Javascript básico](https://developer.mozilla.org/pt-BR/docs/Web/JavaScript)\\n \\n\\n\\n## 🚀 Let's code! 🚀\\n\"},\n",
       " {'repo': 'soxofaan/duviz',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '.. image:: https://img.shields.io/pypi/pyversions/duviz\\n    :target: https://pypi.org/project/duviz/\\n    :alt: PyPI - Python Version\\n.. image:: https://github.com/soxofaan/duviz/actions/workflows/unittests.yml/badge.svg?branch=master\\n    :target: https://github.com/soxofaan/duviz/actions/workflows/unittests.yml\\n    :alt: unit tests\\n.. image:: https://github.com/soxofaan/duviz/actions/workflows/pre-commit.yml/badge.svg?branch=master\\n    :target: https://github.com/soxofaan/duviz/actions/workflows/pre-commit.yml\\n    :alt: pre-commit\\n\\n\\nWhat is duviz?\\n--------------\\n\\n``duviz`` is a (Python 3) command-line tool to visualize disk space usage.\\n\\nIt\\'s like the plethora of desktop applications and widgets\\n(e.g. Filelight, DaisyDisk, WinDirStat, JDiskReport, TreeSize, SpaceSniffer, ...),\\nbut instead of a fancy GUI with animated pie charts and shaded boxes\\nyou get a funky \"ASCII art style hierarchical tree of bars\" in your shell.\\nIf that didn\\'t make a lot of sense to you, look at this example of this ``/opt`` folder::\\n\\n    $ duviz /opt\\n    ________________________________________________________________________________\\n    [                                     /opt                                     ]\\n    [____________________________________3.30GB____________________________________]\\n    [                                    local                                     ]\\n    [____________________________________3.30GB____________________________________]\\n    [              var              ][        lib         ][ share  ][Libr][lib][]|\\n    [_____________1.36GB____________][______925.47MB______][411.37MB][231.][222][]|\\n    [           macports           ]|[gcc][gcc4][]|||      [][]||||||[Fra]|[gc] |\\n    [____________1.36GB____________]|[250][226.][]|||      [][]||||||[231]|[21] |\\n    [    software    ][distfile][]| |           ||  |      | ||||||||[Pyt] [x8]\\n    [____785.31MB____][421.56MB][]| |           ||  |      | ||||||||[231] [21]\\n    [gc][][]||||||||||||||||||||[]               |            ||| |  [Ve]  ||[]\\n    [17][][]||||||||||||||||||||[]               |            ||| |  [23]  ||[]\\n\\n\\nFeatures\\n--------\\n\\n- Basically it consists of just one Python 3 script ``duviz.py``.\\n  No installation required: put it where you want it. Use it how you want it.\\n- Only uses standard library and just depends on ``du`` and ``ls`` utilities,\\n  which are available out of the box on a typical Unix platform (Linux, macOS)\\n- Speed. No need to wait for a GUI tool to get up and running, let alone scanning your disk.\\n  The hard work is done by ``du`` (or ``ls``), which run an C-speed.\\n- Progress reporting while you wait. Be hypnotized!\\n- Detects your terminal width for maximum visualization pleasure.\\n- Not only supports \"disk usage\" based on file size,\\n  but also allows to count files (inode count mode)\\n  or give a size breakdown of ZIP or tar files.\\n- Option to use terminal colors for the boxes instead of ASCII art\\n\\n\\nInstallation\\n------------\\n\\nPip based\\n    duviz can be installed with pip in a desired virtual environment::\\n\\n        pip install duviz\\n\\n    which will install a ``duviz`` command line utility in your environment.\\n\\n    If you already have `pipx <https://pypa.github.io/pipx/>`_ on your toolbelt,\\n    you might prefer to install duviz in an automatically managed,\\n    isolated environment with ``pipx install duviz``.\\n\\nWith Homebrew\\n    duviz can also be installed with `Homebrew <https://brew.sh/>`_::\\n\\n        brew install https://raw.github.com/soxofaan/duviz/master/extra/homebrew/duviz.rb\\n\\nNo installation\\n    The file ``duviz.py`` is also designed to be usable as a standalone Python script,\\n    without having to install it.\\n    Download ``duviz.py`` and just run it::\\n\\n        python path/to/duviz.py\\n\\n\\nPython 2 Support\\n~~~~~~~~~~~~~~~~\\n\\n``duviz`` was originally (2009) a Python 2 script, and started supporting Python 3 around 2016.\\nWith the end of life of Python 2 nearing in 2019, support for Python 2 was dropped.\\nThe Python 2 compatible version can be found in the ``py2-compatible`` branch (last release: 1.1.1).\\n\\nUsage\\n-----\\n\\nIf you run ``duviz`` without arguments, it will render the disk usage of the current working folder.\\nIf you specify one or more directories, it will render the usage of those directories, how intuitive is that!\\n\\nInstead of size in bytes, you can also get inode usage: just use the option ``--inodes`` (or ``-i`` in short).\\n\\nIf you directly pass ``duviz`` a ZIP or tar file,\\nit will visualize the size breakdown of the file tree in the ZIP/tar file.\\nIn case of ZIP files, the compressed size will be shown by default\\n(option ``--unzip-size`` will toggle showing of decompressed size).\\nFor tar files, only the decompressed size is available.\\n\\nRun it with option ``--help`` for more options.\\n'},\n",
       " {'repo': 'MattLoftus/threejs-space-simulations',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': 'This repository is a collection of recent experiments I\\'ve been working on in three.js.\\n\\nThree.js is a JavaScript library built on top of the WebGL graphics language.  WebGL is a low level, verbose language used to create graphics in the browser that can be both very performant and very hard to use.  Three.js greatly reduces the amount of boilerplate code you have to write to build rich 3D graphics, and wraps common operations into intuitive constructor functions.  If you\\'re interested in learning three.js, I recently completed two new tutorials on getting started with the three.js library.  You can find them at [loftus.xyz](http://loftus.xyz)\\n\\nYou will find the different simulations in this repo in the examples folder.  To develop and test out the simulations locally, first clone the repo down to your local machine.  \\n\\n```\\ngit clone https://github.com/MattLoftus/threejs-space-simulations.git\\n```\\n\\nTo avoid cross-origin errors when using textures(every example in this repo), you will need to host the files on a local server.  I recommend using python simple server or npm live server.  First navigate to the root of the directory, then run the following command.\\n\\n```\\npython -m SimpleHTTPServer\\n```\\n\\nThis will host the folder on port 8000, so you can head over to the browser and type \"localhost:8000\" into the address bar, and you will see a listing for the directory. If you happen to have a version of python on your machine >= 3.0, you may need to run the following command instead.\\n\\n```\\npython2.7 -m SimpleHTTPServer\\n```\\n\\n'},\n",
       " {'repo': 'misternebula/quantum-space-buddies',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '![logo](unknown.png)\\n\\n![GitHub](https://img.shields.io/github/license/misternebula/quantum-space-buddies?style=flat-square)\\n![GitHub release (latest by date)](https://img.shields.io/github/v/release/misternebula/quantum-space-buddies?style=flat-square)\\n![GitHub Release Date](https://img.shields.io/github/release-date/misternebula/quantum-space-buddies?label=last%20release&style=flat-square)\\n![GitHub all releases](https://img.shields.io/github/downloads/misternebula/quantum-space-buddies/total?style=flat-square)\\n![GitHub release (latest by date)](https://img.shields.io/github/downloads/misternebula/quantum-space-buddies/latest/total?style=flat-square)\\n![GitHub last commit (branch)](https://img.shields.io/github/last-commit/misternebula/quantum-space-buddies/dev?label=last%20commit%20to%20dev&style=flat-square)\\n\\n[![Support on Patreon](https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=%23e85b46&label=Patreon&query=data.attributes.patron_count&suffix=%20patrons&url=https%3A%2F%2Fwww.patreon.com%2Fapi%2Fcampaigns%2F8528628&logo=patreon)](https://www.patreon.com/qsb)\\n[![Donate with PayPal](https://img.shields.io/badge/PayPal-Donate%20(nebula)-blue?style=for-the-badge&color=blue&logo=paypal)](https://www.paypal.com/paypalme/nebula2056/5)\\n[![Donate with PayPal](https://img.shields.io/badge/PayPal-Donate%20(johncorby)-blue?style=for-the-badge&color=blue&logo=paypal)](https://www.paypal.com/paypalme/johncorby/5)\\n\\nQuantum Space Buddies (QSB) is a multiplayer mod for Outer Wilds. The mod uses the OWML mod loader and Mirror for networking.\\n\\nSpoilers within!\\n\\n## Installation\\n\\n### Easy installation (recommended)\\n\\n- [Install the Outer Wilds Mod Manager](https://github.com/Raicuparta/ow-mod-manager#how-do-i-use-this);\\n- Install Quantum Space Buddies from the mod list displayed in the application;\\n- If you can\\'t get the mod manager to work, follow the instructions for manual installation.\\n\\n### Manual installation\\n\\n- [Install OWML](https://github.com/amazingalek/owml#installation);\\n- [Download the latest Quantum Space Buddies release](https://github.com/misternebula/quantum-space-buddies/releases/latest);\\n- Extract the `QSB` directory to the `OWML/Mods` directory;\\n- Run `OWML.Launcher.exe` to start the game.\\n\\n## Hosting / Connecting\\n\\n#### Connecting to a server\\n\\n- On the title screen, click the option `CONNECT TO MULTIPLAYER`.\\n- Enter the Product User ID of the person you are trying to connect to.\\n- Enjoy!\\n\\n#### Hosting a server\\n\\n- On the title screen, click the option `OPEN TO MULTIPLAYER`.\\n- Share your Product User ID with the people who want to connect.\\n- Enjoy!\\n\\n## Frequently Asked Questions\\n\\n### Requirements\\n- Latest version of OWML.\\n- Latest version of Mod Manager. (If using)\\n- Latest version of Outer Wilds. **We cannot guarantee QSB, or OWML, will work on cracked/pirated versions of Outer Wilds. Do not come asking us for help when using pirated versions.**\\n- Fast and stable internet connection, upload and download.\\n- Above minimum Outer Wilds system requirements.\\n\\n### How complete is this mod? How far through the game can I play?\\n\\nThe base game is around 95% done, whereas EotE is around 80% done.\\n\\n### Compatibility with other mods\\nTL;DR - Don\\'t use any mods with QSB that aren\\'t marked as QSB compatible. \\n\\nQSB relies on object hierarchy to sync objects, so any mod that changes that risks breaking QSB. Also, QSB relies on certain game events being called when things happen in-game. Any mod that makes these things happen without calling the correct events will break QSB. Some mods will work fine and have been tested, like CrouchMod. Others may only work partly, like EnableDebugMode and TAICheat.\\n\\n### Will you make this compatible with NomaiVR?\\n\\nMaybe.\\n\\n### Why do I keep getting thrown around the ship?\\n\\nBoring boring physics stuff. The velocity of the ship is synced, as well as the angular velocity. However, this velocity is not also applied to the player. (Or it is sometimes. I don\\'t 100% know.) This means the ship will accelerate, leaving the player \"behind\". Which makes you fly into the walls alot.\\n\\n**Update**: you can attach/detach yourself to/from the ship using the prompt in the center of the screen.\\n\\n### What\\'s the difference between QSB and Outer Wilds Online?\\n\\nTL;DR - QSB is multiplayer co-op, Outer Wilds Online is multiplayer not co-op.\\n\\nQSB is a fully synced game. The other players are actually there in the world, and can affect things. The loop starts/ends at the same time for everyone, and you share ship logs / signal discoveries.\\n\\nOuter Wilds Online is easier to set up, but much more basic in its features. The other players cannot affect your game, and do not contribute to anything in your save. The loop is entirely per-player.\\n\\n### Why would someone make this mod? Seems like a lot of effort for no reward.\\n\\nGood question.\\n\\nLet me know if you find an answer.\\n\\n**Update**: a plausible answer is the enjoyment you get seeing/hearing about others playing with their friends :) \\n\\n## Translating\\n\\nSee [TRANSLATING.md](TRANSLATING.md)\\n\\n## Development Setup\\n\\n- [Download the Outer Wilds Mod Manager](https://github.com/raicuparta/ow-mod-manager) and install it anywhere you like;\\n- Install OWML using the Mod Manager\\n- Clone QSB\\'s source\\n- Open the file `DevEnv.targets` in your favorite text editor\\n- (optional if copying built dlls manually) Edit the entry `<OwmlDir>` to point to your OWML directory (it is installed inside the Mod Manager directory)\\n- (optional if no unity project) Edit the entry `<UnityAssetsDir>` to point to the Assets folder of the QSB unity project\\n- Open the project solution file `QSB.sln` in Visual Studio 2022\\n\\nIf developing with the Steam version of Outer Wilds you can\\'t run multiple instances of the game by default. To do so, create a file called `steam_appid.txt` in your Outer Wilds directory and write `753640` inside it, then run the exe directly.\\n\\nA powerful PC is needed for development, due to the high amount of RAM and CPU needed to run 2 or 3 instances of modded Outer Wilds.\\n\\nIt is also recommended to lower all graphics settings to minimum, be in windowed mode, and lower resolution to roughly a quarter of your monitor space. This lets you run multiple instances of Outer Wilds to quickly test QSB.\\n\\nSome debugging options exist to make things easier. These come in the form of actions and settings.\\n### Debug Actions :\\n\\nHold Q and press :\\n\\n- Numpad 1 - Teleport to nearest player.\\n- Numpad 2 - If holding LeftShift, warp to the dreamworld Vault fire. If not, warp to the Endless Canyon.\\n- Numpad 3 - Unlock the Sealed Vault.\\n- Numpad 4 - Damage the ship\\'s electrical system.\\n- Numpad 5 - Trigger the supernova.\\n- Numpad 6 - Set the flags for having met Solanum and the Prisoner.\\n- Numpad 7 - Warp to the Vessel.\\n- Numpad 8 - Insert the Advanced Warp Core into the Vessel.\\n- Numpad 9 - If holding LeftShift, load the SolarSystem scene. If not, load the EyeOfTheUniverse scene.\\n- Numpad 0 - Revive a random dead player.\\n\\n### Debug Settings :\\n\\nCreate a file called `debugsettings.json` in the mod folder.\\nThe template for this file is this :\\n\\n```\\n{\\n  \"useKcpTransport\": false,\\n  \"dumpWorldObjects\": false,\\n  \"instanceIdInLogs\": false,\\n  \"hookDebugLogs\": false,\\n  \"avoidTimeSync\": false,\\n  \"autoStart\": false,\\n  \"kickEveryone\": false,\\n  \"disableLoopDeath\": false,\\n  \"debugMode\": false,\\n  \"drawGui\": false,\\n  \"drawLines\": false,\\n  \"drawLabels\": false,\\n  \"drawQuantumVisibilityObjects\": false,\\n  \"drawGhostAI\": false,\\n  \"greySkybox\": false\\n}\\n```\\n\\n- useKcpTransport - Allows you to directly connect to IP addresses, rather than use the Epic relay.\\n- dumpWorldObjects - Creates a file with information about the WorldObjects that were created.\\n- instanceIdInLogs - Appends the game instance id to every log message sent.\\n- hookDebugLogs - Print Unity logs and warnings.\\n- avoidTimeSync - Disables the syncing of time.\\n- autoStart - Host/connect automatically for faster testing.\\n- kickEveryone - Kick anyone who joins a game.\\n- disableLoopDeath - Make it so the loop doesn\\'t end when everyone is dead.\\n- debugMode - Enables debug mode. If this is set to `false`, none of the following settings do anything.\\n- drawGui - Draws a GUI at the top of the screen that gives information on many things.\\n- drawLines - Draws gizmo-esque lines around things. Indicates reference sectors/transforms, triggers, etc. LAGGY.\\n- drawLabels - Draws GUI labels attached to some objects. LAGGY.\\n- drawQuantumVisibilityObjects - Indicates visibility objects with an orange shape.\\n- drawGhostAI - Draws debug lines and labels just for the ghosts.\\n- greySkybox - Turns the skybox grey. Useful in the Eye, where it\\'s pretty dark.\\n\\n**Warning : Mod development can lead to unexpected errors in your computer system.** \\n- **When editing the networking code, mistakes can lead to QSB overwhelming your network connection with excess packets**.\\n- **Too high RAM usage will lead to Outer Wilds sticking at ~31% loading, then crashing**.\\n- **There have been instances of graphics cards crashing, and needing to be disabled/re-enabled from Device Manager.**\\n\\n## Authors and Special Thanks\\n\\n### Authors\\n\\n- [\\\\_nebula](https://github.com/misternebula) - Developer of v0.3.0 onwards\\n- [JohnCorby](https://github.com/JohnCorby) - Co-developer of v0.13.0 onwards.\\n- [AmazingAlek](https://github.com/amazingalek) - Developer of v0.1.0 - v0.7.1.\\n- [Raicuparta](https://github.com/Raicuparta) - Developer of v0.1.0 - v0.2.0.\\n\\n### Contributers\\n\\n- [Chris Yeninas](https://github.com/PhantomGamers) - Help with project files and GitHub workflows.\\n- [Tlya](https://github.com/Tllya) - Russian translation.\\n- [Xen](https://github.com/xen-42) - French translation, and help with particle effects and sounds.\\n- [ShoosGun](https://github.com/ShoosGun) - Portuguese translation.\\n- [DertolleDude](https://github.com/DertolleDude) - German translation.\\n- [SakuradaYuki](https://github.com/SakuradaYuki) - Chinese translation.\\n- [poleshe](https://github.com/poleshe) - Spanish translation.\\n\\n### Special Thanks\\n- Thanks to Logan Ver Hoef for help with the game code, and for helping make the damn game in the first place.\\n- Thanks to all the people who helped in public tests.\\n\\n### Dependencies\\n\\n- [OWML](https://github.com/amazingalek/owml)\\n- [Mirror](https://mirror-networking.com/)\\n    - [kcp2k](https://github.com/vis2k/kcp2k)\\n    - [Telepathy](https://github.com/vis2k/Telepathy)\\n    - [where-allocation](https://github.com/vis2k/where-allocation)\\n- [EpicOnlineTransport](https://github.com/FakeByte/EpicOnlineTransport)\\n- [HarmonyX](https://github.com/BepInEx/HarmonyX)\\n- [UniTask](https://github.com/Cysharp/UniTask)\\n- Modified code from [Popcron\\'s Gizmos](https://github.com/popcron/gizmos)\\n\\n## Help / Discuss development / Whatever\\n\\n[Join the Outer Wilds Modding Discord](https://discord.gg/9vE5aHxcF9), we have a nice `#qsb-bugs-and-questions` channel for support, and other channels to discuss modding!\\n\\n## License and legal stuff\\n\\nCopyright (C) 2020 - 2023 : \\n- Henry Pointer (_nebula or misternebula)\\n- Will Corby (JohnCorby)\\n- Aleksander Waage (AmazingAlek)\\n- Ricardo Lopes (Raicuparta)\\n\\nThis program is free software: you can redistribute it and/or modify\\nit under the terms of the GNU Affero General Public License as published\\nby the Free Software Foundation, either version 3 of the License, or\\n(at your option) any later version.\\n\\nThis program is distributed in the hope that it will be useful,\\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\nGNU Affero General Public License for more details.\\n\\nYou should have received a copy of the GNU Affero General Public License\\nalong with this program.  If not, see <https://www.gnu.org/licenses/>.\\n\\nThis work is unofficial Fan Content created under permission from the Mobius Digital Fan Content Policy. It includes materials which are the property of Mobius Digital and it is neither approved nor endorsed by Mobius Digital.\\n'},\n",
       " {'repo': 'ethancjackson/FaceSpace',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': ''},\n",
       " {'repo': 'CirclesUBI/circles-contracts',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '<div align=\"center\">\\n\\t<img width=\"80\" src=\"https://raw.githubusercontent.com/CirclesUBI/.github/main/assets/logo.svg\" />\\n</div>\\n\\n<h1 align=\"center\">circles-contracts</h1>\\n\\n<div align=\"center\">\\n <strong>\\n   Smart Contracts for Circles UBI\\n </strong>\\n</div>\\n\\n<br />\\n\\n<div align=\"center\">\\n  <!-- npm -->\\n  <a href=\"https://www.npmjs.com/package/circles-contracts\">\\n    <img src=\"https://img.shields.io/npm/v/circles-contracts?style=flat-square&color=%23f14d48\" height=\"18\">\\n  </a>\\n  <!-- Licence -->\\n  <a href=\"https://github.com/CirclesUBI/circles-contracts/blob/main/LICENSE\">\\n    <img src=\"https://img.shields.io/github/license/CirclesUBI/circles-contracts?style=flat-square&color=%23cc1e66\" alt=\"License\" height=\"18\">\\n  </a>\\n  <!-- CI status -->\\n  <a href=\"https://github.com/CirclesUBI/circles-contracts/actions/workflows/run-tests.yml\">\\n    <img src=\"https://img.shields.io/github/workflow/status/CirclesUBI/circles-contracts/run-tests?label=tests&style=flat-square&color=%2347cccb\" alt=\"CI Status\" height=\"18\">\\n  </a>\\n  <!-- Discourse -->\\n  <a href=\"https://aboutcircles.com/\">\\n    <img src=\"https://img.shields.io/discourse/topics?server=https%3A%2F%2Faboutcircles.com%2F&style=flat-square&color=%23faad26\" alt=\"chat\" height=\"18\"/>\\n  </a>\\n  <!-- Twitter -->\\n  <a href=\"https://twitter.com/CirclesUBI\">\\n    <img src=\"https://img.shields.io/twitter/follow/circlesubi.svg?label=twitter&style=flat-square&color=%23f14d48\" alt=\"Follow Circles\" height=\"18\">\\n  </a>\\n</div>\\n\\n<div align=\"center\">\\n  <h3>\\n    <a href=\"https://github.com/CirclesUBI/circles-handbook/blob/master/docs/about/whitepaper.md\">\\n      Whitepaper\\n    </a>\\n    <span> | </span>\\n    <a href=\"https://handbook.joincircles.net\">\\n      Handbook\\n    </a>\\n    <span> | </span>\\n    <a href=\"https://github.com/CirclesUBI/circles-contracts/releases\">\\n      Releases\\n    </a>\\n    <span> | </span>\\n    <a href=\"https://github.com/CirclesUBI/.github/blob/main/CONTRIBUTING.md\">\\n      Contributing\\n    </a>\\n  </h3>\\n</div>\\n\\n<br/>\\n\\nEthereum Smart Contracts for the [`Circles`] UBI system: A decentralised Universal Basic Income platform based on personal currencies.\\n\\n[`circles`]: https://joincircles.net\\n\\n## Basic design\\n\\nThere are several components:\\n\\n### Token contract\\n\\nThis is derived from standard `ERC20` implementations, with two main differences: The ability to mint UBI to the token owner, and the `hubTransfer` function that allows transitive transfers. `Token` contracts store the address of the `Hub` that deployed them, and can only transact transitively with tokens from the same hub. Tokens have owners, which can be an external account or any kind of contract - in our deployment, they are [`Gnosis Safes`].\\n\\n[`Gnosis Safes`]: https://github.com/gnosis/safe-contracts\\n\\n### Hub contract\\n\\nThis is the location of system-wide variables and the trust graph. It has special permissions on all tokens that were deployed through it and have authorized it to perform transitive exchanges. All the parameters in a `Hub` are immutable and it has no owner.\\n\\n![Circles contract diagram](/assets/ContractDiagram.jpg)\\n\\nIllustrated here are some of the main available calls:\\n\\n- `signup` method of the `Hub` deploys a Circles token\\n- Safe or external accounts make trust connections within the hub with the trust method\\n- Users send transitive transactions with the hub, which has special permissions on tokens\\n\\n## Installation\\n\\n```bash\\nnpm i @circles/circles-contracts\\n```\\n\\nRequires [`Node`] version 14.\\n\\n[`Node`]: https://nodejs.org/en/download\\n\\n## Development\\n\\nRequires [Node](https://nodejs.org/en/download/) version 14. You can change your node version to the tested version with `nvm use`.\\n\\nInstall all required dependencies via `npm install`.\\n\\n`npm test` will re-build the contracts / tests and run all of the tests in the [test](test) directory.\\n\\nTests are executed with the help of [`Truffle`] and written in javascript using [`Mocha`] with the [`Chai`] assertion library. \\n\\nWhen you run `npm test` a new local blockchain will be started with ganache-cli (unless you already have one running). The contracts will be deployed and the javascript tests will make transactions to this chain.\\n\\nHelper functions defined in [`test/helpers`](test/helpers) provides functionality for more complicated tests such as: reading the event log, or checking for an EVM \"revert / throw\", or changing the blockstamp times.\\n\\nNote that: We commit the build dir on purpose, because the rest of our stack pulls this repo in from npm and gets the abis from them.\\n\\n[`Truffle`]: https://truffleframework.com/docs/truffle/testing/writing-tests-in-javascript\\n[`Mocha`]: https://mochajs.org\\n[`Chai`]: https://www.chaijs.com\\n\\n## License\\n\\nGNU Affero General Public License v3.0 [`AGPL-3.0`]\\n\\n[`AGPL-3.0`]: LICENSE\\n'},\n",
       " {'repo': 'simeonradivoev/ComputeStochasticReflections',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# Compute Stochastic Screen Space Reflections\\nCompute Stochastic Screen Space Reflections for Unity post processing. Utilizing shared memory for performance.\\nShould be mostly production ready, except for a few Hierarchical Z-depth casting artifacts.\\n\\n# Contents\\n* [Features](#Features)\\n* [Requirements](#Requirements)\\n* [Installation](#Installation)\\n* [Usage](#Usage)\\n* [References](#References)\\n* [Showcase](#Showcase)\\n* [Real life usage](#Real-life-usage)\\n* [Screenshots](#Screenshots)\\n* [Before and After](#Before-and-After)\\n* [Performance](#Performance)\\n\\n# Features\\n* Hierarchical Z-depth casting\\n* Temporal reflection depth reprojection\\n* Median Filtering for extra denoising\\n* Reflection Color Mipmap Pyramid\\n* Raycast and resolve pass downsampling\\n* Frame reprojection for faking multiple bounces\\n* Specular elongation\\n* Contact hardening\\n\\n# Requirements\\n* Works only with deferred rendering\\n* Compute shader capable video card\\n* [Unity Post Processing v2](https://github.com/Unity-Technologies/PostProcessing)\\n* Tested with Unity 2018.2\\n\\n# Installation\\nIn a unity project go to your `Packages` folder. Open `manifest.json` and add into the dependencies the following line: \\n\\n```\\n\"com.simeonradivoev.stochastic-reflections\": \"https://github.com/simeonradivoev/ComputeStochasticReflections.git\"\\n```\\n\\nIt should look something like this:\\n\\n```\\n{\\n    \"dependencies\": {\\n        \"com.unity.ugui\": \"1.0.0\",\\n        \"com.unity.modules.ui\": \"1.0.0\",\\n        \"com.simeonradivoev.stochastic-reflections\": \"https://github.com/simeonradivoev/ComputeStochasticReflections.git\",\\n    } \\n}\\n```\\n\\n# Usage\\nJust add a new effect in a post processing profile under `Custom/Stochastic Screen Space Reflections`\\nFor VR use the test branch called `StereoRendering`. It currently only supports multi pass rendering.\\n\\n# References\\n* Rewritten from [Xerxes1138](https://github.com/Xerxes1138/StochasticScreenSpaceReflection)\\n* Based mainly on [Tomasz Stachowiak and Yasin Uludag, Siggraph15](https://www.ea.com/frostbite/news/stochastic-screen-space-reflections)\\n\\n# Showcase\\n[![](https://img.youtube.com/vi/9D0kRA7vSCQ/default.jpg)](https://www.youtube.com/watch?v=9D0kRA7vSCQ)\\n[![](https://img.youtube.com/vi/LuLO25cPwyI/default.jpg)](https://www.youtube.com/watch?v=LuLO25cPwyI)\\n\\n# Real life usage\\n[![](https://img.youtube.com/vi/MtAYmqzJM5g/default.jpg)](https://www.youtube.com/watch?v=MtAYmqzJM5g)\\n\\n# Screenshots\\n\\n![](https://i.imgur.com/Fxfu70R.png)\\n![](https://i.imgur.com/C37mrdB.png)\\n![](https://i.imgur.com/QcsCOpf.png)\\n![](https://i.imgur.com/4NefLT8.png)\\n\\n# Before and After\\n![Before](https://i.imgur.com/DzvKm5I.png) ![After](https://i.imgur.com/Ua2Ng1R.png)\\n\\n# Performance\\nTested on a GTX 1070 at 1080p\\n\\n* Highest Quality, High Quality Blur\\n\\t* Raycasting: 1.4 ms\\n\\t* Blur: 0.73 ms\\n\\t* Temporal: 0.67 ms\\n\\t* Resolve: 0.55 ms\\n\\t* **Total + Others: 4.15 ms**\\n\\n* Raycast and Resolved downsampled, Low Quality Blur\\n\\t* Raycasting: 0.5 ms\\n\\t* Temporal: 0.19 ms\\n\\t* Resolve: 0.19 ms\\n\\t* **Total + Others: 1.7 ms**\\n'},\n",
       " {'repo': 'glotzerlab/signac',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# <img src=\"https://raw.githubusercontent.com/glotzerlab/signac/master/doc/images/palette-header.png\" width=\"75\" height=\"58\"> signac - simple data management\\r\\n\\r\\n[![Affiliated with NumFOCUS](https://img.shields.io/badge/NumFOCUS-affiliated%20project-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://numfocus.org/sponsored-projects/affiliated-projects)\\r\\n[![PyPI](https://img.shields.io/pypi/v/signac.svg)](https://pypi.org/project/signac/)\\r\\n[![conda-forge](https://img.shields.io/conda/vn/conda-forge/signac.svg?style=flat)](https://anaconda.org/conda-forge/signac)\\r\\n[![GitHub Actions](https://github.com/glotzerlab/signac/actions/workflows/run-pytest.yml/badge.svg)](https://github.com/glotzerlab/signac/actions)\\r\\n[![RTD](https://img.shields.io/readthedocs/signac.svg?style=flat)](https://docs.signac.io)\\r\\n[![License](https://img.shields.io/github/license/glotzerlab/signac.svg)](https://github.com/glotzerlab/signac/blob/master/LICENSE.txt)\\r\\n[![PyPI-downloads](https://img.shields.io/pypi/dm/signac.svg?style=flat)](https://pypistats.org/packages/signac)\\r\\n[![Slack](https://img.shields.io/badge/Slack-chat%20support-brightgreen.svg?style=flat&logo=slack)](https://signac.io/slack-invite/)\\r\\n[![Twitter](https://img.shields.io/twitter/follow/signacdata?style=social)](https://twitter.com/signacdata)\\r\\n[![GitHub Stars](https://img.shields.io/github/stars/glotzerlab/signac?style=social)](https://github.com/glotzerlab/signac/)\\r\\n\\r\\nThe [**signac** framework](https://signac.io) helps users manage and scale file-based workflows, facilitating data reuse, sharing, and reproducibility.\\r\\n\\r\\nIt provides a simple and robust data model to create a well-defined indexable storage layout for data and metadata.\\r\\nThis makes it easier to operate on large data spaces, streamlines post-processing and analysis and makes data collectively accessible.\\r\\n\\r\\n## Resources\\r\\n\\r\\n- [Framework documentation](https://docs.signac.io/):\\r\\n  Examples, tutorials, topic guides, and package Python APIs.\\r\\n- [Package documentation](https://docs.signac.io/projects/core/):\\r\\n  API reference for the **signac** package.\\r\\n- [Slack Chat Support](https://signac.io/slack-invite/):\\r\\n  Get help and ask questions on the **signac** Slack workspace.\\r\\n- [**signac** website](https://signac.io/):\\r\\n  Framework overview and news.\\r\\n\\r\\n## Installation\\r\\n\\r\\nThe recommended installation method for **signac** is through **conda** or **pip**.\\r\\nThe software is tested for Python 3.8+ and is built for all major platforms.\\r\\n\\r\\nTo install **signac** *via* the [conda-forge](https://conda-forge.github.io/) channel, execute:\\r\\n\\r\\n```bash\\r\\nconda install -c conda-forge signac\\r\\n```\\r\\n\\r\\nTo install **signac** *via* **pip**, execute:\\r\\n\\r\\n```bash\\r\\npip install signac\\r\\n```\\r\\n\\r\\n**Detailed information about alternative installation methods can be found in the [documentation](https://docs.signac.io/en/latest/installation.html).**\\r\\n\\r\\n## Quickstart\\r\\n\\r\\nThe framework facilitates a project-based workflow.\\r\\nSet up a new project:\\r\\n\\r\\n```bash\\r\\n$ mkdir my_project\\r\\n$ cd my_project\\r\\n$ signac init MyProject\\r\\n```\\r\\n\\r\\nand access the project handle:\\r\\n\\r\\n```python\\r\\n>>> project = signac.get_project()\\r\\n```\\r\\n\\r\\n## Testing\\r\\n\\r\\nYou can test this package by executing:\\r\\n\\r\\n```bash\\r\\n$ python -m pytest tests/\\r\\n```\\r\\n\\r\\n## Acknowledgment\\r\\n\\r\\nWhen using **signac** as part of your work towards a publication, we would really appreciate that you acknowledge **signac** appropriately.\\r\\nWe have prepared examples on how to do that [here](https://docs.signac.io/en/latest/acknowledge.html).\\r\\n**Thank you very much!**\\r\\n\\r\\nThe signac framework is a [NumFOCUS Affiliated Project](https://numfocus.org/sponsored-projects/affiliated-projects).\\r\\n'},\n",
       " {'repo': 'Nefelim4ag/systemd-swap',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# systemd-swap\\n[![Code Style: Black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/ambv/black)\\n\\n## ⚠️ Current code quality and commit frequency is low ⚠️\\n\\n### Users should migrate to [systemd/zram-generator](https://github.com/systemd/zram-generator) since zram should be enough in most systems\\n\\nScript to manage swap on:\\n\\n- [zswap](https://www.kernel.org/doc/Documentation/vm/zswap.txt) - Enable/Configure\\n- [zram](https://www.kernel.org/doc/Documentation/blockdev/zram.txt) - Autoconfigurating for swap\\n- files - (sparse files for saving space, supports btrfs)\\n- block devices - auto find and do swapon\\n\\n:information_source: It is configurable in `/etc/systemd/swap.conf`.\\n\\nAdditional terms:\\n\\n- **SwapFC** (File Chunked) - provides a dynamic swap file allocation/deallocation\\n\\n## File location\\n\\n```text\\n/etc/systemd/swap.conf\\n/usr/lib/systemd/system/systemd-swap.service\\n/usr/bin/systemd-swap\\n```\\n\\n## Please don\\'t forget to enable and start with\\n\\n```shell\\nsudo systemctl enable --now systemd-swap\\n```\\n\\n## Install\\n\\n- <img src=\"https://www.monitorix.org/imgs/archlinux.png\" weight=\"16\" height=\"16\"> **Arch Linux**: in the [community](https://www.archlinux.org/packages/community/any/systemd-swap/) repo.\\n\\n- <img src=\"https://www.monitorix.org/imgs/debian.png\" weight=\"16\" height=\"16\"> **Debian based distros**\\n\\n  ```shell\\n  git clone https://github.com/Nefelim4ag/systemd-swap.git\\n  cd systemd-swap\\n  make deb\\n  sudo apt install ./systemd-swap_*_all.deb\\n  ```\\n\\n- <img src=\"https://www.monitorix.org/imgs/fedora.png\" weight=\"16\" height=\"16\"> **Fedora based distros**\\n\\n  ```shell\\n  sudo dnf copr enable zeno/systemd-swap\\n  sudo dnf install systemd-swap\\n  ```\\n  \\n  or\\n  \\n  ```shell\\n  git clone https://github.com/Nefelim4ag/systemd-swap.git\\n  cd systemd-swap\\n  make rpm\\n  sudo dnf install ./systemd-swap-*noarch.rpm\\n\\n- **Manual**\\n\\n  Install dependencies:\\n  - `python3` >= 3.7\\n  - `python3` packages: `systemd-python` and `sysv_ipc`\\n\\n  ```shell\\n  git clone https://github.com/Nefelim4ag/systemd-swap.git\\n  cd systemd-swap\\n  sudo make install\\n\\n  # or into /usr/local:\\n  sudo make prefix=/usr/local install\\n  ```\\n\\n## About configuration\\n\\n**Q**: Do we need to activate both zram and zswap?\\\\\\n**A**: Nope, it\\'s useless, as zram is a compressed RAM DISK, but zswap is a compressed _\"writeback\"_ CACHE on swap file/disk. Also having both activated can lead to inverse LRU as noted [here](https://askubuntu.com/questions/471912/zram-vs-zswap-vs-zcache-ultimate-guide-when-to-use-which-one/472227#472227)\\n\\n**Q**: Do I need to use `swapfc_force_use_loop` on swapFC?\\\\\\n**A**: Nope, as you wish really, native swapfile should work faster and it\\'s safer in OOM condition in comparison to loop backed scenario.\\n\\n**Q**: When would we want a certain configuration?\\\\\\n**A**: In most cases zram is enough since it on average compresses 2-3x and is much faster than disk based swap.\\n\\n**Q**: How many zram devices should one use?\\n**A**: If you are not using an ancient kernel (pre 4.7) there\\'s no benefit from using multiple zram devices.\\n\\n**Q**: Can we use this to enable hibernation?\\\\\\n**A**: Nope as hibernation wants a persistent fs blocks and wants access to swap data directly from disk, this will not work on: _swapfc_ (without some magic of course, see [#85](https://github.com/Nefelim4ag/systemd-swap/issues/85)).\\n\\n## Switch on systemd-swap:s automatic swap management\\n\\n- Enable swapfc if wanted (note, you should **never** use zram and zswap at the same time)\\n\\n  ```shell\\n  vim /etc/systemd/swap.conf.d/overrides.conf\\n  ```\\n\\n  ```ini\\n  swapfc_enabled=1\\n  ```\\n\\n- Stop any external swap:\\n\\n  ```shell\\n  sudo swapoff -a\\n  ```\\n\\n- Remove swap entry from fstab:\\n\\n  ```shell\\n  vim /etc/fstab\\n  ```\\n\\n- Remove your swap\\n\\n  ```shell\\n  # For Ubuntu\\n  sudo rm -f /swapfile\\n\\n  # For Centos 7 (if using a swap partition and lvm)\\n  lvremove -Ay /dev/centos/swap\\n  lvextend -l +100%FREE centos/root\\n  ```\\n\\n- Remove swap from Grub:\\n\\n  ```shell\\n  # For Ubuntu remove resume* in grub\\n  vim /etc/default/grub\\n\\n  # For Centos 7 remove rd.lvm.lv=centos/swap*\\n  vim /etc/default/grub\\n\\n  # For Manjaro remove resume* in grub & mkinitcpio\\n  vim /etc/default/grub\\n  vim /etc/mkinitcpio.conf\\n  ```\\n\\n  ```shell\\n  # For Ubuntu\\n  update-grub\\n\\n  # For Centos 7\\n  grub2-mkconfig -o /boot/grub2/grub.cfg\\n\\n  # For Manjaro\\n  update-grub\\n  mkinitcpio -P\\n  ```\\n'},\n",
       " {'repo': 'enmerk4r/SpaceMonkey',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# SpaceMonkey\\nA sample Rhino plugin to showcase MVVM, MaterialDesign and calling into a REST API. For the purposes of this exercise we\\'re writing a tool that calls into the N2YO API in order to get current coordinates and metadata for satellites in Earth\\'s orbit, which we can then bake to Rhino. This project very purposely **does not use IoC**, which would normally be another vital component of scalable modern applications. This is because IoC is a whole other animal to tackle and we\\'re just trying to focus on the WPF side of things here.\\n\\n[[Recording of the YouTube Livestream](https://www.youtube.com/watch?v=s7WC-3DGpoI&t=1589s&ab_channel=EngineeringArchiTECHure)]\\n\\n![](https://github.com/enmerk4r/SpaceMonkey/blob/main/Assets/FrontPage.PNG)\\n\\n## Requirements\\n- Visual Studio 2019 or later (Community is fine)\\n- Rhino 6\\n- .NET framework 4.8 or higher\\n\\n## Installation\\n1. After you clone the repo, go to [N2YO](https://www.n2yo.com/login/register/), create a new free account and generate an API key. \\n2. In Visual Studio create a file named \"secrets.json\" at the root of the SpaceMonkey.Rhinoceros project (VS will complain about a missing file there, so you\\'ll see it)\\n3. Inside that file paste your API key in the following format:\\n```\\n{\\n    \"api_key\": \"PUT_YOUR_API_KEY_HERE\"\\n}\\n```\\n4. Makse sure that SpaceMonkey.Rhinoceros is set as the startup project. \\n5. Make sure that if you right-click on the project, then go to Properties > Debug, the \"Start action\" field is set to Start External Program and is pointing to `C:\\\\Program Files\\\\Rhino 6\\\\System\\\\Rhino.exe` (or wherever your Rhino 6 is installed)\\n6. Hit \"Run\"\\n7. [ATTN: DO THIS ONLY ONCE] After you build the project and start Rhino for the first time, go to the `bin` folder of SpaceMonkey.Rhinoceros (`\\\\SpaceMonkey\\\\SpaceMonkey.Rhinoceros\\\\bin`), find a file called `SpaceMonkey.Rhinoceros.rhp` and drag it into the Rhino canvas. That will register the plugin to your instance of Rhino.\\n8. Run the `SpaceMonkey` command in Rhino\\'s command line\\n\\n## Resources\\n![](https://github.com/enmerk4r/SpaceMonkey/blob/main/Assets/SpaceMonkey_1.gif)\\n\\n### Livestream\\nThe original livestream where this project and its underlying concepts are discussed [can be found here](https://www.youtube.com/watch?v=s7WC-3DGpoI&t=1589s&ab_channel=EngineeringArchiTECHure)\\n\\n### Satellite Tracking API\\nThe satellite tracking API is provided by [N2YO](https://www.n2yo.com/)\\nTheir REST API documentation can be found [here](https://www.n2yo.com/api/)\\n\\n### McNeel\\'s Rhino plugin sample\\nWe all know that McNeel are awesome, which is why they provide a [wealth of code samples](https://github.com/mcneel/rhino-developer-samples) for Rhino and Grasshopper developers on their GitHub page. For instance, you can find a very barebones implementation of a dockable Rhino plugin that uses WPF [via this link](https://github.com/mcneel/rhino-developer-samples/tree/7/rhinocommon/cs/SampleCsWpf). SpaceMonkey also uses that sample.\\n\\n### Deep dive into MVVM and IoC\\nIf you want to learn more about MVVM and start learning about IoC, then check out [this great YouTube playlist](https://www.youtube.com/playlist?list=PLrW43fNmjaQVYF4zgsD0oL9Iv6u23PI6M) by [AngelSix](https://www.youtube.com/c/AngelSix). This is an in-depth, step-by-step walk through the process of building an MVVM application that follows the IoC pattern. SpaceMonkey uses some of the MVVM-related base classes from these tutorials.\\n\\n### WPF Material Design\\nThe project\\'s website can be found [here](http://materialdesigninxaml.net/)\\n\\nWPF Material Design library has been built by [James Willock](https://github.com/ButchersBoy)\\n\\nAnd here\\'s the project\\'s [GitHub page](https://github.com/MaterialDesignInXAML/MaterialDesignInXamlToolkit) with links to documentation and Wikis\\n\\n## Troubleshooting\\n### Fody throws build errors\\nIf you see an error saying `Fody is only supported on MSBuild 16 and above`, then you\\'re running an older version of Visual Studio: you need VS 2019 or older. That\\'s no big deal, because Visual Studio community is free. If, however, you really really really don\\'t want to install new VS, you can simply downgrade Fody to v3.x.x.\\n\\n### Rhino 6 says the plugin is not compatible\\nYou have an old Service Pack. Just upgrade your Rhino.\\n\\n### Visual Studio doesn\\'t build because of a missing \"secrets.json\" file\\nRead the installation instructions above. That file has to be created by you and contain your free API key for N2YO\\n'},\n",
       " {'repo': 'Stefan-Korner/SpacePyLibrary',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': 'SpacePyLibrary\\n==============\\n\\nPython library for AeroSpace applications.\\n\\nThere are 2 GIT branches in the repository:\\n- master....implementation for Python 3\\n- python2...implementation for Python 2\\n\\nThis branch is the master branch for Python 3.\\n'},\n",
       " {'repo': 'satellite-game/Satellite',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Satellite [![Build Status](https://travis-ci.org/satellite-game/Satellite.png?branch=master)](https://travis-ci.org/satellite-game/Satellite)\\n\\n> A 3D multiplayer space warfare game centered around moon bases and space stations\\n\\n## How to play online\\n\\nGo to [http://satellite.io](http://satellite.io)\\n\\n\\n## How to play locally.\\n\\nJust run the default grunt task from the root directory by issuing `grunt`\\n\\n## Testing\\n\\nRunning `grunt test` will initiate mocha/chai/sinon testing on both client side and server side. Running `grunt` will initiate the same testing suite. Both `grunt test` and `grunt` will not execute all tests until changes to a file are made to files.\\n\\n\\n## Dev Staging\\n\\nPushes to Azure with every merge to dev branch. [Link can be found here](http://satellite.azurewebsites.net/)\\n\\n\\n# README Directory\\n[Server concerns](/server)\\n[Client concerns](/client/game/README.md)\\n'},\n",
       " {'repo': 'HavenFeng/photometric_optimization',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Photometric FLAME Fitting\\n\\nThis repository provides an analysis-by-synthesis framework to fit a textured [FLAME](http://flame.is.tue.mpg.de/) model to an image. FLAME is a lightweight generic 3D head model learned from over 33,000 head scans, but it does not come with an appearance space (see the [scientific publication](https://ps.is.tuebingen.mpg.de/uploads_file/attachment/attachment/400/paper.pdf) for details). \\n\\n<p align=\"center\"> \\n<img src=\"images/tex_0_-3.0.png\" width=\"150\"/>\\n<img src=\"images/tex_1_-3.0.png\" width=\"150\"/>\\n<img src=\"images/tex_2_-3.0.png\" width=\"150\"/>\\n<img src=\"images/tex_3_-3.0.png\" width=\"150\"/>  \\n<img src=\"images/tex_4_-3.0.png\" width=\"150\"/>  \\n</p>\\n<p align=\"center\"> \\n<img src=\"images/tex_0_3.0.png\" width=\"150\"/>\\n<img src=\"images/tex_1_3.0.png\" width=\"150\"/>\\n<img src=\"images/tex_2_3.0.png\" width=\"150\"/>\\n<img src=\"images/tex_3_3.0.png\" width=\"150\"/>  \\n<img src=\"images/tex_4_3.0.png\" width=\"150\"/>  \\n</p>\\n<p style=\"text-align: justify;\"><em><font color=\"grey\">Variations of the texture space for the first five principal components. Each column shows the variation for &plusmn;2 standard deviations along one axis.</font></em></p>\\n \\nThis repository \\n1) describes how to build a texture space for FLAME from in-the-wild images, and provides\\n2) code to fit a textured FLAME model to in-the-wild images, optimizing for FLAME\\'s parameters, appearance, and lighting, and\\n3) code to optimize for the FLAME texture to match an in-the-wild image. \\n\\n**The FLAME model and the texture space can be downloaded from the [FLAME project website](https://flame.is.tue.mpg.de). You need to sign up and agree to the license for access.**\\n\\n**The demos will be released soon.**\\n\\n## Build FLAME texture space from in-the-wild images\\n\\nThe goal is to build a texture space from in-the-wild images in order to cover large range of ethnicities, age groups, etc. We therefore randomly select 1500 images from the [FFHQ dataset](https://github.com/NVlabs/ffhq-dataset) in order to build a texture space. This is done in following steps\\n\\n***1. Initialization***\\n\\nBuilding a texture space from in-the-wild images is a chicken-and-egg problem. Given a texture space, it can be used in an analysis-by-synthesis fashion to fit the 3D model to images, where these fits then can be used to build a texture space. To get an initial texture space, we fit FLAME to the [Basel Face Model (BFM)](https://faces.dmi.unibas.ch/bfm/index.php?nav=1-0&id=basel_face_model) template, and project the BFM vertex colors onto the FLAME mesh, to get an initial texture basis.\\n\\n***2. Model fitting***\\n\\nWe then fit FLAME to the FFHQ images, optimizing for the FLAME shape, pose, and expression parameters, the parameters of the initial texture space, the parameters for Spherical Harmonics (SH) lighting (we optimize for 9 SH coefficient only, shared across all three color channels), and a texture offset to capture texture details deviating from the initial texture space. The fitting minimizes a landmark loss, a photometric loss, and diverse regularizers for shape, pose, expression, appearance, and the texture offset. \\n\\nThe landmark loss minimizes the difference between the landmarks projected from the face model\\'s surface, and predicted 2D landmarks (predicted using the [FAN landmark predictor](https://github.com/1adrianb/face-alignment)). The photometric loss is optimized for the skin region only (provided by the [face segmentation network](https://github.com/YuvalNirkin/face_segmentation)) to gain robustness to partial occlusions. See the provided code for details how to fit a textured FLAME model to an image. \\n\\n***3. Texture completion***\\n\\nAfter fitting, the computed texture offsets capture for each image the facial appearance of the non-occluded skin region. To complete the texture maps, we train an inpainting network adapted from [GMCNN](https://github.com/shepnerd/inpainting_gmcnn) (across all texture maps) supervisely by adding random strokes (i.e. strokes of random size and location) in the visible face region(visibility obtained from the fitted reconstruction) and learning to inpaint these strokes. Once trained, we inpaint all missing regions with the resulting inpainting network.\\n\\n***4. Texture space computation***\\n\\nAfter completing these 1500 texture maps, we use principal component analysis (PCA) to compute a texture space. \\n\\n## Demos\\nThe single image photometric fitting demo is implemented and tested in a conda environment with PyTorch 1.5 and PyTorch3D 0.2 in Python 3.8.3. For better CUDA supports, we recommend you to install PyTorch3D 0.2 via conda, \\n\\n```\\nconda create -n pytorch3d python=3.8\\nconda activate pytorch3d\\nconda install -c pytorch pytorch=1.5.0 torchvision cudatoolkit=10.2\\nconda install -c conda-forge -c fvcore fvcore\\nconda install pytorch3d -c pytorch3d\\n```\\n\\nATTENTION: The pip and conda packages of PyTorch3D have different dependencies, please follow their installation guide.\\n\\nRun this demo with specified FFHQ image name and computing device,\\n```\\npython photometric_fitting.py 00000 cuda\\n```\\n\\nAnother simple demo to sample the texture space can be found [here](https://github.com/TimoBolkart/TF_FLAME).\\n\\n\\n## License\\nThe code is available for non-commercial scientific research purposes. The texture model is available under [Creative Commons BY-NC-SA 4.0 license](https://creativecommons.org/licenses/by-nc-sa/4.0/). For details see the [Texture license](https://flame.is.tue.mpg.de/texturelicense.html).\\n\\n## Notes\\nWe use the FLAME.py from [FLAME_PyTorch](https://github.com/soubhiksanyal/FLAME_PyTorch) and the renderer.py from [DECA](https://github.com/YadiraF/DECA).\\n\\n## Citation\\n\\nWhen using this code or the texture model in a scientific publication, please cite **this GitHub repository** and the **FFHQ dataset**. When using the FLAME geometry model, please cite the model (you find the up-to-date bibtex [here](https://flame.is.tue.mpg.de/)).\\n\\n## Contact\\nFor questions regarding the provided fitting code please contact haiwen.feng@tuebingen.mpg.de, for FLAME related questions please contact flame@tuebingen.mpg.de.\\n'},\n",
       " {'repo': 'helemaalbigt/DesignSpace',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': \"# DesignSpace\\n\\nhttps://www.youtube.com/watch?v=VzfZSJFIbxY\\n\\nDesignSpace is a prototype design tool for the HTC Vive for urban designers and architects, first developed during the 2016 AEC Hackathon in London.\\nThe tool demonstrates how you could load in and position reference images, work at scale, add simple block volumes, and sketch directly on the model.\\n\\nNote that the current version still has quite a few bugs in both the UI and model interaction.\\n\\n## Usage\\n\\nThis Repo contains the Unity3D project files. To open the project, launch Unity3D, select 'Open' and navigate to this project folder.\\n\\n## Controls\\n\\nGrip buttons: hold one to pan the entire scene, hold two to zoom/rotate the entire scene\\nTrigger buttons: click buttons, select objects, move/scale/rotate objects\\n\\n## Credits\\n\\nAEC hackathon team: Carlos de la Barrera, Lorenzo Greco, Thomas Phillips, Thomas Van Bouwel <br>\\n3D Model London: Michal Konicek - Vertex Modelling <br>\\nOBJ import script: AARO4130 <br>\\nSky box created by Hazel Whorley\\n\\n## License\\nMIT License\\n\"},\n",
       " {'repo': 'Mikeware/SpaceBattleArena',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': 'Space Battle Arena\\n============\\n\\nSpace Battle Arena is a ‘[Programming Game](http://en.wikipedia.org/wiki/Programming_game)‘ where you must write code (in Java) to autonomously control a space ship to accomplish specified objectives.  \\n\\nTo find out more visit our **[GitHub Website](http://mikeware.github.io/SpaceBattleArena)**.  Complete Learning materials and other guides are available there as well.\\n\\nSpace Battle Arena is [licensed](LICENSE) under the GPLv2.  [Gson](https://github.com/Mikeware/SpaceBattleArena/releases/download/v1.1.0.1111/gson-2.2.jar) is licensed under the Apache license and provided for convenience on our [release](https://github.com/Mikeware/SpaceBattleArena/releases) page.\\n\\nStudent Environment\\n-------------------------\\nIt is expected that students have completed a full year of Java programming in high school or just over a semester of programming at the college level.\\n\\nWe use [jGRASP](http://www.jgrasp.org/) as our IDE of choice when working with High School students, but any Java IDE can be used that is capable of adding a jar to a classpath and executing a class from within the jar as the main class.\\n\\nResources\\n------------\\n* [Client Setup](http://mikeware.github.io/SpaceBattleArena/client/)\\n    * [Client Java Docs](http://mikeware.github.io/SpaceBattleArena/client/java_doc/)\\n    * [Initial Guides](http://mikeware.github.io/SpaceBattleArena/client/guides/)\\n* [Server Setup](http://mikeware.github.io/SpaceBattleArena/server/)\\n* [Mac OS X Server from Source](https://github.com/Mikeware/SpaceBattleArena/blob/master/SBA_Serv/README.md)\\n* Talks\\n    * [You Have Died of Dysentery: Games in Education Are Still Alive - PAXDev 2014](http://www.mikeware.com/2014/08/you-have-died-of-dysentery-games-in-education-are-still-alive/)\\n    * [Reach for the Stars - PAXDev 2012](http://www.mikeware.com/2012/09/reach-for-the-stars-educating-the-next-generation-using-games/)\\n* [Development](http://mikeware.github.io/SpaceBattleArena/dev)\\n\\nDevelopment Tools\\n----------------------\\n* Visual Studio 2013 w/ [Python Tools](http://pytools.codeplex.com/)\\n* [Eclipse](https://eclipse.org/) or [jGRASP](http://www.jgrasp.org/)\\n\\nDependencies\\n----------------\\nSpace Battle was built against the following versions of libraries:\\n\\n* Java Client\\n    * [Gson 2.2](https://github.com/google/gson)\\n* Python Server\\n    * [Python 2.7.12](https://www.python.org/downloads/release/python-2712/)\\n    * [PyMunk 3.0.0](https://pypi.python.org/pypi/pymunk/3.0.0)\\n    * [PyGame 1.9.1](http://www.pygame.org/download.shtml)\\n    * [Py2Exe 0.6.9](http://sourceforge.net/projects/py2exe/files/py2exe/0.6.9/) (for server executable build)\\n\\nSee the [Development Guide](http://mikeware.github.io/SpaceBattleArena/dev/) for more information on extending Space Battle Arena.\\n'},\n",
       " {'repo': 'Majiir/Kethane',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '\\ufeffKethane\\n=======\\n\\nResource mining and processing plugin for [Kerbal Space Program](http://www.kerbalspaceprogram.com/).\\n\\nForum thread: [Kethane Pack](http://forum.kerbalspaceprogram.com/showthread.php/23979-Kethane-Pack)  \\nMaintainer: [Majiir](http://forum.kerbalspaceprogram.com/member.php/7556-Majiir)\\n\\nBuilding\\n--------\\n\\nTo build Kethane you can use the [Kethane Utilities](https://github.com/LaylConway/KethaneUtilities) scripts to automatically build the plugin and assemble a mod folder.\\n\\nIf you don\\'t want to use the automatic build scripts, you can build manually using the following steps:\\n\\n1. Build the plugin DLL. Make sure to reference the Assembly-CSharp and UnityEngine assemblies from the version of KSP you wish to target. (A plugin build targeted to one version may not work on another, even if no code changes are necessary for compatibility.)\\n2. Copy the part .cfg files from the repository Parts/ directory.\\n3. Copy any other assets from the latest public release. In particular, .wav, .mu and .mbm files are currently excluded from the repository.\\n\\nReporting Issues\\n----------------\\n\\nPlease provide as much detail as possible when reporting an issue. That said, if you encounter an issue and aren\\'t able to pin down the cause, post it and explain what you\\'ve tried so far. Some bugs are difficult to reproduce, and we need to know about them anyway.\\n\\nFeature suggestions are also welcome. However, don\\'t be surprised if these issues are closed. The project will be expanding, but some features are out of the intended scope and won\\'t be included.\\n\\nCoding and Pull Request Guidelines\\n----------------------------------\\n\\n- The project follows [\"A successful Git branching model\"](http://nvie.com/posts/a-successful-git-branching-model/) with some minor modifications. Namely, before a release branch may be merged into `master`, any changes on `master` must have been merged into the release branch (or other branches upstream).\\n- Pull requests should be tested before submission.\\n- Keep your commits clean. Before committing, check your changes and make sure you\\'re only committing the absolute minimum changes necessary. In particular, avoid committing changes to the .csproj file unless you\\'re absolutely sure that those changes should be included in the repository.\\n- Don\\'t commit binary files, including DLLs and models.\\n- No merges should be included in pull requests unless the pull request\\'s purpose is a merge.\\n- No tabs, please. Use four spaces instead.\\n- No trailing whitespaces.\\n- No CRLF line endings, LF only, put your gits \\'core.autocrlf\\' on \\'true\\'.\\n- No 80 column limit or \\'weird\\' midstatement newlines.\\n\\nKeep in mind that some pull requests will be rejected at first, but with changes, may be accepted again. Don\\'t be offended if your pull is rejected; it\\'s just an effort to maintain a consistent code base and feature growth.\\n\\nModel and Part Asset Submission Guidelines\\n------------------------------------------\\n\\nPart assets should be submitted by private message to [Majiir](http://forum.kerbalspaceprogram.com/member.php/7556-Majiir) for review.\\n\\nThe following contents should be packaged in a .zip or .rar archive:\\n\\n- Model to be submitted as a .obj or .FBX, must include low-poly convex node_collider mesh\\n    - Meshes of appropriate resolution/polycount (<1000 tris small part, <2000 tris large part recommended)\\n    - Objects named properly (there\\'s no strict convention, but make sure they\\'re clearly named; e.g. fuelTank_geo, geo_engine, enginePoly, etc.)\\n    - All transformations must be cleared/frozen\\n    - No n-gons (five or greater sided polygons)\\n    - UVs mapped to 0-1 space, not overlapping (you can overlap UV shells, just not UVs of the same shell)\\n- Texture map and Normal map (optional) to be submitted as a .png or .jpeg\\n    - Appropriate resolution for part size, not to exceed 1024\\n    - AO baked textures recommended\\n- Notes in .txt file (optional)\\n\\nPart submissions may be returned for any of the following reasons, in approximately this order:\\n\\n- Part doesn\\'t fall within the scope of the plugin\\n- Assets submission doesn\\'t meet above guidelines\\n- Part concept doesn\\'t fit the art style of the plugin\\n- Assets need further work to improve quality\\n\\nOur goal is to help you submit your parts, but at the same time, contributions can\\'t add significantly to our workload. These guidelines are here to ensure a high standard of quality and to make the process as painless as possible for everyone involved.'},\n",
       " {'repo': 'awslabs/damo',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': 'DAMO: Data Access Monitoring Operator\\n=====================================\\n\\n`damo` is a user space tool for [DAMON](https://damonitor.github.io).  Using\\nthis, you can monitor the data access patterns of your system or workloads and\\nmake data access-aware memory management optimizations.\\n\\n![damo monitor demo for water_nsquared](images/damo_monitor_water_nsquared.gif)\\n\\n\\nDemo Video\\n==========\\n\\nPlease click the below thumbnail to show the short demo video.\\n\\n[![DAMON: a demo for the Kernel Summit 2020](\\nhttp://img.youtube.com/vi/l63eqbVBZRY/0.jpg)](\\nhttp://www.youtube.com/watch?v=l63eqbVBZRY\\n\"DAMON: a demo for the Kernel Summit 2020\")\\n\\n\\nGetting Started\\n===============\\n\\nFollow below instructions and commands to monitor and visualize the access\\npattern of your workload.\\n\\n    $ # ensure your kernel is built with CONFIG_DAMON_*=y\\n    $ sudo mount -t debugfs none /sys/kernel/debug/\\n    $ sudo pip3 install damo\\n    $ sudo damo record $(pidof <your workload>)\\n    $ damo report heats --heatmap stdout --stdout_heatmap_color emotion\\n\\nThe last command will show the access pattern of your workload, like below:\\n\\n![masim zigzag heatmap in ascii](images/masim_zigzag_heatmap_ascii.png)\\n![masim stairs heatmap in ascii](images/masim_stairs_heatmap_ascii.png)\\n\\n\\nFAQs\\n====\\n\\nHow can I install a kernel that is built with `CONFIG_DAMON_*=y`?\\n-----------------------------------------------------------------\\n\\nPlease refer to \\'Install\\' section of https://damonitor.github.io/.\\n\\nWhere can I get more detailed usage?\\n------------------------------------\\n\\nThe below sections provide quick introductions for `damo`\\'s major features.\\nFor more detailed usage, please refer to [USAGE.md](USAGE.md) file.\\n\\n\\nWhat does the version number mean?\\n----------------------------------\\n\\nNothing at all but indicate which version is more fresh.  A higher version\\nnumber means it is more recently released.\\n\\n\\nWill `pip3 install damo` installs the latest version of `damo`?\\n---------------------------------------------------------------\\n\\nIt will install the latest _stable_ version of `damo`.  If you want, you can\\nalso install less stable but more fresh `damo` from source code.  For that,\\nfetch the `next` branch of the source tree and use `damo` executable file in\\nthe tree.\\n\\n    $ git clone https://github.com/awslabs/damo -b next\\n\\n\\nHow can I participate in the development of `damo`?\\n---------------------------------------------------\\n\\nPlease refer to\\n[CONTRIBUTING](https://github.com/awslabs/damo/blob/next/CONTRIBUTING) file.\\n\\n\\n\\nRecording Data Access Patterns\\n==============================\\n\\nBelow commands record memory access patterns of a program and save the\\nmonitoring results in `damon.data` file.\\n\\n    $ git clone https://github.com/sjp38/masim\\n    $ cd masim; make; ./masim ./configs/zigzag.cfg &\\n    $ sudo damo record -o damon.data $(pidof masim)\\n\\nThe first two lines of the commands get an artificial memory access generator\\nprogram and run it in the background.  It will repeatedly access two 100\\nMiB-sized memory regions one by one.  You can substitute this with your real\\nworkload.  The last line asks ``damo`` to record the access pattern in\\n``damon.data`` file.\\n\\n\\nVisualizing Recorded Patterns\\n=============================\\n\\nBelow three commands visualize the recorded access patterns into three\\nimage files.\\n\\n    $ damo report heats --heatmap access_pattern_heatmap.png\\n    $ damo report wss --range 0 101 1 --plot wss_dist.png\\n    $ damo report wss --range 0 101 1 --sortby time --plot wss_chron_change.png\\n\\n- ``access_pattern_heatmap.png`` will show the data access pattern in a\\n  heatmap, which shows when (x-axis) what memory region (y-axis) is how\\n  frequently accessed (color).\\n- ``wss_dist.png`` will show the distribution of the working set size.\\n- ``wss_chron_change.png`` will show how the working set size has\\n  chronologically changed.\\n\\nYou can show the images on a web page [1].  Those made with other realistic\\nworkloads are also available [2,3,4].\\n\\n[1] https://damonitor.github.io/doc/html/latest/admin-guide/mm/damon/start.html#visualizing-recorded-patterns  \\n[2] https://damonitor.github.io/test/result/visual/latest/rec.heatmap.1.png.html  \\n[3] https://damonitor.github.io/test/result/visual/latest/rec.wss_sz.png.html  \\n[4] https://damonitor.github.io/test/result/visual/latest/rec.wss_time.png.html\\n\\n\\nData Access Pattern Aware Memory Management\\n===========================================\\n\\nBelow three commands make every memory region of size >=4K that hasn\\'t accessed\\nfor >=60 seconds in your workload to be swapped out.  By doing this, you can\\nmake your workload more memory efficient with only a modest performance\\noverhead.\\n\\n    $ echo \"#min-size max-size min-acc max-acc min-age max-age action\" > my_scheme\\n    $ echo \"4K        max      0       0       60s     max     pageout\" >> my_scheme\\n    $ sudo damo schemes -c my_scheme <pid of your workload>\\n'},\n",
       " {'repo': 'jasonwebb/2d-space-colonization-experiments',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '> [Read my Medium article](https://medium.com/@jason.webb/space-colonization-algorithm-in-javascript-6f683b743dc5) to learn more about the space colonization algorithm and this project.\\n>\\n> [Additional media is available on my portfolio](https://jasonwebb.io/2020/03/space-colonization-branching-experiments-in-javascript/)\\n\\n## About space colonization\\n\\n<img src=\"https://i.imgur.com/WQoYWBx.gif\" width=\"300\" align=\"right\">\\n\\nSpace colonization is a process for iteratively growing networks of branching lines based on the distribution of growth hormone sources (called \"auxin\" sources) to which the lines are attracted. [Originally described](http://algorithmicbotany.org/papers/venation.sig2005.pdf) (PDF) by Adam Runions and collaborators at the Algorithmic Botany group at the University of Calgary, this system can be used to simulate the growth of leaf venation patterns and tree-like structures, as well as many other vein-like systems like Gorgonian sea fans, circulatory systems, root systems, and more.\\n\\nThe original algorithm describes methods for generating both \"open\" (as seen in the example GIF) and \"closed\" venation networks, referring to whether or not secondary or tertiary branches connect together to form loops (or anastomoses).\\n\\n### Algorithm at a glance:\\n\\nFor both the open and closed variants of this algorithm, begin by placing a set of points on the canvas representing sources of either the auxin growth hormone (as in leaves) or ambient nutrients (as in trees).\\n\\n#### Open venation:\\n\\n* Associate each attractor with the single closest node within a pre-defined attraction distance.\\n* For each node that is associated with at least one attractor, calculate the average direction towards them as a normalized vector and generate a new node that extends in that direction at a pre-defined segment length (by scaling the normalized direction vector by that length).\\n* Remove any attractors that have nodes within a pre-defined kill distance around it.\\n\\n#### Closed venation:\\n\\n* Associate each attractor with all of the nodes that are both within a pre-defined attraction distance and within the attractor\\'s relative neighborhood.\\n* For each node that is associated with at least one attractor, calculate the average direction towards them as a normalized vector and generate a new node that extends in that direction at a pre-defined segment length (by scaling the normalized direction vector by that length).\\n* Remove any attractors that have been reached by all of their associated nodes.\\n\\n#### Auxin flux canalization:\\nThis is a process by which veins become thicker as they grow longer. The longer a vein gets, the more auxin flows through it (\"flux\"), causing veins to progressively thicken from their tips to their roots. \"Canalization\" references the process by which \"canals\" of water form.\\n\\n* Give each branch segment a uniform default thickness to start with.\\n* Beginning at each terminal node (that is, segments with no child segments), traverse \"upwards\" through each parent node, adding their child node thickness to their own until you reach a root node (a segment with no parent segment).\\n\\n## Features\\n\\n1. Supports both _open_ and _closed_ venation. Configurable via `./core/Defaults.js` or a local `Setting.js` file.\\n2. Growth can be constrained within _bounding shapes_. See `./core/Path.js` and `./core/Network.js`.\\n3. _Obstacles_ can be defined that growth must avoid. See `./core/Path.js` and `./core/Network.js`.\\n4. Simple SVG files can be loaded and parsed into either \"bounds\" or \"obstacle\" paths. See `./core/SVGLoader.js`.\\n5. Attractors can be placed along the edges of paths, which can in turn be scaled and moved, in order to model _marginal growth_. See `./marginal-growth/js/entry.js`.\\n6. Multiple vein networks can be created (just add more than one \"root\" vein to kick off growth).\\n7. Veins can be progressively thickened as they grow using a process called _auxin flux canalization_. Press `c` in any sketch to toggle it.\\n8. Vein transparency can be smoothly blended from tip to root using _opacity blending_ (a variation of auxin flux canalization). Press `p` in any sketch to toggle it.\\n9. Vein networks can be exported using `e` at any time. However, these networks get so complex so quickly that this can easily cause your browser to freeze - use at your own risk!\\n\\n## Implementation notes\\n\\nSee `./core` for common modules:\\n* `Attractor.js` - location of a single source of auxin growth hormone or other growth-promoting influence\\n* `Network.js` - manages the growth of nodes based on attractors and provided bounds and obstacles\\n* `Path.js` - arbitrary path consisting of points, used for either constraining growth (\"bounds\") or defining areas for growth to avoid (\"obstacle\").\\n* `AttractorPatterns.js` - functions for generating attractors arranged in various patterns (grids, noise, etc)\\n* `Node.js` - a single point in a branch\\n\\nA couple additional helper modules are also included there:\\n* `KeyboardInteractions.js` - a structure for handling common keyboard commands that every sketch should have\\n* `Utilities.js` - small helper functions like `random` and `lerp`\\n* `ColorPresets.js` - collection of pre-made color palettes for use in `Defaults.js`\\n* `Defaults.js` - collection of global variables used for configuring the behavior and display of the algorithm\\n  * Any variable can be overridden on a per-sketch basis using a local `Setting.js` file\\n* `SVGLoader.js` - utility for loading and parsing simple SVG documents to create Paths\\n\\n## Technologies used\\n* Native [Canvas API](https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API), specifically the [CanvasRenderingContext2D](https://developer.mozilla.org/en-US/docs/Web/API/CanvasRenderingContext2D) interface, for all drawing\\n* Vanilla ES6 JavaScript\\n* Webpack build system with live-reloading dev server\\n\\n## Packages used\\n* [KDBush](https://www.npmjs.com/package/kdbush) for KD-tree based spatial index\\n* [vec2](https://www.npmjs.com/package/vec2) for simple, fast 2D vector math\\n* [Webpack](https://webpack.js.org/) for modern JS (ES6) syntax, code modularization, bundling, and serving locally.\\n\\n## Install and run notes\\n1. Run `npm install` to get all packages\\n2. Run `npm run serve` to start up Webpack and launch the application in a browser window\\n\\n## References\\n\\n### Articles and papers:\\n\\n* [Modeling and visualization of leaf venation patterns](http://algorithmicbotany.org/papers/venation.sig2005.pdf) (PDF) paper by Adam Runions, Martin Fuhrer, Brendan Lane, Pavol Federl, Anne−Gaëlle Rolland−Lagan, and Przemyslaw Prusinkiewicz\\n* [Modeling Trees with a Space Colonization Algorithm](http://algorithmicbotany.org/papers/colonization.egwnp2007.large.pdf) (PDF) paper by Adam Runions, Brendan Lane, and Przemyslaw Prusinkiewicz\\n* [Procedurally Generated Trees with Space Colonization Algorithm in XNA C#](http://www.jgallant.com/procedurally-generating-trees-with-space-colonization-algorithm-in-xna/) by Jon Gallant\\n* [Part 26: Trees](http://www.sea-of-memes.com/LetsCode26/LetsCode26.html) by Michael Goodfellow\\n* [Hyphae](https://github.com/inconvergent/hyphae) (Python) by Anders Hoff (inconvergent) ([live examples and short writeup](https://inconvergent.net/generative/hyphae/))\\n* [Space Colonization Algorithm Part 1](https://bastiaanolij.blogspot.com/2014/12/space-colonization-algorithm-part-1.html) [[Part II](https://bastiaanolij.blogspot.com/2014/12/space-colonization-algorithm-part-2.html)] [[Part III](https://bastiaanolij.blogspot.com/2015/01/space-colonization-algorithm-part-3.html)] by Bastiaan Olij\\n* [Space colonization](https://github.com/jasonwebb/morphogenesis-resources#space-colonization) in Jason Webb\\'s Morphogenesis Resources repo.\\n\\n### Creative projects:\\n\\n* [Hyphae](https://n-e-r-v-o-u-s.com/shop/line.php?code=8), [Xylem](https://n-e-r-v-o-u-s.com/shop/line.php?code=6), [Folium](https://n-e-r-v-o-u-s.com/blog/?p=3983) series by Nervous System\\n  * Also see their [Xylem Experiments and Improvements](https://n-e-r-v-o-u-s.com/blog/?p=1218) write-up\\n* [Bromeliad](https://n-e-r-v-o-u-s.com/shop/product.php?code=286) and [Calyx](https://n-e-r-v-o-u-s.com/shop/product.php?code=285&search=download) lamps by Nervous System\\n* [Space Colonization Experiments](https://www.youtube.com/watch?v=D9Z3jJ87kzg) by David Ferreira\\n\\n### Code projects:\\n\\n* [ofxSpaceColoinzation](https://github.com/edap/ofxSpaceColonization) add-on for openFrameworks\\n* [space-colonization](https://github.com/nicknikolov/space-colonization) (JavaScript) by Nick Nikolov\\n* [Dendrite](https://github.com/mattatz/Dendrite) (Unity) by mattatz\\n* [Grower](https://github.com/joesfer/Grower) (Maya plugin) by Jose Esteve\\n* [Venation](https://github.com/nielmclaren/Venation) (Processing) by Niel McLaren\\n* [hyphae](https://github.com/jblondin/hyphae) (Julia) by Jamie Blondin\\n* [hyphae](https://github.com/inconvergent/hyphae) and [hyphae_ani](https://github.com/inconvergent/hyphae_ani) by Anders Hoff (inconvergent)\\n\\n### Videos:\\n\\n* [Coding Challenge #17: Fractal Trees - Space Colonization](https://www.youtube.com/watch?v=kKT0v3qhIQY) by Daniel Shiffman ([Github repo](https://github.com/CodingTrain/website/tree/master/CodingChallenges/CC_017_SpaceColonizer) with source code for p5.js and Processing)\\n* [VEX in Houdini: Space Colonization](https://vimeo.com/231315378) (Houdini + VEX) by Entagma\\n  * [SCA-2.1](https://vimeo.com/252447953) variation by IQCoo with obstacle avoidance, increasing path widths, heat transfer visualization.\\n\\n## Screenshots\\n\\n![Basic experiment screenshot](images/basic-screenshot.png)\\n\\n![Bounds experiment screenshot](images/bounds-screenshot.png)\\n\\n![Obstacles experiment screenshot](images/obstacles-screenshot.png)\\n\\n![Marginal growth experiment screenshot](images/marginal-growth-screenshot.png)'},\n",
       " {'repo': 'facebookresearch/TimeSformer',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"# TimeSformer\\n\\nThis is an official pytorch implementation of our ICML 2021 paper [Is Space-Time Attention All You Need for Video Understanding?](https://arxiv.org/pdf/2102.05095.pdf). In this repository, we provide PyTorch code for training and testing our proposed TimeSformer model. TimeSformer provides an efficient video classification framework that achieves state-of-the-art results on several video action recognition benchmarks such as Kinetics-400.\\n\\nIf you find TimeSformer useful in your research, please use the following BibTeX entry for citation.\\n\\n```BibTeX\\n@inproceedings{gberta_2021_ICML,\\n    author  = {Gedas Bertasius and Heng Wang and Lorenzo Torresani},\\n    title = {Is Space-Time Attention All You Need for Video Understanding?},\\n    booktitle   = {Proceedings of the International Conference on Machine Learning (ICML)}, \\n    month = {July},\\n    year = {2021}\\n}\\n```\\n\\n# Model Zoo\\n\\nWe provide TimeSformer models pretrained on Kinetics-400 (K400), Kinetics-600 (K600), Something-Something-V2 (SSv2), and HowTo100M datasets.\\n\\n| name | dataset | # of frames | spatial crop | acc@1 | acc@5 | url |\\n| --- | --- | --- | --- | --- | --- | --- |\\n| TimeSformer | K400 | 8 | 224 | 77.9 | 93.2 | [model](https://www.dropbox.com/s/g5t24we9gl5yk88/TimeSformer_divST_8x32_224_K400.pyth?dl=0) |\\n| TimeSformer-HR | K400 | 16 | 448 | 79.6 | 94.0 | [model](https://www.dropbox.com/s/6f0x172lpqy3oxt/TimeSformer_divST_16x16_448_K400.pyth?dl=0) |\\n| TimeSformer-L | K400 | 96 | 224 | 80.6 | 94.7 | [model](https://www.dropbox.com/s/r1iuxahif3sgimo/TimeSformer_divST_96x4_224_K400.pyth?dl=0) |\\n\\n| name | dataset | # of frames | spatial crop | acc@1 | acc@5 | url |\\n| --- | --- | --- | --- | --- | --- | --- |\\n| TimeSformer | K600 | 8 | 224 | 79.1 | 94.4 | [model](https://www.dropbox.com/s/4h2qt41m2z3aqrb/TimeSformer_divST_8x32_224_K600.pyth?dl=0) |\\n| TimeSformer-HR | K600 | 16 | 448 | 81.8 | 95.8 | [model](https://www.dropbox.com/s/ft1e92g2vhvxecv/TimeSformer_divST_16x16_448_K600.pyth?dl=0) |\\n| TimeSformer-L | K600 | 96 | 224 | 82.2 | 95.6 | [model](https://www.dropbox.com/s/857rx6xeclxfhdg/TimeSformer_divST_96x4_224_K600.pyth?dl=0) |\\n\\n| name | dataset | # of frames | spatial crop | acc@1 | acc@5 | url |\\n| --- | --- | --- | --- | --- | --- | --- |\\n| TimeSformer | SSv2 | 8 | 224 | 59.1 | 85.6 | [model](https://www.dropbox.com/s/tybhuml57y24wpm/TimeSformer_divST_8_224_SSv2.pyth?dl=0) |\\n| TimeSformer-HR | SSv2 | 16 | 448 | 61.8 | 86.9 | [model](https://www.dropbox.com/s/9t68uzk8w2fpfnv/TimeSformer_divST_16_448_SSv2.pyth?dl=0) |\\n| TimeSformer-L | SSv2 | 64 | 224 | 62.0 | 87.5 | [model](https://www.dropbox.com/s/3f1rm2al8mhprwa/TimeSformer_divST_64_224_SSv2.pyth?dl=0) |\\n\\n| name | dataset | # of frames | spatial crop | single clip coverage | acc@1 | url |\\n| --- | --- | --- | --- | --- | --- | --- |\\n| TimeSformer | HowTo100M | 8 | 224 | 8.5s | 56.8 | [model](https://www.dropbox.com/s/9v8hcm88b9tc6ff/TimeSformer_divST_8x32_224_HowTo100M.pyth?dl=0) |\\n| TimeSformer | HowTo100M | 32 | 224 | 34.1s | 61.2 | [model](https://www.dropbox.com/s/4roflx4q1gscu85/TimeSformer_divST_32x32_224_HowTo100M.pyth?dl=0) |\\n| TimeSformer | HowTo100M | 64 | 448 | 68.3s | 62.2 | [model](https://www.dropbox.com/s/15bvqltl1j5vyp3/TimeSformer_divST_64x32_224_HowTo100M.pyth?dl=0) |\\n| TimeSformer | HowTo100M | 96 | 224 | 102.4s | 62.6 | [model](https://www.dropbox.com/s/t2mzgahnfhgakma/TimeSformer_divST_96x32_224_HowTo100M.pyth?dl=0) |\\n\\nWe note that these models were re-trained using a slightly different implementation than the one used in the paper. Therefore, there might be a small difference in performance compared to the results reported in the paper.\\n\\nYou can load the pretrained models as follows:\\n\\n```python\\nimport torch\\nfrom timesformer.models.vit import TimeSformer\\n\\nmodel = TimeSformer(img_size=224, num_classes=400, num_frames=8, attention_type='divided_space_time',  pretrained_model='/path/to/pretrained/model.pyth')\\n\\ndummy_video = torch.randn(2, 3, 8, 224, 224) # (batch x channels x frames x height x width)\\n\\npred = model(dummy_video,) # (2, 400)\\n```\\n\\n# Installation\\n\\nFirst, create a conda virtual environment and activate it:\\n```\\nconda create -n timesformer python=3.7 -y\\nsource activate timesformer\\n```\\n\\nThen, install the following packages:\\n\\n- torchvision: `pip install torchvision` or `conda install torchvision -c pytorch`\\n- [fvcore](https://github.com/facebookresearch/fvcore/): `pip install 'git+https://github.com/facebookresearch/fvcore'`\\n- simplejson: `pip install simplejson`\\n- einops: `pip install einops`\\n- timm: `pip install timm`\\n- PyAV: `conda install av -c conda-forge`\\n- psutil: `pip install psutil`\\n- scikit-learn: `pip install scikit-learn`\\n- OpenCV: `pip install opencv-python`\\n- tensorboard: `pip install tensorboard`\\n\\nLastly, build the TimeSformer codebase by running:\\n```\\ngit clone https://github.com/facebookresearch/TimeSformer\\ncd TimeSformer\\npython setup.py build develop\\n```\\n\\n# Usage\\n\\n## Dataset Preparation\\n\\nPlease use the dataset preparation instructions provided in [DATASET.md](timesformer/datasets/DATASET.md).\\n\\n## Training the Default TimeSformer\\n\\nTraining the default TimeSformer that uses divided space-time attention, and operates on 8-frame clips cropped at 224x224 spatial resolution, can be done using the following command:\\n\\n```\\npython tools/run_net.py \\\\\\n  --cfg configs/Kinetics/TimeSformer_divST_8x32_224.yaml \\\\\\n  DATA.PATH_TO_DATA_DIR path_to_your_dataset \\\\\\n  NUM_GPUS 8 \\\\\\n  TRAIN.BATCH_SIZE 8 \\\\\\n```\\nYou may need to pass location of your dataset in the command line by adding `DATA.PATH_TO_DATA_DIR path_to_your_dataset`, or you can simply add\\n\\n```\\nDATA:\\n  PATH_TO_DATA_DIR: path_to_your_dataset\\n```\\n\\nTo the yaml configs file, then you do not need to pass it to the command line every time.\\n\\n## Using a Different Number of GPUs\\n\\nIf you want to use a smaller number of GPUs, you need to modify .yaml configuration files in [`configs/`](configs/). Specifically, you need to modify the NUM_GPUS, TRAIN.BATCH_SIZE, TEST.BATCH_SIZE, DATA_LOADER.NUM_WORKERS entries in each configuration file. The BATCH_SIZE entry should be the same or higher as the NUM_GPUS entry. In [`configs/Kinetics/TimeSformer_divST_8x32_224_4gpus.yaml`](configs/Kinetics/TimeSformer_divST_8x32_224_4gpus.yaml), we provide a sample configuration file for a 4 GPU setup.\\n\\n\\n## Using Different Self-Attention Schemes\\n\\nIf you want to experiment with different space-time self-attention schemes, e.g., space-only or joint space-time attention, use the following commands:\\n\\n\\n```\\npython tools/run_net.py \\\\\\n  --cfg configs/Kinetics/TimeSformer_spaceOnly_8x32_224.yaml \\\\\\n  DATA.PATH_TO_DATA_DIR path_to_your_dataset \\\\\\n  NUM_GPUS 8 \\\\\\n  TRAIN.BATCH_SIZE 8 \\\\\\n```\\n\\nand\\n\\n```\\npython tools/run_net.py \\\\\\n  --cfg configs/Kinetics/TimeSformer_jointST_8x32_224.yaml \\\\\\n  DATA.PATH_TO_DATA_DIR path_to_your_dataset \\\\\\n  NUM_GPUS 8 \\\\\\n  TRAIN.BATCH_SIZE 8 \\\\\\n```\\n\\n## Training Different TimeSformer Variants\\n\\nIf you want to train more powerful TimeSformer variants, e.g., TimeSformer-HR (operating on 16-frame clips sampled at 448x448 spatial resolution), and TimeSformer-L (operating on 96-frame clips sampled at 224x224 spatial resolution), use the following commands:\\n\\n```\\npython tools/run_net.py \\\\\\n  --cfg configs/Kinetics/TimeSformer_divST_16x16_448.yaml \\\\\\n  DATA.PATH_TO_DATA_DIR path_to_your_dataset \\\\\\n  NUM_GPUS 8 \\\\\\n  TRAIN.BATCH_SIZE 8 \\\\\\n```\\n\\nand\\n\\n```\\npython tools/run_net.py \\\\\\n  --cfg configs/Kinetics/TimeSformer_divST_96x4_224.yaml \\\\\\n  DATA.PATH_TO_DATA_DIR path_to_your_dataset \\\\\\n  NUM_GPUS 8 \\\\\\n  TRAIN.BATCH_SIZE 8 \\\\\\n```\\n\\nNote that for these models you will need a set of GPUs with ~32GB of memory.\\n\\n## Inference\\n\\nUse `TRAIN.ENABLE` and `TEST.ENABLE` to control whether training or testing is required for a given run. When testing, you also have to provide the path to the checkpoint model via TEST.CHECKPOINT_FILE_PATH.\\n```\\npython tools/run_net.py \\\\\\n  --cfg configs/Kinetics/TimeSformer_divST_8x32_224_TEST.yaml \\\\\\n  DATA.PATH_TO_DATA_DIR path_to_your_dataset \\\\\\n  TEST.CHECKPOINT_FILE_PATH path_to_your_checkpoint \\\\\\n  TRAIN.ENABLE False \\\\\\n```\\n\\n## Single-Node Training via Slurm\\n\\nTo train TimeSformer via Slurm, please check out our single node Slurm training script [`slurm_scripts/run_single_node_job.sh`](slurm_scripts/run_single_node_job.sh).\\n\\n\\n## Multi-Node Training via Submitit\\n\\nDistributed training is available via Slurm and submitit\\n\\n```\\npip install submitit\\n```\\n\\nTo train TimeSformer model on Kinetics using 4 nodes with 8 gpus each use the following command:\\n```\\npython tools/submit.py --cfg configs/Kinetics/TimeSformer_divST_8x32_224.yaml --job_dir  /your/job/dir/${JOB_NAME}/ --num_shards 4 --name ${JOB_NAME} --use_volta32\\n```\\n\\nWe provide a script for launching slurm jobs in [`slurm_scripts/run_multi_node_job.sh`](slurm_scripts/run_multi_node_job.sh).\\n\\n## Finetuning\\n\\nTo finetune from an existing PyTorch checkpoint add the following line in the command line, or you can also add it in the YAML config:\\n\\n```\\nTRAIN.CHECKPOINT_FILE_PATH path_to_your_PyTorch_checkpoint\\nTRAIN.FINETUNE True\\n```\\n\\n## HowTo100M Dataset Split\\n\\nIf you want to experiment with the long-term video modeling task on HowTo100M, please download the train/test split files from [here](https://www.dropbox.com/sh/ttvsxwqypijjuda/AACmJx1CnddW6cVBoc21eSuva?dl=0).\\n\\n\\n# Environment\\n\\nThe code was developed using python 3.7 on Ubuntu 20.04. For training, we used four GPU compute nodes each node containing 8 Tesla V100 GPUs (32 GPUs in total). Other platforms or GPU cards have not been fully tested.\\n\\n# License\\n\\nThe majority of this work is licensed under [CC-NC 4.0 International license](LICENSE). However portions of the project are available under separate license terms: [SlowFast](https://github.com/facebookresearch/SlowFast) and [pytorch-image-models](https://github.com/rwightman/pytorch-image-models) are licensed under the Apache 2.0 license.\\n\\n# Contributing\\n\\nWe actively welcome your pull requests. Please see [CONTRIBUTING.md](CONTRIBUTING.md) and [CODE_OF_CONDUCT.md](CODE_OF_CONDUCT.md) for more info.\\n\\n# Acknowledgements\\n\\nTimeSformer is built on top of [PySlowFast](https://github.com/facebookresearch/SlowFast) and [pytorch-image-models](https://github.com/rwightman/pytorch-image-models) by [Ross Wightman](https://github.com/rwightman). We thank the authors for releasing their code. If you use our model, please consider citing these works as well:\\n\\n```BibTeX\\n@misc{fan2020pyslowfast,\\n  author =       {Haoqi Fan and Yanghao Li and Bo Xiong and Wan-Yen Lo and\\n                  Christoph Feichtenhofer},\\n  title =        {PySlowFast},\\n  howpublished = {\\\\url{https://github.com/facebookresearch/slowfast}},\\n  year =         {2020}\\n}\\n```\\n\\n```BibTeX\\n@misc{rw2019timm,\\n  author = {Ross Wightman},\\n  title = {PyTorch Image Models},\\n  year = {2019},\\n  publisher = {GitHub},\\n  journal = {GitHub repository},\\n  doi = {10.5281/zenodo.4414861},\\n  howpublished = {\\\\url{https://github.com/rwightman/pytorch-image-models}}\\n}\\n```\\n\"},\n",
       " {'repo': 'AlexanderDzhoganov/ksp-advanced-flybywire',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '## Advanced Fly-By-Wire v1.7-beta\\n### Kerbal Space Program input system overhaul mod\\n\\nLatest version - 1.7-beta for KSP 1.1 and later\\n\\n[Download v1.7-beta (Windows, KSP x64)](https://github.com/AlexanderDzhoganov/ksp-advanced-flybywire/raw/master/builds/ksp-advanced-flybywire_v1.7_x64.zip)\\n\\n[Download v1.7-beta (Linux, KSP x86/x64)](https://github.com/AlexanderDzhoganov/ksp-advanced-flybywire/raw/master/builds/ksp-advanced-flybywire_v1.7-linux.zip) (Requires libsdl2)\\n\\n[Licensed under the MIT License](https://github.com/AlexanderDzhoganov/ksp-advanced-flybywire/blob/master/LICENSE)\\n\\nYou can also find the mod on:\\n- [Official forums](http://forum.kerbalspaceprogram.com/threads/95022-0-24-2-Advanced-Fly-by-wire-v1-0-%28Better-controller-support%29)\\n- [KerbalStuff](https://kerbalstuff.com/mod/232/Advanced%20Fly-By-Wire)\\n\\n#### Installation\\nOn Windows, simply extract the ZIP file into the root installation folder for your game. Often this is directly in `%ProgramFiles%`. If you are using Steam, it will be located in `%ProgramFiles%\\\\Steam\\\\steamapps\\\\common\\\\Kerbal Space Program`.\\n\\n### What is this?\\nThis is a mod for [Kerbal Space Program](http://kerbalspaceprogram.com), a spaceship building/ space exploration game by Squad.\\nIt dramatically enhances the stock input system with a bunch of fixes and many new features.\\n\\n### How is it better than stock?\\n\\n- Edit your control setup at any time during flight, no need to go back to the main menu to change bindings.\\n- Supports almost all controller types through the SDL wrapper.\\n- Native support for the Xbox 360 controller (and PS3 controller using MotionJoy) through the XInput wrapper. A built-in example preset is available.\\n- Unlimited number of controller buttons and axes\\n- Full keyboard & mouse support\\n- Lower latency and better analog precision than KSP\\'s stock input\\n- Smart dead-zone detection and analog calibration \\n- Remaps analog inputs to achieve better precision\\n- Acceleration-based discrete inputs for precise keyboard flight\\n- Supports key combinations with an infinite number of keys\\n- Multiple presets per controller\\n- Extremely simple to configure and use\\n- Works alongside the stock input system. The mod will not override or break your current setup.\\n\\n### How to use\\n![screenshot](http://i.imgur.com/NgIVDIQ.png)\\n\\nThe mod adds a new button to the mod toolbar.\\n\\nClick the game controller icon or press Shift + L during flight to bring up Fly-By-Wire\\'s main configuration screen.\\n\\nFrom there you will see a list of detected controllers. You can click on \"Enable\" to enable a controller from the list - two new buttons will appear - \"Presets\" and \"Configure\".\\n\\n\"Presets\" will open up the preset editor which is very similar to KSP\\'s stock bindings editor. Using the preset editor you can modify your controller layout at any time.\\n\\n\"Configure\" will open up the controller configuration screen. It allows you to set some configuration values as well as calibrate the controller if necessary.\\n\\nAfter setting up your controller you should save your game (by using quicksave or exiting to space center), which will automatically save your controller configuration as well.\\n\\n### Operating system compatibility\\nFully tested and compatible with KSP x64 on Windows and Linux operating systems.\\nXInput support unavailable on Linux.\\n\\n### Performance considerations\\nThe mod is extremely lightweight both on performance and memory. It only does a bit of arithmetic and remapping of incoming inputs which should have\\nno noticeable effect on CPU usage. Memory usage is in the order of a few megabytes.\\n\\n### Bug reports\\nPlease report any bugs using [GitHub\\'s issue tracker](https://github.com/AlexanderDzhoganov/ksp-advanced-flybywire/issues).\\n\\n### Changelog\\n\\n[Click here for changelog](https://raw.githubusercontent.com/AlexanderDzhoganov/ksp-advanced-flybywire/master/CHANGELOG)\\n\\n'},\n",
       " {'repo': 'jeffreymorganio/d3-space-filler-explorer',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# d3-space-filler-explorer\\n\\nThe **D3 Space Filler Explorer** is an [Electron](http://electron.atom.io \"Visit the Electron website\") application that uses multiple [D3](http://d3js.org \"Visit the D3 website\")  visualizations to present the space consumed by the files in a hierarchical directory structure.\\n\\nThe used and free space of each disk on the system is visualized with a two-segment [pie chart](https://github.com/mbostock/d3/wiki/Pie-Layout \"Learn more about the D3 Pie layout at GitHub\"). The space consumed by each of the files in a user-selectable directory is visualized with a [treemap](https://github.com/mbostock/d3/wiki/Treemap-Layout \"Learn more about the D3 Treemap layout at GitHub\"). The directory data structure visualized by the D3 treemap is generated by my [file-size-tree-js](https://www.npmjs.com/package/file-size-tree-js \"Visit the file-size-tree-js NPM website\") package.\\n\\nThe Space Filler Explorer demonstrates how to organise the architecture of an Electron app and how to integrate multiple D3 visualizations.\\n\\nWatch the  [Space Filler Explorer](https://www.youtube.com/watch?v=IORmY0rn2S8) video on YouTube.\\n\\n<img src=\"http://jeffreymorganio.github.io/d3-space-filler-explorer/d3-space-filler-explorer.png\" alt=\"D3 Space Filler Explorer\" width=\"100%\">\\n\\n\\n## Requirements\\n\\nAs an Electron application, the Space Filler Explorer is built from cross-platform web technologies (HTML, CSS and JavaScript) and [Node.js](https://nodejs.org \"Visit the Node.js website\") modules hosted on the [npmjs.org](https://www.npmjs.com \"Visit the npm registry\") registry. To run the Space Filler Explorer, you will need to [install Node.js](https://docs.npmjs.com/getting-started/installing-node \"Learn how to install Node.js at npmjs.org\").\\n\\nAfter cloning the `d3-space-filler-explorer` repository, run `npm install` to download the Node.js modules on which the Space Filler Explorer application depends.\\n\\n## Running\\n\\nTo run the Space Filler Explorer application, invoke the `start` script in the `package.json` file:\\n\\n```\\nnpm start\\n```\\n\\nThe Space Filler Explorer is not currently compatible with Windows because one of its Node.js dependencies ([nodejs-disks](https://www.npmjs.com/package/nodejs-disks \"Learn more about nodejs-disks at NPM\")) is not compatible with Windows.\\n\\n## Packaging\\n\\nElectron applications can be packaged to run as native, double-clickable applications. The following commands package the Space Filler Explorer as native applications on Mac OS X and Linux:\\n\\n- `npm run package-osx`\\n- `npm run package-linux`\\n\\n### How Packaging Works\\n\\nThis section explains how packaging for Mac OS X works. However, packaging for Linux (and Windows) follows a similar pattern. For more information about packaging on all three platforms, see the documentation for the [`electron-packager`](https://www.npmjs.com/package/electron-packager \"Learn more about electron-packager at NPM\") tool.\\n\\nThe `package.json` script contains the `package-osx` script for packaging the Space Filler Explorer as a Mac OS X application:\\n\\n```\\n\"package-osx\": \"electron-packager ./ SpaceFillerExplorer --platform=darwin --arch=x64 --icon=app/icons/Icon.icns --version=0.30.4 --ignore=node_modules/electron-* --overwrite\",\\n```\\n\\nThe `package-osx` script uses the `electron-packager` tool to package the Space Filler Explorer as a 64-bit (`--arch=x64`) Mac OS (`--platform=darwin`) application into the `SpaceFillerExplorer.app` file, which is stored in the `SpaceFillerExplorer-darwin-x64` folder.\\n\\nMac OS X applications require icons in [various sizes and resolutions](https://developer.apple.com/library/mac/documentation/UserExperience/Conceptual/OSXHIGuidelines/Designing.html \"Learn more about Mac OS X application icons at Apple\") to be stored in a `.icns` file. The icons for the Space Filler Explorer are included with the command-line parameter `--icon=app/Icon.icns`.\\n\\nThe remaining command-line parameters provide `electron-packager` with the Electron version (`--version=0.30.4`), tell `electron-packager` which folders to exclude from the `SpaceFillerExplorer.app` file (`--ignore=node_modules/electron-*`), and allow `electron-packager` to overwrite a previously-created `SpaceFillerExplorer-darwin-x64` folder without a warning (`--overwrite`).\\n\\n## Performance\\n\\nTwo actions contribute to the time taken to visualize the space occupied by the files in a directory structure:\\n\\n1. Building the JSON data structure that describes the directory structure.\\n2. Building and rendering the D3 treemap.\\n\\nFor small folder structures, building and rendering the D3 treemap takes most of the time. For large folder structures, building the JSON data structure takes most of the time.\\n'},\n",
       " {'repo': 'spacetypegenerator/spacetypegenerator.github.io',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': ''},\n",
       " {'repo': 'ugurkanates/SpaceXReinforcementLearning',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# SpaceX Falcon 9 Landing with Reinforcement Learning \\n\\n# ![GIF](videogifwhatever.gif)\\n\\nThe Falcon 9, developed by aerospace company SpaceX, means it is now possible to reuse the first-stage of the rocket, by flying it safely back to Earth.\\n\\nAn achievement once seemed so impossible that it lead creation of multiple “fake SpaceX landing videos-explanations” now is widely agreed upon about how amazing behind the tech related to it.\\n\\nWhile today I’m not here nor capable of you giving rocket engineering course just wanted to show this quick little diagram from SpaceX to understand a little bit more.\\n\\nEnvironment is created by [Sven Niederberger](https://github.com/EmbersArc) and it\\'s based on LunarLander environment by OpenAI.\\n\\n \\n\\nIt offers you to include or exclude Velocity information for your Agent\\'s Observation. \\n\\nYou can either train in **DISCRETE ACTION**  or **CONTINUOUS ACTION** modes.\\nYou can find more information at (gym/env/box2d/rocket_lander.py)\\n\\n\\n\\n### STATE VARIABLES  \\nThe state consists of the following variables:\\n  * X position  \\n  * Y position  \\n  * Angle  \\n  * First Leg Ground Contact Indicator  \\n  * Second Leg Ground Contact Indicator  \\n  * Throttle  \\n  * Engine Gimbal  \\n  \\nIf VEL_STATE is set to true, the velocities are included:  \\n  * X velocity  \\n  * Y velocity  \\n  * Angular Velocity  \\n  \\nAll state variables are normalized for improved training.\\n    \\n\\n## Discrete control inputs are:\\n* Gimbal Left\\n* Gimbal Right\\n* Throttle Up\\n* Throttle Down\\n* Use First Control Thruster\\n* Use Second Control Thruster\\n* No Action\\n## Continuous control inputs are:\\n* Gimbal (Left/Right)\\n* Throttle (Up/Down)\\n* Control Thruster (Left/Right)\\n\\nSince this is repository includes a modified version of **OpenAI GYM** , I won\\'t recommend to install with PIP.\\n\\nYou can basically struct your code to execute from this directory so your own installed GYM package wouldn\\'t interfere with this project. \\n\\nThere are also Google Colab Notebooks that\\'s ready to train and can record videos & save your network modules into your Google Drive.\\nIf you work on local machine you can basically skip Drive related commands.\\n\\nThere are 3 agents in this project.I left ***PPO*** as default. If you want to run ***D4PG*** or ***SAC*** algorithms just rename LIB folders correctly\\n(lib -> lib_PPO ,  libSAC-> lib  or libD4PG -> lib) you get the idea.\\n\\nAgent creation done thanks to PTan library. You can find more information about it link below.\\n\\n[PTan Agent Network Library](https://github.com/Shmuma/ptan)\\n\\nIf you have anyting to ask regarding to project or RL in general feel free to ask me via e-mail in my profile.\\nHave a nice flight !\\n\\n\\n## Tensorboard X \\n\\nIn order to see logs of your train run , you can execute command below\\n\\ntensorboard --logdir runs/ --host localhost\\n\\n## Name of training run\\n\\nDon\\'t forget to change training run each time you execute code or otherwise it will overwrite of older files.\\nAlso don\\'t forget to change video_directory if you want to save videos.\\n\\n## How to test \\n\\nI already saved bunch of network models in **/rocket_saved_networks/** folder.  You can try them out with play function ( feel free to edit if you have custom agents or if you try other networks you may need to load actor model etc.)\\n\\n\\n[![Everything Is AWESOME](youtube.png)](https://www.youtube.com/watch?v=z-j82WmZojY \"Everything Is AWESOME\")\\n\\n## Detailed Medium Post\\n\\nhttps://medium.com/@paypaytr/spacex-falcon-9-landing-with-rl-7dde2374eb71\\n'},\n",
       " {'repo': 'ThatEidolon/Python',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Python\\nScratch space for learning python\\n'},\n",
       " {'repo': 'mathieudutour/scroll-through-time',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': \"# scroll-through-time\\n\\nTwo fingers (horizontal) scroll moves through time instead of space in [Atom](https://atom.io/)\\n\\n![demo](https://github.com/mathieudutour/scroll-through-time/raw/master/screencast.gif)\\n\\n## Installation\\n\\n```\\napm install scroll-through-time\\n```\\n\\n## Why?\\n![If used with software that could keep up, a scroll wheel mapped to send a stream of 'undo' and 'redo' events could be kind of cool.](https://imgs.xkcd.com/comics/borrow_your_laptop.png)\\n_ xkcd ([#1806](https://xkcd.com/1806/))\\n\"},\n",
       " {'repo': 'contentful/gallery-app-react',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Gallery\\n\\nThis is a React application example for the [Contentful][1] gallery space template.\\n\\n![The gallery App](./screenshot.png)\\n\\n\\n[Contentful](https://www.contentful.com) provides a content infrastructure for digital teams to power content in websites, apps, and devices. Unlike a CMS, Contentful was built to integrate with the modern software stack. It offers a central hub for structured content, powerful management and delivery APIs, and a customizable web app that enable developers and content creators to ship digital products faster.\\n\\n## Usage\\n\\n- create a space with the \"Gallery\" space template on [Contentful][1]\\n- clone this repo and run `npm i`\\n- edit `deliveryAccessToken`, `spaceId` and `galleryTypeId` (id of the content type `Photo Gallery`) in `config.js` included in the project root\\n- run `npm start`\\n- open `localhost:9020/` to see it in action\\n\\n## License\\n\\nCopyright (c) 2016 Contentful GmbH. See LICENSE for further details.\\n\\n[1]: https://www.contentful.com\\n'},\n",
       " {'repo': 'Robinseibold/Unity-URP-Outlines',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': \"# Outlines\\nA custom renderer feature for screen space outlines based on [Erik Roystan Ross Outline Shader](https://roystan.net/articles/outline-shader.html).\\n\\nfor more specific implementation details see [YouTube video](https://youtu.be/LMqio9NsqmM). Apart from minor adjustments no further improvements has been added.\\n\\n## Usage\\n- Add files to your project\\n- Enable depth texture in the Universal Render Pipeline Asset\\n- Add the 'Screen Space Outlines' renderer feature to the Universal Renderer Data\\n- Adjust 'Screen Space Outlines' settings\\n- Go to Project Settings > Graphics and add the 3 new ones (Outlines, UnlitColor, and ViewSpaceNormals) to the list of Always Included Shaders. This is needed to make sure the shaders are included when building the project for a real device.\\n\\n## Known Issues\\n- Does not work with objects that utilizes displacement shaders, the normal texture generation does not take the displacement into account\\n- Does not work with MSAA, the current work around is to use FXAA or SMAA on the camera instead.\\n\\n## Future Plans\\nI'm continuously expanding on this implementation, below are some things planned for future releases:\\n- Outline generation anti aliasing\\n- Varying thickness of outlines based on distance from camera\\n- Fog affected outlines\\n- and much more\\n\"},\n",
       " {'repo': 'johnymontana/spacetime-reviews',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '## spacetime-reviews\\n\\nA React app to demonstrate how to use the spatial and temporal functionality introduced in Neo4j 3.4. It makes use of:\\n\\n* [create-react-app](https://github.com/facebook/create-react-app)\\n* [neo4j-javascript-driver](https://github.com/neo4j/neo4j-javascript-driver)\\n* [Mapbox GL JS](https://www.mapbox.com/mapbox-gl-js/api/)\\n* [Nivo charts](http://nivo.rocks/)\\n\\n![](img/screenshot.png)\\n\\n## Installation \\n\\nSet environment variables:\\n\\n```\\nREACT_APP_NEO4J_URI=XXX\\nREACT_APP_NEO4J_USER=XXX\\nREACT_APP_NEO4J_PASSWORD=XXX\\nREACT_APP_MAPBOX_TOKEN=XXX\\n```\\n\\nthese can be added to `.env`\\n\\nClone this git repo, and then\\n\\n```\\nnpm install\\nnpm start\\n```\\n\\n'},\n",
       " {'repo': 'Helioviewer-Project/JHelioviewer-SWHV',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': \"JHelioviewer\\n============\\n\\n[![Coverity Scan Build Status](https://scan.coverity.com/projects/9940/badge.svg)](https://scan.coverity.com/projects/9940)\\n[![DOI](https://zenodo.org/badge/50179170.svg)](https://zenodo.org/badge/latestdoi/50179170)\\n\\nWebsite: http://www.jhelioviewer.org/\\n\\nUser manual: http://swhv.oma.be/user_manual/\\n\\nAbout\\n-----\\n\\nJHelioviewer is a visualization tool for solar physics data based on the JPEG\\n2000 image compression standard, and part of the open source ESA/NASA Helioviewer\\nProject (https://github.com/Helioviewer-Project/).\\n\\nJPEG 2000 offers many useful features and has the potential to revolutionize the\\nway high-resolution image data are disseminated and analyzed. Using JPEG 2000, data\\ncan be served to a client in highly compressed, progressive in quality, and\\nregion-of-interest based form. These features make it possible to minimize the data\\ntransmitted while maximizing the use of the data that is transmitted.\\n\\nSolar observatories are providing the world-wide community with a wealth of data,\\ncovering wide time ranges (e.g. Solar and Heliospheric Observatory, SOHO), multiple\\nviewpoints (Solar TErrestrial RElations Observatory, STEREO), and returning large\\namounts of data (Solar Dynamics Observatory, SDO). In particular, the large volume\\nof SDO data presents challenges; the data are available only from a few repositories,\\nand full-disk, full-cadence data for reasonable durations of scientific interest are\\ndifficult to download, due to their size and the download rates available to most\\nusers. From a scientist's perspective this poses three problems: accessing, browsing,\\nand finding interesting data as efficiently as possible.\\n\\nWith JHelioviewer users can visualise the Sun for any time period between September\\n1991 and today; they can perform basic image processing in real time, track features\\non the Sun, and interactively overlay magnetic field extrapolations. The software\\nintegrates solar event data and a timeline display. Once an interesting event has\\nbeen identified, science quality data can be accessed for in-depth analysis.\\n\\nAs support for the science planning of the Solar Orbiter mission, JHelioviewer\\noffers a virtual camera model that enables users to set the vantage point to the\\nlocation of a spacecraft or celestial body at any given time.\\n\\nReferences:\\n\\n- JHelioviewer paper by Mueller et al., Astronomy & Astrophysics, 2017:\\n  https://doi.org/10.1051/0004-6361/201730893\\n\\n- JHelioviewer paper by Mueller et al., Computing Science and Engineering, 2009:\\n  http://jhelioviewer.org/pub/Mueller+al_CiSE2009.pdf\\n\\n- Information about JPEG 2000:\\n  http://wiki.helioviewer.org/wiki/JPEG_2000\\n\\nReport a Problem or Idea\\n------------------------\\n\\nIf you have a problem or have ideas for improvements please report them at:\\n\\nhttps://github.com/Helioviewer-Project/JHelioviewer-SWHV/issues\\n\\nWe will try to solve your problem as fast as possible. The more details you can\\nprovide, the easier it is for us. For example your system specifications\\n(hardware and software with version numbers) as well as a detailed description\\nwhat you did (so we can reproduce the problem) are very helpful. If possible,\\nplease also be ready to provide the log files from the session where the problem\\noccured. The log files can be found in the Logs/ directory of your JHelioviewer\\nhome folder. You can find the correct log file by searching for the correct\\nstart date. The files have the form:\\n\\nJHV_'year'-'month'-'day'_'hours'.'minutes'.'seconds'.log\\n\"},\n",
       " {'repo': 'Thinkful-Ed/spaced-repetition-api',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': \"# Spaced repetition API!\\n\\n## Local dev setup\\n\\nIf using user `dunder-mifflin`:\\n\\n```bash\\nmv example.env .env\\ncreatedb -U dunder-mifflin spaced-repetition\\ncreatedb -U dunder-mifflin spaced-repetition-test\\n```\\n\\nIf your `dunder-mifflin` user has a password be sure to set it in `.env` for all appropriate fields. Or if using a different user, update appropriately.\\n\\n```bash\\nnpm install\\nnpm run migrate\\nenv MIGRATION_DB_NAME=spaced-repetition-test npm run migrate\\n```\\n\\nAnd `npm test` should work at this point\\n\\n## Configuring Postgres\\n\\nFor tests involving time to run properly, configure your Postgres database to run in the UTC timezone.\\n\\n1. Locate the `postgresql.conf` file for your Postgres installation.\\n   1. E.g. for an OS X, Homebrew install: `/usr/local/var/postgres/postgresql.conf`\\n   2. E.g. on Windows, _maybe_: `C:\\\\Program Files\\\\PostgreSQL\\\\11.2\\\\data\\\\postgresql.conf`\\n   3. E.g  on Ubuntu 18.04 probably: '/etc/postgresql/10/main/postgresql.conf'\\n2. Find the `timezone` line and set it to `UTC`:\\n\\n```conf\\n# - Locale and Formatting -\\n\\ndatestyle = 'iso, mdy'\\n#intervalstyle = 'postgres'\\ntimezone = 'UTC'\\n#timezone_abbreviations = 'Default'     # Select the set of available time zone\\n```\\n\\n## Scripts\\n\\nStart the application `npm start`\\n\\nStart nodemon for the application `npm run dev`\\n\\nRun the tests mode `npm test`\\n\\nRun the migrations up `npm run migrate`\\n\\nRun the migrations down `npm run migrate -- 0`\\n\"},\n",
       " {'repo': 'HebaruSan/Astrogator',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': \"# Astrogator\\n\\n![mainScreenshot](screenshots/mainScreenshot.png)\\n\\nA space-navigational aide for [Kerbal Space Program](https://www.kerbalspaceprogram.com/).\\n\\nSee all the transfers that you could choose from your current location at a glance, including the time till the burn and delta V, and turn them into maneuvers with one click.\\n\\nSee the [README in the download] for installation and usage instructions.\\n\\nSee the [README in the Localization folder] for instructions for adding or improving translations for languages other than English.\\n\\n[README in the download]: GameData/Astrogator/README.md\\n\\n[README in the Localization folder]: GameData/Astrogator/Localization/README.md\\n\\n## Download\\n\\nThe [latest version] is available on Github.\\n\\n[latest version]: https://github.com/HebaruSan/Astrogator/releases/latest\\n\\n## How to donate\\n\\n[![Donate][Donation image]][Donation link]\\n\\n[Donation link]: https://paypal.me/HebaruSan\\n\\n[Donation image]: https://i.imgur.com/M9m07Qw.png\\n\\n## Building\\n\\n1. Install mono\\n2. If you don't have Steam, make `Source/KSP_Data` a symbolic link to your game's `KSP_Data` folder\\n3. `make`\\n\\n## References\\n\\n### Plug-in authoring\\n\\n- https://kerbalspaceprogram.com/api/index.html\\n- https://forum.kerbalspaceprogram.com/index.php?/topic/153765-getting-started-the-basics-of-writing-a-plug-in/\\n- https://forum.kerbalspaceprogram.com/index.php?/topic/151354-unity-ui-creation-tutorial/\\n- https://forum.kerbalspaceprogram.com/index.php?/topic/149324-popupdialog-and-the-dialoggui-classes/\\n- https://forum.kerbalspaceprogram.com/index.php?/topic/78231-application-launcher-and-mods/\\n- https://forum.kerbalspaceprogram.com/index.php?/topic/154006-solved-texture-issues/&do=findComment&comment=2904233\\n\\n### Localization\\n\\n- https://forum.kerbalspaceprogram.com/index.php?/topic/158018-addon-localization-home/\\n\\n### Physics and math\\n\\n- https://en.wikipedia.org/wiki/Hohmann_transfer_orbit\\n- https://en.wikipedia.org/wiki/Orbital_speed#Precise_orbital_speed\\n- https://www.reddit.com/r/KerbalAcademy/comments/35wtv1/how_do_i_calculate_phase_and_ejection_angle/crf3kc4/\\n- https://www.bogan.ca/orbits/kepler/orbteqtn.html\\n- https://d2r5da613aq50s.cloudfront.net/wp-content/uploads/411616.image0.jpg\\n- https://en.wikipedia.org/wiki/Orbital_inclination_change#Calculation\\n- https://en.wikipedia.org/wiki/Hyperbolic_trajectory#Hyperbolic_excess_velocity\\n- https://www.dtic.mil/dtic/tr/fulltext/u2/a200383.pdf\\n- https://forum.kerbalspaceprogram.com/index.php?/topic/122779-changing-orbital-angle-without-changing-apoapsis/\\n- https://upload.wikimedia.org/wikipedia/commons/e/eb/Orbit1.svg\\n\\n### Performance\\n\\n- https://www.somasim.com/blog/2015/04/csharp-memory-and-performance-tips-for-unity/\\n- https://forum.kerbalspaceprogram.com/index.php?/topic/142712-devnote-tuesday-smashing-buttons/&do=findComment&comment=2653161\\n\\n## Credits, acknowledgements, and dedication\\n\\nSpecial thanks to all those who have helped to translate Astrogator to other languages!\\n\\n| Language | Translators |\\n| --- | --- |\\n| Spanish | Iván ([hashashin](https://github.com/hashashin)), [Deltathiago98](https://github.com/Deltathiago98) |\\n| Chinese | [QAQdong](https://github.com/QAQdong) |\\n| Russian | [Niev](https://github.com/Niev), [Sooll3](https://github.com/Sooll3) |\\n| French  | [valens](https://forum.kerbalspaceprogram.com/index.php?/profile/163224-valens/) |\\n\\n- AN/DN logic and plane change calculations borrowed from MechJeb\\n- Phase angle logic and some icons borrowed from Kerbal Alarm Clock\\n- `.gitignore` borrowed from Transfer Window Planner\\n- `csproj` file adapted from Transfer Window Planner and Craft Import\\n- r4m0n for making it easy to find plane change nodes\\n- [TCShipInfo](http://forum.kerbalspaceprogram.com/index.php?/topic/59724-112-v04-resource-details-in-tracking-center/) for figuring out the tracking station API\\n- Main app icon modified from https://fontawesome.io/icon/map/\\n- Close X icon under Apache 2 license from https://material.io/icons/#ic_close and modified\\n- Back icon under Apache 2 license from https://material.io/icons/#ic_chevron_left\\n- ProjectGuid generated by https://www.guidgenerator.com/online-guid-generator.aspx\\n- Kepler, Hohmann, Tsiolkovsky, and Oberth for giving us the math\\n- In admiration of [Sailing Master Kevin Renner](https://en.wikipedia.org/wiki/The_Mote_in_God's_Eye)\\n\"},\n",
       " {'repo': 'Amegma/Galaxy-Attack',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '![[object Object]](https://socialify.git.ci/Amegma-Games/Space-Invaders/image?description=1&descriptionEditable=An%20inspiration%20of%20the%20original%20Atari%20Space%20Invaders%20game%20built%20in%20Pygame&font=KoHo&forks=1&issues=1&language=1&name=1&owner=1&pattern=Charlie%20Brown&pulls=1&stargazers=1&theme=Dark)\\n\\n# Galaxy Attack\\n\\nThis project comes under an open-source gaming organization called [Amegma](https://github.com/Amegma), and it is an inspiration of an arcade game which was Atari Space Invaders\\n\\n[Demo Game Video](https://youtu.be/ejA89NtTH2k)\\n\\n## Technologies Used\\n\\n- Python\\n- Framework - PyGame\\n- Other modules - time, os, sys, ctypes, etc.\\n\\n## Guidelines to setup\\n\\n### Prerequisites\\n\\n- Python should be installed: `Python version > 3.10`\\n- Pygame should be installed: `Pygame version > 1.9.5`\\n\\n### Run the game\\n```\\npython <file_path>/main.py\\n```\\n\\n### Create a release\\n```\\npyinstaller.exe .\\\\main.spec\\n```\\n\\n## DEMO\\n\\n#### IMAGES:\\n\\n<img src=\"/images/home.png\" width=340px /><img src=\"/images/game2.png\" width=340px />\\n<img src=\"/images/scoreboard2.png\" width=340px /><img src=\"/images/paused.png\" width=340px />\\n<img src=\"/images/settings.png\" width=340px /><img src=\"/images/ships.png\" width=340px />\\n<img src=\"/images/controls1.png\" width=340px /><img src=\"/images/controls2.png\" width=340px />\\n<img src=\"/images/controls3.png\" width=340px /><img src=\"/images/controls4.png\" width=340px />\\n'},\n",
       " {'repo': '42skillz/dddeu-workshop-from-problem-space-to-solution-space-',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# Workshop: from problem space to solution space\\n\\nCreating multiple models for the same problem is one of the more important lessons that Domain Driven Design teaches us. It is a lot cheaper to quickly iterate over them and throw away less useful prototypes before we even start coding. However, creating multiple models can be hard. When we begin gaining insight from our domain, we suffer a lot from cognitive biases that get in our way to gain new insights. We need these insights before we even start thinking about modelling. Tools like event storming and example mapping can help us to deliberate discover, and battle these biases. They help you quickly gain insight into the problem space. But the fallacy here is that we can get locked into the tool, and get stuck again.\\nWhat you will learn\\n\\nIn this two day workshop, you will learn the essentials of event storming and how it can help you gain the necessary insights you need to deliver quality software. With our newly acquired domain knowledge, we can then start exploring the solution space. During the exploration, we begin to design and model multiple models for the same problem with Domain Driven Design patterns. This way of visualising gives us the power to quickly iterate over the different models and figure out which model will be the best to use for now. Eventually, we start our coding journey TDD style, iterating over the model to refactor towards deeper insights while discovering how hexagonal architecture may help us to protect our domain code from the technical concerns, in the long run.\\nTarget Audience\\nThis workshop is for you if you are a software architect or software developer.\\nProgram\\n\\n## Day 1:\\n    Introduction\\n    Domain Driven Design a brief tour\\n    Event Storming - essential workshop\\n    Example Mapping - essential workshop\\n    Discovery Phase: Process Modeling Event Storming / Example Mapping\\n    Recap\\n\\n## Day 2:\\n    Technical DDD patterns\\n    Software modelling\\n    Test-first coding flow (from problem space to solution space)\\n    Hexagonal architecture overview and implementation\\n    Refactor towards deeper insights\\n    Recap\\n\\n### Prerequisites\\n\\n    Read the free ebook The Anatomy Of Domain-Driven Design: https://leanpub.com/theanatomyofdomain-drivendesign\\n    It is nice to have read Domain Driven Design by Eric Evans (the blue book), it is not required.\\n    Able to properly remove a sticky note\\n    For the second day: Bring a laptop with your favourite IDE and programming language\\n\\n## About the instructors\\n\\n### Kenny Baas-Schwegler\\nKenny Baas-Schwegler is a Software Engineer and Consultant focusing on software quality at Xebia. He mentors and coaches teams by using practices and techniques from Domain Driven Design, Behaviour Driven Development, Test Driven Development, and Continuous Delivery.\\n\\nHe is an advocate for multidisciplinary collaboration in open spaces. By using techniques such as Event Storming, and Example Mapping, he helps engineer requirements to design and model software. With these methods, he aims to create a transparent, collaborative domain space with constant and instant feedback when delivering software.\\n\\nBesides his daily work, he also helps organise meetups for Behaviour Driven Development NL and Domain Driven Design Nederland.\\n\\n### Bruno Boucard\\nBruno Boucard is developer, trainer, agile coach and speaker. He loves to explain with concrete examples with live-coding, if needed. He is organizer of the BDD Paris user group. He is a long-time Microsoft MVP, but he still is coding java with a C# accent ;-)\\n\\nHis company 42skillz which aims to help organizations to make software and to work differently, he provides trainings, coaching, consulting about TDD, BDD, legacy code refactoring techniques and DDD with an extra soul.\\n\\n### Thomas Pierrain\\nThomas Pierrain is co-organizer of the DDD Paris user group and co-founder of 42skillz, Thomas is an eXtreme Programmer & technical architect obsessed by use cases (Vs. solution-driven approaches). He likes using DDD & TDD to boost his efficiency at work.\\n\\nPolymath wanna be, Thomas loves to inspire his work with philosophy, astrophysics, psychology and music (to name a few).\\n'},\n",
       " {'repo': 'Yecats/SuperSpaceShooter',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '![alt text](SuperSpaceShooterLogo.png \"Super Space Shooter\")\\n\\n[![Join the chat at https://gitter.im/staceyhaffner/SuperSpaceShooter](https://badges.gitter.im/staceyhaffner/SuperSpaceShooter.svg)](https://gitter.im/staceyhaffner/SuperSpaceShooter?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\\n\\nSuper Space Shooter is a Unity demo game showing simple mechanics such as audio, animations, AI and UI. The purpose of this project is to give beginner developers a stepping stone into creating their own games. \\n\\nI\\'d love to see what you\\'ve created with the development challenges or on your own! Tweet them to [@yecats131](https://twitter.com/yecats131).\\n## Development Challenges\\n\\nSuper Space Shooter is a bare bones game that has many opportunities to be extended and enhanced to draw players in. Development challenges are designed to get new developers used to breaking down a large problem (making a game) into smaller, bite sized chunks. It is important to note that the challenges are not meant to be step-by-step instructions on how to solve a problem in Unity but instead encourage research and experimentation. By the end of the challenges you\\'ll have a complete space shooter game with better visuals, power ups, customizations, player profiles and AI improvements.\\n\\n*Note: Some challenges may depend on another being completed. For example, adding achievements depend on a player profile.*\\n\\n* [Easy Development Challenges](DevelopmentChallenges/Easy.md)\\n* [Medium Development Challenges](DevelopmentChallenges/Medium.md)\\n* [Hard Development Challenges](DevelopmentChallenges/Hard.md)\\n\\n## Credit\\nThis project uses the [Luckiest Guy](http://www.1001fonts.com/luckiest-guy-font.html) Font and graphics/sounds provied by Kenney. Assets can be found at http://kenney.nl/. The exact asset packs used are:\\n* [Space Shooter Redux](http://kenney.nl/assets/space-shooter-redux)\\n* [Digital Audio](http://kenney.nl/assets/digital-audio)\\n* [UI Pack](http://kenney.nl/assets/ui-pack)\\n'},\n",
       " {'repo': 'Fire-Oceann/CS-Space',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '<h1 align=\"center\">\\n  <a href=\"https://cs-space.vercel.app\"><img src=\"./static/img/slash-introducing.png\" alt=\"Computer Science  - Space\"></a>\\n</h1>\\n\\n<h3 align=\"center\">\\n  <a href=\"https://github.com/Fire-Oceann/CS-Space/issues/new?assignees=&labels=bug&template=bug_report.yml&title=\">Report a Bug</a>\\n  <span> · </span>\\n  <a href=\"https://github.com/Fire-Oceann/CS-Space/issues/new?assignees=&labels=feature+request&template=feature_request.yml&title=\">Request a Feature</a>\\n  <span> · </span>\\n  <a href=\"https://github.com/Fire-Oceann/CS-Space/discussions\">Ask a Question</a>\\n</h3>\\n\\n<div align=\"center\">\\n<br />\\n<a herf=\"./LICENSE\"><img alt=\"GitHub\" src=\"https://img.shields.io/github/license/Fire-Oceann/BB-TR-Kaynak?style=for-the-badge\"/></a>\\n<!-- ALL-CONTRIBUTORS-BADGE:START - Do not remove or modify this section -->\\n<a href=\"https://img.shields.io/badge/all_contributors-0-orange.svg?style=for-the-badge\"><img alt=\"All Contributors\" src=\"https://img.shields.io/badge/all_contributors-5-orange.svg?style=for-the-badge\"/></a>\\n<!-- ALL-CONTRIBUTORS-BADGE:END -->\\n<a herf=\".URL_\"><img alt=\"Open Source - ❤️\" src=\"https://img.shields.io/badge/Open_Source-❤️-00d59b?style=for-the-badge\"/></a>\\n<a href=\"https://github.com/Fire-Oceann/CS-Space/issues\"><img alt=\"GitHub issues\" src=\"https://img.shields.io/github/issues-raw/Fire-Oceann/BB-TR-Kaynak?color=%23F2625A&style=for-the-badge\"/></a>\\n\\n<a href=\"https://prettier.io/\"><img src=\"https://img.shields.io/badge/code%20style-prettier-%23d971de?style=for-the-badge\" alt=\"Prettier\" /></a>\\n<a href=\"https://github.com/Fire-Oceann/CS-Space/pulls\"><img src=\"https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=for-the-badge\" alt=\"PRs Welcome\" /></a>\\n\\n</div>\\n\\n<kbd>[<img title=\"Türkçe\" alt=\"Türkçe\" src=\"https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/tr.svg\" width=\"22\">](README/README-TR.md)</kbd>\\n<kbd>[<img title=\"Française\" alt=\"Française\" src=\"https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/fr.svg\" width=\"22\">](README/README-FR.md)</kbd>\\n<kbd>[<img title=\"Deutsch\" alt=\"Deutsch\" src=\"https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/de.svg\" width=\"22\">](README/README-DE.md)</kbd>\\n<kbd>[<img title=\"Русский язык\" alt=\"Русский язык\" src=\"https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/ru.svg\" width=\"22\">](README/README-RU.md)</kbd>\\n<kbd>[<img title=\"Italiano\" alt=\"Italiano\" src=\"https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/it.svg\" width=\"22\">](README/README-IT.md)</kbd>\\n\\n> A Computer Science portal for enthusiasts. It contains well-written, well-thought-out and well-explained computer science and programming articles.\\n\\n## Prerequisites\\n\\nMake sure you have installed all of the following prerequisites on your development machine:\\n\\n- Git - [Download & Install Git](https://git-scm.com/downloads). OSX and Linux machines typically have this already installed.\\n- Node.js and npm - [Download & Install Node.js and npm](https://nodejs.org/en/download/). If you encounter any problems, you can also use this [GitHub Gist](https://gist.github.com/isaacs/579814) to install Node.js and npm.\\n\\n## Installation & Setup\\n\\n```bash\\ngit clone https://github.com/Fire-Oceann/CS-Space.git\\ncd CS-Space\\nnpm install\\nnpm start\\n```\\n\\n## Project assistance\\n\\nIf you want to say thank you or/and support active development of `CS-SPACE`:\\n\\n- Add a ⭐️ [GitHub Star](https://github.com/Fire-Oceann/CS-Space) to the project.\\n- Tweet about the `CS-SPACE`.\\n- Write interesting articles about the project on [Dev.to](https://dev.to/), [Medium](https://medium.com/) or your personal blog.\\n\\nTogether, we can make `CS-SPACE` better!\\n\\n## 🤝 Contributing\\n\\nFirst off, thanks for taking the time to contribute! Contributions are what make the open-source community such an amazing place to learn, inspire, and create. Any contributions you make will benefit everybody else and are greatly appreciated.\\n\\nPlease read [our contribution guidelines ](CONTRIBUTING.md) for details on our [code of conduct](CODE_OF_CONDUCT.md), and the process for submitting pull requests to us.\\n\\n## Contributors ✨\\n\\nThanks goes to these wonderful people ([emoji key](https://allcontributors.org/docs/en/emoji-key)):\\n\\n<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->\\n<!-- prettier-ignore-start -->\\n<!-- markdownlint-disable -->\\n\\n<table>\\n  <tr>\\n    <td align=\"center\"><a href=\"https://github.com/ziarparvaiz\"><img src=\"https://avatars.githubusercontent.com/u/50423368?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>ZIAR Parvaiz</b></sub></a><br /><a href=\"https://github.com/Fire-Oceann/CS-Space/issues?q=author%3Aziarparvaiz\" title=\"Bug reports\">🐛</a> <a href=\"https://github.com/Fire-Oceann/CS-Space/commits?author=ziarparvaiz\" title=\"Code\">💻</a> <a href=\"#design-ziarparvaiz\" title=\"Design\">🎨</a> <a href=\"https://github.com/Fire-Oceann/CS-Space/commits?author=ziarparvaiz\" title=\"Documentation\">📖</a> <a href=\"#ideas-ziarparvaiz\" title=\"Ideas, Planning, & Feedback\">🤔</a> </td>\\n    <td align=\"center\"><a href=\"https://github.com/Burak-Atak\"><img src=\"https://avatars.githubusercontent.com/u/71793345?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Burak</b></sub></a><br /><a href=\"https://github.com/Fire-Oceann/CS-Space/commits?author=Burak-Atak\" title=\"Code\">💻</a> <a href=\"https://github.com/Fire-Oceann/CS-Space/commits?author=Burak-Atak\" title=\"Documentation\">📖</a></td>\\n    <td align=\"center\"><a href=\"https://medium.com/@beyzatekinli\"><img src=\"https://avatars.githubusercontent.com/u/64313175?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Beyzanur Tekinli</b></sub></a><br /><a href=\"#blog-b-tekinli\" title=\"Blogposts\">📝</a> <a href=\"https://github.com/Fire-Oceann/CS-Space/commits?author=b-tekinli\" title=\"Code\">💻</a> <a href=\"https://github.com/Fire-Oceann/CS-Space/commits?author=b-tekinli\" title=\"Documentation\">📖</a></td>\\n    <td align=\"center\"><a href=\"https://github.com/TalhaAksoy\"><img src=\"https://avatars.githubusercontent.com/u/56833887?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Talha Aksoy</b></sub></a><br /><a href=\"https://github.com/Fire-Oceann/CS-Space/commits?author=TalhaAksoy\" title=\"Code\">💻</a> <a href=\"https://github.com/Fire-Oceann/CS-Space/commits?author=TalhaAksoy\" title=\"Documentation\">📖</a></td>\\n    <td align=\"center\"><a href=\"https://github.com/beyzanur-seyhan\"><img src=\"https://avatars.githubusercontent.com/u/80166639?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Beyzanur Seyhan</b></sub></a><br /><a href=\"#blog-beyzanur-seyhan\" title=\"Blogposts\">📝</a> <a href=\"https://github.com/Fire-Oceann/CS-Space/commits?author=beyzanur-seyhan\" title=\"Code\">💻</a> <a href=\"https://github.com/Fire-Oceann/CS-Space/commits?author=beyzanur-seyhan\" title=\"Documentation\">📖</a></td>\\n  </tr>\\n</table>\\n\\n<!-- markdownlint-restore -->\\n<!-- prettier-ignore-end -->\\n\\n<!-- ALL-CONTRIBUTORS-LIST:END -->\\n\\nThis project follows the [all-contributors](https://github.com/all-contributors/all-contributors) specification. Contributions of any kind welcome!\\n\\n## Versioning\\n\\nWe use [SemVer](https://semver.org/) for versioning. For the versions available, see the [tags on this repository](https://github.com/Fire-Oceann/CS-Space/tags).\\n\\n## License\\n\\nThis project is licensed under the [CS-SPACE](https://github.com/Fire-Oceann/CS-Space) License - see the [LICENSE](./LICENSE) file for details\\n'},\n",
       " {'repo': 'sonologic/SpaceState',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': 'SpaceState\\n==========\\n\\n\"Koen Martens\" <gmc@hackerspaces.nl>\\n\\nhttps://market.android.com/details?id=com.sonologic.spacestatus\\n\\nAndroid app and widget showing spacestatus by retrieving json objects\\nthat are formatted according to the hackerspaces.nl SpaceAPI (which\\ncan be found on https://hackerspaces.nl/spaceapi/).\\n\\nToDo\\n====\\n\\n- Working on importing the space status directory, and let users choose\\n  which space to follow\\n- Multiple widgets for different spaces\\n- Visual appealing sauce\\n- Show logo\\'s and other details from the json object \\n\\nChangelog\\n=========\\n\\nv1.2:\\n- persistent preferences\\n- app launches when widget is tapped\\n- show when space status changed\\nv1.1:\\n- add slider to set update frequency\\n- add \\'force update\\' button\\n- force update when activity becomes visible'},\n",
       " {'repo': 'NVlabs/edm',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '## Elucidating the Design Space of Diffusion-Based Generative Models (EDM)<br><sub>Official PyTorch implementation of the NeurIPS 2022 paper</sub>\\n\\n![Teaser image](./docs/teaser-1920x640.jpg)\\n\\n**Elucidating the Design Space of Diffusion-Based Generative Models**<br>\\nTero Karras, Miika Aittala, Timo Aila, Samuli Laine\\n<br>https://arxiv.org/abs/2206.00364<br>\\n\\nAbstract: *We argue that the theory and practice of diffusion-based generative models are currently unnecessarily convoluted and seek to remedy the situation by presenting a design space that clearly separates the concrete design choices. This lets us identify several changes to both the sampling and training processes, as well as preconditioning of the score networks. Together, our improvements yield new state-of-the-art FID of 1.79 for CIFAR-10 in a class-conditional setting and 1.97 in an unconditional setting, with much faster sampling (35 network evaluations per image) than prior designs. To further demonstrate their modular nature, we show that our design changes dramatically improve both the efficiency and quality obtainable with pre-trained score networks from previous work, including improving the FID of a previously trained ImageNet-64 model from 2.07 to near-SOTA 1.55, and after re-training with our proposed improvements to a new SOTA of 1.36.*\\n\\nFor business inquiries, please visit our website and submit the form: [NVIDIA Research Licensing](https://www.nvidia.com/en-us/research/inquiries/)\\n\\n## Requirements\\n\\n* Linux and Windows are supported, but we recommend Linux for performance and compatibility reasons.\\n* 1+ high-end NVIDIA GPU for sampling and 8+ GPUs for training. We have done all testing and development using V100 and A100 GPUs.\\n* 64-bit Python 3.8 and PyTorch 1.12.0 (or later). See https://pytorch.org for PyTorch install instructions.\\n* Python libraries: See [environment.yml](./environment.yml) for exact library dependencies. You can use the following commands with Miniconda3 to create and activate your Python environment:\\n  - `conda env create -f environment.yml -n edm`\\n  - `conda activate edm`\\n* Docker users:\\n  - Ensure you have correctly installed the [NVIDIA container runtime](https://docs.docker.com/config/containers/resource_constraints/#gpu).\\n  - Use the [provided Dockerfile](./Dockerfile) to build an image with the required library dependencies.\\n\\n## Getting started\\n\\nTo reproduce the main results from our paper, simply run:\\n\\n```.bash\\npython example.py\\n```\\n\\nThis is a minimal standalone script that loads the best pre-trained model for each dataset and generates a random 8x8 grid of images using the optimal sampler settings. Expected results:\\n\\n| Dataset  | Runtime | Reference image\\n| :------- | :------ | :--------------\\n| CIFAR-10 | ~6 sec  | [`cifar10-32x32.png`](./docs/cifar10-32x32.png)\\n| FFHQ     | ~28 sec | [`ffhq-64x64.png`](./docs/ffhq-64x64.png)\\n| AFHQv2   | ~28 sec | [`afhqv2-64x64.png`](./docs/afhqv2-64x64.png)\\n| ImageNet | ~5 min  | [`imagenet-64x64.png`](./docs/imagenet-64x64.png)\\n\\nThe easiest way to explore different sampling strategies is to modify [`example.py`](./example.py) directly. You can also incorporate the pre-trained models and/or our proposed EDM sampler in your own code by simply copy-pasting the relevant bits. Note that the class definitions for the pre-trained models are stored within the pickles themselves and loaded automatically during unpickling via [`torch_utils.persistence`](./torch_utils/persistence.py). To use the models in external Python scripts, just make sure that `torch_utils` and `dnnlib` are accesible through `PYTHONPATH`.\\n\\n**Docker**: You can run the example script using Docker as follows:\\n\\n```.bash\\n# Build the edm:latest image\\ndocker build --tag edm:latest .\\n\\n# Run the generate.py script using Docker:\\ndocker run --gpus all -it --rm --user $(id -u):$(id -g) \\\\\\n    -v `pwd`:/scratch --workdir /scratch -e HOME=/scratch \\\\\\n    edm:latest \\\\\\n    python example.py\\n```\\n\\nNote: The Docker image requires NVIDIA driver release `r520` or later.\\n\\nThe `docker run` invocation may look daunting, so let\\'s unpack its contents here:\\n\\n- `--gpus all -it --rm --user $(id -u):$(id -g)`: with all GPUs enabled, run an interactive session with current user\\'s UID/GID to avoid Docker writing files as root.\\n- ``-v `pwd`:/scratch --workdir /scratch``: mount current running dir (e.g., the top of this git repo on your host machine) to `/scratch` in the container and use that as the current working dir.\\n- `-e HOME=/scratch`: specify where to cache temporary files. Note: if you want more fine-grained control, you can instead set `DNNLIB_CACHE_DIR` (for pre-trained model download cache). You want these cache dirs to reside on persistent volumes so that their contents are retained across multiple `docker run` invocations.\\n\\n## Pre-trained models\\n\\nWe provide pre-trained models for our proposed training configuration (config F) as well as the baseline configuration (config A):\\n\\n- [https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/](https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/)\\n- [https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/baseline/](https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/baseline/)\\n\\nTo generate a batch of images using a given model and sampler, run:\\n\\n```.bash\\n# Generate 64 images and save them as out/*.png\\npython generate.py --outdir=out --seeds=0-63 --batch=64 \\\\\\n    --network=https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-cifar10-32x32-cond-vp.pkl\\n```\\n\\nGenerating a large number of images can be time-consuming; the workload can be distributed across multiple GPUs by launching the above command using `torchrun`:\\n\\n```.bash\\n# Generate 1024 images using 2 GPUs\\ntorchrun --standalone --nproc_per_node=2 generate.py --outdir=out --seeds=0-999 --batch=64 \\\\\\n    --network=https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-cifar10-32x32-cond-vp.pkl\\n```\\n\\nThe sampler settings can be controlled through command-line options; see [`python generate.py --help`](./docs/generate-help.txt) for more information. For best results, we recommend using the following settings for each dataset:\\n\\n```.bash\\n# For CIFAR-10 at 32x32, use deterministic sampling with 18 steps (NFE = 35)\\npython generate.py --outdir=out --steps=18 \\\\\\n    --network=https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-cifar10-32x32-cond-vp.pkl\\n\\n# For FFHQ and AFHQv2 at 64x64, use deterministic sampling with 40 steps (NFE = 79)\\npython generate.py --outdir=out --steps=40 \\\\\\n    --network=https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-ffhq-64x64-uncond-vp.pkl\\n\\n# For ImageNet at 64x64, use stochastic sampling with 256 steps (NFE = 511)\\npython generate.py --outdir=out --steps=256 --S_churn=40 --S_min=0.05 --S_max=50 --S_noise=1.003 \\\\\\n    --network=https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-imagenet-64x64-cond-adm.pkl\\n```\\n\\nBesides our proposed EDM sampler, `generate.py` can also be used to reproduce the sampler ablations from Section 3 of our paper. For example:\\n\\n```.bash\\n# Figure 2a, \"Our reimplementation\"\\npython generate.py --outdir=out --steps=512 --solver=euler --disc=vp --schedule=vp --scaling=vp \\\\\\n    --network=https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/baseline/baseline-cifar10-32x32-uncond-vp.pkl\\n\\n# Figure 2a, \"+ Heun & our {t_i}\"\\npython generate.py --outdir=out --steps=128 --solver=heun --disc=edm --schedule=vp --scaling=vp \\\\\\n    --network=https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/baseline/baseline-cifar10-32x32-uncond-vp.pkl\\n\\n# Figure 2a, \"+ Our sigma(t) & s(t)\"\\npython generate.py --outdir=out --steps=18 --solver=heun --disc=edm --schedule=linear --scaling=none \\\\\\n    --network=https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/baseline/baseline-cifar10-32x32-uncond-vp.pkl\\n```\\n\\n## Calculating FID\\n\\nTo compute Fr&eacute;chet inception distance (FID) for a given model and sampler, first generate 50,000 random images and then compare them against the dataset reference statistics using `fid.py`:\\n\\n```.bash\\n# Generate 50000 images and save them as fid-tmp/*/*.png\\ntorchrun --standalone --nproc_per_node=1 generate.py --outdir=fid-tmp --seeds=0-49999 --subdirs \\\\\\n    --network=https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-cifar10-32x32-cond-vp.pkl\\n\\n# Calculate FID\\ntorchrun --standalone --nproc_per_node=1 fid.py calc --images=fid-tmp \\\\\\n    --ref=https://nvlabs-fi-cdn.nvidia.com/edm/fid-refs/cifar10-32x32.npz\\n```\\n\\nBoth of the above commands can be parallelized across multiple GPUs by adjusting `--nproc_per_node`. The second command typically takes 1-3 minutes in practice, but the first one can sometimes take several hours, depending on the configuration. See [`python fid.py --help`](./docs/fid-help.txt) for the full list of options.\\n\\nNote that the numerical value of FID varies across different random seeds and is highly sensitive to the number of images. By default, `fid.py` will always use 50,000 generated images; providing fewer images will result in an error, whereas providing more will use a random subset. To reduce the effect of random variation, we recommend repeating the calculation multiple times with different seeds, e.g., `--seeds=0-49999`, `--seeds=50000-99999`, and `--seeds=100000-149999`. In our paper, we calculated each FID three times and reported the minimum.\\n\\nAlso note that it is important to compare the generated images against the same dataset that the model was originally trained with. To facilitate evaluation, we provide the exact reference statistics that correspond to our pre-trained models:\\n\\n* [https://nvlabs-fi-cdn.nvidia.com/edm/fid-refs/](https://nvlabs-fi-cdn.nvidia.com/edm/fid-refs/)\\n\\nFor ImageNet, we provide two sets of reference statistics to enable apples-to-apples comparison: `imagenet-64x64.npz` should be used when evaluating the EDM model (`edm-imagenet-64x64-cond-adm.pkl`), whereas `imagenet-64x64-baseline.npz` should be used when evaluating the baseline model (`baseline-imagenet-64x64-cond-adm.pkl`); the latter was originally trained by Dhariwal and Nichol using slightly different training data.\\n\\nYou can compute the reference statistics for your own datasets as follows:\\n\\n```.bash\\npython fid.py ref --data=datasets/my-dataset.zip --dest=fid-refs/my-dataset.npz\\n```\\n\\n## Preparing datasets\\n\\nDatasets are stored in the same format as in [StyleGAN](https://github.com/NVlabs/stylegan3): uncompressed ZIP archives containing uncompressed PNG files and a metadata file `dataset.json` for labels. Custom datasets can be created from a folder containing images; see [`python dataset_tool.py --help`](./docs/dataset-tool-help.txt) for more information.\\n\\n**CIFAR-10:** Download the [CIFAR-10 python version](https://www.cs.toronto.edu/~kriz/cifar.html) and convert to ZIP archive:\\n\\n```.bash\\npython dataset_tool.py --source=downloads/cifar10/cifar-10-python.tar.gz \\\\\\n    --dest=datasets/cifar10-32x32.zip\\npython fid.py ref --data=datasets/cifar10-32x32.zip --dest=fid-refs/cifar10-32x32.npz\\n```\\n\\n**FFHQ:** Download the [Flickr-Faces-HQ dataset](https://github.com/NVlabs/ffhq-dataset) as 1024x1024 images and convert to ZIP archive at 64x64 resolution:\\n\\n```.bash\\npython dataset_tool.py --source=downloads/ffhq/images1024x1024 \\\\\\n    --dest=datasets/ffhq-64x64.zip --resolution=64x64\\npython fid.py ref --data=datasets/ffhq-64x64.zip --dest=fid-refs/ffhq-64x64.npz\\n```\\n\\n**AFHQv2:** Download the updated [Animal Faces-HQ dataset](https://github.com/clovaai/stargan-v2/blob/master/README.md#animal-faces-hq-dataset-afhq) (`afhq-v2-dataset`) and convert to ZIP archive at 64x64 resolution:\\n\\n```.bash\\npython dataset_tool.py --source=downloads/afhqv2 \\\\\\n    --dest=datasets/afhqv2-64x64.zip --resolution=64x64\\npython fid.py ref --data=datasets/afhqv2-64x64.zip --dest=fid-refs/afhqv2-64x64.npz\\n```\\n\\n**ImageNet:** Download the [ImageNet Object Localization Challenge](https://www.kaggle.com/competitions/imagenet-object-localization-challenge/data) and convert to ZIP archive at 64x64 resolution:\\n\\n```.bash\\npython dataset_tool.py --source=downloads/imagenet/ILSVRC/Data/CLS-LOC/train \\\\\\n    --dest=datasets/imagenet-64x64.zip --resolution=64x64 --transform=center-crop\\npython fid.py ref --data=datasets/imagenet-64x64.zip --dest=fid-refs/imagenet-64x64.npz\\n```\\n\\n## Training new models\\n\\nYou can train new models using `train.py`. For example:\\n\\n```.bash\\n# Train DDPM++ model for class-conditional CIFAR-10 using 8 GPUs\\ntorchrun --standalone --nproc_per_node=8 train.py --outdir=training-runs \\\\\\n    --data=datasets/cifar10-32x32.zip --cond=1 --arch=ddpmpp\\n```\\n\\nThe above example uses the default batch size of 512 images (controlled by `--batch`) that is divided evenly among 8 GPUs (controlled by `--nproc_per_node`) to yield 64 images per GPU. Training large models may run out of GPU memory; the best way to avoid this is to limit the per-GPU batch size, e.g., `--batch-gpu=32`. This employs gradient accumulation to yield the same results as using full per-GPU batches. See [`python train.py --help`](./docs/train-help.txt) for the full list of options.\\n\\nThe results of each training run are saved to a newly created directory, for example `training-runs/00000-cifar10-cond-ddpmpp-edm-gpus8-batch64-fp32`. The training loop exports network snapshots (`network-snapshot-*.pkl`) and training states (`training-state-*.pt`) at regular intervals (controlled by `--snap` and `--dump`). The network snapshots can be used to generate images with `generate.py`, and the training states can be used to resume the training later on (`--resume`). Other useful information is recorded in `log.txt` and `stats.jsonl`. To monitor training convergence, we recommend looking at the training loss (`\"Loss/loss\"` in `stats.jsonl`) as well as periodically evaluating FID for `network-snapshot-*.pkl` using `generate.py` and `fid.py`.\\n\\nThe following table lists the exact training configurations that we used to obtain our pre-trained models:\\n\\n| <sub>Model</sub> | <sub>GPUs</sub> | <sub>Time</sub> | <sub>Options</sub>\\n| :-- | :-- | :-- | :--\\n| <sub>cifar10&#8209;32x32&#8209;cond&#8209;vp</sub>   | <sub>8xV100</sub>  | <sub>~2&nbsp;days</sub>  | <sub>`--cond=1 --arch=ddpmpp`</sub>\\n| <sub>cifar10&#8209;32x32&#8209;cond&#8209;ve</sub>   | <sub>8xV100</sub>  | <sub>~2&nbsp;days</sub>  | <sub>`--cond=1 --arch=ncsnpp`</sub>\\n| <sub>cifar10&#8209;32x32&#8209;uncond&#8209;vp</sub> | <sub>8xV100</sub>  | <sub>~2&nbsp;days</sub>  | <sub>`--cond=0 --arch=ddpmpp`</sub>\\n| <sub>cifar10&#8209;32x32&#8209;uncond&#8209;ve</sub> | <sub>8xV100</sub>  | <sub>~2&nbsp;days</sub>  | <sub>`--cond=0 --arch=ncsnpp`</sub>\\n| <sub>ffhq&#8209;64x64&#8209;uncond&#8209;vp</sub>    | <sub>8xV100</sub>  | <sub>~4&nbsp;days</sub>  | <sub>`--cond=0 --arch=ddpmpp --batch=256 --cres=1,2,2,2 --lr=2e-4 --dropout=0.05 --augment=0.15`</sub>\\n| <sub>ffhq&#8209;64x64&#8209;uncond&#8209;ve</sub>    | <sub>8xV100</sub>  | <sub>~4&nbsp;days</sub>  | <sub>`--cond=0 --arch=ncsnpp --batch=256 --cres=1,2,2,2 --lr=2e-4 --dropout=0.05 --augment=0.15`</sub>\\n| <sub>afhqv2&#8209;64x64&#8209;uncond&#8209;vp</sub>  | <sub>8xV100</sub>  | <sub>~4&nbsp;days</sub>  | <sub>`--cond=0 --arch=ddpmpp --batch=256 --cres=1,2,2,2 --lr=2e-4 --dropout=0.25 --augment=0.15`</sub>\\n| <sub>afhqv2&#8209;64x64&#8209;uncond&#8209;ve</sub>  | <sub>8xV100</sub>  | <sub>~4&nbsp;days</sub>  | <sub>`--cond=0 --arch=ncsnpp --batch=256 --cres=1,2,2,2 --lr=2e-4 --dropout=0.25 --augment=0.15`</sub>\\n| <sub>imagenet&#8209;64x64&#8209;cond&#8209;adm</sub> | <sub>32xA100</sub> | <sub>~13&nbsp;days</sub> | <sub>`--cond=1 --arch=adm --duration=2500 --batch=4096 --lr=1e-4 --ema=50 --dropout=0.10 --augment=0 --fp16=1 --ls=100 --tick=200`</sub>\\n\\nFor ImageNet-64, we ran the training on four NVIDIA DGX A100 nodes, each containing 8 Ampere GPUs with 80 GB of memory. To reduce the GPU memory requirements, we recommend either training the model with more GPUs or limiting the per-GPU batch size with `--batch-gpu`. To set up multi-node training, please consult the [torchrun documentation](https://pytorch.org/docs/stable/elastic/run.html).\\n\\n## License\\n\\nCopyright &copy; 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\\n\\nAll material, including source code and pre-trained models, is licensed under the [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-nc-sa/4.0/).\\n\\n`baseline-cifar10-32x32-uncond-vp.pkl` and `baseline-cifar10-32x32-uncond-ve.pkl` are derived from the [pre-trained models](https://github.com/yang-song/score_sde_pytorch) by Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. The models were originally shared under the [Apache 2.0 license](https://github.com/yang-song/score_sde_pytorch/blob/main/LICENSE).\\n\\n`baseline-imagenet-64x64-cond-adm.pkl` is derived from the [pre-trained model](https://github.com/openai/guided-diffusion) by Prafulla Dhariwal and Alex Nichol. The model was originally shared under the [MIT license](https://github.com/openai/guided-diffusion/blob/main/LICENSE).\\n\\n`imagenet-64x64-baseline.npz` is derived from the [precomputed reference statistics](https://github.com/openai/guided-diffusion/tree/main/evaluations) by Prafulla Dhariwal and Alex Nichol. The statistics were\\noriginally shared under the [MIT license](https://github.com/openai/guided-diffusion/blob/main/LICENSE).\\n\\n## Citation\\n\\n```\\n@inproceedings{Karras2022edm,\\n  author    = {Tero Karras and Miika Aittala and Timo Aila and Samuli Laine},\\n  title     = {Elucidating the Design Space of Diffusion-Based Generative Models},\\n  booktitle = {Proc. NeurIPS},\\n  year      = {2022}\\n}\\n```\\n\\n## Development\\n\\nThis is a research reference implementation and is treated as a one-time code drop. As such, we do not accept outside code contributions in the form of pull requests.\\n\\n## Acknowledgments\\n\\nWe thank Jaakko Lehtinen, Ming-Yu Liu, Tuomas Kynk&auml;&auml;nniemi, Axel Sauer, Arash Vahdat, and Janne Hellsten for discussions and comments, and Tero Kuosmanen, Samuel Klenberg, and Janne Hellsten for maintaining our compute infrastructure.\\n'},\n",
       " {'repo': 'developit/preact-portal',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# 🌌 preact-portal 🌠\\n\\n[![NPM](https://img.shields.io/npm/v/preact-portal.svg?style=flat)](https://www.npmjs.org/package/preact-portal)\\n[![travis-ci](https://travis-ci.org/developit/preact-portal.svg?branch=master)](https://travis-ci.org/developit/preact-portal)\\n\\n### **Render [Preact] components into SPACE**\\\\*\\n\\n_\\\\* a space in the DOM. Sorry._\\n\\n> Use this if you have a component that needs to render children into some other place in the DOM.\\n>\\n> An example of this would be modal dialogs, where you may need to render `<Dialog />` into `<body>`.\\n\\n\\n| [Demo #1] | [Demo #2] |\\n|:---------:|:---------:|\\n| _Moving around the DOM by changing `into`._ | _Open a full-page modal from within a thumbnail._ |\\n| <img src=\"https://i.gyazo.com/c08ff6fb5b3dc7da41099cb5c743ac86.gif\" width=\"232\"> | <img src=\"https://i.gyazo.com/afe7ebdaa2591dac92753af7066ac437.gif\" width=\"176\"> |\\n\\n\\n\\n---\\n\\n\\n## Installation\\n\\nVia npm:\\n\\n`npm install --save preact-portal`\\n\\n\\n\\n## Usage\\n\\n```js\\nimport { h, Component, render } from \\'preact\\';\\nimport Portal from \\'preact-portal\\';\\n\\nclass Thumbnail extends Component {\\n  open = () => this.setState({ open:true });\\n  close = () => this.setState({ open:false });\\n\\n  render({ url }, { open }) {\\n    return (\\n      <div class=\"thumb\" onClick={this.open}>\\n        <img src={url} />\\n\\n        { open ? (\\n          <Portal into=\"body\">\\n            <div class=\"popup\" onClick={this.close}>\\n              <img src={url} />\\n            </div>\\n          </Portal>\\n        ) : null }\\n      </div>\\n    );\\n  }\\n}\\n\\nrender(<Thumbnail url=\"//i.imgur.com/6Rp4hbs.gif\" />, document.body);\\n```\\n\\n\\n---\\n\\n\\nOr, wrap up a very common case into a simple high order function:\\n\\n```js\\nconst Popup = ({ open, into=\"body\", children }) => (\\n  open ? <Portal into={into}>{ children }</Portal> : null\\n);\\n\\n// Example: show popup on error.\\nclass Form extends Component {\\n  render({}, { error }) {\\n    return (\\n      <form>\\n        <Popup open={error}>\\n          <p>Error: {error}</p>\\n        </Popup>\\n        ...etc\\n      </form>\\n    );\\n  }\\n}\\n```\\n\\n\\n[preact]: https://github.com/developit/preact\\n[Demo #1]: http://jsfiddle.net/developit/bsr7gmdd/\\n[Demo #2]: http://jsfiddle.net/developit/f1jmxtvg/\\n'},\n",
       " {'repo': 'SAP-archive/cloud-sample-spaceflight-node',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '![](https://img.shields.io/badge/STATUS-NOT%20CURRENTLY%20MAINTAINED-red.svg?longCache=true&style=flat)\\n\\n# Important Notice\\nThis public repository is read-only and no longer maintained.\\n\\n# Description\\nLearn how to build node applications with the application programming model on SAP Cloud Platform.\\n\\n## Scenario:\\n\\nSPICY (Space Itinerary Company) is the most popular travel technology company in our milky way. Owing to the increasing demand in space travel, SPICY is now building a website where it offers users from all planets to login and book their next space flight. You, as a part of the software engineering team of SPICY, have chosen SAP Cloud Platform to build this website. \\n\\nClick [here](./-exercises-/docs/CNA375.pdf) for the presentation.\\n\\n![SPICY](./-exercises-/images/scenario.png?raw=true)\\n\\n### [Exercise 01: Create a new Business Application using local development tools](./-exercises-/exercise01/README.md)\\nIn this exercise, we will learn to set up our local development environment and create a new node.js project. This will be our codebase to create the Space travel web application.\\n\\n### [Exercise 02: Understand and extend the data model](./-exercises-/exercise02/README.md)\\nIn this exercise, we will understand the data model of the [base space-flight project](https://github.com/SAP/cloud-sample-spaceflight). Using the base model, we create tables in a local SQLite database and load data from CSV files. We will further extend the data model to include payment information for space travel bookings.\\n\\n### [Exercise 03: Add custom logic restricting the number of travellers in a spacecraft](./-exercises-/exercise03/README.md)\\nIn this exercise, we will learn to expose our data model entities as OData services. We will also include custom logic to limit the number of passengers in each space craft. \\n\\n### [Exercise 04: Build a User Interface using SAP UI5 from SAP Web IDE](./-exercises-/exercise04/README.md)\\nIn this exercise, we will import the code into SAP Web IDE and build a User Interface based on SAP UI5 so that users can create a booking from the UI of our app.\\n\\n### [Exercise 05: Deploy the application to SAP Cloud Platform Cloud Foundry environment](./-exercises-/exercise05/README.md)\\nIn this exercise, we will deploy the application to SAP Cloud Platform Cloud Foundry environment. The multiple components of the app are visualized and monitored from SAP Cloud Platform Cockpit.\\n\\n## Requirements\\n\\n### For SAP TechEd 2018\\nSAP TechEd will provide you with a full environment to develop this sample application. The instructions below are only needed if you wish to run the application in your own account on SAP Cloud Platform.\\n\\n### Development in SAP Cloud Platform Web IDE\\n\\nSAP Web IDE Full-Stack access is needed. For more information, see [Open SAP Web IDE](https://help.sap.com/viewer/825270ffffe74d9f988a0f0066ad59f0/CF/en-US/51321a804b1a4935b0ab7255447f5f84.html).\\n\\nRead the [getting started tutorial](https://help.sap.com/viewer//65de2977205c403bbc107264b8eccf4b/Cloud/en-US/5ec8c983a0bf43b4a13186fcf59015fc.html) to learn more about working with SAP Cloud Platform Web IDE.\\n\\nA **HANA instance** is needed in your account, so that you can deploy the persistence assets.\\n\\nNow clone your fork of this repository (*File -> Git -> Clone Repository*).\\n\\n#### Develop, Build, Deploy\\n\\nTo build and deploy your application or modify it and redeploy, use any of the following options:\\n\\n* Build and deploy the DB module by choosing *Build* from the context menu of the db folder.\\n\\n* Build and deploy the Node.js service by choosing *Run -> Run as -> Node.js application* from the context menu of the srv folder. To test the service, click the URL displayed in the Run Console. Use the endpoint of the service *clouds.products.CatalogService* to call $metadata or CRUD requests.\\n\\n* Test the UI by choosing *Run -> Run as -> Web Application* from the context menu of the ui folder.\\n\\n## Known Issues\\nNone\\n\\n## Support\\nThis project is provided \"as-is\": there is no guarantee that raised issues will be answered or addressed in future releases.\\n\\n## License\\n\\nCopyright (c) 2018 SAP SE or an SAP affiliate company. All rights reserved. This file is licensed under the Apache Software License, version 2.0 except as noted otherwise in the [LICENSE](LICENSE) file.\\n'},\n",
       " {'repo': 'yiqing-95/yiiSpace',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '基于Yii的sns 测试项目\\r\\n=============================\\r\\n\\r\\n感谢选择Yii 作为您的web应用程序框架.\\r\\n\\r\\n\\r\\n安装\\r\\n------------\\r\\n\\r\\n下载本程序 解压到您的web服务器www目录\\r\\n用你最喜欢的数据库管理工具（导入）执行protected/data/newYiiSpace.mysql.sql\\r\\n\\r\\n根据你服务器配置情况 修改.htaccess Ngix用户请参考此文件修改\\r\\n\\r\\n环境需求\\r\\n------------\\r\\n\\r\\nThe minimum requirement by Yii is that your Web server supports\\r\\nPHP 5.1.0 or above. Yii has been tested with Apache HTTP server\\r\\non Windows and Linux operating systems.\\r\\n\\r\\nPlease access the following URL to check if your Web server reaches\\r\\nthe requirements by Yii, assuming \"YiiPath\" is where Yii is installed:\\r\\n\\r\\n      http://hostname/YiiPath/requirements/index.php\\r\\n\\r\\n\\r\\nQUICK START\\r\\n-----------\\r\\n\\r\\n等待开发差不多了 在考虑提供自动安装功能 现在只能手工安装 ^-^\\r\\n\\r\\nWHAT\\'s NEXT\\r\\n-----------\\r\\n\\r\\n.....\\r\\n\\r\\n\\r\\n贡献者列表\\r\\n-----------\\r\\n\\r\\n\\r\\n\\nThis file was modified by JetBrains PhpStorm 5.0.4 for binding GitHub repository'},\n",
       " {'repo': 'idsc-frazzoli/owl',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '# ch.ethz.idsc.owl <a href=\"https://travis-ci.org/idsc-frazzoli/owl\"><img src=\"https://travis-ci.org/idsc-frazzoli/owl.svg?branch=master\" alt=\"Build Status\"></a>\\n\\nLibrary for motion planning in Java, version `0.5.9`\\n\\n![owl](https://user-images.githubusercontent.com/4012178/63221687-dc80d380-c19c-11e9-8aa4-8f7b36f5a4d4.png)\\n\\nThe library was developed with the following objectives in mind\\n* suitable for use in safety-critical real-time systems\\n* trajectory planning for an autonomous vehicle\\n* implementation of theoretical concepts with high level of abstraction\\n* simulation and visualization\\n\\n<table>\\n<tr>\\n<td>\\n\\n![usecase_motionplan](https://user-images.githubusercontent.com/4012178/35968244-96577dee-0cc3-11e8-80a1-b38691e863af.png)\\n\\nMotion planning\\n\\n<td>\\n\\n![shadow_regions](https://user-images.githubusercontent.com/4012178/42315433-b53034de-8047-11e8-8fc2-87fa504460c5.png)\\n\\nObstacle anticipation\\n\\n<td>\\n\\n![usecase_gokart](https://user-images.githubusercontent.com/4012178/35968269-a92a3b46-0cc3-11e8-8d5e-1276762cdc36.png)\\n\\n[Trajectory pursuit](https://www.youtube.com/watch?v=XgmS8CP6gqw)\\n\\n<td>\\n\\n![planning_obstacles](https://user-images.githubusercontent.com/4012178/40268689-2af06cd4-5b72-11e8-95cf-d94edfdc3dd1.png)\\n\\n[Static obstacles](https://www.youtube.com/watch?v=xLZeKFeAokM)\\n\\n</tr>\\n</table>\\n\\n## Student Projects\\n\\n### 2017\\n\\n* Jonas Londschien (MT): *An Anytime Generalized Label Correcting Method for Motion Planning*\\n\\n### 2018\\n\\n* Yannik Nager (MT): *What lies in the shadows? Safe and computation-aware motion planning for autonomous vehicles using intent-aware dynamic shadow regions*\\n\\n### 2019\\n\\n* André Stoll (MT): *Multi-Objective Optimization Using Preference Structures*\\n* Oliver Brinkmann (MT): *Averaging on Lie Groups: Applications of Geodesic Averages and Biinvariant Means*\\n* Joel Gächter (MT): *Subdivision-Based Clothoids in Autonomous Driving*\\n\\n## Features\\n\\n* Motion planning algorithms: [GLC](src/main/java/ch/ethz/idsc/owl/glc/std/StandardTrajectoryPlanner.java), and [RRT*](src/main/java/ch/ethz/idsc/owl/rrts/core/DefaultRrts.java)\\n* integrators: Euler, Midpoint, Runge-Kutta 4-5th order, exact integrator for the group SE2\\n* state-space models: car-like, two-wheel-drive, pendulum-swing-up, Lotka-Volterra, etc.\\n* efficient heuristic for goal regions: sphere, conic section\\n* visualizations and animations, see [video](https://www.youtube.com/watch?v=lPQW3GqQqSY)\\n\\n## Motion Planning\\n\\n### GLC\\n\\nRice2: 4-dimensional state space + time\\n\\n<table>\\n<tr>\\n<td>\\n\\n![rice2dentity_1510227502495](https://user-images.githubusercontent.com/4012178/32603926-dd317aea-c54b-11e7-97ab-82df23b52fa5.gif)\\n\\n<td>\\n\\n![rice2dentity_1510234462100](https://user-images.githubusercontent.com/4012178/32608146-b6106d1c-c55b-11e7-918d-e0a1d1c8e400.gif)\\n\\n</tr>\\n</table>\\n\\n---\\n\\nSE2: 3-dimensional state space\\n\\n<table>\\n<tr>\\n<td>\\n\\nCar\\n\\n![se2entity_1510232282788](https://user-images.githubusercontent.com/4012178/32606961-813b05a6-c557-11e7-804c-83b1c5e94a6f.gif)\\n\\n<td>\\n\\nTwo-wheel drive (with Lidar simulator)\\n\\n![twdentity_1510751358909](https://user-images.githubusercontent.com/4012178/32838106-2d88fa2c-ca10-11e7-9c2a-68b34b1717cc.gif)\\n\\n</tr>\\n</table>\\n\\n---\\n\\nSimulation: autonomous gokart or car\\n\\n<table>\\n<tr>\\n<td>\\n\\nGokart\\n\\n![_1530775215911](https://user-images.githubusercontent.com/4012178/42308510-10283bf0-8036-11e8-8a42-b8f1f807bb88.gif)\\n\\n<td>\\n\\nCar\\n\\n![_1530775403211](https://user-images.githubusercontent.com/4012178/42308523-1ae4ea8e-8036-11e8-8067-83bdd67a2d33.gif)\\n\\n</tr>\\n</table>\\n\\n\\n### RRT*\\n\\nR^2\\n\\n![r2ani](https://cloud.githubusercontent.com/assets/4012178/26282173/06dccee8-3e0c-11e7-930f-fedab34fe396.gif)\\n\\n![r2](https://cloud.githubusercontent.com/assets/4012178/26045794/16bd0a54-394c-11e7-9d11-19558bc3be88.png)\\n\\n### Nearest Neighbors\\n\\n<table>\\n<tr>\\n<td>\\n\\n![nearest_r2](https://user-images.githubusercontent.com/4012178/64911097-dc351300-d71d-11e9-9a92-5ce1fcd8c42f.png)\\n\\nR^2\\n\\n<td>\\n\\n![nearest_dubins](https://user-images.githubusercontent.com/4012178/64911102-e7883e80-d71d-11e9-96d2-11273b892775.png)\\n\\nDubins\\n\\n<td>\\n\\n![nearest_clothoid](https://user-images.githubusercontent.com/4012178/64911109-f242d380-d71d-11e9-83cf-358a4047175b.png)\\n\\nClothoid\\n\\n</tr>\\n</table>\\n\\n## Integration\\n\\nSpecify `repository` and `dependency` of the owl library in the `pom.xml` file of your maven project:\\n\\n```xml\\n<repositories>\\n  <repository>\\n    <id>owl-mvn-repo</id>\\n    <url>https://raw.github.com/idsc-frazzoli/owl/mvn-repo/</url>\\n    <snapshots>\\n      <enabled>true</enabled>\\n      <updatePolicy>always</updatePolicy>\\n    </snapshots>\\n  </repository>\\n</repositories>\\n\\n<dependencies>\\n  <dependency>\\n    <groupId>ch.ethz.idsc</groupId>\\n    <artifactId>owl</artifactId>\\n    <version>0.5.9</version>\\n  </dependency>\\n</dependencies>\\n```\\n\\n## Contributors\\n\\nJan Hakenberg, Jonas Londschien, Yannik Nager, André Stoll, Joel Gaechter\\n\\n> The code in the repository operates a heavy and fast robot that may endanger living creatures. We follow best practices and coding standards to protect from avoidable errors.\\n\\n## Publications\\n\\n* *What lies in the shadows? Safe and computation-aware motion planning for autonomous vehicles using intent-aware dynamic shadow regions*\\nby Yannik Nager, Andrea Censi, and Emilio Frazzoli,\\n[video](https://www.youtube.com/watch?v=3w6zQF9HOAM)\\n\\n## References\\n\\n* *A Generalized Label Correcting Method for Optimal Kinodynamic Motion Planning*\\nby Brian Paden and Emilio Frazzoli, \\n[arXiv:1607.06966](https://arxiv.org/abs/1607.06966),\\n[video](https://www.youtube.com/watch?v=4-r6Oi8GHxc)\\n* *Sampling-based algorithms for optimal motion planning*\\nby Sertac Karaman and Emilio Frazzoli,\\n[IJRR11](http://ares.lids.mit.edu/papers/Karaman.Frazzoli.IJRR11.pdf)\\n\\n---\\n\\n![ethz300](https://user-images.githubusercontent.com/4012178/45925071-bf9d3b00-bf0e-11e8-9d92-e30650fd6bf6.png)\\n\\n# ch.ethz.idsc.sophus <a href=\"https://travis-ci.org/idsc-frazzoli/owl\"><img src=\"https://travis-ci.org/idsc-frazzoli/owl.svg?branch=master\" alt=\"Build Status\"></a>\\n\\nLibrary for non-linear geometry computation in Java\\n\\n![sophus](https://user-images.githubusercontent.com/4012178/64911180-9f1d5080-d71e-11e9-9490-ae484d0399f3.png)\\n\\nThe library was developed with the following objectives in mind\\n* trajectory design for autonomous robots\\n* suitable for use in safety-critical real-time systems\\n* implementation of theoretical concepts with high level of abstraction\\n\\n<table>\\n<tr>\\n<td>\\n\\n![curve_se2](https://user-images.githubusercontent.com/4012178/47631757-8f693d80-db47-11e8-9c00-7796b07c48fc.png)\\n\\nCurve Subdivision\\n\\n<td>\\n\\n![smoothing](https://user-images.githubusercontent.com/4012178/47631759-91cb9780-db47-11e8-9dc7-a2631a144ecc.png)\\n\\nSmoothing\\n\\n<td>\\n\\n![wachspress](https://user-images.githubusercontent.com/4012178/62423041-7c7a2f80-b6bc-11e9-874e-414ae13be3ab.png)\\n\\nWachspress\\n\\n<td>\\n\\n![dubinspathcurvature](https://user-images.githubusercontent.com/4012178/50681318-5d72cc80-100b-11e9-943e-e168d0463eca.png)\\n\\nDubins path curvature\\n\\n</tr>\\n</table>\\n\\n## Features\\n\\n* geodesics in Lie-groups and homogeneous spaces: Euclidean space `R^n`, special Euclidean group `SE(2)`, hyperbolic half-plane `H2`, n-dimensional sphere `S^n`, ...\\n* parametric curves defined by control points in non-linear spaces: `GeodesicBSplineFunction`, ...\\n* non-linear smoothing of noisy localization data `GeodesicCenterFilter`\\n* Dubins path\\n\\n### Geodesic DeBoor Algorithm\\n\\n![loops5](https://user-images.githubusercontent.com/4012178/51076078-3c0d8280-1694-11e9-9857-2166598c09b2.png)\\n\\nB-Spline curves in `SE(2)` produced by DeBoor Algorithm or curve subdivision produce curves in the planar subspace `R^2` with appealing curvature.\\n\\n### Smoothing using Geodesic Averages\\n\\n![smoothing](https://user-images.githubusercontent.com/4012178/51090026-283a4d00-1776-11e9-81d3-aae3e34402f1.png)\\n\\nThe sequence of localization estimates of a mobile robot often contains noise.\\nInstead of using a complicated extended Kalman filter, geodesic averages based on conventional window functions denoise the uniformly sampled signal of poses in `SE(2)`.\\n\\n### Curve Decimation in Lie Groups\\n\\n![curve_decimation](https://user-images.githubusercontent.com/4012178/64847671-cf29fe00-d60f-11e9-8993-9f5549388ceb.png)\\n\\nThe pose of mobile robots is typically recorded at high frequencies.\\nThe trajectory can be faithfully reconstructed from a fraction of the samples. \\n\\n### Visualization of Geodesic Averages\\n\\n![deboor5](https://user-images.githubusercontent.com/4012178/51075948-ade4cc80-1692-11e9-9c9a-1e75084df796.png)\\n\\nA geodesic average is the generalization of an affine combination from the Euclidean space to a non-linear space.\\nA geodesic average consists of a nested binary averages.\\nGenerally, an affine combination does not have a unique expression as a geodesic average.\\nInstead, several geodesic averages reduce to the same affine combination when applied in Euclidean space. \\n\\n## Contributors\\n\\nJan Hakenberg, Oliver Brinkmann, Joel Gächter\\n\\n## Publications\\n\\n* *Curve Subdivision in SE(2)*\\nby Jan Hakenberg,\\n[viXra:1807.0463](http://vixra.org/abs/1807.0463),\\n[video](https://www.youtube.com/watch?v=2vDciaUgL4E)\\n* *Smoothing using Geodesic Averages*\\nby Jan Hakenberg,\\n[viXra:1810.0283](http://vixra.org/abs/1810.0283),\\n[video](https://www.youtube.com/watch?v=dmFO72Pigb4)\\n* *Curve Decimation in SE(2) and SE(3)*\\nby Jan Hakenberg,\\n[viXra:1909.0174](http://vixra.org/abs/1909.0174)\\n\\n## References\\n\\n* *Bi-invariant Means in Lie Groups. Application to Left-invariant Polyaffine Transformations.* by Vincent Arsigny, Xavier Pennec, Nicholas Ayache\\n* *Exponential Barycenters of the Canonical Cartan Connection and Invariant Means on Lie Groups* by Xavier Pennec, Vincent Arsigny\\n* *Lie Groups for 2D and 3D Transformations* by Ethan Eade\\n* *Manifold-valued subdivision schemes based on geodesic inductive averaging* by Nira Dyn, Nir Sharon\\n* *Power Coordinates: A Geometric Construction of Barycentric Coordinates on Convex Polytopes* by Max Budninskiy, Beibei Liu, Yiying Tong, Mathieu Desbrun\\n\\n---\\n\\n![ethz300](https://user-images.githubusercontent.com/4012178/45925071-bf9d3b00-bf0e-11e8-9d92-e30650fd6bf6.png)\\n'},\n",
       " {'repo': 'kontur-web-courses/space-y',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# SpaceY\\n\\nВ задании будем делать серверную часть и клиенсткие запросы для приложения SpaceY. Приложение представляет собой SPA (Single Page Application), которое умеет работать без перезагрузки страниц браузером.\\n\\nВ SPA серверу достаточно возвращать единственную html страницу, на которой подключаются скрипты и стили приложения.\\n\\nЭти скрипты определят, какой контент нужно показать пользователю, сделают запросы к серверу, получат данные и сформируют страницу \"на лету\".\\n\\nВсе запросы на сервер приложение будет посылать через файл `client.mjs`.\\n\\nКод приложения лежит в папке `spa`, собранная версия уже лежит в папке `spa/build`. Для выполнения задания трогать код приложения не потребуется, но если захочешь что-то поменять, не забудь установить зависимости (`npm install` в папке `spa`) и собрать новую версию приложения (`npm run build`).\\n\\n0. Поставь зависимости и запусти сервер.\\n\\n- Для этого перейди в директорию задачи и выполни команду `npm install`.\\n- После установки зависимостей, выполни команду `npm run start`.\\n- После запуска, перейди по адресу [localhost:3000](http://localhost:3000)\\n\\n1. Сделай так, чтобы сервер смог отдавать статические файлы из директории `spa/build`. В express для этого есть middleware `express.static`. Подробнее можно прочитать [здесь](https://expressjs.com/en/starter/static-files.html)\\n\\n2. Сделай так, чтобы при заходе на любой неизвестный адрес, сервер возвращал файл `spa/build/index.html`. В этом помогут специальные символы [в путях](https://expressjs.com/en/guide/routing.html#route-paths)\\n\\n3. Сделай так, чтобы наш сайт работал по https. В этом поможет [этот небольшой пост](https://timonweb.com/posts/running-expressjs-server-over-https/). Сертификат уже сгенерирован и лежит в папке `/certs`.\\n\\nОбрати внимание, что придётся разрешить Chrome работать с само-подписанными сертификатами для localhost. Это можно сделать включив флаг `chrome://flags/#allow-insecure-localhost`.\\n\\n4. Изучи файл `client.mjs`. В нём лежит заготовка клиента, который будет делать запросы на сервер.\\n\\nСделай так, чтобы работали методы, работы с пользователем (`.getUser()`, `.loginUser()`, `.logoutUser()`). На этом этапе имя пользователя можно хранить на сервере.\\n\\nВсе адреса, по которым этот клиент будет слать запросы лучше начинать с `/api/...`, чтобы показать, что они являются частью API, к которому делают AJAX запросы.\\n\\nЕсли в методе `.loginUser()` будешь посылать имя пользователя в теле запроса, то не забудь подключить `express.json` [middleware](https://expressjs.com/en/4x/api.html#express.json) или `body-parser` [middleware](https://expressjs.com/en/resources/middleware/body-parser.html).\\n\\nОтправлять ответ можно с помощью [res.json](https://expressjs.com/en/4x/api.html#res.json).\\n\\n5. Сохрани имя пользователя в [cookie](https://expressjs.com/en/4x/api.html#req.cookies) (не забудь подключить `cookie-parser` [middleware](https://expressjs.com/en/resources/middleware/cookie-parser.html)).\\n\\nСделай так, чтобы методы `.getUser()`, `.loginUser()`, `.logoutUser()` работали с cookie\\n\\n6. Сделай так, чтобы cookie с именем пользователя была `HttpOnly`, `Secure`, и имела `SameSite` политику `Strict`. В этом помогут дополнительные опции [res.cookie](https://expressjs.com/en/4x/api.html#res.cookie).\\n\\n7. Сделай так, чтобы при заходе на любой роут приложения, кроме api, статики и `/login` без cookie происходил редирект на страницу `/login`.\\n\\nДля этого придётся написать `middleware` и проверять наличие cookie в запросе. Как написать узнай [здесь](https://expressjs.com/en/guide/writing-middleware.html).\\n\\nСделай так, чтобы middleware применялось только для путей, которые непосредственно отдают `index.html`\\n\\n8. Оживи остальные страницы кроме `/sendToMars`. А именно `About`, `History`, `Rockets`, `Roadster`.\\n\\nВ качестве источника данных используй [публичное API](https://docs.spacexdata.com/). Методы в нём названы похожим образом.\\n\\nПосылать с сервера запросы к публичному API можно с помощью [https.request](https://nodejs.org/api/https.html#https_https_request_url_options_callback) или, как в браузере, с помощью `fetch`. Так этот это браузерный стандарт, его нет в стандартной библиотеке node.js, но существует реализация [node-fetch](https://github.com/bitinn/node-fetch), которая уже подключена в качестве зависимости в `package.json`.\\n\\nПо возможности, не запрашивай лишних данных из API и не возвращай лишних данных на клиент.\\n\\nФормат данных, который ожидает клиент описан в файле `client.mjs` в формате [JSDoc](https://jsdoc.app/).\\n\\n9. \\\\* Оживи страницу `/sendToMars`. Объект каждого предмета посылай в теле запроса в формате json. Для того, чтобы прочитать и распарсить тело запроса придётся подключить `express.json` [middleware](https://expressjs.com/en/4x/api.html#express.json). Храни данные в памяти на сервере, придумай как гарантировать уникальность полей `id` для каждого предмета.\\n'},\n",
       " {'repo': 'Boris-Barboris/AtmosphereAutopilot',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': 'AtmosphereAutopilot\\n===================\\n\\nPlugin for Kerbal Space Program.\\n\\nOriginal author: Boris-Barboris.\\n\\nContributors:\\n* radistmorse (aka Morse on KSP forum) - Neo-GUI design and implementation.\\n* CraigCottingham - Cruise flight and speed control GUI refactoring, coordinate input to waypoint mode.\\n* Hotel26 - usability fixes for old-GUI Cruise flight waypoint control.\\n* Boop from KSP forum - vessel.LandedOrSplashed bug squasher.\\n\\nLicense: GNU GPL version 3\\n\\n# Dependencies\\n\\nModuleManager.\\n\\n# For developers\\n\\n## How to build:\\nYou need to build two dlls: AtmosphereAutopilot.UI.dll and AtmosphereAutopilot.dll. Both can be built from MS Visual studio on Windows using AtmosphereAutopilot.sln.   \\nOn Linux you need to run build.sh from root directory, wich requires the following packages:\\n* monodevelop (look for msbuild binary availability)\\n* zip\\nResults will be in AtmosphereAutopilot/bin/Release folder.\\n\\n# General description\\nAtmosphere autopilot is a modular atmospheric flight control system library. It\\'s meant to be a foundation for multiple high-level programs - \"Autopilots\", wich will aid KSP player in one way or another, implying atmospheric flight. Autopilots are mutually exclusive - only one or none at all may be active at the active vessel at one given moment. They are provided by the library with means of automatic reflection-based serialization\\\\deserialization and ugly, but lazy and customizable GUI interaction.\\n\\nAutopilots are modular entities. They can use basic, provided by main library components (like controllers and models), or they can define their own components and share them with other Autopilots. Those components will be called \"Autopilot modules\", or simply - \"Modules\". Every sealed child of AutopilotModule wich is not a StateController is treated by the library like a Module. Sealed StateController children are treated as Autopilots.\\n\\nStock and FAR aerodynamics are supported.\\n\\n# GUI concept\\nAA icon is placed in Application Launcher toolbar during flight. It\\'s contents will visualize a list of all Autopilots and Modules, created for active vessel. For every vessel \"Autopilot Module Manager\" will be created regardless. Turning on a \"MASTER SWITCH\" on it\\'s window will create required context of Autopilots and Modules for this vessel. Under the master switch all Autopilots will be listed, for the user to choose one of them as an active one. Hotkey for Master switch is letter P, autoPilot. Can be changed in Global_settings.cfg file, Autopilot_module_manager section.\\n\\nCraft settings window contains shotrcuts to most used moderation and tuning parameters of the craft, as well as provides basic preset functionality. Presets are saved in \"Global_settings.cfg\"/settings_wnd/profiles section.\\n\\nEach Autopilot and Module has it\\'s own GUI window. All of them (even inactive ones) are accessible from AA button in Application Launcher, some of them are accessible from Autopilot window hierarchy (that\\'s up to Autopilot developer to decide, what particular Modules should be accessible from it\\'s GUI). Window positions are serialized (preserved between flights and game sessions) in \"Global_settings.cfg\" file.\\n\\n# Neo-GUI\\nAlternative, more condensed but less powerfull way of representing AppLauncher window can be turned on by setting AtmosphereAutopilot/use_neo_gui to _true_ in Global_settings.txt config file. It is read every scene change, so the shift can be made without shutting KSP down. While it\\'s active, \"Autopilot Module Manager\" is still accessible using hotkeys. Standard GUI has logical priority over Neo-GUI.\\n\\n# Hotkeys\\n\"Hotkey manager\" window is placed into Application Launcher window list. It\\'s contents are registered hotkeys, wich can be changed during runtime.\\nThere are two main hotkeys: \\n* \"Master switch\" - toggles Master Switch.\\n* Shift + \"Master switch\" - toggles GUI of \"Autopilot Module Manager\".\\n\\nOthers are very module-specific and will not be described here.\\n\\n# Craft implications and limitations\\n\"Control from here\" part is facing prograde, with close-to-zero angle of attack bias. Planar symmetry is implied (left and right side of the plane are mirrored), as well as good degree of pitch-yaw and pitch-roll control isolation. Axial engine symmetry is strongly recommended. No wind mods are supported, as well as any mods, wich are changing control surface, rcs and engine gimbaling behaviour.\\n\\n**WARNING: DO NOT USE AEROBRAKES AS CONTROL SURFACES, USE THEM ONLY AS BRAKES!**\\n\\n# Default Autopilots descriptions\\n\\n## Standard Fly-By-Wire\\nIn general, FBW (Fly-By-Wire) is an abstraction Autopilot. It is designed to aid in player-controlled flight on generic (space)plane, providing a soft layer between user joystick\\\\keyboard input and control surface outputs.\\nMain goals:\\n* Auto-trimming.\\n* AoA and G-force moderation.\\n* Sideslip handling.\\n* Fighting oscillations.\\n\\nFBW uses three controllers - pitch, roll and yaw. Pitch is handled by \"Pitch ang vel controller\", roll by \"Roll ang vel controller\" and yaw is handled by \"Sideslip controller\" in plane mode, or directly by \"Yaw ang vel controller\" in \"Rocket mode\". In Rocket mode pitch and yaw axes are treated the same - it\\'s usefull in case player wants to use FBW for rocket launches. FBW is effective only on small (<25 degrees) AoA values, though control is possible on all regimes. It\\'s just that it\\'s quality will degrade from inadequacy of linearization assumptions. \"Moderation\" button is toggling all pitch and yaw moderations - usefull for low speed VTOL action or for fighting overmoderation bugs. Pitch moderation is turned off for 2 seconds after taking-off to prevent overmoderation-related crashes.\\n\\n\"Coordinated turn\" - pseudo-pitch hold to assist in performing coordinated turns.\\n\\nHotkeys: \\n* \"FBW moderation\" - default hotkey for Moderation is letter O, mOderation.\\n* \"FBW rocket mode\" - default hotkey unassigned.\\n* \"FBW coord turn\" - default hotkey unassigned.\\n\\nSpeed control - throttle automation to maintain speed setpoint. Handeled by \"Prograde thrust controller\".\\n\\n## Mouse Director\\nMouse Director (MD) is declarative autopilot, crafted with idea to let the user to define desired airspeed direction with camera position. Autopilot then tries to comply with this surface-relative velocity setpoint. MD is inherently-linear, so only relatively small angles of attack are allowed. All AoA moderations are forcefully turned on during it\\'s work.\\n\\nMD uses \"Director controller\", wich uses two AoA controllers: pitch \"AoA controller\" and yaw \"Sideslip controller\", and \"Roll ang vel controller\" for roll. Currently, planar asymmetry of a plane is not taken into account (sideslip noise is still too noticeable in zero-lift convergence problem), sideslip is always at zero setpoint. If your craft requires nonzero sideslip to fly straight, MD is not a very good solution right now, use FbW in the _rocket mode_.\\n\\nShort GUI description:\\n\\nSpeed control - throttle automation to maintain speed setpoint. Handeled by \"Prograde thrust controller\".\\n\\n## Cruise Flight controller\\nCruise Flight (CF) is high-level autopilot, designet for travel automation. Just like MD, CF is inherently-linear, so only relatively small angles of attack are allowed. All AoA moderations are forcefully turned on during it\\'s work.\\n\\nCF uses \"Director controller\" for controlling velocity vector and \"Prograde thrust controller\" for throttle automation.\\nFunctions:\\n* Simple leveling.\\n* Baromethric height and airspeed control.\\n* Primitive waypoint functionality, picking point on planet surface (mouse click) on the map and flying to it.\\n\\nShort GUI description:\\n* _Level_ - simple leveling regime. Upon activation, CF will save surface-relative inclination of velocity and will follow it. If altitude is not set, will keep vertical speed at zero.\\n* _Course_ - follows azimuth setpoint, set in field _desired course_. If altitude is not set, will keep vertical speed at zero. On high latitudes (>80 degrees) will switch to _Level_ mode.\\n* _Waypoint_ - primitive waypoint following. Designed for pick-and-fly functionality. When activated, _pick waypoint_ button appears under mode tabs, as well as waypoint latitude-longtitude representation and distance to it in straight line (through planet core). Waypoint control is turned off when destination is closer than 200 meters to be followed by _Level_ mode activation.\\n* _desired course_ - azimuth in degrees to follow in _Course_ mode.\\n* _Speed control_ - throttle automation to maintain speed setpoint. Handeled by \"Prograde thrust controller\\n* _Vertical motion control_ - activate altitude or vertical speed control. Otherwise vertical speed is kept at zero.\\n* _Altitude_ - hold altitude, meters above sea level.\\n* _Vertical speed_ - hold vertical speed, meters per second.\\n\\n\"Advanced options\" description:\\n* _pseudo-FLC_ - toggle for pseudo-FLC (Flight Level Change) control law for ascend. Will force CF to respect speed setpoint and craft thrust parameters when choosing ascent angle.\\n* _flc margin_ - default value 15 m/s. Span of pseudo-FLC algorithm relaxation region. Decrease if don\\'t want to tolerate errors in speed. Algorithm will not converge below some minimal value, so be careful.\\n* _strength mult_ - default value 0.75. Will be multiplied in the runtime on Director controller\\'s strength to restrain maneuvers. Tune to achieve slover or faster behaviour.\\n* _height relax time_ - default value 6.0 seconds. Time frame of proportional control law jurisdiction, related to relaxation behaviour. Tune to prevent overshooting, if really needed.\\n* _height relax Kp_ - gain for proportional law, decrease to slow down relaxation.\\n* _max climb angle_ - default value 30 degrees. Global limit on climb and drop maneuver velocity pitch. Will sometimes be exceeded, it\\'s okay.\\n* _use keys_ - use pitch and yaw keys to control course and altitude\\\\vertical speed setpoints. This flag is toggled by \"CF keys input mode\" hotkey.\\n* _hotkey course sens_ - tweak to manage course setpoint change speed.\\n* _hotkey altitude sens_ - tweak to manage altitude setpoint change speed.\\n* _hotkey vertspeed sens_ - tweak to manage vertical speed setpoint change speed.\\n* _hotkey vertspeed snap_ - tweak to manage vertical speed snap to zero margin.\\n\\nHotkeys:\\n* \"Pitch keys\" - alter vertical motion setpoint, altitude or vertical speed (whatever is active at the moment).\\n* \"Yaw keys\" - alter course setpoint.\\n* \"CF keys input mode\" - default hotkey is _Right Alt_, toggles whether Pitch and yaw is used to control setpoints.\\n* \"CF vertical control\" - toggles _Vertical motion control_.\\n* \"CF altitude\\\\vertical speed\" - toggles between _Altitude_ and _Vertical speed_ modes.\\n\\n# Default Modules descriptions\\n\\n## Flight Model\\nIt is a fundamental craft analysis module. It performs motion and dynamics evaluation, as well as analysis of craft aerodynamics. VTOL engine balancing is also handled by Flight Model (though it will probably change in the future). This Module will probably be used by every single other Autopilot and module.\\n\\nShort GUI description (consult source code for more deatils and insight):\\n* Three sections for three craft principal axes, each contains:\\n  * _ang vel_ - angular velocity of a craft as a mechanical system of rigid bodies, radians / second. Positive for pitch up, yaw right, roll right.\\n  * _ang acc_ - angular acceleration, produced by numerical diffirentiation.\\n  * AoA - angle of attack, degrees. Positive for pitch up, yaw right. For roll it\\'s the angle between wing chord and airspeed vector, projected on frontal plane.\\n* _has csurf_ - is true if Flight Model has found control surfaces on the craft. It is important for aerodynamics regressor to know it.\\n* Five \"trainers\", linear regressors. They are analyzing craft performance history and try (and fail horribly) to produce linear models of aerodynamic torques and forces. Their GUIs are filled with magic numbers you should never need to change.\\n* _balance engines_ - toggles engine-balancing algorithm for VTOLs. Has a hotkey.\\n* _balancer steering k_ - gain for attitude control using engines. Use zero to keep them static. Default value 1.\\n* _Lift acc_ - acceleration, provided by aerodynamic lift in the direction of plane spine.\\n* _Slide acc_ - acceleration, provided by aerodynamic lift in the direction of plane right wing.\\n* _sum acc_ - vector of total craft acceleration in PhysX reference frame.\\n* _pitch gravity acc_ - gravitational acceleration, projected on craft spine vector.\\n* _pitch engine acc_ - engines-induced acceleration, projected on craft spine vector.\\n* _pitch noninert acc_ - coriolis + centrifugal acceleration, projected on craft spine vector.\\n* _yaw gravity acc_ - gravitational acceleration, projected on craft right wing vector.\\n* _yaw engine acc_ - engines-induced acceleration, projected on craft right wing vector.\\n* _yaw noninert acc_ - coriolis + centrifugal acceleration, projected on craft right wing vector.\\n* _aoa virtual gain_ - default value 0.95. Gain of virtual rotation filter. Used to provide virtual craft rotation in case of interpart oscillations. 0.0 - pure control from part rotation. 1.0 - pure virtual.\\n* _MOI_ - moment of inertia of the craft.\\n* _CoM_ - center of mass of the craft in PhysX reference frame.\\n* _Vessel mass_ - self explanatory.\\n* _Reaction wheels_ - overall torque capability of reaction wheel systems.\\n* _RCS pos_ - estimated torque capability of RCS system when user input is positive.\\n* _RCS neg_ - estimated torque capability of RCS system when user input is negative.\\n* _e torque_ - engines-induced torque in craft principal reference frame.\\n* _e thrust_ - engines thrust in craft principal reference frame.\\n* two vectors on engine torque linear estimations. They are used to adress gimbaling capabilities of a craft.\\n\\nHotkeys:\\n* \"Thrust balancing\" - toggles _balance engines_ button.\\n\\n## Director controller\\nMiddle-level model-reference controller, follows a setpoint of surface velocity and acceleration vectors. Input: velocity vector and acceleration vector. Output: AoA, sideslip and roll angular velocity.\\n\\nShort GUI description:\\n* _strength_ - default value 0.95. Measure of agressiveness of acceleration output of MD. Precise control multiplies output acceleration by the factor of 0.4. Serialized per vessel design.\\n* _roll stop k_ - default value 1.0, used to prevent overshooting, magic number.\\n* _angular error_ - error in radians between desired velocity vector and current one.\\n* _max angular v_ - estimate on current maneuver maximum angular velocity.\\n* _stop time roll_ - estimate on 90-degrees bank maneuver stop time.\\n* _relaxation margin_ - default value 0.01 radians. Margin of relaxed acceleration output. Magic number. Increase to fight overshooting (rarely needed).\\n* _angle relaxation k_ - default value 0.1. Relaxation gain, magic number. Decrease to fight oscillations.\\n* _max neg g_ - default value 8.0. Maximum negative g-force tolerate. May be useful for players, who are using G-force effects mods. Serialized per vessel design.\\n* _min rollover alt_ - default value 150.0 meters. Under this terrain altitude setpoint rolling over to prevent large negative g-force will be forbiden to decrease probability of deadly maneovers.\\n* _desired pitch lift_ - desired lift-induced acceleration, projected on spinal vector.\\n* _desired pitch acc_ - desired total acceleration, projected on spinal vector.\\n* _desired pitch v_ - desired angular velocity for pitch, calculated from previous value.\\n* _allow spine down_ - global flag to allow turning spine down to prevent negative G-force.\\n* _roll acc factor_ - angular acceleration factor estimate of roll rotation model.\\n* _roll acc filter_ - default value 4.0. filter gain for smoothing _roll acc factor_ evolution noise.\\n* _roll cubic K_ - default value 0.3. Cubic descent gain for roll. Increase for faster roll control, decrease for lower overshooting and oscillations.\\n* _roll cubic relax frame_ - default value 10.0. Relaxation frame for cubic descent phase. Magic nubmer.\\n* _roll relax Kp_ - default value 0.1. Relaxation gain for roll.\\n* _roll error filter margin_ - default value 3.0. Margin for smoothing _roll angle_ oscillations. Magic number.\\n* _roll error filter k_ - default value 0.5. Filter gain for _roll angle_ smoothing on relaxation regime.\\n* _max roll v_ - estimate of constrained maximum roll angular velocity.\\n* _roll error_ - current bank error in radians.\\n* _roll_cubic_ - true when in cubic descent regime for roll.\\n* _snapping boundary_ - default vaulue 3 degrees. On low bank error modes we will transition from cubic relaxation to proportional relaxation (like in roll controller wing leveler code).\\n* _desired aoa_ - output to \"AoA controller\".\\n* _desired sideslip_ - output to \"Sideslip controller\".\\n\\n## Pitch, roll and yaw angular acceleration controllers\\nLow level dynamics inversion angular acceleration controllers. Input: desired angular acceleration (and yaw output for roll controller). Output: pitch\\\\roll\\\\yaw control state.\\n\\nShort GUI description:\\n* _Csurf output_ - current expected virtual control surface position, wich is usually lagged from control signal.\\n* _write telemetry_ button - primitive logging capability for further matlab analysis. .csv logs are saved in KSP\\\\Resources directory to be read by plotter.m viewer. It is a debug utility.\\n* _desired acc_ - desired acceleration, passed to this controller from above.\\n* _model predicted acc_ - predicted by model acceleration for the next frame.\\n* _authority_ - linear axial authority, complicated thing, do not bother. Should always be positive though.\\n* _angular acc_ - angular acceleration, duplicate of Flight Model _ang acc_ field.\\n* _output_ - control state output, is passed to vessel in FlightCtrlState object.\\n\\n## Pitch and yaw angular velocity controllers\\nModel-reference controllers, that perform pitch and yaw angular velocity control with respect to moderation and controllability restrictions. Input: [-1, 1] user input or desired angular velocity. Output: desired angular acceleration, passed to angular acceleration controller.\\n\\nWhen navball is in surface mode, controller is dealing with surface-oriented reference frame. Zero input will keep zero angular velocity relative to the ground - useful on planes. In orbit navball mode inertial reference frame will be used - usefull for spacecrafts. Precision mode (CAPS LOCK) multiplies input by the factor of 0.33 (_precision mode factor_ option in global_settings.txt) to provide more precise control, or to aid with control on high physical warp regimes. To ignore precision mode, unser _watch precision mode_ toggle in respected ang vel controllers.\\n\\nShort GUI description:\\n* _Auto trim_ button - turn on of you want control trim to preserve after controller shutdown. Off by default.\\n* _max\\\\min input aoa_ - estimated maximum angle of attack (radians), achievable by locking control to 1.0 or -1.0. When craft is statically unstable, this value is 0.6 of the controllability region boundary - it helps to stay reliable on unstable planes.\\n* _max\\\\min input v_ - equilibrium angular velocities on max\\\\min input aoa flight regimes.\\n* _max\\\\min g aoa_ - estimated maximum angle of attack considering g-force moderation.\\n* _max\\\\min g v_ - respective equilibrium angular velocities.\\n* _max\\\\min aoa v_ - equlibrium angular velocities for set by user AoA limit.\\n* _moder filter_ - default value - 3.0. Used to filter out rapid changes or oscillations in flight model to provide more smooth boundary condition evolution. Magic number.\\n* _quadr Kp_ - default value - 0.3. Contoller uses parabolic descent model of angular velocity to it\\'s desired value. Those descent parameters are governed by this koefficient. Larger values may cause overshoot from wrong control surface lag handling. Lower values will slow down control. Magic number.\\n* _kacc quadr_ - parabollic steepness of control, governed by control utilities authority and craft characteristics. Should be positive.\\n* _kacc smoothing_ - default value - 10.0. Filter gain for slow and smooth \"kacc quadr\" evolution. Magic number.\\n* _relaxation k_ - default value - 1.0. Controller uses relaxed linear descent on very small velocity error regimes. This koefficient governs relaxation frame size.\\n* _relaxation Kp_ - default value - 0.5. Relaxation gain itself.\\n* _relaxation frame_ - default value - 1. How many velocity frames will be averaged as current angular velocity. This is an old deprecated way of fighting oscillations, keep it 1.\\n* _relax count_ - for how many frames velocity is in relaxation state.\\n* _transit max v_ - very rough, but safe estimation of maximum non-overshooting velocity in transit maneuver (from 0.0 AoA to maximum AoA).\\n* _res max\\\\min aoa_ - AoA limits, that will actually govern controller in the current frame. Are chosed as the most strict ones from previously listed.\\n* _res max\\\\min v_ - respective equilibrium angular velocities.\\n* _scaled aoa_ - how far away current AoA is from it\\'s limit.\\n* _scaled restr v_ - result of moderation algorithm.\\n* _Moderate AoA_ button - toggle angle of attack moderation. Is necessary for safe flight, but can be turned off, for example, during re-entry to provide maximum-drag profile. Required to be ON, if this controller is governed by upper-level AoA controller.\\n* _Moderate G-force_ button - toggle G-force moderation. Moderates centifugal acceleration of trajectory, not the full one, so G-meeter on navball will sometimes exceed maximum value (it is a correct behaviour).\\n* _max AoA_ - default value - 15.0 degrees. User-entered AoA limit. Recommended values [5.0, 25.0]. Serialized on per-design basis.\\n* _max G-force_ - default value - 10.0 g\\'s. Self-explanatory. Serialized on per-design basis.\\n* _angular vel_ - current angular velocity of a craft in corresponding axis.\\n* _output acceleration_ - output, produced by controller.\\n* _input deriv limit_ - default value - 5.0. Artificial inertia gain. User input derivative is clamped by this value. Decrease for more inertia, increase for sharpness. Serialized globally.\\n* _prev input_ - previous controller input.\\n* _Max v construction_ - default value - 0.5 (rad/sec). Global angular velocity restriction. Is introduced to provide comfortable control by limiting vessel rotation capabilities. 0.5 is good for most crafts. Serialized on per-design basis.\\n* _desired v_ - desired angular velocity, not yet processed by moderation.\\n\\n## Roll angular velocity controllers\\nModel-reference controller, that perform roll angular velocity control and wing leveling. Input: [-1, 1] user input or desired angular velocity. Output: desired angular acceleration, passed to angular acceleration controller.\\n\\nPrecision mode (CAPS LOCK) divides input by the factor of 3 to provide more precise control, or to aid with control on high physical warp regimes.\\n\\nShort GUI description (except identical from previous module):\\n* _Wing leveler_ - toggle to level wings automaticly, if craft is close to zero bank angle. Zero angle is not horizontal one, but the normal one to the trajectory plane - good for leveling on non-zero pitch while yawing.\\n* _Snap angle_ - default value - 3.0 degrees. Decides, when to activate wing leveler.\\n* _angle btw hor_ - when wings are close to snapped state, this is the angle in radians. Is needed if snap angle is large and sin(x)<>x.\\n* _angle btw hor sin_ - sinus of the horizont angle.\\n* _snapping Kp_ - snapping speed gain. Default avlue - 0.25. Larger values seem to be too agressive, too large oscillate.\\n\\n## AoA and Sideslip controllers\\nModel-reference controllers with self-explanatory names. Input: [-1, 1] user input or desired AoA. Output: desired angular velocity. Both require respective angular velocity controllers to have AoA moderation on, because it uses respective angular velocity controller limitation values as governers.\\n\\nShort GUI description:\\n* _AoA_ - respective angle of attack in radians.\\n* _desired aoa_ - processed by controller input in radians.\\n* _output v_ - controller output.\\n* _desired aoa equilibr v_ - equilibrium angular velocity on desired angle of attack. For example, nosedive angular velocity on nose-heavy plane, wich will keep AoA at zero.\\n* _filter k_ - filter gain to smooth changes in equilibrium v estimation. Default value - 4.0.\\n* _relaxation frame_ - relaxation frame count, used for close-to desired behaviour. Default value - 2.\\n* _relaxation factor_ - default value 0.1. Proportional gain of relaxation smoothing.\\n* _cubic barrier_ - default value 1.0 seconds. AoA controller uses quadratic descent profile for long evolutions and cubic for short (less than \"cubic barrier\" seconds). Used to prevent overshooting.\\n* _cubic KP_ - default value 0.3. Gain for cubic profile steepness evaluation.\\n* _cubic mode_ - true if controller is now in cubic mode.\\n\\n## Prograde thrust controller\\nHybrid model-reference or PID controller. Input: desired surface velocity. Output: throttle. Can be switched to PID control and manually tuned, if user is not satisfied with it\\'s performance.\\n\\nShort GUI description:\\n* _pid Kp_ - if used in PID mode, it\\'s the proportional PID gain.\\n* _pid Ki_ - integrad PID gain.\\n* _pid Kd_ - derivative PID gain.\\n* _desired v_ - self explanatory.\\n* _current v_ - self explanatory.\\n* _break spd margin_ - when surface speed is exceeding desired by this margin, brakes will be used. On groud breaks are used without margin.\\n* _Use breaks_ - controller is using \"Breaks\" action group.\\n* _prograde thrust_ - thrust vector projection on prograde direction.\\n* _Kp v_ - proportional gain on velocity error. Default value - 0.5, e.g. on 1 m/s error it will be 0.5 m/s^2 desired acceleration. Decrease if don\\'t like overshooting on very slow jets.\\n* _acc filter k_ - default value 1. Filter gain for acceleration moving average. Magic number.\\n* _relaxation acc error_ - default value 0.1 m/s^2. Error margin for filter activation. Magic number.\\n* _use PID_ - toggle if you want to manually tune controller, or using strange engines.\\n* _hotkey_speed_factor_ - tweak to change throttle hotkey sensitivity.\\n* _use_throttle_hotkeys_ - toggle speed setpoint handling by hotkeys.\\n\\nHotkeys:\\n* \"Throttle keys\" - alter velocity setpoint by using stock throttle hotkeys (Shift and LCntrl by default).\\n* \"Speed control toggle\" - toggles speed control ON and OFF.\\n'},\n",
       " {'repo': 'cortesi/scurve',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"A collection of algorithms and visualisation tools related to space-filling\\ncurves.\\n\\n# See [binvis.io](http://binvis.io) for my more recent interactive binary visualisation tool\\n\\n# The Curves \\n\\nThe following traversals of all points in a space are supported (some are true\\nspace-filling curves, some are not):\\n    \\n- __hilbert__:    Hilbert curve\\n- __natural__:    A natural-order traversal of all points, where each co-ordinate is simply treated as a digit.\\n- __zigzag__:     A traversal of all points that zig-zags to ensure that each point differs from the previous point by a unit-offset in only one dimension.\\n- __zorder__:     Z-order curve\\n\\n# The Tools \\n\\n- __binvis__: Visualize binaries using space-filling curves.  \\n- __colorswatch__: Creates a swatch with a visual breakdown of the colours\\ncontained in a specified image.\\n- __cube__: Outputs a POV-Ray definition file for drawing 3-dimensional curves.\\n- __drawcurve__: Generates two dimensional lines-and-vertexes drawings of\\nspace-filling curves.\\n- __gray__: Prints a bit representation of the Gray codes of a specified bit\\nwidth.\\n- __testpattern__: Projects a 3-dimensional traversal of the RGB colour cube onto\\na specified two-dimensional curve.\\n\\n\\n# More info\\n\\nDevelopment on Scurve is usually spurred along by posts on my blog. Some of\\nscurve's features are documented and illustrated in the following posts:\\n\\n- [Portrait of the Hilbert Curve](http://corte.si/posts/code/hilbert/portrait/index.html) \\n- [Generating colour maps with space-filling curves](http://corte.si/posts/code/hilbert/swatches/index.html)\\n- [Hilbert Curve + Sorting Algorithms + Procrastination = ?](http://corte.si/posts/code/sortvis-fruitsalad/index.html)\\n\"},\n",
       " {'repo': 'Rynchodon/ARMS',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': \"# ARMS\\nARMS is a mod for [Space Engineers](http://www.spaceengineersgame.com/)\\nthat provides a simple interface for automated sequences of piloting actions\\nlike navigation, engagement, collision avoidance, docking, and more.\\n\\nIt also includes radar functionality for picking up non-broadcasting objects\\ndepending on their radar signature and distance. Radar information is\\ndistributed across antenna networks.\\n\\nPlease see the [steam page]\\n(http://steamcommunity.com/sharedfiles/filedetails/?id=363880940) for a full\\nfeature list.\\n\\n## Getting started\\nIf you'd simply like to use the mod, please subscribe to it via the Steam\\nWorkshop as you normally would and download [Load-ARMS](https://github.com/Rynchodon/Load-ARMS) and follow the readme instructions.\\n\\nTo work with a local development copy of this mod instead, follow the steps\\nlisted [on the wiki](https://github.com/Rynchodon/Autopilot/wiki/Developing-Autopilot).\\n\\n## Roadmap\\n\\nPlease see the [steam page]\\n(http://steamcommunity.com/sharedfiles/filedetails/?id=363880940) for the\\ncurrent Roadmap.\\n\\n## Contributing\\n\\nPlease submit bug reports and feature requests on the [steam page]\\n(http://steamcommunity.com/sharedfiles/filedetails/?id=363880940) and its related\\ndiscussions.\\n\\n## License\\nCC0 1.0 Universal (CC0 1.0), see LICENSE.\\n\"},\n",
       " {'repo': 'lance-gg/spaaace',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '![spaaace](https://cloud.githubusercontent.com/assets/3951311/21784604/ffc2d282-d6c4-11e6-97f0-0ada12c4fab7.gif)\\n\\n# Spaaace\\nAn online HTML5 multiplayer space shooter built with [Lance](http://lance.gg) game server\\n\\n[Play now!](http://spaaace.herokuapp.com)\\n'},\n",
       " {'repo': 'conceptdev/MonkeySpace',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': 'MonkeySpace 2012\\n================\\n\\nConference app for MonkeySpace 2012 in Boston.\\n\\nSee [monkeyspace.org](http://monkeyspace.org) for conference details.'},\n",
       " {'repo': 'hkchengrex/STCN',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"# STCN\\n\\n## Rethinking Space-Time Networks with Improved Memory Coverage for Efficient Video Object Segmentation\\n\\n[Ho Kei Cheng](https://hkchengrex.github.io/), Yu-Wing Tai, Chi-Keung Tang\\n\\nNeurIPS 2021\\n\\n[[arXiv]](https://arxiv.org/abs/2106.05210) [[PDF]](https://arxiv.org/pdf/2106.05210) [[Project Page]](https://hkchengrex.github.io/STCN/) [[Papers with Code]](https://paperswithcode.com/task/semi-supervised-video-object-segmentation)\\n\\n## Check out our new work on long-term VOS [XMem](https://github.com/hkchengrex/XMem)!\\n\\n![bmx](https://imgur.com/SIFq5c1.gif) ![pigs](https://imgur.com/nHvWuzi.gif)\\n\\n*News:* In the [YouTubeVOS 2021 challenge](https://youtube-vos.org/challenge/2021/leaderboard/), STCN achieved 1st place accuracy in novel (unknown) classes and 2nd place in overall accuracy. Our solution is also fast and light.\\n\\nWe present Space-Time Correspondence Networks (STCN) as the new, effective, and efficient framework to model space-time correspondences in the context of video object segmentation.\\nSTCN achieves SOTA results on multiple benchmarks while running fast at 20+ FPS without bells and whistles. Its speed is even higher with mixed precision.\\nDespite its effectiveness, the network itself is very simple with lots of room for improvement. See the paper for technical details.\\n\\n**UPDATE (15-July-2021)**\\n\\n1. CBAM block: We tried without CBAM block and I would say that we don't really need it. For s03 model, we get -1.2 in DAVIS and +0.1 in YouTubeVOS. For s012 model, we get +0.1 in DAVIS and +0.1 in YouTubeVOS. You are welcome to drop this block (see `no_cbam` branch). Overall, the much larger YouTubeVOS seems to be a better evaluation benchmark for consistency.\\n\\n**UPDATE (22-Aug-2021)**\\n\\n2. Reproducibility: We have updated the package requirements below. With that environment, we obtained DAVIS J&F in the range of [85.1, 85.5] across multiple runs on two different machines.\\n\\n**UPDATE (27-Apr-2022)**\\n\\nMulti-scale testing code (as in the paper) has been added [here](https://github.com/hkchengrex/STCN/tree/ms/ms).\\n\\n\\n## What do we have here?\\n\\n1. [A gentle introduction](#a-gentle-introduction)\\n\\n2. [Quantitative results and precomputed outputs](#results)\\n    1. DAVIS 2016\\n    2. DAVIS 2017 validation/test-dev\\n    3. YouTubeVOS 2018/2019\\n\\n3. [Try our model on your own data (Interactive GUI available)](#try-your-own-data)\\n\\n4. Steps to reproduce\\n   1. [Pretrained models](#pretrained-models)\\n   2. [Inference](#inference)\\n   3. [Training](#training)\\n\\n5. [If you want to look closer](#looking-closer)\\n\\n6. [Citation](#citation)\\n\\n## A Gentle Introduction\\n\\n![framework](https://imgur.com/TY1ScRy.jpg)\\n\\nThere are two main contributions: STCN framework (above figure), and L2 similarity. We build affinity between images instead of between (image, mask) pairs -- this leads to a significantly speed up, memory saving (because we compute one, instead of multiple affinity matrices), and robustness. We further use L2 similarity to replace dot product, which improves the memory bank utilization by a great deal.\\n\\n### Perks\\n\\n- Simple, runs fast (30+ FPS with mixed precision; 20+ without)\\n- High performance\\n- Still lots of room to improve upon (e.g. locality, memory space compression)\\n- Easy to train: just two 11GB GPUs, no V100s needed\\n\\n## Requirements\\n\\nWe used these packages/versions in the development of this project. \\n\\n- PyTorch `1.8.1`\\n- torchvision `0.9.1`\\n- OpenCV `4.2.0`\\n- [Pillow-SIMD](https://github.com/uploadcare/pillow-simd) `7.0.0.post3`\\n- progressbar2\\n- [thinspline](https://github.com/cheind/py-thin-plate-spline) for training (`pip install git+https://github.com/cheind/py-thin-plate-spline`)\\n- gitpython for training\\n- gdown for downloading pretrained models\\n- [Other packages in my environment](docs/packages.txt), for reference only.\\n\\nRefer to the official [PyTorch guide](<https://pytorch.org/>) for installing PyTorch/torchvision, and the [pillow-simd](https://github.com/uploadcare/pillow-simd) guide to install Pillow-SIMD. The rest can be installed by:\\n\\n`pip install progressbar2 opencv-python gitpython gdown git+https://github.com/cheind/py-thin-plate-spline`\\n\\n## Results\\n\\n### Notations\\n\\n- FPS is amortized, computed as total processing time / total number of frames irrespective of the number of objects, aka multi-object FPS, and measured on an RTX 2080 Ti with IO time excluded.\\n- We also provide inference speed when Automatic Mixed Precision (AMP) is used -- the performance is almost identical. Speed in the paper are measured without AMP.\\n- All evaluations are done in the 480p resolution. FPS for test-dev is measured on the validation set under the same memory setting (every third frame as memory) for consistency.\\n\\n**[[Precomputed outputs - Google Drive]](https://drive.google.com/drive/folders/1V4wslwiGaFHwq09k019tXU1HpG-kODnZ?usp=sharing)**\\n\\n**[[Precomputed outputs - OneDrive]](https://uillinoisedu-my.sharepoint.com/:f:/g/personal/hokeikc2_illinois_edu/EtzYCTCKG1FBoeocJ7Q_tUUB3jivfQE_2DnrybPNC6dTRA?e=hD7IKi)**\\n\\n**s012 denotes models with BL pretraining while s03 denotes those without** (used to be called s02 in MiVOS).\\n\\n### Numbers (s012)\\n\\n| Dataset | Split |  J&F | J | F | FPS | FPS (AMP)\\n| --- | --- | :--:|:--:|:---:|:---:|:---:|\\n| DAVIS 2016 | validation | 91.7 | 90.4 | 93.0 | 26.9 | 40.8 |\\n| DAVIS 2017 | validation | 85.3 | 82.0 | 88.6 | 20.2 | 34.1 |\\n| DAVIS 2017 | test-dev | 79.9 | 76.3 | 83.5 | 14.6 | 22.7 |\\n\\n| Dataset | Split | Overall Score | J-Seen | F-Seen | J-Unseen | F-Unseen\\n| --- | --- | :--:|:--:|:---:|:---:|:---:|\\n| YouTubeVOS 18 | validation | 84.3 | 83.2 | 87.9 | 79.0 | 87.2 |\\n| YouTubeVOS 19 | validation | 84.2 | 82.6 | 87.0 | 79.4 | 87.7 |\\n\\n| Dataset | AUC-J&F | J&F @ 60s\\n| --- |:---:| :--:|\\n| DAVIS Interactive | 88.4 | 88.8 |\\n\\nFor DAVIS interactive, we changed the propagation module of [MiVOS](https://github.com/hkchengrex/MiVOS) from STM to STCN. See [this link](https://github.com/hkchengrex/MiVOS/tree/MiVOS-STCN) for details.\\n\\n## Try on your own data (Interactive GUI available)\\n\\nIf you (somehow) have the first-frame segmentation (or more generally, segmentation of each object when they first appear), you can use `eval_generic.py`. Check the top of that file for instructions.\\n\\nIf you just want to play with it interactively, I highly recommend [our extension to MiVOS](https://github.com/hkchengrex/MiVOS/tree/MiVOS-STCN) :yellow_heart: -- it comes with an interactive GUI, and is highly efficient/effective.\\n\\n## Reproducing the results\\n\\n### Pretrained models\\n\\nWe use the same model for YouTubeVOS and DAVIS. You can download them yourself and put them in `./saves/`, or use `download_model.py`.\\n\\ns012 model (better): [[Google Drive]](https://drive.google.com/file/d/1mRrE0uCI2ktdWlUgapJI_KmgeIiF2eOm/view?usp=sharing) [[OneDrive]](https://uillinoisedu-my.sharepoint.com/:u:/g/personal/hokeikc2_illinois_edu/Eav35v3GZIZFiq6dv9BM8n0BHtR1hD7QU9tcxH7hylG3dA?e=NXJpTu)\\n\\ns03 model: [[Google Drive]](https://drive.google.com/file/d/1JllXPJZola0G-g1aUQfXe9nLMBioj-aH/view?usp=sharing) [[OneDrive]](https://uillinoisedu-my.sharepoint.com/:u:/g/personal/hokeikc2_illinois_edu/EdhurrdqNTFIoK43NsboxOgBPTsMlJBoKOirflGUn-JqBA?e=8NATDz)\\n\\ns0 pretrained model: [[GitHub]](https://github.com/hkchengrex/STCN/releases/tag/1.0)\\n\\ns01 pretrained model: [[GitHub]](https://github.com/hkchengrex/STCN/releases/tag/1.0)\\n\\n### Inference\\n\\n- `eval_davis_2016.py` for DAVIS 2016 validation set\\n- `eval_davis.py` for DAVIS 2017 validation and test-dev set (controlled by `--split`)\\n- `eval_youtube.py` for YouTubeVOS 2018/19 validation set (controlled by `--yv_path`)\\n\\nThe arguments tooltip should give you a rough idea of how to use them. For example, if you have downloaded the datasets and pretrained models using our scripts, you only need to specify the output path: `python eval_davis.py --output [somewhere]` for DAVIS 2017 validation set evaluation. For YouTubeVOS evaluation, point `--yv_path` to the version of your choosing.\\n\\nMulti-scale testing code (as in the paper) has been added [here](https://github.com/hkchengrex/STCN/tree/ms/ms).\\n\\n### Training\\n\\n#### Data preparation\\n\\nI recommend either softlinking (`ln -s`) existing data or use the provided `download_datasets.py` to structure the datasets as our format. `download_datasets.py` might download more than what you need -- just comment out things that you don't like. The script does not download BL30K because it is huge (>600GB) and we don't want to crash your harddisks. See below.\\n\\n```bash\\n├── STCN\\n├── BL30K\\n├── DAVIS\\n│   ├── 2016\\n│   │   ├── Annotations\\n│   │   └── ...\\n│   └── 2017\\n│       ├── test-dev\\n│       │   ├── Annotations\\n│       │   └── ...\\n│       └── trainval\\n│           ├── Annotations\\n│           └── ...\\n├── static\\n│   ├── BIG_small\\n│   └── ...\\n├── YouTube\\n│   ├── all_frames\\n│   │   └── valid_all_frames\\n│   ├── train\\n│   ├── train_480p\\n│   └── valid\\n└── YouTube2018\\n    ├── all_frames\\n    │   └── valid_all_frames\\n    └── valid\\n```\\n\\n#### BL30K\\n\\nBL30K is a synthetic dataset proposed in [MiVOS](https://github.com/hkchengrex/MiVOS/#bl30k).\\n\\nYou can either use the automatic script `download_bl30k.py` or download it manually from [MiVOS](https://github.com/hkchengrex/MiVOS/#bl30k). Note that each segment is about 115GB in size -- 700GB in total. You are going to need ~1TB of free disk space to run the script (including extraction buffer).\\n\\n#### Training commands\\n\\n`CUDA_VISIBLE_DEVICES=[a,b] OMP_NUM_THREADS=4 python -m torch.distributed.launch --master_port [cccc] --nproc_per_node=2 train.py --id [defg] --stage [h]`\\n\\nWe implemented training with Distributed Data Parallel (DDP) with two 11GB GPUs. Replace `a, b` with the GPU ids, `cccc` with an unused port number,  `defg` with a unique experiment identifier, and `h` with the training stage (0/1/2/3).\\n\\nThe model is trained progressively with different stages (0: static images; 1: BL30K; 2: 300K main training; 3: 150K main training). After each stage finishes, we start the next stage by loading the latest trained weight.\\n\\n(Models trained on stage 0 only cannot be used directly. See `model/model.py: load_network` for the required mapping that we do.)\\n\\nThe `.pth` with `_checkpoint` as suffix is used to resume interrupted training (with `--load_model`) which is usually not needed. Typically you only need `--load_network` and load the last network weights (without `checkpoint` in its name).\\n\\n<details> \\n<summary>\\n\\nSo, to train a s012 model, we launch three training steps sequentially as follows:\\n\\n</summary>\\n\\nPre-training on static images: `CUDA_VISIBLE_DEVICES=0,1 OMP_NUM_THREADS=4 python -m torch.distributed.launch --master_port 9842 --nproc_per_node=2 train.py --id retrain_s0 --stage 0`\\n\\nPre-training on the BL30K dataset: `CUDA_VISIBLE_DEVICES=0,1 OMP_NUM_THREADS=4 python -m torch.distributed.launch --master_port 9842 --nproc_per_node=2 train.py --id retrain_s01 --load_network [path_to_trained_s0.pth]  --stage 1`\\n\\nMain training: `CUDA_VISIBLE_DEVICES=0,1 OMP_NUM_THREADS=4 python -m torch.distributed.launch --master_port 9842 --nproc_per_node=2 train.py --id retrain_s012 --load_network [path_to_trained_s01.pth]  --stage 2`\\n\\n</details>\\n\\n<details> \\n<summary>\\n\\nAnd to train a s03 model, we launch two training steps sequentially as follows:\\n\\n</summary>\\n\\nPre-training on static images: `CUDA_VISIBLE_DEVICES=0,1 OMP_NUM_THREADS=4 python -m torch.distributed.launch --master_port 9842 --nproc_per_node=2 train.py --id retrain_s0 --stage 0`\\n\\nMain training: `CUDA_VISIBLE_DEVICES=0,1 OMP_NUM_THREADS=4 python -m torch.distributed.launch --master_port 9842 --nproc_per_node=2 train.py --id retrain_s03 --load_network [path_to_trained_s0.pth]  --stage 3`\\n\\n</details>\\n\\n## Looking closer\\n\\n- To add your datasets, or do something with data augmentations: `dataset/static_dataset.py`, `dataset/vos_dataset.py`\\n- To work on the similarity function, or memory readout process: `model/network.py: MemoryReader`, `inference_memory_bank.py`\\n- To work on the network structure: `model/network.py`, `model/modules.py`, `model/eval_network.py`\\n- To work on the propagation process: `model/model.py`, `eval_*.py`, `inference_*.py`\\n\\n## Citation\\n\\nPlease cite our paper (MiVOS if you use top-k) if you find this repo useful!\\n\\n```bibtex\\n@inproceedings{cheng2021stcn,\\n  title={Rethinking Space-Time Networks with Improved Memory Coverage for Efficient Video Object Segmentation},\\n  author={Cheng, Ho Kei and Tai, Yu-Wing and Tang, Chi-Keung},\\n  booktitle={NeurIPS},\\n  year={2021}\\n}\\n\\n@inproceedings{cheng2021mivos,\\n  title={Modular Interactive Video Object Segmentation: Interaction-to-Mask, Propagation and Difference-Aware Fusion},\\n  author={Cheng, Ho Kei and Tai, Yu-Wing and Tang, Chi-Keung},\\n  booktitle={CVPR},\\n  year={2021}\\n}\\n```\\n\\nAnd if you want to cite the datasets:\\n\\n<details> \\n<summary>\\n\\nbibtex\\n\\n</summary>\\n\\n```bibtex\\n@inproceedings{shi2015hierarchicalECSSD,\\n  title={Hierarchical image saliency detection on extended CSSD},\\n  author={Shi, Jianping and Yan, Qiong and Xu, Li and Jia, Jiaya},\\n  booktitle={TPAMI},\\n  year={2015},\\n}\\n\\n@inproceedings{wang2017DUTS,\\n  title={Learning to Detect Salient Objects with Image-level Supervision},\\n  author={Wang, Lijun and Lu, Huchuan and Wang, Yifan and Feng, Mengyang \\n  and Wang, Dong, and Yin, Baocai and Ruan, Xiang}, \\n  booktitle={CVPR},\\n  year={2017}\\n}\\n\\n@inproceedings{FSS1000,\\n  title = {FSS-1000: A 1000-Class Dataset for Few-Shot Segmentation},\\n  author = {Li, Xiang and Wei, Tianhan and Chen, Yau Pun and Tai, Yu-Wing and Tang, Chi-Keung},\\n  booktitle={CVPR},\\n  year={2020}\\n}\\n\\n@inproceedings{zeng2019towardsHRSOD,\\n  title = {Towards High-Resolution Salient Object Detection},\\n  author = {Zeng, Yi and Zhang, Pingping and Zhang, Jianming and Lin, Zhe and Lu, Huchuan},\\n  booktitle = {ICCV},\\n  year = {2019}\\n}\\n\\n@inproceedings{cheng2020cascadepsp,\\n  title={{CascadePSP}: Toward Class-Agnostic and Very High-Resolution Segmentation via Global and Local Refinement},\\n  author={Cheng, Ho Kei and Chung, Jihoon and Tai, Yu-Wing and Tang, Chi-Keung},\\n  booktitle={CVPR},\\n  year={2020}\\n}\\n\\n@inproceedings{xu2018youtubeVOS,\\n  title={Youtube-vos: A large-scale video object segmentation benchmark},\\n  author={Xu, Ning and Yang, Linjie and Fan, Yuchen and Yue, Dingcheng and Liang, Yuchen and Yang, Jianchao and Huang, Thomas},\\n  booktitle = {ECCV},\\n  year={2018}\\n}\\n\\n@inproceedings{perazzi2016benchmark,\\n  title={A benchmark dataset and evaluation methodology for video object segmentation},\\n  author={Perazzi, Federico and Pont-Tuset, Jordi and McWilliams, Brian and Van Gool, Luc and Gross, Markus and Sorkine-Hornung, Alexander},\\n  booktitle={CVPR},\\n  year={2016}\\n}\\n\\n@inproceedings{denninger2019blenderproc,\\n  title={BlenderProc},\\n  author={Denninger, Maximilian and Sundermeyer, Martin and Winkelbauer, Dominik and Zidan, Youssef and Olefir, Dmitry and Elbadrawy, Mohamad and Lodhi, Ahsan and Katam, Harinandan},\\n  booktitle={arXiv:1911.01911},\\n  year={2019}\\n}\\n\\n@inproceedings{shapenet2015,\\n  title       = {{ShapeNet: An Information-Rich 3D Model Repository}},\\n  author      = {Chang, Angel Xuan and Funkhouser, Thomas and Guibas, Leonidas and Hanrahan, Pat and Huang, Qixing and Li, Zimo and Savarese, Silvio and Savva, Manolis and Song, Shuran and Su, Hao and Xiao, Jianxiong and Yi, Li and Yu, Fisher},\\n  booktitle   = {arXiv:1512.03012},\\n  year        = {2015}\\n}\\n```\\n\\n</details>\\n\\nContact: <hkchengrex@gmail.com>\\n\"},\n",
       " {'repo': 'Divested-Mobile/Extirpater',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '![Banner](https://divestos.org/images/featureGraphics/Extirpater.png)\\n\\nExtirpater\\n==========\\n\\nOverview\\n--------\\nA high performance free space eraser.\\n\\n[<img src=\"https://fdroid.gitlab.io/artwork/badge/get-it-on.png\"\\n     alt=\"Get it on F-Droid\"\\n     height=\"80\">](https://f-droid.org/packages/us.spotco.extirpater/)\\n\\nWhat is free space erasing?\\n---------------------------\\n- Typically when you delete a file it is not really deleted, it is merely removed from the file system\\'s index.\\n- A free space eraser tool such as this one fills the remaining space of your drive with random noise files and then deletes them.\\n- This process makes deleted files for the most part irrecoverable.\\n\\nWhat is file table filling?\\n---------------------------\\n- On some file systems, deleted file names can still be accessible in backup index databases.\\n- By creating many tens or hundreds of thousands of empty files with different random names you can push out the old files.\\n- This process makes deleted file names for the most part irrecoverable.\\n\\nRequirements\\n------------\\n- Android KitKit 4.4.4 and higher\\n\\nUses\\n----\\n- Before selling your device\\n- After enabling encryption\\n- After deleting many apps/files\\n- To maintain good data hygiene\\n\\nWarnings\\n-------\\n- Do not overuse this tool\\n- Excessive use will destroy your NAND flash storage\\n- Ensure important files are backed up before use\\n\\nInstructions\\n------------\\n0. (Optional) Delete unnecessary files, clear histories, clear app cache, factory reset, etc.\\n1. Launch the app\\n2. (Optional) Change the erase options from the menu\\n3. Click \"Start\" on either storage location\\n4. The status of drives are shown via the status label and progress bar\\n\\n\"Data Output\" Option\\n--------------------\\n- The list is ranked roughly by how \"secure\" the output is\\n- If you want a super quick erase use \"Zeroes\", but be warned that it might not do anything due to various factors (flash, cache, compression, etc.)\\n- If you want a quick erase use \"Random\"\\n- If you want a quick but more secure erase use \"CMWC4096RNG\"\\n- If you want a cryptographically secure, but very slow erase use \"SecureRandom\"\\n\\nKnown Issues\\n------------\\n- On devices without real external storage, the two shown are both internal\\n- The file table of the secondary drive will never really be filled\\n- The last 20MB aren\\'t erased\\n\\nLimitations\\n-----------\\n- Due to how flash drives work and the partition layout of Android devices, it\\'ll never be possible to fully fill the drive\\n\\nPlanned Updates\\n---------------\\n- Better GUI\\n- Add a fast csprng data source\\n- Root support for filling /cache and /system\\n- Root support for fstrim\\'ing partitions\\n\\nGoals\\n-----\\n- Be fast\\n- Don\\'t eat batteries\\n- Use minimal permissions\\n- Use libraries only when necessary\\n\\nCredits\\n-------\\n- @inkhorn for the Portuguese translations.\\n- Library: Uncommons Maths, License: Apache 2.0, https://maths.uncommons.org\\n- Icons: Google/Android/AOSP, License: Apache 2.0, https://google.github.io/material-design-icons/\\n\\nDonate\\n-------\\n- https://divested.dev/donate\\n'},\n",
       " {'repo': 'EntitySpaces/EntitySpaces-CompleteSource',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': 'EntitySpaces-CompleteSource\\n===========================\\n\\n###NOT READY FOR PRIME TIME YET###\\n* DO NOT DOWNLOAD THE SOURCE CODE YET !!!\\n* IT IS NOT READY YET\\n* YOU WILL NOT BE ABLE TO BUILD AND THE NEXT TIME YOU SYNC YOUR REPOSITORY WILL BE SEVERELY MESSED UP\\n* WAIT FOR THE ANNOUNCEMENT ON OUR BLOG\\n* WE WILL BE WORKING ON THIS ALL WEEKEND\\n* DETAILED BUILD INSTRUCTION ARE COMING\\n\\n###LICENSE###\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tNew BSD License\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tCopyright (c) 2006-2012, EntitySpaces, LLC\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  All rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n    * Redistributions of source code must retain the above copyright\\n      notice, this list of conditions and the following disclaimer.\\n    * Redistributions in binary form must reproduce the above copyright\\n      notice, this list of conditions and the following disclaimer in the\\n      documentation and/or other materials provided with the distribution.\\n    * Neither the name of the EntitySpaces, LLC nor the\\n      names of its contributors may be used to endorse or promote products\\n      derived from this software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL EntitySpaces, LLC BE LIABLE FOR ANY\\nDIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\\nON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n\\n\\n\\n###INSTALL THE OFFICIAL RELEASE###\\n* [To install click here - use the direct download link] \\n(http://download.cnet.com/EntitySpaces-Studio/3000-10250_4-10590953.html?tag=mncol;1)\\n\\n\\n<img src=\"https://raw.github.com/EntitySpaces/EntitySpaces-CompleteSource/master/logo.png\" border=\"0\">\\n\\n'},\n",
       " {'repo': 'CustomPhase/CP_SSSSS',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# Custom Phase Screen-Space Subsurface Scattering\\nNaive screen-space subsurface scattering solution for Unity 5.<br><br>\\n<img src=\"http://customphase.ru/download/img/CP_SSSSS_1.PNG\" alt=\"In action\" width=\"360\"/> <br>\\nAnimated gif: http://imgur.com/Nc8VyDv\\n\\n<h3>Tested only in Unity 5.4.2, works with deferred/forward, gamma/linear, HDR/LDR, DX11/DX9, perspective/ortho.</h3>\\n\\n<h1>How to use:</h1>\\n<ol>\\n<li>Put the files into any folder in your .../Assets/Resources folder</li>\\n<li>Attach the CP_SSSSS_Main script to your main camera</li>\\n<li>Attach CP_SSSSS_Object script to any Renderer object that you want to have subsurface scattering on</li>\\n</ol>\\n\\n<h1>Basic idea behind algorithm:</h1>\\n<ol>\\n<li>Blur the source image separably, based on the distance from the camera, and attenuate surrounding sample\\'s influence based on the depth difference between this sample and the center sample (Soft Depth Bias parameter controls the maximum depth difference allowed)</li>\\n<li>Render the scene with replaced shader, using the mask set in CP_SSSSS_Object script multiplied by the subsurface color</li>\\n<li>Composite the blurred stuff on top of the original, multiplying it by mask from step 2, and substracting the original based on the Affect Direct parameter</li>\\n</ol>\\n\\n<hr>\\nMIT License\\n\\nCopyright (c) 2016 Evgeny Erzutov\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'},\n",
       " {'repo': 'plamere/SpotifyPopcorn',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Music Popcorn\\nA dynamic visualization of the music genre space\\n'},\n",
       " {'repo': 'gheja/trilateration.js',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# trilateration.js\\n[![Build Status](https://travis-ci.org/gheja/trilateration.js.svg?branch=master)](https://travis-ci.org/gheja/trilateration.js)\\n\\nTrilateration in 3D space, implemented in JavaScript.\\n\\n[Try it here](https://gheja.github.io/trilateration.js/example.html)\\n\\n## license\\nMIT License\\n\\nCopyright (c) 2015-2017 Gabor Heja\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'},\n",
       " {'repo': 'robot527/add-spaces',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '给文本文件的中文英文之间添加合理的空格\\n==================================================\\n\\n## 文档排版需求\\n\\n  - [中英文之间需要增加空格](#中英文之间需要增加空格)\\n  - [中文与数字之间需要增加空格](#中文与数字之间需要增加空格)\\n  - [数字与单位之间需要增加空格](#数字与单位之间需要增加空格)\\n  - [全角标点与其他字符之间不加空格](#全角标点与其他字符之间不加空格)\\n\\n## 脚本用法\\n\\n```\\n\\tpython add_spaces.py /path/to/file code  # code 为文件编码，如：gbk, utf8\\n\\t# 或者自动猜测文本文件的编码\\n\\tpython add_spaces.py /path/to/file\\n```\\n\\n## 更新历史  \\n### 更新时间：2016-08-28\\n  - 支持对中文里有粗体或斜体英文单词的语句的处理\\n  - 支持对中文里有粗体或斜体中文字词的语句的处理\\n\\n## 空格\\n\\n「有研究显示，打字的时候不喜欢在中文和英文之间加空格的人，感情路都走得很辛苦，有七成的比例会在 34 岁的时候跟自己不爱的人结婚，而其余三成的人最后只能把遗产留给自己的猫。毕竟爱情跟书写都需要适时地留白。\\n\\n与大家共勉之。」——[vinta/paranoid-auto-spacing](https://github.com/vinta/pangu.js)\\n\\n### 中英文之间需要增加空格\\n\\n正确：\\n\\n> 在 LeanCloud 上，数据存储是围绕 `AVObject` 进行的。\\n\\n错误：\\n\\n> 在LeanCloud上，数据存储是围绕`AVObject`进行的。\\n\\n> 在 LeanCloud上，数据存储是围绕`AVObject` 进行的。\\n\\n完整的正确用法：\\n\\n> 在 LeanCloud 上，数据存储是围绕 `AVObject` 进行的。每个 `AVObject` 都包含了与 JSON 兼容的 key-value 对应的数据。数据是 schema-free 的，你不需要在每个 `AVObject` 上提前指定存在哪些键，只要直接设定对应的 key-value 即可。\\n\\n例外：「豆瓣FM」等产品名词，按照官方所定义的格式书写。\\n\\n### 中文与数字之间需要增加空格\\n\\n正确：\\n\\n> 今天出去买菜花了 5000 元。\\n\\n错误：\\n\\n> 今天出去买菜花了 5000元。\\n\\n> 今天出去买菜花了5000元。\\n\\n### 数字与单位之间需要增加空格\\n\\n正确：\\n\\n> 我家的光纤入户宽带有 10 Gbps，SSD 一共有 20 TB。\\n\\n错误：\\n\\n> 我家的光纤入户宽带有 10Gbps，SSD 一共有 10TB。\\n\\n例外：度／百分比与数字之间不需要增加空格：\\n\\n正确：\\n\\n> 今天是 233° 的高温。\\n\\n> 新 MacBook Pro 有 15% 的 CPU 性能提升。\\n\\n错误：\\n\\n> 今天是 233 ° 的高温。\\n\\n> 新 MacBook Pro 有 15 % 的 CPU 性能提升。\\n\\n### 全角标点与其他字符之间不加空格\\n\\n正确：\\n\\n> 刚刚买了一部 iPhone，好开心！\\n\\n错误：\\n\\n> 刚刚买了一部 iPhone ，好开心！\\n\\n## [贡献](./Contributing.md)\\n\\n## 参考\\n\\n- [中文文案排版指北](https://github.com/LCTT/TranslateProject/blob/master/%E4%B8%AD%E6%96%87%E6%8E%92%E7%89%88%E6%8C%87%E5%8C%97.md#%E4%B8%AD%E8%8B%B1%E6%96%87%E4%B9%8B%E9%97%B4%E9%9C%80%E8%A6%81%E5%A2%9E%E5%8A%A0%E7%A9%BA%E6%A0%BC)\\n'},\n",
       " {'repo': 'novemberfiveco/symbol-spacer-sketch-plugin',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': 'symbol-spacer-sketch-plugin\\n=========\\n[![GitHub release](https://badge.fury.io/gh/novemberfiveco%2Fsymbol-spacer-sketch-plugin.svg?maxAge=3600)](https://github.com/novemberfiveco/symbol-spacer-sketch-plugin/releases)\\n\\nAutomatically resizes symbol to original size when switching from one spacing to another spacing symbol\\n\\n## Blog post\\nHow we use this plugin in combination with AnimApp: \\nhttps://novemberfive.co/blog/animapp-sketch-spacings/\\n\\n## Functionality\\nWhen you change a symbol that starts with the string `@spacing` (default) and deselect, our plugin will automatically trigger Sketch’s “reset to original size” function. The symbol will be updated, and its size will be updated to the new symbol’s original size.\\n \\nAs a default, the spacings layer names should always start with \"@spacing\": @spacing-16, spacing / @spacing-32,... This is the naming we use, but you can change this to match your own conventions in the Settings.\\n \\nUnlike Sketch’s own default setting, **our plugin updates the layer name to match the name of the updated symbol**.\\n\\n![Symbol Spacer](https://raw.githubusercontent.com/novemberfiveco/symbol-spacer-sketch-plugin/master/src/images/spacing-plugin.gif)\\n\\n## Installation\\n\\n### From a release (simplest)\\n\\n* [Download](https://github.com/novemberfiveco/symbol-spacer-sketch-plugin/releases/latest) the latest release of the plugin\\n* Un-zip\\n* Double-click on novemberfive-symbol-spacer.sketchplugin\\n* Install\\n\\n### From the sources\\n\\n* Clone the repo\\n* Install the dependencies (`npm install`)\\n* Build (`npm run build`)\\n* Double-click on novemberfive-symbol-spacer.sketchplugin\\n'},\n",
       " {'repo': 'MikeGriffinReborn/EntitySpaces',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': 'Click here for the [Glossy Site ...](https://mikegriffinreborn.github.io/EntitySpaces/)\\n\\n<img src=\"https://repository-images.githubusercontent.com/194275145/55b5b080-1ccf-11ea-8609-15b9de0d2351\" alt=\"EntitySpaces\" width=\"531\" height=\"268\">\\n\\nSupports .NET Standard 2.x, .NET Core 2.x & 3.x .NET Framework 4.5.1, 4.6, 4.6.1\\n\\nAvailable on Nuget for [SqlServer](https://www.nuget.org/packages/EntitySpaces.ORM.SqlServer), [SQLite](https://www.nuget.org/packages/EntitySpaces.ORM.SQLite/ \"NuGet\"), [MySQL](https://www.nuget.org/packages/EntitySpaces.ORM.MySQL/ \"NuGet\") or [PostgreSQL](https://www.nuget.org/packages/EntitySpaces.ORM.PostgreSQL)\\n\\nSee [Mike\\'s Blog](https://saltycode.blogspot.com/) for EntitySpaces news ...\\n\\n# EntitySpaces - A Fluent SQL API\\nEntitySpaces is a Fluent API for SQL Server, SQLite, MySQL, PostgreSQL and more on the way. If you are familiar with the SQL syntax then you are already an expert in EntitySpaces. EntitySpaces is also high performance, transactional, and very intuitive. EntitySpaces Studio is used to generate your C# classes from your database schema.\\n\\n## Example Query\\nIn this example we are going to sum the total # of items for each order. Each order can have many order detail records so we group our query by OrderId and sum up the quantity as \\'TotalQuantity\\'. Notice that we can access the derived \\'TotalQuantity\\' column through the dynamic property.\\n\\n**Use of \\'out var\\'**\\n\\n*Notice the judicial use of the \"our var\" syntax of C# in the example code below. The \"out var\" syntax allows you to delcare a variable that is created for you such as the \\'OrderDetailQuery\\' object \\'od\\' in the InnerJoin() below. Then you are then free to use the \\'od\\' variable throughout the query as is done in the Select() statement. This is also true for constructors. For example, notice how \"out var o\" is used on the creation of the OrdersQuery().*\\n\\n```c#\\nOrdersCollection coll = new OrdersQuery(\"o\", out var o)\\n    .InnerJoin<OrderDetailsQuery>(\"od\", out var od).On(o.OrderID == od.OrderID)\\n    .Select(o.OrderID, od.Quantity.Sum().As(\"TotalQuantity\"))\\n    .GroupBy(o.OrderID)\\n    .OrderBy(o.OrderID.Ascending)\\n    .ToCollection<OrdersCollection>();\\n\\nforeach(Orders order in coll)\\n{\\n    Console.WriteLine(order.OrderID + \" : \" + order.dynamic.TotalQuantity);\\n}\\n```\\n\\nThe SQL generated is just as you would expect.\\n\\n```sql\\nSELECT o.[OrderID], SUM(od.[Quantity]) AS \\'TotalQuantity\\'  \\nFROM [Orders] o \\nINNER JOIN [Order Details] od ON o.[OrderID] = od.[OrderID] \\nGROUP BY o.[OrderID] \\nORDER BY o.[OrderID] ASC\\n```\\n\\nThe output is as follows is ...\\n\\n|OrderID | TotalQuantity |\\n|-|-|\\n|10248\\t |27|\\n|10249\\t |49|\\n|10250\\t |60|\\n\\n## InnerJoin, RightJoin, LeftJoin, CrossJoin, and FullJoin\\n\\nThe sample below demonstrates a self join on the Employees table which is looking for all employees whose Supervisor has an \\'a\\' in their last name. Kind of silly but it shows off the syntax. \\n\\n```c#\\nEmployeesCollection coll = new EmployeesQuery(\"e\", out var e)   // Employees\\n    .InnerJoin<EmployeesQuery>(\"r\", out var reportsTo).On(e.ReportsTo == reportsTo.EmployeeID)\\n    .Select(e.EmployeeID, e.LastName, reportsTo.LastName.As(\"SupervisorName\"))\\n    .Where(reportsTo.LastName.Like(\"%a%\"))\\n    .OrderBy(reportsTo.LastName.Descending).Distinct()\\n    .ToCollection<EmployeesCollection>();\\n\\nif (coll.Count > 0)\\n{\\n    // Then we loaded at least one record\\n}\\n```\\n\\nNotice that the SQL is extremely lean.\\n\\nResults from the Query Above. SQL Parameters are always used to avoid SQL Injection Attacks.\\n\\n```sql\\nSELECT  DISTINCT e.[EmployeeID],e.[LastName],r.[LastName] AS \\'SupervisorName\\'  \\nFROM [Employees] e \\nINNER JOIN [Employees] r ON e.[ReportsTo] = r.[EmployeeID] \\nWHERE r.[LastName] LIKE @LastName1 \\nORDER BY r.[LastName] DESC\\n```\\n\\n## Any, All, and Some \\nAny, All, and Some all follow the same rules. You them with operators (==, !=, >, >=, <, or <=) in the \"nested\" syntax as shown below.\\n\\n```c#\\nEmployeesCollection coll = new EmployeesQuery(\"q\", out var q)\\n.Where(q.EmployeeID > (() =>\\n    {\\n        return new EmployeesQuery(\"e\", out var q1)\\n        .Select(q1.EmployeeID)\\n        .Where(q1.EmployeeID.IsNotNull()).Any();  // <= Any indicated here !\\n    })\\n)\\n.ToCollection<EmployeesCollection>();\\n```\\n\\nSQL Generated:\\n\\n```sql\\nSELECT * FROM [Employees] q \\nWHERE q.[EmployeeID] > ANY \\n(\\n    SELECT e.[EmployeeID] \\n    FROM [Employees] e \\n    WHERE e.[EmployeeID] IS NOT NULL\\n)\\n```\\n\\n## CrossApply and OuterApply\\nThis example uses OuterApply to select each customer and their last 2 orders.\\n\\n```c#\\nCustomersCollection coll = new CustomersQuery(\"c\", out var c)\\n    .OuterApply<OrdersQuery>(out var o, () =>\\n    {\\n        return new OrdersQuery(\"o\", out var subQuery)\\n        .Select(subQuery.OrderID, subQuery.OrderDate)\\n        .Top(2)\\n        .Where(subQuery.CustomerID == c.CustomerID)\\n        .OrderBy(subQuery.OrderDate.Descending, subQuery.OrderID.Ascending);\\n\\n    })\\n    .Select(c.CustomerID, c.CompanyName, o.OrderID, o.OrderDate)\\n    .ToCollection<CustomersCollection>();\\n\\n// Notice the \"dynamic\" property accessor for accessing the columns brought \\n// back from the Orders table.\\nforeach(Customers cust in coll)\\n{\\n    Console.WriteLine(cust.CustomerID);\\n    Console.WriteLine(cust.CompanyName);\\n    Console.WriteLine(cust.dynamic.OrderID);\\n    Console.WriteLine(cust.dynamic.OrderDate);\\n}    \\n```\\nSQL Generated:\\n\\n```sql\\nSELECT c.[CustomerID],c.[CompanyName],o.[OrderID],o.[OrderDate]\\nFROM [Customers] c \\nOUTER APPLY \\n(\\n    SELECT TOP 2 o.[OrderID],o.[OrderDate]\\n\\tFROM [Orders] o \\n\\tWHERE o.[CustomerID] = c.[CustomerID] \\n\\tORDER BY o.[OrderDate] DESC,o.[OrderID] ASC\\n) AS o\\n```\\n\\nEach customer and their last 2 orders.\\n\\n\\n|CustomerID | CompanyName | OrderID | OrderDate|\\n|:-|:-|:-|:-|\\n|ALFKI|Alfreds Futterkiste|11011|04/09/1998 12:00:00 AM|\\n|ALFKI|Alfreds Futterkiste|10952|03/16/1998 12:00:00 AM|\\n|ANATR|Ana Trujillo Emparedados y helados|10926|03/04/1998 12:00:00 AM|\\n|ANATR|Ana Trujillo Emparedados y helados|10759|11/28/1997 12:00:00 AM|\\n|ANTON|Antonio Moreno Taquería|10856|01/28/1998 12:00:00 AM|\\n|ANTON|Antonio Moreno Taquería|10682|09/25/1997 12:00:00 AM|\\n|AROUT|Around the Horn|11016|04/10/1998 12:00:00 AM|\\n|AROUT|Around the Horn|10953|03/16/1998 12:00:00 AM|\\n\\n## Union, Intersect, and Except\\nHere we use Union to find employees whose first name begins with F, C, or M. Of course, this isn\\'t a great way to determine this data but it demonstrate syntax.\\n\\n```c#\\nEmployeesCollection coll = new EmployeesQuery(\"q1\", out var q1)\\n    .Select(q1.EmployeeID, q1.FirstName, q1.LastName)\\n    .Where(q1.FirstName.Like(\"F%\"))\\n    .Union(() =>\\n    {\\n        return new EmployeesQuery(\"q2\", out var q2)\\n        .Select(q2.EmployeeID, q2.FirstName, q2.LastName)\\n        .Where(q2.FirstName.Like(\"C%\"));\\n    })\\n    .Union(() =>\\n    {\\n        return new EmployeesQuery(\"q3\", out var q3)\\n        .Select(q3.EmployeeID, q3.FirstName, q3.LastName)\\n        .Where(q3.FirstName.Like(\"M%\"));\\n    })\\n    .ToCollection<EmployeesCollection>();\\n\\nif (coll.Count > 0)\\n{\\n    // Then we loaded at least one record\\n}\\n```\\n\\nSQL Generated:\\n\\n```sql\\nSELECT q1.[EmployeeID],q1.[FirstName],q1.[LastName]  \\nFROM [Employees] q1 WHERE q1.[FirstName] LIKE @FirstName1 \\n  UNION SELECT q2.[EmployeeID],q2.[FirstName],q2.[LastName]  \\n  FROM [Employees] q2 WHERE q2.[FirstName] LIKE @FirstName2 \\n  UNION SELECT q3.[EmployeeID],q3.[FirstName],q3.[LastName]  \\n  FROM [Employees] q3 WHERE q3.[FirstName] LIKE @FirstName3\\n```\\n\\n## Using In() and NotIn() via Nested Queries\\n\\n```c#\\nOrdersCollection coll = new OrdersQuery(\"o\", out var oQuery)\\n.Select(oQuery.OrderID, oQuery.EmployeeID)\\n.InnerJoin<OrderDetailsQuery>(\"od\", out var od).On(oQuery.OrderID == od.OrderID)\\n.InnerJoin<EmployeesQuery>(\"e\", out var e).On(e.EmployeeID == oQuery.EmployeeID \\n  && oQuery.EmployeeID.In(() =>\\n  {\\n     return new EmployeesQuery(\"ee\", out var ee)\\n      .InnerJoin<OrdersQuery>(\"eo\", out var eo).On(ee.EmployeeID == eo.EmployeeID)\\n      .InnerJoin<OrderDetailsQuery>(\"eod\", out var eod).On(eo.OrderID == eod.OrderID)\\n      .Select(eo.EmployeeID)\\n      .Distinct();\\n  })\\n)\\n.ToCollection<OrdersCollection>();\\n\\nif (coll.Count > 0)\\n{\\n    // We loaded some records\\n}\\n```\\n\\nSQL Generated:\\n\\n```sql\\nSELECT o.[OrderID],  o.[EmployeeID]\\nFROM [Orders] o\\nINNER JOIN [Order Details] od ON o.[OrderID] = od.[OrderID]\\nINNER JOIN [Employees] e ON (e.[EmployeeID] = o.[EmployeeID] AND o.[EmployeeID] IN \\n(\\n    SELECT DISTINCT eo.[EmployeeID]\\n    FROM [Employees] ee\\n    INNER JOIN [Orders] eo ON ee.[EmployeeID] = eo.[EmployeeID]\\n    INNER JOIN [Order Details] eod ON eo.[OrderID] = eod.[OrderID]\\n)\\n```\\n\\n## Exists() \\n\\nExists evaluates to true if the SubQuery returns a result set.\\n\\n```c#\\nEmployeesCollection coll = new EmployeesQuery(\"e\", out var eq)\\n.Select(eq.EmployeeID, eq.ReportsTo)\\n.Where(eq.Exists(() =>\\n{\\n    // SubQuery of Employees with a null Supervisor column.\\n    return new EmployeesQuery(\"s\", out var sq)\\n    .Select(sq.EmployeeID).Where(sq.ReportsTo.IsNull()).Distinct();\\n}))\\n.ToCollection<EmployeesCollection>();\\n\\nif (coll.Count > 0)\\n{\\n    // Then we loaded at least one record\\n}\\n```\\n\\nSQL Generated:\\n\\n```sql\\nSELECT e.[EmployeeID], e.[ReportsTo]\\nFROM [Employees] e\\nWHERE EXISTS (\\n    SELECT DISTINCT s.[EmployeeID]\\n    FROM [Employees] s\\n    WHERE s.[ReportsTo] IS NULL\\n)\\n```\\n\\n## Where() with Nested Query\\n\\nIn and NotIn are two of the most common operators used in a Where SubQuery. The following produces a result set containing Territories that an Employee is not associated with.\\n\\n```c#\\n// Territories that Employee 1 is not assigned to.\\nTerritoriesCollection coll = new TerritoriesQuery(\"t\", out var tq)\\n  .Select(tq.TerritoryID, tq.TerritoryDescription);\\n  .Where(tq.TerritoryID.NotIn(() =>\\n  {\\n      return new EmployeeTerritoriesQuery(\"et\", out var etq)\\n      .Select(etq.TerritoryID)\\n      .Where(etq.EmployeeID == 1);\\n  }))\\n  .ToCollection<TerritoriesCollection>();\\n\\nif (coll.Count > 0)\\n{\\n    // Then we loaded at least one record\\n}\\n```\\n\\nSQL Generated:\\n\\n```sql\\nSELECT t.[Description]  \\nFROM [dbo].[Territory] t \\nWHERE t.[TerritoryID] NOT IN \\n(\\n    SELECT et.[TerrID]  \\n    FROM .[dbo].[EmployeeTerritory] et \\n    WHERE et.[EmpID] = @EmpID1\\n) \\n```\\n\\n## From() with Nested Query\\nNotice how in the Select() statement we use the \"escape hatch\" mechanism and declare \"<sub.OrderTotal>\" as a string. What does this do? Anything you pass in within \"<>\" brackets is take \"as-is\". We need to do this here because the nested query in the From() clause is aliased as \"sub\" and we need to access the derived \"OrderTotal\" column. In an upcoming version the \"out var\" syntax will be supported on the Alias and you will no longer have to use the escape hatch. This isn\\'t always true of the From clause it only has to do with this particular query.\\n\\n```c#\\nOrdersCollection coll = new OrdersQuery(\"o\", out var o)\\n    .Select(o.CustomerID, o.OrderDate, \"<sub.OrderTotal>\")\\n    .From<OrderDetailsQuery>(out var od, () =>\\n    {\\n        return new OrderDetailsQuery(\"od\", out var subQuery)\\n        .Select(subQuery.OrderID, (subQuery.UnitPrice * subQuery.Quantity).Sum().As(\"OrderTotal\"))\\n        .GroupBy(subQuery.OrderID);\\n    }).As(\"sub\")\\n    .InnerJoin(o).On(o.OrderID == od.OrderID)\\n    .ToCollection<OrdersCollection>();\\n\\nif (coll.Count > 0)\\n{\\n    // Then we loaded at least one record\\n}\\n```\\n\\nSQL Generated:\\n\\n```sql\\nSELECT o.[CustomerID], o.[OrderDate], sub.OrderTotal\\nFROM \\n(\\n    SELECT od.[OrderID],SUM((od.[UnitPrice] * od.[Quantity])) AS \\'OrderTotal\\'  \\n\\tFROM [Order Details] od \\n\\tGROUP BY od.[OrderID]\\n) AS sub \\nINNER JOIN [Orders] o ON o.[OrderID] = sub.[OrderID]\\n```\\n\\n## Nested Query within Select Clause\\n\\nA Nested Query in a Select clause must return a single value.\\n\\n```c#\\nOrdersCollection coll = new OrdersQuery(\"o\", out var orders)\\n.Select\\n(\\n    orders.OrderID, \\n    orders.OrderDate,\\n    // Embed another query (see \\'SQL Generated\\' below)\\n    new OrderDetailsQuery(\"oi\", out var details).Select(details.UnitPrice.Max())\\n    .Where(orders.OrderID == details.OrderID).As(\"MaxUnitPrice\")\\n)\\n.ToCollection<OrdersCollection>();\\n\\nif (coll.Count > 0)\\n{\\n    // Then we loaded at least one record\\n}\\n```\\n\\nSQL Generated:\\n\\n```sql\\nSELECT o.[OrderID],o.[OrderDate], \\n(\\n   SELECT MAX(oi.[UnitPrice]) AS \\'UnitPrice\\'  \\n   FROM [Order Details] oi \\n   WHERE o.[OrderID] = oi.[OrderID]\\n) AS MaxUnitPrice  \\nFROM [Orders] o\\n```\\n\\n## OVER Clause Examples\\nDetermines the partitioning and ordering of a rowset before the associated window function is applied. That is, the OVER clause defines a window or user-specified set of rows within a query result set. A window function then computes a value for each row in the window. You can use the OVER clause with functions to compute aggregated values such as moving averages, cumulative aggregates, running totals, or a top N per group results.\\n\\n```c#\\nOrdersCollection coll = new OrdersQuery(\"o\", out var o)\\n.Select\\n(\\n    o.Over.Sum(o.Freight).PartitionBy(o.EmployeeID).As(\"FreightByEmployee\"),\\n    o.Over.Sum(o.Freight).PartitionBy(o.EmployeeID, o.ShipCountry).As(\"FreightByEmployeeAndCountry\")\\n)\\n.OrderBy(o.EmployeeID.Ascending, o.ShipCountry.Ascending)\\n.ToCollection<OrdersCollection>();\\n\\nif (coll.Count > 0)\\n{\\n    // Then we loaded at least one record\\n}\\n```\\nSQL Generated:\\n\\n```sql\\nSELECT \\n    SUM(o.[Freight]) OVER( PARTITION BY o.[EmployeeID] ) AS \\'FreightByEmployee\\',\\n    SUM(o.[Freight]) OVER( PARTITION BY o.[EmployeeID], o.[ShipCountry] ) AS \\'FreightByEmployeeAndCountry\\'  \\nFROM [Orders] o \\nORDER BY o.[EmployeeID] ASC,o.[ShipCountry] ASC\\n```\\n\\n## OVER Clauses with esAlias and Rows Syntax\\nThis might look like a complicated query but it\\'s really quite simple. As you look at the code below think of it this way. Within the From() statement is a nested query. Also, notice how we grab aliased columns via the \"out\" parameter. We also give our nested query an alias of \"sub\". Finally, the outer query selects and orders columns from the nested query using the aliased columns as well as applies the OVER syntax over the aliased columns. The outer query also uses the sophisticated ROWS syntax. Notice how simple the SQL generated from this query actually is, and it looks just like the C# code.\\n\\n```c#\\n// We grab these aliases in the nested query via \"out\" parameters\\nesAlias aliasCompany = null, aliasPeriod = null, aliasAmount = null, aliasItemCount = null;\\n\\nOrdersCollection coll = new OrdersQuery(\"q\", out var q)\\n.From<OrdersQuery>(out var sub, () => // mimic a CTE\\n{\\n    // Nested Query\\n    return new OrdersQuery(\"o\", out var o)\\n    .InnerJoin<CustomersQuery>(\"c\", out var c).On(c.CustomerID == o.CustomerID)\\n    .InnerJoin<OrderDetailsQuery>(\"od\", out var od).On(od.OrderID == o.OrderID)\\n    .Select\\n    (\\n        // We\\'re going to grab the aliased columns here for re-use in the outer query later\\n        o.Count().As(\"TotalItems\", out aliasItemCount),\\n        c.CompanyName.As(\"CompanyName\", out aliasCompany),\\n        o.OrderDate.DatePart(\"year\").As(\"Period\", out aliasPeriod),\\n        ((1.00M - od.Discount) * od.UnitPrice * od.Quantity).Cast(esCastType.Decimal, 19, 2)\\n\\t\\t.Sum().Round(2).As(\"Amount\", out aliasAmount)\\n    )\\n    .GroupBy(c.CompanyName, o.OrderDate.DatePart(\"year\"));\\n}).As(\"sub\")\\n// Now act on \"sub\" query columns\\n.Select(\\n   aliasCompany(), aliasPeriod(), aliasAmount(), aliasItemCount(),  \\n   q.Over.Sum(aliasAmount()).PartitionBy(aliasCompany()).OrderBy(aliasPeriod().Ascending)\\n      .Rows.UnBoundedPreceding.As(\"CumulativeAmount\"),\\n   q.Over.Sum(aliasAmount()).PartitionBy(aliasCompany()).As(\"TotalAmount\")\\n)\\n.OrderBy(aliasCompany().Ascending, aliasPeriod().Ascending)\\n.ToCollection<OrdersCollection>();\\n\\nif(coll.Count > 0)\\n{\\n    // we loaded data\\n}\\n```\\n\\nSQL Generated:\\n\\n```sql\\nSELECT\\n   sub.[CompanyName],\\n   sub.[Period],\\n   sub.[Amount],\\n   sub.[TotalItems],\\n   SUM([Amount]) OVER( PARTITION BY [CompanyName] ORDER BY sub.[Period] ASC \\n      ROWS UNBOUNDED PRECEDING ) AS \\'CumulativeAmount\\',\\n   SUM([Amount]) OVER( PARTITION BY [CompanyName] ) AS \\'TotalAmount\\' \\nFROM\\n   (\\n      SELECT\\n         COUNT(*) AS \\'TotalItems\\',\\n         c.[CompanyName] AS \\'CompanyName\\',\\n         DATEPART(year, o.[OrderDate]) AS \\'Period\\',\\n         CAST(SUM(ROUND((((1.00 - od.[Discount]) * od.[UnitPrice]) * od.[Quantity]), 2)) \\n\\t    AS decimal(19, 2)) AS \\'Amount\\' \\n      FROM [Orders] o \\n         INNER JOIN [Customers] c ON c.[CustomerID] = o.[CustomerID] \\n         INNER JOIN [Order Details] od ON od.[OrderID] = o.[OrderID] \\n      GROUP BY c.[CompanyName], DATEPART(year, o.[OrderDate])\\n   )\\n   AS sub \\nORDER BY sub.[CompanyName] ASC, sub.[Period] ASC\\n```\\n\\nThe output is as follows is ...\\n\\n| CompanyName | Period | Amount | TotalItems  | CumulativeAmount  | TotalAmount  |\\n|:-|:-|:-|:-|:-|:-|\\n|Alfreds Futterkiste|1997|2022.50|6|2022.50|4273.00|\\n|Alfreds Futterkiste|1998|2250.50|6|4273.00|4273.00|\\n|Ana Trujillo Emparedados y helados|1996|88.80|2|88.80|1402.95|\\n|Ana Trujillo Emparedados y helados|1997|799.75|4|888.55|1402.95|\\n|Ana Trujillo Emparedados y helados|1998|514.40|4|1402.95|1402.95|\\n|Antonio Moreno Taquería|1996|403.20|1|403.20|7023.97|\\n|Antonio Moreno Taquería|1997|5960.77|14|6363.97|7023.97|\\n|Antonio Moreno Taquería|1998|660.00|2|7023.97|7023.97|\\n|Around the Horn|1996|1379.00|5|1379.00|13390.65|\\n|Around the Horn|1997|6406.90|18|7785.90|13390.65|\\n|Around the Horn|1998|5604.75|7|13390.65|13390.65|\\n|Berglunds snabbköp|1996|4324.40|9|4324.40|24927.58|\\n|Berglunds snabbköp|1997|13849.02|27|18173.42|24927.58|\\n|Berglunds snabbköp|1998|6754.16|16|24927.58|24927.58|\\n\\n## AND and OR and Concatentation\\nAnd and Or work just as you would expect, use parenthesis to control the order of precedence. You can also concatentat and use all kinds of operators in your queries. See the tables at the end of this document.\\n\\n```c#\\nEmployeesCollection coll = new EmployeesQuery(\"e\", out var q)\\n    .Select(q.EmployeeID, (q.LastName + \", \" + q.FirstName).As(\"FullName\"))\\n    .Where(q.EmployeeID > 4 && (q.EmployeeID < 10 || q.EmployeeID == 100))\\n    .ToCollection<EmployeesCollection>();\\n\\nif (coll.Count > 0)\\n{\\n\\n}\\n```\\n\\nSQL Generated:\\n\\n```sql\\nSELECT \\n   e.[EmployeeID],\\n  (e.[LastName] + \\', \\' + e.[FirstName]) AS \\'FullName\\'  \\nFROM [Employees] e \\nWHERE e.[EmployeeID] > @EmployeeID1 \\n  AND \\n  (\\n      e.[EmployeeID] < @EmployeeID2 OR e.[EmployeeID] = @EmployeeID3\\n  )\\n```\\n\\n## Select * from a Joined Table\\nHere the Orders table is joined with the OrderDetails table. The Orders.OrderID column is brought back along with all columns from the OrderDetails table. Notice how the Select() statement uses \\'od\\' without a column declared. This results in \\'od.*\\' in the SQL.\\n\\n```c#\\nOrdersCollection coll = new OrdersQuery(\"oq\", out var o)\\n.InnerJoin<OrderDetailsQuery>(\"od\", out var od).On(o.OrderID == od.OrderID)\\n.Select(o.OrderID, od) // Notice the \\'od\\' results in \\'od.*\\'\\n.Where(od.Discount > 0)\\n.ToCollection<OrdersCollection>();\\n\\nif (coll.Count > 0)\\n{\\n    // data was loaded\\n}\\n```\\n\\nSQL Generated:\\n\\n```sql\\nSELECT oq.[OrderID], od.*\\nFROM [Orders] oq \\nINNER JOIN [Order Details] od ON oq.[OrderID] = od.[OrderID]\\nWHERE od.[Discount] > @Discount1\\n```\\n\\n## Select Top\\n\\n```c#\\nEmployees emp = new EmployeesQuery(\"q\", out var q)\\n   .Where(q.ReportsTo.IsNotNull())\\n   .OrderBy(q.LastName.Descending).Top(1)\\n   .ToEntity<Employees>();\\n\\nif (emp != null)\\n{\\n    // Then we loaded at least one record\\n}\\n```\\n\\nSQL Generated:\\n\\n```sql\\nSELECT TOP 1 * \\nFROM [Employees] \\nWHERE [ReportsTo] IS NOT NULL \\nORDER BY [LastName] DESC\\n```\\n\\n\\n## SelectAllExcept\\n\\nSelectAllExcept() is just a convenient way to select all columns except one or more listed columns.\\n\\n```c#\\n// We don\\'t want to bring back the huge photo\\nEmployeesCollection coll = new EmployeesQuery(\"q\", out var q)\\n    .SelectAllExcept(q.Photo)\\n    .ToCollection<EmployeesCollection>();\\n\\nif (coll.Count > 0)\\n{\\n    // Then we loaded at least one record\\n}\\n```\\n\\nSQL Generated:\\n\\n```sql\\nSELECT q.[EmployeeID],q.[LastName],q.[FirstName],q.[Title], -- all except q.Photo\\nFROM [Employees] q\\n```\\n\\n## Paging\\n\\n**PageSize / PageNumber**\\n\\nThis is the traditional way of paging and works on all versions of SQL Server. You always need an OrderBy when sorting.\\n\\n```c#\\nEmployeesCollection coll = new EmployeesQuery(\"q\", out var q)\\n .Select(q.EmployeeID, q.LastName)\\n .OrderBy(q.LastName.Ascending)\\n .PageNumber(2).PageSize(20)\\n .ToCollection<EmployeesCollection>();\\n\\nif (coll.Count > 0)\\n{\\n\\n}\\n```\\n\\nSQL Generated:\\n\\n```sql\\nWITH [withStatement] AS \\n(\\n   SELECT [EmployeeID],[LastName],\\n      ROW_NUMBER() OVER( ORDER BY [LastName] ASC) AS ESRN \\n\\t  FROM [Employees]\\n)\\nSELECT * \\nFROM [withStatement] \\nWHERE ESRN BETWEEN 21 AND 40 \\nORDER BY ESRN ASC\\n```\\n\\n**Skip / Take**\\n\\nSkip and Take Require Microsoft SQL 2012 at a minimum and is a much nicer syntax.\\n\\n```c#\\nEmployeesCollection coll = new EmployeesQuery(\"q\", out var q)\\n .Select(q.EmployeeID, q.LastName)\\n .OrderBy(q.LastName.Ascending)\\n .Skip(40).Take(20)\\n .ToCollection<EmployeesCollection>();\\n\\nif (coll.Count > 0)\\n{\\n\\n}\\n```\\n\\nSQL Generated:\\n\\n```sql\\nSELECT [EmployeeID],[LastName]\\nFROM [Employees] \\nORDER BY [LastName] ASC \\nOFFSET 40 ROWS  \\nFETCH NEXT 20 ROWS ONLY \\n```\\n\\n## Distinct\\n\\nSelectT DISTINCT clause to retrieve the only distinct values in a specified list of columns.\\n\\n```c#\\n// Distinct list of Employee\\'s who have orders ...\\nEmployeesCollection coll = new EmployeesQuery(\"e\", out var e)\\n  .Select(e.EmployeeID)\\n  .InnerJoin<OrdersQuery>(\"o\", out var o).On(e.EmployeeID == o.EmployeeID)\\n  .Distinct()\\n  .ToCollection<EmployeesCollection>();\\n```\\n\\nSQL Generated:\\n\\n```sql\\nSELECT DISTINCT e.[EmployeeID]\\nFROM [Employees] e \\nINNER JOIN [Orders] o ON e.[EmployeeID] = o.[EmployeeID]\\n```\\n\\n## With NoLock\\n\\n```c#\\nEmployeesCollection coll = new EmployeesQuery(\"e\", out var e)\\n  .Select(e.EmployeeID)\\n  .InnerJoin<OrdersQuery>(\"o\", out var o).On(e.EmployeeID == o.EmployeeID)\\n  .Where(o.Freight > 20)\\n  .es.WithNoLock()\\n  .ToCollection<EmployeesCollection>();\\n```\\n\\nNotice that even though many query objects are being used you only need to set WithNoLock to true for the parent or main query object. The SQL generated is as follows:\\n\\nSQL Generated: (Notice that \"WITH (NOLOCK)\" was applied on both tables involved in the query)\\n\\n```sql\\nSELECT e.[EmployeeID]  \\nFROM [Employees] e WITH (NOLOCK) \\nINNER JOIN [Orders] o WITH (NOLOCK) ON e.[EmployeeID] = o.[EmployeeID] \\nWHERE o.[Freight] > @Freight1\\n```\\n\\n## Full Expressions\\n\\nThis query doesn’t really make sense, but we wanted to show you what will is possible.\\n\\n```c#\\nEmployeesQuery q = new EmployeesQuery(); \\nq.Select(q.LastName.Substring(2, 4).ToLower()); \\nq.OrderBy(q.LastName.Substring(2, 4).ToLower().Descending); \\nq.GroupBy(q.LastName.Substring(2, 4).ToLower());\\n\\nEmployeesCollection coll = new EmployeesCollection();\\nif(coll.Load(q))\\n{\\n    // Then we loaded at least one record\\n}\\n```\\n\\nSQL Generated:\\n\\n```sql\\nSELECT SUBSTRING(LOWER([LastName]),2,4) AS \\'LastName\\' \\nFROM [Employees] \\nGROUP BY SUBSTRING(LOWER([LastName]),2,4) \\nORDER BY SUBSTRING(LOWER([LastName]),2,4) DESC\\n```\\n\\n## Casting\\nYou can cast your types to other SQL types using Cast()\\n\\n```c#\\nOrderDetailsCollection coll = new OrderDetailsQuery(\"o\", out var o)\\n.Select\\n(\\n    (o.Quantity * o.UnitPrice).Cast(esCastType.Decimal, 34, 4).As(\"Cost\")\\n)\\n.ToCollection<OrderDetailsCollection>();\\n```\\n\\nSQL Generated:\\n\\n```sql\\nSELECT\\n   CAST((o.[Quantity] * o.[UnitPrice]) AS decimal(34, 4)) AS \\'Cost\\' \\nFROM\\n   [Order Details] o\\n```\\n\\n## Case().When().Then().End() Syntax\\n\\n```c#\\nEmployeesQuery q = new EmployeesQuery();\\nq.Select(q.EmployeeID, q.FirstName);\\nq.Where(q.EmployeeID == 2);\\n\\nOrderDetailsQuery oq = new OrderDetailsQuery();\\noq.Select\\n(\\n  oq.UnitPrice.Case()\\n    .When(\"yay\").Then(\"wow\")\\n    .When(oq.Exists(q)).Then(\"Exists!!\")\\n    .When(oq.Quantity >= 50).Then(oq.UnitPrice)\\n    .When(oq.Quantity  / 50 / 50 == 0).Then(oq.UnitPrice)\\n    .When(oq.Quantity >= 50 && oq.Quantity < 250).Then(1)\\n    .When(oq.Quantity >= 250 && \\n            oq.Quantity < 1000).Then(oq.UnitPrice * .80)\\n    .Else(\"Huh?\")\\n    .End()\\n);\\noq.Where(oq.Quantity.Sum() >= 50 && oq.Quantity.Avg() < 250);\\noq.OrderBy(oq.OrderID.Descending, oq.Quantity.Descending);\\n\\nOrderDetailsCollection coll = new OrderDetailsCollection();\\nif(coll.Load(OrderDetails))\\n{\\n    // Then we loaded at least one record\\n}\\n```\\n\\nSQL Generated:\\n\\n```sql\\nSELECT \\n  CASE UnitPrice  \\n    WHEN \\'yay\\' THEN \\'wow\\' \\n    WHEN  EXISTS \\n    (\\n        SELECT [EmployeeID],[FirstName]  \\n        FROM [Employees] \\n        WHERE [EmployeeID] = @EmployeeID1\\n    ) THEN \\'Exists!!\\' \\n    WHEN [Quantity] >= @Quantity2 THEN [UnitPrice] \\n    WHEN (([Quantity] / 50) / 50) = @Expr3 THEN [UnitPrice] \\n    WHEN ([Quantity] >= @Quantity4 AND [Quantity] < @Quantity5) THEN 1 \\n    WHEN ([Quantity] >= @Quantity6 AND [Quantity] < @Quantity7) THEN \\n         ([UnitPrice] * 0.8) \\n    ELSE \\'Huh?\\'  \\n    END    \\nFROM [Order Details] \\nWHERE (SUM([Quantity]) >= @Quantity8 AND AVG([Quantity]) < @Quantity9) \\nORDER BY [OrderID] DESC,[Quantity] DESC\\n```\\n\\n## Having Clause\\n\\n```c#\\nEmployeeCollection coll = new EmployeeQuery(\"e\", out var q)\\n  .Select(q.EmployeeID, q.Age.Sum().As(\"TotalAge\"))\\n  .Where(q.EmployeeID.IsNotNull())\\n  .GroupBy(q.EmployeeID)\\n  .Having(q.Age.Sum() > 5)\\n  .OrderBy(q.EmployeeID.Descending)\\n  .ToCollection<EmployeeCollection>();\\n\\nif(coll.Count > 0)\\n{\\n    // Then we loaded at least one record\\n}\\n```\\n\\nSQL Generated:\\n\\n```sql\\nSELECT e.[EmployeeID] AS \\'EmployeeID\\', SUM([Age]) AS \\'TotalAge\\' \\nFROM [dbo].[Employee] e \\nWHERE e.[EmployeeID] IS NOT NULL \\nGROUP BY e.[EmployeeID] \\nHAVING SUM([Age]) > @Age2 \\nORDER BY e.[EmployeeID] DESC\\n```\\n\\n## Getting the Count\\nHere we are getting the count of Employees who have NULL as their ReportsTo ...\\n```c#\\nint count = new EmployeesQuery(\"e\", out var q)\\n  .Select(q.Count())\\n  .Where(q.ReportsTo.IsNull())\\n  .ExecuteScalar<int>();\\n```\\n\\nSQL Generated:\\n\\n```sql\\nSELECT COUNT(*)\\nFROM [Employees] e \\nWHERE e.[ReportsTo] IS NULL\\n```\\n\\n## Raw SQL Injection Everywhere\\nThere may be times when you need to access some SQL feature that is not supported by the DynamicQuery API. But, now having used and fallen in love with DynamicQuery, the last thing you want to do is stop and go write a stored procedure or create a view. We have always supported the raw injection feature in our Select statement, but it will soon be available almost everywhere. The way it works is you pass in raw SQL in the form of a string surrounded by < > angle brackets. That indicates that you want the raw SQL passed directly to the database engine “as is”.\\n\\nHere is an example query. You would never write a query like this in reality. Tiraggo supports this simple query without having to use < > angle brackets. This is just to show all of the places that can accept the raw SQL injection technique:\\n\\n```c#\\nEmployeesCollection coll = new EmployeesQuery(\"e\", out var q)\\n    .Select(\"<FirstName>\", q.HireDate)\\n    .Where(\"<EmployeeID = 1>\")\\n    .GroupBy(\"<FirstName>\", q.HireDate)\\n    .OrderBy(\"<FirstName ASC>\")\\n    .ToCollection<EmployeesCollection>();\\n\\nif (coll.Count > 0)\\n{\\n    // Then we loaded at least one record\\n}\\n```\\n\\nThe SQL Generated is as follows (and works)\\n\\nSQL Generated:\\n\\n```sql\\nSELECT FirstName, e.[HireDate]\\nFROM [Employees] e \\nWHERE (EmployeeID = 1) \\nGROUP BY FirstName,[HireDate] \\nORDER BY FirstName ASC\\n```\\n\\nOf course, you could easily write the above query without injection, but you get the idea. The escape hatch will be available to you almost everywhere ….\\n\\n```c#\\nEmployeesQuery q = new EmployeesQuery();\\nq.Select(q.FirstName);\\n.Where(q.EmployeeID == 1)\\n.OrderBy(q.FirstName.Ascending)\\n.GroupBy(q.FirstName, q.HireDate)\\n```\\n\\nUsing the raw SQL injection techniques above will allow you to invoke SQL functions that we don’t support, including database vender specific SQL, and so on. Hopefully, you will almost never have to resort to writing a custom load method to invoke a stored procedure or an entirely hand written SQL statement. Of course, you can use our native API everywhere and just inject the raw SQL on the GroupBy for instance. You can mix and match to get the desired SQL.\\n\\n## The \\'Filter\\' Property\\nEntitySpaces collections have a \\'Filter\\' property that allows you to use a Linq query to filter and sort a collection after it has been loaded. Basically, any Linq query is valid, you can use Where() and OrderBy() and so on. Setting the \\'Filter\\' property doesn\\'t remove any records, it just temporarily hides them. If you use foreach() you will only see those records that meet the Filter criteria. To remove the filter just set the \\'Filter\\' property to null, all of the original records then become visible again.\\n\\n```c#\\nusing System.Linq;\\n\\nEmployeesCollection coll = new EmployeesCollection();\\nif (coll.LoadAll())\\n{\\n    // Filter on FirstName containing an \"a\"\\n    coll.Filter = coll.AsQueryable().Where(d => d.FirstName.Contains(\"a\"));\\n\\n    foreach (Employees employee in coll)\\n    {\\n        // Each employee\\'s FirstName contains an \\'a\\' \\n    }\\n\\n    // Clear the filter\\n    coll.Filter = null;\\n\\n    foreach (Employees employee in coll)\\n    {\\n        // All employees are now back in the list\\n    }\\n}\\n```\\n\\n## Old School Syntax\\nIf you prefer you can use the old school syntax which doesn\\'t use the generic methods with the \"out var\" technique. See the example below:\\n\\n```c#\\nEmployeesQuery eQuery = new EmployeesQuery(\"e\");\\nOrdersQuery o = new OrdersQuery(\"o\");\\nOrderDetailsQuery od = new OrderDetailsQuery(\"od\");\\n\\n eQuery.Select(eQuery.EmployeeID)\\n.InnerJoin(o).On(eQuery.EmployeeID == o.EmployeeID)\\n.InnerJoin(od).On(o.OrderID == od.OrderID)\\n.Where(o.Freight > 20);\\n\\nEmployeesCollection coll = new EmployeesCollection();\\nif(coll.Load(eQuery))\\n{\\n    // The data was loaded\\n}\\n```\\n\\n## JSON Serialization of Derived Columns\\nEntitySpaces will serialize any derived columns which are brought back by a query via a JOIN, aggregates, or by creating an extra column on the fly via concatenation such as is done with \"fullName\" column shown in the example below. Even though there is not a \"fullName\" property on the Employees object the \"fullName\" value will still serialize correctly. \\n\\n```c#\\nEmployeesCollection coll = new EmployeesQuery(\"e\", out var e)\\n.Select\\n(\\n    e.EmployeeID, e.LastName, e.FirstName,\\n    (e.LastName + \", \" + e.FirstName).As(\"fullName\") // derived column \\n)\\n.OrderBy(e.LastName.Descending)\\n.ToCollection<EmployeesCollection>();\\n\\nif (coll.Count > 0)\\n{\\n    string json = JsonConvert.SerializeObject(coll);\\n}\\n```\\n\\nNotice the \"fullName\" column is present in the JSON, no need for intermediate classes or \"newing\" up anonymous objects.\\n\\n```json\\n[\\n  {\\n    \"EmployeeID\": 6,\\n    \"LastName\": \"Suyama\",\\n    \"FirstName\": \"Michael\",\\n    \"fullName\": \"Suyama, Michael\"\\n  },\\n  {\\n    \"EmployeeID\": 193,\\n    \"LastName\": \"Smith\",\\n    \"FirstName\": \"Frank\",\\n    \"fullName\": \"Smith, Frank\"\\n  }\\n]\\n``` \\n\\n# Modifying Data\\n\\n## Transaction Support\\nEntitySpaces is both Hiearchical and Transactional. If you are saving a nested set of hierarchical objects then a transaction is implicitly created for you. However, if you need to save two disparate unrelated objects as shown in the sample below then you should use an esTransactionScope to ensure they both succeed or fail as a unit.\\n\\n```c#\\nusing (esTransactionScope scope = new esTransactionScope())\\n{\\n    Employees employee = new Employees();\\n    employee.FirstName = \"Mike\";\\n    employee.LastName = \"Griffin\";\\n    employee.Save();\\n\\n    Products product = new Products();\\n    product.ProductName = \"Some Gadget\";\\n    product.Save();\\n\\n    scope.Complete(); // last line of using statement\\n}\\n```\\n\\nIn this example below we are using the EntitySpaces hierarchical model and there is no need to declare an esTransactionScope.\\n\\n```c#\\n// Create an order\\nOrders order = new Orders\\n{\\n    OrderDate = DateTime.Now\\n};\\n\\n// Add an OrderDetails Record to the Order\\norder.OrderDetailsCollection.Add(new OrderDetails\\n{\\n    UnitPrice = 55.00M,\\n    Quantity = 4,\\n    ProductID = 8\\n});\\n\\norder.Save(); // Saves hierarchically\\n```\\n\\n## CRUD Example\\n```c#\\n// Create a new Employee\\nEmployees newEmp = new Employees();\\nnewEmp.FirstName = \"Joe\";\\nnewEmp.LastName = \"Smith\";\\nnewEmp.Save();\\n\\n// Load that same Employee\\nEmployees employee = new Employees();\\nif (employee.LoadByPrimaryKey(newEmp.EmployeeID.Value))\\n{\\n    // Modify that Employee\\n    employee.FirstName = \"Bob\";\\n    employee.Save();\\n\\n    // Delete that Employee\\n    employee.MarkAsDeleted();\\n    employee.Save();\\n}\\n```\\n\\n## Collections\\nCollection are simple enumerable lists of single entities.\\n```c#\\nEmployeesCollection coll = new EmployeesCollection();\\nif (coll.LoadAll())\\n{\\n    foreach (Employees emp in coll)\\n    {\\n        \\n    }\\n}\\n```\\n\\n## Supported Operators\\n\\nUse the native language syntax, it works as you expect it would.\\n\\n|Operator | Description |\\n|:-|:-|\\n| + |plus operator|\\n| - |minus operator|\\n| * |multiple operator|\\n| / |divison operator|\\n| % |mod operator|\\n| > |greater-than operator|\\n| < |less-than operator|\\n| <= |less-than or equal-to operator|\\n| >= |greater-than or equal to operator|\\n| == |equal to operator|\\n| != |not-equal to operator|\\n| && |and operator|\\n| \\\\|\\\\| |or operator|\\n\\n## Sub Operators\\n\\n|Sub Operator | Description |\\n|:-|:-|\\n| ToUpper() |Convert to lower case|\\n| ToLower() |Left trim any leading spaces|\\n| LTrim() |Left trim any trailing spaces|\\n| RTrim() |Right trim any trailing spaces|\\n| Trim() |Trim both leading and trailing spaces|\\n| SubString() |Return a sub-string|\\n| Coalesce() |Return the first non null evaluating expression|\\n| Date() |Returns only the date of a datetime type|\\n| DatePart() |Returns the value of part of a datetime value|\\n| Length() |Return the length|\\n| Round() |Rounds the numeric-expression to the desired places after the decimal point|\\n| Avg() |Average|\\n| Count() |Count operator|\\n| Max() |Maximum Value|\\n| Min() |Minimum Value|\\n| StdDev() |Standard Deviation|\\n| Var() |Variance|\\n| Sum() |Summation|\\n| Cast() |SQL Cast|\\n\\n## \"Over\" Clause Operators\\nFor information on the following operators see [SELECT - OVER Clause (Transact-SQL)](https://docs.microsoft.com/en-us/sql/t-sql/queries/select-over-clause-transact-sql?view=sql-server-ver15).\\n\\nTypical syntax is **OVER**( **PARTITION BY** \\'clause\\' **ORDER BY** \\'clause\\' **ROWS** or **RANGE** \\'clause\\')\\n\\n### Ranking Functions\\n\\n|Sub Operator | SQL Function | \\n|:-|:-|\\n| Over.RowNumber() |ROW_NUMBER()|\\n| Over.Rank()|RANK()|\\n| Over.DenseRank()|DENSE_RANK()|\\n| Over.PercentRank()|PERCENT_RANK()|\\n| Over.Ntile()|NTILE()|\\n\\n### Aggregate Functions\\n\\n|Sub Operator | SQL Function | \\n|:-|:-|\\n|Over.Avg()| AVG() OVER() |\\n|Over.Count()| COUNT() OVER() |\\n|Over.CountBig()| COUNT_BIG() OVER() |\\n|Over.Max()| MAX() OVER() |\\n|Over.Min()| MIN OVER() |\\n|Over.StdDev()| STDDEV() OVER |\\n|Over.StdDevP()| STDDEVP() OVER() |\\n|Over.Var()| VAR() OVER()|\\n|Over.VarP()| VARP() OVER() |\\n\\n### Analytical Functions\\n\\n|Sub Operator | SQL Function | \\n|:-|:-|\\n|Over.CumeDist()|CUME_DIST()|\\n|Over.FirstValue()|FIRST_VALUE()|\\n|Over.LastValue()|LAST_VALUE()|\\n|Over.Lag()|LAG()|\\n|Over.Lead()|LEAD()|\\n|Over.PercentileCont()|PERCENTILE_CONT()|\\n|Over.PercentileDisc()|PERCENTILE_DISC()|\\n\\n\\n# Setup\\n\\n1. Install [EntitySpaces Studio](https://github.com/MikeGriffinReborn/EntitySpaces/raw/master/EntitySpaces.Studio/EntitySpacesStudio_20191.1218.0.zip?raw=true/ \"Zip File\")\\n\\n## NuGet Package(s)\\n\\n* SQL Server - [EntitySpaces.ORM.SqlServer](https://www.nuget.org/packages/EntitySpaces.ORM.SqlServer/ \"NuGet\") \\n* SQLite - [EntitySpaces.ORM.SQLite](https://www.nuget.org/packages/EntitySpaces.ORM.SQLite/ \"NuGet\") \\n* MySQL - [EntitySpaces.ORM.MySQL](https://www.nuget.org/packages/EntitySpaces.ORM.MySQL/ \"NuGet\")\\n* PostgreSQL - [EntitySpaces.ORM.PostgreSQL](https://www.nuget.org/packages/EntitySpaces.ORM.PostgreSQL)\\n\\n**Generating your Classes via EntitySpaces Studio**\\nIt\\'s very simple. You only need to execute two templates. The Custom classes are generated only once, that is where you can add custom code and overide EntitySpaces functionality if need be. The Generated classes are generated any time your database schema changes, you never edit these classes.\\n\\nHowever, first you will need to go to the \"Settings\" tab and then the \"Connection\" tab and connect to your database, there is a dialog box that can help you do that, it\\'s very simple.\\n\\n<img src=\"https://raw.githubusercontent.com/MikeGriffinReborn/EntitySpaces/master/docs/Studio.PNG\" alt=\"EntitySpaces Studio\" width=\"632\" height=\"406\">\\n\\n**Setup SQL Server connection string in your C# .NET Project**\\n\\n```c#\\n// esDataProviderFactory is a one time setup \\nesProviderFactory.Factory = new EntitySpaces.Loader.esDataProviderFactory();\\n\\n// Add a connection\\nesConnectionElement conn = new esConnectionElement();\\nconn.Provider = \"EntitySpaces.SqlClientProvider\";\\nconn.DatabaseVersion = \"2012\";\\nconn.ConnectionString = \"User ID=sa;Password=blank;Initial Catalog=Northwind;Data Source=localhost\";\\nesConfigSettings.ConnectionInfo.Connections.Add(conn);\\n```\\n\\n**Setup SQLite connection string in your C# .NET Project**\\n\\n```c#\\n// esDataProviderFactory is a one time setup \\nesProviderFactory.Factory = new EntitySpaces.Loader.esDataProviderFactory();\\n\\n// Add a connection\\nesConnectionElement conn = new esConnectionElement();\\nconn.Provider = \"EntitySpaces.SQLiteProvider\";\\nconn.DatabaseVersion = \"2012\";\\nconn.ConnectionString = @\"Data Source=C:\\\\MyFolder\\\\Northwind.db3;Version=3;\";\\nesConfigSettings.ConnectionInfo.Connections.Add(conn);\\n```\\n\\n**Setup MySQL connection string in your C# .NET Project**\\n\\n```c#\\n// esDataProviderFactory is a one time setup \\nesProviderFactory.Factory = new EntitySpaces.Loader.esDataProviderFactory();\\n\\n// Add a connection\\nesConnectionElement conn = new esConnectionElement();\\nconn.Provider = \"EntitySpaces.MySqlProvider\";\\nconn.DatabaseVersion = \"2012\";\\nconn.ConnectionString = \"Database=Northwind;Data Source=localhost;User Id=myuser;Password=mypassword;\";\\nesConfigSettings.ConnectionInfo.Connections.Add(conn);\\n```\\n'},\n",
       " {'repo': 'thompsonb/vecalign',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Vecalign\\n\\nVecalign is an accurate sentence alignment algorithm which is fast even for very long documents.\\nIn conjunction with [LASER](https://github.com/facebookresearch/LASER), Vecalign \\nworks in about 100 languages (i.e. 100^2 language pairs), \\nwithout the need for a machine translation system or lexicon. \\n\\nVecalign uses similarity of multilingual sentence embeddings to judge the similarity of sentences.\\n\\n![multilingual_sentence_embedding image](media/multilingual_sentence_embedding.png)\\n[image based on [this Facebook AI post](https://engineering.fb.com/ai-research/laser-multilingual-sentence-embeddings/)]\\n\\nVecalign uses an approximation to Dynamic Programming based on \\n[Fast Dynamic Time Warping](https://content.iospress.com/articles/intelligent-data-analysis/ida00303)\\nwhich is linear in time and space with respect to the number of sentences being aligned. \\n\\n![dynamic_programing_approximation visualization](media/dynamic_programing_approximation.gif)\\n\\n### License \\n\\nCopyright 2019 Brian Thompson\\n\\nVecalign is released under the [Apache License, Version 2.0](LICENSE).\\nFor convenience, the dev and test datasets from Bleualign are provided. Bleualign is Copyright 2010 Rico Sennrich and is released under the [GNU General Public License Version 2](bleualign_data/LICENSE)\\n\\n### Build Vecalign\\n\\nYou will need python 3.6+ with numpy and cython. You can build an environment using conda as follows:\\n\\n```\\n# Use latest conda\\nconda update conda -y\\n# Create conda environment\\nconda create  --force -y --name vecalign python=3.7\\n# Activate new environment\\nsource `conda info --base`/etc/profile.d/conda.sh # See: https://github.com/conda/conda/issues/7980\\nconda activate vecalign\\n# Install required packages\\nconda install -y -c anaconda cython\\nconda install -y -c anaconda numpy\\npip install mcerp \\n```\\n\\nNote that Vecalign contains cython code, but there is no need to build it manually as it is compiled automatically by [pyximport](https://github.com/cython/cython/tree/master/pyximport).\\n\\n### Run Vecalign (using provided embeddings)\\n```\\n./vecalign.py --alignment_max_size 8 --src bleualign_data/dev.de --tgt bleualign_data/dev.fr \\\\\\n   --src_embed bleualign_data/overlaps.de bleualign_data/overlaps.de.emb  \\\\\\n   --tgt_embed bleualign_data/overlaps.fr bleualign_data/overlaps.fr.emb\\n```\\n\\nAlignments are written to stdout:\\n```\\n[0]:[0]:0.156006\\n[1]:[1]:0.160997\\n[2]:[2]:0.217155\\n[3]:[3]:0.361439\\n[4]:[4]:0.346332\\n[5]:[5]:0.211873\\n[6]:[6, 7, 8]:0.507506\\n[7]:[9]:0.252747\\n[8, 9]:[10, 11, 12]:0.139594\\n[10, 11]:[13]:0.273751\\n[12]:[14]:0.165397\\n[13]:[15, 16, 17]:0.436312\\n[14]:[18, 19, 20, 21]:0.734142\\n[]:[22]:0.000000\\n[]:[23]:0.000000\\n[]:[24]:0.000000\\n[]:[25]:0.000000\\n[15]:[26, 27, 28]:0.840094\\n...\\n```\\n\\nThe first two entries are the source and target sentence indexes for each alignment, respectively. \\nThe third entry in each line is the sentence alignment cost computed by Vecalign. \\nNote that this cost includes normalization but does *not* include the penalties terms for containing more than one sentence. \\nNote that the alignment cost is set to zero for insertions/deletions. \\nAlso note that the results may vary slightly due to randomness in the normalization.\\n\\nTo score against a gold alignment, use the \"-g\" flag.\\nFlags \"-s\", \"-t\", and \"-g\" can accept multiple arguments. This is primarily useful for scoring, as the output alignments will all be concatenated together in stdout. For example, to align and score the bleualign test set: \\n```\\n./vecalign.py --alignment_max_size 8 --src bleualign_data/test*.de --tgt bleualign_data/test*.fr \\\\\\n   --gold bleualign_data/test*.defr  \\\\\\n   --src_embed bleualign_data/overlaps.de bleualign_data/overlaps.de.emb  \\\\\\n   --tgt_embed bleualign_data/overlaps.fr bleualign_data/overlaps.fr.emb > /dev/null\\n```\\nWhich should give you results that approximately match the Vecalign paper:\\n\\n```\\n\\n ---------------------------------\\n|             |  Strict |    Lax  |\\n| Precision   |   0.899 |   0.985 |\\n| Recall      |   0.904 |   0.987 |\\n| F1          |   0.902 |   0.986 |\\n ---------------------------------\\n```\\n\\nNote: Run `./vecalign.py -h` for full sentence alignment usage and options. \\nFor stand-alone scoring against a gold reference, see [score.py](score.py)\\n\\n### Embed your own documents\\n\\nThe Vecalign repository contains overlap and embedding files for the Bluealign dev/test files. \\nThis section shows how those files were made, as an example for running on new data.\\n\\nVecalign requires not only embeddings of sentences in each document, \\nbut also embeddings of *concatenations* of consecutive sentences.\\nThe embeddings of multiple, consecutive sentences are needed to consider 1-many, many-1, and many-many alignments.\\n\\n\\nTo create a file containing all the sentence combinations in the dev and test files from Bleualign:\\n```\\n./overlap.py -i bleualign_data/dev.fr bleualign_data/test*.fr -o bleualign_data/overlaps.fr -n 10\\n./overlap.py -i bleualign_data/dev.de bleualign_data/test*.de -o bleualign_data/overlaps.de -n 10\\n```\\n\\nNote: Run `./overlap.py -h` to see full set of embedding options. \\n\\n`bleualign_data/overlaps.fr` and `bleualign_data/overlaps.de` are text files containing one or more sentences per line. \\n\\nThese files must then be embedded using a multilingual sentence embedder.\\n\\nWe recommend the [Language-Agnostic SEntence Representations (LASER)](https://github.com/facebookresearch/LASER) \\ntoolkit from Facebook, as it has strong performance and comes with a pretrained model which works well in about 100 languages. \\nHowever, Vecalign should also work with other embedding methods as well. Embeddings should be provided as a binary file containing float32 values.\\n\\nThe following assumes LASER is installed and the LASER environmental variable has been set.\\n\\nTo embed the Bleualign files using LASER:\\n```\\n$LASER/tasks/embed/embed.sh bleualign_data/overlaps.fr bleualign_data/overlaps.fr.emb [fra]\\n$LASER/tasks/embed/embed.sh bleualign_data/overlaps.de bleualign_data/overlaps.de.emb [deu]\\n```\\n\\n> Please always refer [here](https://github.com/facebookresearch/LASER/blob/main/tasks/embed/README.md) for the latest usage of this script. The usage may vary across the different versions of LASER.\\n\\nNote that LASER will not overwrite an embedding file if it exsts, so you may need to run first `rm bleualign_data/overlaps.fr.emb bleualign_data/overlaps.de.emb`.\\n\\n### Document Alignment\\n\\n[We propose](https://aclanthology.org/2020.emnlp-main.483) using Vecalign to rescore document alignment candidates, \\nin conjunction with candidate generation using a document embedding method that retains sentence order information.\\nExample code for our document embedding method is provided [here](standalone_document_embedding_demo.py).\\n\\n### Publications\\n\\nIf you use Vecalign, please cite our [Vecalign paper](https://www.aclweb.org/anthology/D19-1136):\\n\\n```\\n@inproceedings{thompson-koehn-2019-vecalign,\\n    title = \"{V}ecalign: Improved Sentence Alignment in Linear Time and Space\",\\n    author = \"Thompson, Brian and Koehn, Philipp\",\\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)\",\\n    month = nov,\\n    year = \"2019\",\\n    address = \"Hong Kong, China\",\\n    publisher = \"Association for Computational Linguistics\",\\n    url = \"https://www.aclweb.org/anthology/D19-1136\",\\n    doi = \"10.18653/v1/D19-1136\",\\n    pages = \"1342--1348\",\\n}\\n```\\n\\nIf you use the provided document embedding code or use Vecalign for document alignment, please cite our [document alignment paper](https://aclanthology.org/2020.emnlp-main.483):\\n\\n```\\n@inproceedings{thompson-koehn-2020-exploiting,\\n    title = \"Exploiting Sentence Order in Document Alignment\",\\n    author = \"Thompson, Brian  and\\n      Koehn, Philipp\",\\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)\",\\n    month = nov,\\n    year = \"2020\",\\n    address = \"Online\",\\n    publisher = \"Association for Computational Linguistics\",\\n    url = \"https://aclanthology.org/2020.emnlp-main.483\",\\n    doi = \"10.18653/v1/2020.emnlp-main.483\",\\n    pages = \"5997--6007\",\\n}\\n```\\n'},\n",
       " {'repo': 'rileyjshaw/terra',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': \"terra\\n=====\\n\\nJS library for cellular automata and simple biological simulations. Documentation and examples live [here](http://rileyjshaw.com/terra/).\\n\\n## Hacking this library\\nTo build terra on your machine you'll need [Node.js](http://nodejs.org/), [Bower](http://bower.io/), and [gulp](http://gulpjs.com/) installed. Then...\\n\\n```.bash\\ncd path/to/terra\\nnpm install\\nbower install\\ngulp\\n```\\n\\n## Contributing\\nAt this stage **the most important way you can help is to use the library**. The API is in Beta and still flexible. If you discover something that's confusing or hard to work with, document it [here](https://github.com/rileyjshaw/terra/issues). Come up with an idea and try to build it; by using and testing the library you'll find bugs or usability issues that would otherwise go unnoticed.\\nIf you want to make a pull-request on anything labeled 'major', be sure to join the discussion first so we can talk architecture.\\nIf anyone's willing to get the ball rolling on [unit tests](https://github.com/rileyjshaw/terra/issues/16), [you will be my hero](http://youtu.be/koJlIGDImiU).\\n\\n\\nThat's all, folks! MIT, remixing strongly encouraged.\\n\"},\n",
       " {'repo': 'theSage21/lorentz-embeddings',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"Lorentz Embeddings\\n==================\\n\\n\\nA pytorch implementation of [Learning Continuous Hierarchies in the Lorentz Model of Hyperbolic Geometry](https://arxiv.org/pdf/1806.03417.pdf?noredirect=1).\\n\\n> We are concerned with the discovery of hierarchical relationships from large-scale unstructured similarity scores. For this purpose, we study different models of hyperbolic space and find that learning embeddings in the Lorentz model is substantially more efficient than in the Poincaré-ball model. We show that the proposed approach allows us to learn high-quality embeddings of large taxonomies which yield improvements over Poincaré embeddings, especially in low dimensions. Lastly, we apply our model to discover hierarchies in two real-world datasets: we show that an embedding in hyperbolic space can reveal important aspects of a company’s organizational structure as well as reveal historical relationships between language families.\\n\\nAn example of a binary tree being embedded in the Lorentz space and then visualized using Poincaré space.\\n![Binary Tree Embedding](embeddings/binary_tree.png)\\n\\n**NOTE** :  [@lambdaofgod](https://github.com/lambdaofgod) has generously ported this work to use sparse matrices and a bunch of other nice things! Go check out the PR at https://github.com/theSage21/lorentz-embeddings/pull/15 . We haven't merged that work since it does not have updated examples. If you have the time please go ahead and submit a PR to either their repo or this one.\\n\\nUsage\\n-----\\n\\nBinary tree embedding and visualization.\\n\\n```bash\\n# See this for more options\\npython lorentz.py --help\\n\\n\\npython lorentz.py bin_mat  # run binary tree\\n\\n\\n# plot the checkpoint's embeddings for all saved checkpoints\\n# in poincare space\\npython lorentz.py bin_mat -plot -ckpt ckpt  # plot only embeddings\\npython lorentz.py bin_mat -plot -ckpt ckpt -plot_graph  # plot graph also\\npython lorentz.py bin_mat -plot -ckpt ckpt -plot_graph  -overwrite_plots # overwrite plots\\npython lorentz.py bin_mat -plot -ckpt ckpt -plot_graph  -plot_size 10 # make a large plot\\n```\\n\\nTo embed an arbitrary graph\\n\\n1. Add a numpy matrix in the `datasets.py` file with a unique name (`my_graph` for example). This represents a directed adjacency matrix\\n2. Now you can simply call `python lorentz.py my_graph` to embed your graph.\\n3. You can use tensorboard to watch the progress with `tensorboard --logdir runs`.\\n4. You can plot the embeddings using `python lorentz.py my_graph -plot -ckpt ckpt`\\n\\n\\nFor anything else `python lorentz.py --help`\\n\"},\n",
       " {'repo': 'jbengtson/ksp-precisenode',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': 'ksp-precisenode\\n===============\\n\\nPreciseNode for Kerbal Space Program\\n\\nProvides a more precise widget for maneuver node editing.  Whenever the map view is open and a maneuver node exists for the currently active vessel, the PreciseNode window will show.  You can edit the time that the maneuver will take place, edit prograde, radial, and normal vector magnitudes, see the final delta-V required for the maneuver, and also see the ejection angle of the maneuver from the orbited body.  This makes the mod excellent for use with porkchop plots that require precision burns.  PreciseNode allows you to change the conics drawing mode for better maneuver planning.  You can also control the PreciseNode widget from user-configurable keyboard shortcuts.\\n'},\n",
       " {'repo': 'hsharrison/pypsr',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': 'pypsr\\n=====\\n\\nA work-in-progress library for phase-space reconstruction in Python!\\n'},\n",
       " {'repo': 'zalandoresearch/probrnn',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Probabilistic RNNs for sequential data with missing values\\n\\nEU project 732328: \"Fashion Brain\".\\n\\nD1.4: \"Software Requirements: SSM library for time-series modeling and trend prediction\".\\n\\n## Tasks\\n\\n### Prediction and forecasting\\n\\nWe assume we are given sample paths from a time-series: \\nThus:\\n\\n![alt text](./img/timeseries.png)\\n\\nWhere the sample paths are all drawn from an underlying model:\\n\\n![alt text](./img/dist.png)\\n\\nThe aim in forecasting is to predict future time-points from sample past time points.\\nI.e. we would be interested in estimating:\\n\\n![alt text](./img/expectation.png)\\n\\nor\\n\\n![alt text](./img/conditional.png)\\n\\nIn this package we follow the ansatz that the dependency is completely determined by the hidden state of a recurrent neural network.\\n\\n![alt text](./img/hiddep.png)\\n\\nwhere the RNN recurrence relation is given by:\\n\\n![alt text](./img/recur.png)\\n\\n### Distribution modeling with NADE\\n\\nHere we are interested in estimating the distribution of a multivariate vector without necessarily assuming a temporal or sequential ordering.\\nNevertheless we still have a decomposition of the joint distribution:\\n\\n![alt text](./img/decomposition.png)\\n\\nWhich may be modeled again using an RNN:\\n\\n![alt text](./img/nade.png)\\n\\nThis approach is known as [neural autoregressive distribution estimation (NADE)](http://proceedings.mlr.press/v15/larochelle11a/larochelle11a.pdf).\\n\\n### Missing value estimation\\n\\nIn many applications missing data is an issue which hampers both training and inference.\\nThus we are given a sequence of data where only a subsequence is available. For example:\\n\\n![alt text](./img/missing.png)\\n\\nAssume we are given an RNN model:\\n\\n![alt text](./img/recur.png)\\n\\nand a parameterized estimate of the conditional distribution:\\n\\n![alt text](./img/kernel.png)\\n\\nIn this package we take a [sequential importance sampling (SIS)](https://en.wikipedia.org/wiki/Particle_filter) approach to inferring missing data given this model of the time-series.\\nIf, additionally, data is missing at training time, we employ an [expectation-maximization (EM)](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm) training algorithm, rather than a standard backpropagation through (BPTT) time algorithm.\\n\\n### Multivariate temporal dependencies\\n\\nIn the simplest case suppose we have two time series. Temporal dependencies may be modeled by treating the interleaved sequence:\\n\\n![alt text](./img/interleaved.png)\\n\\nAt test time, predicting one unseen time-series given an observed time-series may be treated as a missing value problem and applying SIS to the sequence:\\n\\n![alt text](./img/missinginterleaved.png)\\n\\n\\n## Python package\\n\\n### Structure\\n\\n![alt text](./img/flow.png)\\n\\n### Getting started\\n\\nInstallation:\\n\\n``` bash \\ngit clone https://github.com/zalandoresearch/probrnn.git\\ncd probnn/\\nmake install\\n```\\n\\nInstallation in development mode:\\n\\n``` bash \\ngit clone https://github.com/zalandoresearch/probrnn.git\\ncd probnn/\\nmake develop\\n```\\n\\nRunning the tests:\\n\\n``` bash\\nmake clean\\nmake test\\n```\\n\\n### Usage\\n\\nSetting up NADE data\\n\\n```python\\nfrom probrnn import data\\nimport numpy as np\\n\\nx = np.random.randn(1000, 10)\\ndatastruct = data.NadeWrapper(x)\\n```\\n\\nSetting up time-series data\\n\\n```python\\nx = np.random.randn(10000)\\ndatastruct = data.TimeSeries(x)\\n```\\n\\nSetting up parameters for learning\\n```python\\nparams = \\\\\\n    {\\n        \"N_ITERATIONS\": 10 ** 5, # no of batches to pass in total\\n        \"VALIDATE_EACH\": 100, # how often to check error on validation data\\n        \"SAVE_EACH\": 1000, # how often to save model\\n        \"LOG_EVERY\": 50, # how often to log\\n        \"LEARNING_RATE\": 0.0001, # learning rate of learning\\n        \"N_HIDDEN\": 256, # number of hidden units in RNN\\n        \"N_BINS\": 50, # number of bins to discretize data\\n        \"BATCH_SIZE\": 50, # number of samples per batch\\n    }\\n```\\n\\nGet a NADE model\\n```python\\nfrom probrnn import models\\n\\nmodel = models.NADE(datastruct, params=params)\\n```\\n\\nDo the training\\n```python\\ntraining = models.Training(model, \"test_model\", \"test_log.json\")\\ncallback = lambda err, i, _: print \"loss: {err}; iteration {i}\".format(err=err, i=i)\\ntraining.train(callback)\\n```\\n\\nSame thing but with missing values\\n```python\\nfrom probrnn import inference\\n\\nimputer = lambda a, b: inference.NaiveSIS(a, b)\\ntraining = models.Training(model, \"test_model\", \"test_log.json\", imputer=imputer)\\ntraining.train(callback)\\n```\\n\\nDo imputation at test time\\n\\n```python\\nx[np.random.choice(len(x), replace=False, size=50)] = np.nan\\nestimate = imputer(model, x).estimate()\\n```\\n### Examples\\n\\nExample notebooks are in ```./examples/```\\n\\n## License\\n\\nThe MIT License (MIT)\\nCopyright (c) 2016 Zalando SE\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n\\n'},\n",
       " {'repo': 'trehn/termtrack',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"TermTrack\\n---------\\n\\nTrack orbiting objects (such as the International Space Station) in your terminal!\\n\\n.. image:: https://raw.githubusercontent.com/trehn/termtrack/master/screenshot.png\\n    :alt: Screenshot\\n\\nRequires Python 3.3+ and a terminal with 256 colors. A black background is highly recommended.\\n\\n.. code-block::\\n\\n\\tpip3 install termtrack\\n\\ttermtrack -figmntxo 1 iss\\n\\n.. code-block::\\n\\n\\tUsage: termtrack [OPTIONS] [SATELLITE]\\n\\n\\t  Shows a world map tracking SATELLITE. Valid values for SATELLITE are\\n\\t  numbers from http://www.celestrak.com/NORAD/elements/master.php (for\\n\\t  your convenience, a number of aliases have been provided).\\n\\n\\t  Example satellite aliases (find more with --aliases):\\n\\t      hubble          Hubble Space Telescope\\n\\t      iss             International Space Station\\n\\n\\t  Hotkeys:\\n\\t      a       Toggle apsides markers\\n\\t      c       Toggle next-orbit coverage overlay\\n\\t      d       Toggle ascent/descent markers\\n\\t      f       Toggle footprint (satellite horizon)\\n\\t      g       Toggle latitude/longitude grid\\n\\t      i       Toggle info panels\\n\\t      n       Toggle night shading\\n\\t      o       Cycle through drawing 0-3 next orbits\\n\\t      p       Pause/resume\\n\\t      q       Quit\\n\\t      r       Reset plotted time to current\\n\\t      t       Toggle topography\\n\\t      x       Toggle crosshair\\n\\t      left    Small step back in time\\n\\t      right   Small step forward in time\\n\\t      down    Large step back in time\\n\\t      up      Large step forward in time\\n\\n\\tOptions:\\n\\t  --aliases                 Show all satellite aliases and exit\\n\\t  --apsides                 Draw apoapsis and periapsis markers\\n\\t  -b, --body BODY           Which celestial body to draw: Earth, Moon or Mars\\n\\t                            (defaults to Earth)\\n\\t  -c, --coverage            Show next-orbit coverage overlay\\n\\t  -f, --footprint           Draw satellite footprint/horizon\\n\\t  --fps N                   Frames per second (defaults to 1)\\n\\t  -g, --grid                Draw latitude/longitude grid\\n\\t  -i, --info                Show info panels\\n\\t  -m, --me                  Auto-detect your location as observer\\n\\t  -n, --night               Shade night side\\n\\t  -o, --orbits N            Draw this many orbits ahead of the satellite\\n\\t  --orbit-ascdesc           Draw orbits with ascent/descent markers\\n\\t  -O, --observer 'LAT LON'  Space-separated latitude and longitude of an\\n\\t                            observer; overrides IP-geolocation\\n\\t  -p, --paused              Start paused\\n\\t  -P, --planets PLANETS     Comma-separated list of celestial objects to draw\\n\\t                            (e.g. 'sun,moon')\\n\\t  -r, --orbit-res [/]N[+]   Set distance of orbit markers: 'N' means N\\n\\t                            minutes, '/N' means 1/Nth of orbital period,\\n\\t                            append a plus sign to interpolate in between\\n\\t                            markers (defaults to /70)\\n\\t  -t, --topo                Enable coloring of topographical features\\n\\t  --tle FILE                read TLE data from FILE instead of downloading it\\n\\t                            (SATELLITE will have no effect and can be omitted)\\n\\t  -x, --crosshair           Draw crosshair around satellite location\\n\\t  --version                 Show version and exit\\n\\t  --help                    Show this message and exit\\n\\nCredit goes to `vain/asciiworld <https://github.com/vain/asciiworld>`_ for inspiration and some tasty pieces of code.\\n\\n------------------------------------------------------------------------\\n\\n.. image:: http://img.shields.io/pypi/v/termtrack.svg\\n    :target: https://pypi.python.org/pypi/termtrack/\\n    :alt: Latest Version\\n\\n.. image:: http://img.shields.io/badge/Python-3.3+-green.svg\\n    :target: https://pypi.python.org/pypi/termtrack/\\n    :alt: Python 3.3+\\n\\n.. image:: http://img.shields.io/badge/License-GPLv3-red.svg\\n    :target: https://pypi.python.org/pypi/termtrack/\\n    :alt: License\\n\\n------------------------------------------------------------------------\\n\\nHow Stuff Works\\n===============\\n\\nTo draw the map, TermTrack will look at a shapefile from `Natural Earth <http://www.naturalearthdata.com>`_ in order to find coordinates that are within a landmass. While computationally expensive, this method yields the most accurate and good-looking maps at all terminal sizes. To determine the color of each pixel, a relatively low-resolution and low-quality JPEG image is used. If you look at the image (``termtrack/data/earth.jpg``), you'll notice it has green oceans. This is to ensure that ocean blue will not spill over into coastal areas during downsampling. Same goes for the expanded white coast of Antarctica. Finally, the image has been tuned to produce good-looking colors against a black background. The resolution and quality of the image is not really a concern since we do not need maximum per-pixel precision to make the Sahara appear yellow. After computing land/ocean status and land color, this information is cached in ``~/.termtrack_map_cache``, so it will not have to be rendered again for the current terminal size.\\n\\nFor Mars and the Moon there is no shapefile to read and the entire area is colored according to similar JPEG color maps.\\n\\nNight shading for each pixel is done by looking at the Sun's elevation (as computed by `pyephem <http://rhodesmill.org/pyephem/>`_) and shifting the color of the pixel towards blue accordingly. Twilight starts when the Sun is 18° below the horizon (`astronomical twilight <https://en.wikipedia.org/wiki/Twilight#Astronomical_twilight>`_) and ends when it has risen to 0°.\\n\\nSatellite locations are derived from `TLE <https://en.wikipedia.org/wiki/Two-line_element_set>`_ data downloaded from `CelesTrak <https://celestrak.com/>`_. The data is fed into pyephem where the current position of the satellite is computed using `SGP4 <https://en.wikipedia.org/wiki/Simplified_perturbations_models>`_. Most of the data you see in the info panels is provided by pyephem, but the apsides' locations as well as the satellite footprint outline are computed by TermTrack itself.\\n\\n\\nKnown Issues\\n============\\n\\nWhen looking at the ISS, you may notice some inconsistencies:\\n\\n* the apoapsis/periapsis altitudes from the info panel do not match up with live altitude values when the satellite actually is at that point\\n* sometimes the current altitude is lower/higher than periapsis/apoapsis altitude\\n* the location of apoapsis/periapsis markers from ``--apsides`` are not located at the transition points between plus and minus signs drawn by ``--orbit-ascdesc``\\n\\nWhere do these errors come from? The locations of the apsides are derived from the true anomaly which matches values from http://www.satellite-calculations.com/TLETracker/SatTracker.htm so I'm assuming that's not the source of the error. The shape of the Earth also does not explain the deviations in altitude.\\n\\nInterestingly enough, when you look at more eccentric orbits like that of QZS-1 (37158) the errors seem to disappear, suggesting that the issue is merely inaccuracy instead of a plain wrong calculation somewhere.\\n\"},\n",
       " {'repo': 'fdb/space-game',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Space Invaders\\n\\n![Screenshot](https://raw.githubusercontent.com/HackYourFutureBelgium/JavaScript2/master/Projects/space-game/screenshot.png)\\n\\nA clone of [Space Invaders](https://en.wikipedia.org/wiki/Space_Invaders), specially built for teaching purposes!\\n\\n[Play the game here!](https://www.enigmeta.com/spacegame/final/)\\n\\n## Step By Step\\n\\nThe project is divided into multiple steps. The `final` folder shows the full game.\\n\\n* [**Step 1**](./step01): Creating and moving the player\\n* [**Step 2**](./step02): Correct key input\\n* [**Step 3**](./step03): Correct timing\\n* [**Step 4**](./step04): Firing lasers\\n* [**Step 5**](./step05): Removing lasers\\n* [**Step 6**](./step06): Creating the enemies\\n* [**Step 7**](./step07): Hit detection\\n* [**Step 8**](./step08): Enemy lasers\\n* [**Step 9**](./step09): Random cooldown\\n* [**Step 10**](./step10): Losing the game\\n* [**Step 11**](./step11): Winning the game\\n\\n## Credits\\n\\n* Game assets by [Kenney](http://kenney.nl/assets/space-shooter-redux)\\n'},\n",
       " {'repo': 'fabiospampinato/phoenix',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '\\n# Phoenix\\n\\nMy [Phoenix](https://github.com/kasper/phoenix) setup. Powerful, easy to customize, tuned for web development, adds a space switcher.\\n\\n## Installation\\n\\nThis configuration uses a remap of the Caps Lock key to something more useful: the Hyper key <kbd>⇪</kbd> (basically just <kbd>ctrl + alt + cmd</kbd> combined into one key) if used in combination with other keys, otherwise it gets mapped to <kbd>F18</kbd>, which is used to trigger the space switcher. If you prefer you may skip these remap steps while you try the configuration, but if you usually have many spaces opened I highly recommend you not to miss out the awesome space switcher, for which this hack is a requirement.\\n\\n1. Install [Phoenix](https://github.com/kasper/phoenix#install)\\n2. Install [Karabiner Elements](https://github.com/tekezo/Karabiner-Elements) via its [dmg](https://pqrs.org/latest/karabiner-elements-latest.dmg)\\n3. Replace Caps Lock with Hyper/F18 using [this](http://tinyurl.com/yc8m5qe8) Karabiner Elements configuration (if the link doesn\\'t work copy and paste this in a browser: `karabiner://karabiner/assets/complex_modifications/import?url=https%3A%2F%2Fraw.githubusercontent.com%2Ffabiospampinato%2Fphoenix%2Fmaster%2Fconfig%2Fkarabiner.json`)\\n4. `$ mkdir ~/.config`\\n5. `$ cd ~/.config`\\n6. `$ git clone git@github.com:fabiospampinato/phoenix.git`\\n7. Restart Phoenix\\n8. Enjoy!\\n\\n## Customization\\n\\nTo disable specific features, just comment out their correspondent `require` call in [phoenix.js](https://github.com/fabiospampinato/phoenix/blob/master/phoenix.js).\\n\\nTo tweak some numbers, simply edit [constants.js](https://github.com/fabiospampinato/phoenix/blob/master/config/constants.js).\\n\\nChanging the specific shortcut used to trigger an action is pretty trivial.\\n\\nDon\\'t forget to make a PR if you fixed something or implemented something cool :)\\n\\n## Shortcuts\\n\\n### Sides\\n\\n<p align=\"center\">\\n\\t<img src=\"resources/sides.gif\" alt=\"Sides\" style=\"width:690px\">\\n</p>\\n\\n| Shortcut | Description |\\n| -------- | ----------- |\\n| <kbd>⇪ + ↑</kbd> | Move window to the top side |\\n| <kbd>⇪ + →</kbd> | Move window to the right side |\\n| <kbd>⇪ + ↓</kbd> | Move window to the bottom side |\\n| <kbd>⇪ + ←</kbd> | Move window to the left side |\\n\\n### Corners\\n\\n<p align=\"center\">\\n\\t<img src=\"resources/corners.gif\" alt=\"Corners\" style=\"width:690px\">\\n</p>\\n\\n| Shortcut | Description |\\n| -------- | ----------- |\\n| <kbd>⇪ + Q</kbd> | Move window to the top-left corner |\\n| <kbd>⇪ + W</kbd> | Move window to the top-right corner |\\n| <kbd>⇪ + S</kbd> | Move window to the bottom-right corner |\\n| <kbd>⇪ + A</kbd> | Move window to the bottom-left corner |\\n\\n### Halves\\n\\n<p align=\"center\">\\n\\t<img src=\"resources/halves.gif\" alt=\"Halves\" style=\"width:690px\">\\n</p>\\n\\n| Shortcut | Description |\\n| -------- | ----------- |\\n| <kbd>⇪ + [</kbd> | Move window to the 1st half |\\n| <kbd>⇪ + ]</kbd> | Move window to the 2nd half |\\n\\n### Thirds\\n\\n<p align=\"center\">\\n\\t<img src=\"resources/thirds.gif\" alt=\"Thirds\" style=\"width:690px\">\\n</p>\\n\\n| Shortcut | Description |\\n| -------- | ----------- |\\n| <kbd>⇪ + 1</kbd> | Move window to the 1st column |\\n| <kbd>⇪ + 2</kbd> | Move window to the 2nd column |\\n| <kbd>⇪ + 3</kbd> | Move window to the 3rd column |\\n\\n### Sixths\\n\\n<p align=\"center\">\\n\\t<img src=\"resources/sixths.gif\" alt=\"Sixths\" style=\"width:690px\">\\n</p>\\n\\n| Shortcut | Description |\\n| -------- | ----------- |\\n| <kbd>⇪ + shift + Q</kbd> | Move window to the 1st sixth |\\n| <kbd>⇪ + shift + W</kbd> | Move window to the 2nd sixth |\\n| <kbd>⇪ + shift + E</kbd> | Move window to the 3rd sixth |\\n| <kbd>⇪ + shift + A</kbd> | Move window to the 4th sixth |\\n| <kbd>⇪ + shift + S</kbd> | Move window to the 5th sixth |\\n| <kbd>⇪ + shift + D</kbd> | Move window to the 6th sixth |\\n\\n### Center\\n\\n<p align=\"center\">\\n\\t<img src=\"resources/center.gif\" alt=\"Center\" style=\"width:690px\">\\n</p>\\n\\n| Shortcut | Description |\\n| -------- | ----------- |\\n| <kbd>⇪ + X</kbd> | Center the window |\\n| <kbd>⇪ + shift + X</kbd> | Center the window and set its dimensions to 900x600 |\\n\\n### Grow\\n\\n<p align=\"center\">\\n\\t<img src=\"resources/grow.gif\" alt=\"Grow\" style=\"width:690px\">\\n</p>\\n\\n| Shortcut | Description |\\n| -------- | ----------- |\\n| <kbd>⇪ + shift + ↑</kbd> | Grow window from the top |\\n| <kbd>⇪ + shift + →</kbd> | Grow window from the right |\\n| <kbd>⇪ + shift + ↓</kbd> | Grow window from the bottom |\\n| <kbd>⇪ + shift + ←</kbd> | Grow window from the left |\\n\\n### Expand\\n\\n<p align=\"center\">\\n\\t<img src=\"resources/expand.gif\" alt=\"Expand\" style=\"width:690px\">\\n</p>\\n\\n| Shortcut | Description |\\n| -------- | ----------- |\\n| <kbd>⇪ + space</kbd> | Toggle window expansion to fill the space |\\n| <kbd>⇪ + shift + space</kbd> | Toggle window expansion to fullscreen |\\n\\n### Focus or Open\\n\\n<p align=\"center\">\\n\\t<img src=\"resources/focus.gif\" alt=\"Focus or Open\" style=\"width:690px\">\\n</p>\\n\\n| Shortcut | Description |\\n| -------- | ----------- |\\n| <kbd>⇪ + `</kbd> | Focus to or open [Notable](https://notable.md)|\\n| <kbd>⇪ + C</kbd> | Focus to or open [Chrome](https://www.google.com/chrome)|\\n| <kbd>⇪ + D</kbd> | Focus to or open [Chrome Developer Tools](https://developer.chrome.com/devtools)|\\n| <kbd>⇪ + V</kbd> | Focus to or open [Visual Studio Code](https://code.visualstudio.com)|\\n| <kbd>⇪ + F</kbd> | Focus to or open Finder|\\n| <kbd>⇪ + T</kbd> | Focus to or open [iTerm](http://iterm2.com)|\\n| <kbd>⇪ + G</kbd> | Focus to or open [GitTower](https://www.git-tower.com/)|\\n\\n### Spaces\\n\\n<p align=\"center\">\\n\\t<img src=\"resources/spaces.gif\" alt=\"Spaces\" style=\"width:690px\">\\n</p>\\n\\nIn order to make this work you have to open `System Preferences -> Keyboard -> Shortcuts -> Mission Control` and bind all `Switch to Desktop [NUMBER]` actions to <kbd>ctrl + alt + cmd + shift + [NUMBER]</kbd>. There are actions up to the 9th desktop, but they may not be shown to you if you have less then 9 desktops currently open.\\n\\n**Note**: If you don\\'t need wrapping support, you should just remap the `Move left/right a space` actions under `System Preferences -> Keyboard -> Shortcuts -> Mission Control`.\\n\\n| Shortcut | Description |\\n| -------- | ----------- |\\n| <kbd>⇪ + tab</kbd> | Switch to the next space |\\n| <kbd>⇪ + shift + tab </kbd> | Switch to the previous space |\\n\\n### Applications Icons\\n\\n<p align=\"center\">\\n\\t<img src=\"resources/applications_icons.gif\" alt=\"Applications Icons\" style=\"width:690px\">\\n</p>\\n\\n| Shortcut | Description |\\n| -------- | ----------- |\\n| <kbd>⇪ + I</kbd> | For each window in the current space show an icon indicating its position |\\n| <kbd>⇪ + shift + I</kbd> | Display the current date and time | <!-- //TODO: add to gif -->\\n\\n### Reload Phoenix\\n\\n<p align=\"center\">\\n\\t<img src=\"resources/reload_phoenix.gif\" alt=\"Reload Phoenix\" style=\"width:690px\">\\n</p>\\n\\n| Shortcut | Description |\\n| -------- | ----------- |\\n| <kbd>⇪ + shift + P</kbd> | Reload Phoenix |\\n\\n### Pause/Resume Application\\n\\n<p align=\"center\">\\n\\t<img src=\"resources/pause_resume.gif\" alt=\"Pause/Resume Application\" style=\"width:690px\">\\n</p>\\n\\nThis can be used for saving battery, pausing single-player games etc.\\n\\n| Shortcut | Description |\\n| -------- | ----------- |\\n| <kbd>⇪ + F8</kbd> | Pause or resume the current application |\\n\\n### Quit Application\\n\\n<p align=\"center\">\\n\\t<img src=\"resources/quit.gif\" alt=\"Quit Application\" style=\"width:690px\">\\n</p>\\n\\nDid you ever close 3+ Chrome windows instead of a single tab by mistake? Fear no more! Now in order to quit an app you have to trigger <kbd>⌘Q</kbd> twice in a short timeframe. Stop [wasting 10$](https://clickontyler.com/commandq) for something so basic.\\n\\n| Shortcut | Description |\\n| -------- | ----------- |\\n| <kbd>⌘Q</kbd> <kbd>⌘Q</kbd> | Quit application |\\n\\n### Screens\\n\\n> For this to work you need to have the follwing shell functions available: `resolution-set`\\n\\n//TODO: Record gif\\n\\n| Shortcut | Description |\\n| -------- | ----------- |\\n| <kbd>⇪ + Enter</kbd> | Set the resolution of screens |\\n\\n### Music\\n\\n> For this to work you need to have the follwing shell functions available: `music-current-like`\\n\\n//TODO: Record gif\\n\\n| Shortcut | Description |\\n| -------- | ----------- |\\n| <kbd>⇪ + F6</kbd> | Add the current song to the library Apple Music |\\n\\n### Brightness\\n\\n> For this to work you need to have the follwing shell functions available: `brightness-mod`\\n\\n//TODO: Record gif\\n\\n| Shortcut | Description |\\n| -------- | ----------- |\\n| <kbd>⇪ + F1</kbd> | Increase brightness by 10% |\\n| <kbd>⇪ + F2</kbd> | Decrease brightness by 10% |\\n| <kbd>⇪ + shift + F1</kbd> | Increase brightness by 2.5% |\\n| <kbd>⇪ + shift + F2</kbd> | Decrease brightness by 2.5% |\\n\\n### Volume\\n\\n> For this to work you need to have the follwing shell functions available: `volume-mod`\\n\\n//TODO: Record gif\\n\\n| Shortcut | Description |\\n| -------- | ----------- |\\n| <kbd>⇪ + F11</kbd> | Increase volume by 10% |\\n| <kbd>⇪ + F12</kbd> | Decrease volume by 10% |\\n| <kbd>⇪ + shift + F11</kbd> | Increase volume by 2.5% |\\n| <kbd>⇪ + shift + F12</kbd> | Decrease volume by 2.5% |\\n\\n### Split View\\n\\n//TODO: Unfortunately there\\'s no API available for doing this at the moment, that\\'s a shame given that I would have finally found a use for the <kbd>§</kbd> key, it looks like 2 <kbd>S</kbd> => Side-by-Side :D\\n\\n| Shortcut | Description |\\n| -------- | ----------- |\\n| <kbd>⇪ + §</kbd> | If there are only 2 windows in the current space put them in split view |\\n\\n## Mouse\\n\\n### Snapping\\n\\n<p align=\"center\">\\n\\t<img src=\"resources/snapping.gif\" alt=\"Snapping\" style=\"width:690px\">\\n</p>\\n\\nDrag a window to an edge or corner to snap it into place.\\n\\n## Magic\\n\\n### Chrome\\n\\nIf it gets opened, positionate it to the left side.\\n\\n### Chrome Developer Tools\\n\\nIf it gets opened, positionate it to the bottom-right corner, and shrink Visual Studio Code\\'s height a bit, so that the console will be visible.\\n\\nIf it gets closed, restore Visual Studio Code\\'s height.\\n\\n### Terminal/iTerm2/Finder\\n\\nIf one of these apps\\' windows gets opened, positionate it to bottom-left corner.\\n\\n### Visual Studio Code\\n\\nIf it gets opened, positionate it to the right side.\\n\\n## Space Switcher\\n\\n<p align=\"center\">\\n\\t<img src=\"resources/spaces_switcher.gif\" alt=\"Space Switcher\" style=\"width:690px\">\\n</p>\\n\\nThis is the truly great addition to what was already available on the internet.\\n\\nIt combines [Phoenix](https://github.com/kasper/phoenix), [Alfred](https://www.alfredapp.com) and [alfred-spaces-workflow](https://github.com/fabiospampinato/alfred-spaces-workflow) into the space switcher macOS deserves but never had, always just a double Hyper (<kbd>⇪</kbd> <kbd>⇪</kbd>) away.\\n\\nThis is how it works:\\n- It uses Phoenix to compile a list of your spaces\\n- It tries to guess a name for those spaces, by default that would be the name of the folder opened in the Visual Studio Code instance present in that space ([get_space_name.js](https://github.com/fabiospampinato/phoenix/blob/master/helpers/get_space_name.js))\\n- Refreshes the list and those guessed names when necessary\\n- Listens for the double Hyper (<kbd>⇪</kbd> <kbd>⇪</kbd>) shortcut\\n- Opens [Alfred](https://www.alfredapp.com) and triggers [alfred-spaces-workflow](https://github.com/fabiospampinato/alfred-spaces-workflow)\\n- Which reads the list of spaces compiled with Phoenix and displays it to you\\n- Now just select a space to switch to\\n\\nIn order to make this work you have to open `System Preferences -> Keyboard -> Shortcuts -> Mission Control` and bind all `Switch to Desktop [NUMBER]` actions to <kbd>ctrl + alt + cmd + shift + [NUMBER]</kbd>. There are actions up to the 9th desktop, but they may not be shown to you if you have less then 9 desktops currently open.\\n\\n**Note**: There\\'s no API available for retrieving windows from other spaces, therefor other spaces\\' guessed names won\\'t be refreshed until you visit them. The very first time you load Phoenix you might want to switch the focus to all of them, one by one, to have their names updated.\\n\\n## License\\n\\nMIT © Fabio Spampinato\\n'},\n",
       " {'repo': 'leftfieldlabs/Space-Sketchr',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# Space Sketchr\\n\\n### Releasable build v1.0\\n####\\n#####\\n\\n\\nThis code is able to run / export the latest version of Tango Doodle using Unity 4.\\n\\n\\n\\n### Overview\\n\\nAs with any Tango project, the first step is to import the Tango assets into your project.\\n\\nDownload from here: https://developers.google.com/project-tango/downloads\\n\\nGo to `Assets > Import Package > Custom package` and select the package file. Leave all assets selected and import them. You should now have a `TangoSDK` folder in your project.\\n\\n(NOTE: At this point, unity might be reporting errors in the tango source code. If so, build the project once `File > Build` and they should go away)\\n\\nAs with all Unity / Tango apps, you\\'ll need to add the prefab at `TangoSDK/Core/Prefabs/Tango Manager` to your scene, and take a look at it\\'s components. We only need pose data for this demo, not depth, so leave everything as it is.\\n\\nTake a look at the `DrawingRig` object in the root of the scene. All of the drawing logic happens in the `Sketchpad.cs` script, which is attached to a particle system. The \"drawing\" is actually a regular old ParticleSystem. The only difference is that Looping, Play on awake, Emission, and Shape are all disabled (Leaving Renderer on).\\n\\nNote that `DrawingRig` has a `TangoSketchController` script on it. This is a copy of `TangoSDK/Examples/Scripts/Controllers/SampleController.cs`, with a bunch of `OnGUI` debug info disabled. This script is what takes the latest pose (position + rotation) data from the device and applies it to the game object.\\n\\nThe actual drawing logic, in `Sketchpad.cs`, is fairly standard unity code. Here\\'s how it works:\\n\\n- Listen for mouse events\\n- Project a ray onto the `InvisibleCanvas` plane that is attached to `DrawingRig`\\n- Create a bunch of new points from the last point to the new point\\n- Update the ParticleSystem with the latest points\\n\\nCheck out the source of `Sketchpad.cs` for more detail.\\n'},\n",
       " {'repo': '0XDE57/SpaceProject',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '# a space project\\nWelcome to A Space Project. A project involving space...\\n> Enjoy Galactic Space Exploration in a sate-of-the-art, hyper-realistic physics simulation of the entire universe!\\n\\n> Get a realistic sense of the cosmic scale; there\\'s literally dozens of planets and traveling between them could take up to entire minutes!\\n\\n![screenshot](/Capture.PNG?raw=true)\\n\\n***This game is in pre-alpha prototype phase. More of an sandbox than a game, there\\'s not much content***\\n\\n## Features\\n* Fly around the starsystem in a spaceship\\n* Faster Than Light Travel! (yeah it\\'s real, cuz like quantum anti-dark matter n\\' stuff yo)\\n* Destructable asteroids using cutting edge - ***t r i ▲ n g l e s*** -\\n* Discover a plethora of astronomical bodies including:\\n    * Unary star systems, Binary star systems, Trinary star systems, and even ~~Quadri... quatro? quadrino-ary?~~ \\n    * as-many-as-you-want star systems!\\n    * lonely rogue planets who lost their sun :(\\n* Fight against other ship (disabled for now)\\n  * combat broken placeholder AI while I figure out the rest of the engine.\\n* Then when you\\'re bored of that you can land on a planet I guess (in theory, probably out of scope):\\n   * The planets are flat (ha, take that Round Earthers!)\\n      * Actually the planets are ***donuts*** (ha, take that Flat Earthers!)\\n* Controller Support (works in game but not menus)\\n   * hot plugging!\\n* Sound?\\n  * there is no sound in the vacuum of space silly\\n* Unit Tests?\\n   * pfft... my code is perfect. the first time. every time.\\n* Developer Tools (in progress)\\n* Cross-Platform Desktop and Mobile Support\\n  * Windows, OSX, Linux, Android, IOS\\n  * (it may be difficult to find sensible touchscren controls, focus is desktop and controller)\\n* Feature Creep and Unrealistic Scope!\\n   * Multiplayer is out of current scope :(\\n\\n\\n### Controls\\n| Control                        | Desktop       | Controller/Gamepad    | Mobile (Android, iOS)                   |\\n|------------------------------- | ------------  | ------------------    | ----------------------------------------|\\n| Movement                       | WASD          | Left Stick + L1/R1    | Left Stick                              |\\n| Aim                            | Mouse         | Left Stick            | Left Stick                              |\\n| Boost                          | Space         | A                     | todo: needs button                      |\\n| Attack: Shoot                  | Left-Click    | RT                    | bottom right button                     |\\n| Defense: Shield                | Shift         | LT                    | todo: needs button                      |\\n| Defense: Dodge (Barrel Roll)   | A/D + Space (or) Double Tap A/D | Double Tap R1/L1 | todo: swipe gesture?       |\\n| Switch Weapon                  | E             | D-Pad Right           | idk at this point, mobile might not have space for controls\\n| Toggle HyperDrive              | Hold 1        | Hold B                | todo: needs button                      |\\n| Land/Take Off                  | T             | D-Pad Down            | top center when over planet             |\\n| Enter/Exit vehicle             | G             | Y                     | bottom right small button when in/near vehicle |\\n| Zoom                           | Scroll Wheel  | Right JoyStick        | todo: Pinch Zoom                        |\\n| Reset Zoom                     | Middle-Click  | Click in Right stick  | todo: double tap                        |\\n| Toggle Map State               | M             |                       | top left corner button                  |\\n| Toggle HUD                     | H             |                       |                                         |\\n| Full screen                    | F11           |                       |                                         |\\n| Menu (Pause)                   | Escape        | Start                 | top right corner button                 |\\n| Vsync                          | F8            |                       |                                         |\\n| ECS Debug Viewer               | F9            |                       |                                         |\\n| Misc debug keys I am too lazy to document rn and won\\'t be permanent anyway |                                     |\\n\\n\\n## Current Status\\nA work in progress engine toy sandbox thing: scaffolding for what will hopefully one day be a game.\\nCurrently focused on figuring out some rendering issues with how shaders work.\\nThe code is bit rough in some places, littered with todo\\'s, half-baked features, and of course the occasional bug.\\n  \\n\\n\\n\\n## License\\n   Apache 2.0: see [LICENSE.md](/LICENSE.md)\\n   Credit appreciated.\\n\\n## Libraries\\n- [libGDX](https://github.com/libgdx/libgdx)\\n- [Ashley](https://github.com/libgdx/ashley/wiki)\\n- [OpenSimplexNoise](https://gist.github.com/KdotJPG/b1270127455a94ac5d19)\\n- [VisUI](https://github.com/kotcrab/vis-ui)\\n\\n\\n## Building\\n**General**\\n* Set up your Development Environment: https://libgdx.com/wiki/start/setup\\n* Make sure Android SDK is installed.\\n* Import project in IDE of choice using gradle.\\n* If a \"File not found\" error occurs, check the working directory. Append \"\\\\assets\" to the working directory in run configurations.\\n\\n\\n**Android Studio**\\n* Desktop\\n  * create Run Configuration\\n  * main class = com.spaceproject.desktop.DesktopLauncher\\n  * use classpath of module \\'desktop\\'\\n  * working directory = ...\\\\SpaceProject\\\\assets\\n      * (must ensure working directory includes assets so data like fonts, particles, shaders, configs can be loaded)\\n  * build and run!\\n* Android\\n  * enable dev options, enable usb debugging\\n  * connect phone, android studio should detect it\\n  * build and run!\\n* IOS\\n  * https://libgdx.com/dev/import_and_running/#ios\\n\\n'},\n",
       " {'repo': 'TominoCZ/Sound-Space-Map-Editor',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# Sound-Space-Editor\\nA level editor for Sound Space: https://www.roblox.com/games/2677609345/Sound-Space\\n'},\n",
       " {'repo': 'MechanicPluto24/Terraria_Macrocosm',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# Terraria_Macrocosm\\nSpace\\n'},\n",
       " {'repo': 'TakeshiCho/UI_RenderPipelineInLinearSpace',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# UI Render Pipeline In Linear Space ( Developing... )\\nThis project is an Unity render pipeline and shader framework for UI which is based on *Unity URP*.\\nWe created it for the purpose of fixing the alpha value of the UI images ( opacity of images )  which is wrong in *linear color space*, \\nthat keep the sRGB workflow for the UI designers in Unity.\\n\\n#### 线性空间 UI 渲染管线 ( 开发中... )\\n\\n这个项目是一套基于 Unity URP 的 UI 渲染管线和 Shader 框架，\\n为了修复线性色彩空间中的的 UI 切图的透明度错误，\\n能够让 UI 设计师在 Unity 中保持原有的 sRBG 工作流。\\n\\n#### リニア UI レンダーパイプライン ( 開発中... )\\n\\nこのプロジェクトは Unity URP に基づいて作った UI レンダーパイプラインとシェーダーフレームワークです、\\nUnity リニア の色空間にある UI 画像の透明さをなおす為に作れたものです。\\nならば、UI デザイナーはいつもように sRGB の業務フローをすることができます。\\n\\n# Versions and Schecdule\\n\\n### Unity:\\n* Unity: 2020.3.21 f1c1  \\n* Unity UI: com.unity.render-pipelines.universal@10.6.0  \\n* Universal RP: com.unity.ugui@1.0.0  \\n\\n### Feature:\\n* In our Render Pipeline,the UI images have the same opacity with images which are in the PhotoShop.\\n\\n![Opacity_Comparison](./Readme/Opacity_Comparison.png)\\n\\n* UI Render with Individual resolution.\\n\\n![RenderScaleComparison](./Readme/RenderScaleComparison.png)  \\n  \\n\\n### Update:\\n* 11 / 05 / 2021:  \\n  Supported correct UI alpha gamma in case of Post-Processing;  \\n  Fixed the wrong effects of baked Reflection Probe;  \\n  Fixed the Gamma in Scene view;  \\n  \\n  \\n* 11 / 06 / 2021:   \\n  Supported correct UI alpha gamma in case of FXAA;  \\n \\n  \\n* 11 / 12 / 2021:  \\n  Fixed the Color Depth resolution of 3D render.  \\n  \\n  \\n* 11 / 19 / 2021:  \\n  Fixed the the wrong effects of Reflection Probe in scene view.  \\n  Fixed the the wrong effects of Transparent Objects in scene view.\\n  \\n* 11 / 28 / 2021:  \\n  Supported UI Render with Individual resolution.\\n\\n* 03 / 24 / 2022:  \\n  Changed the UI Render Target.\\n\\n### Plan:\\n* To Develop Camera Managing Script. \\n\\n# Pipeline Flowchart\\n![UI_RenderPipeline](./Readme/UI_RenderPipeline.png)\\n\\n## Why using *RGBA32 UNorm* for the UI Buffer\\nWhen the final 3D render image is blit into the UI buffer, and transform to the Gamma Space, \\nwe can compare the resolutions of Color Depth in different graphics format of the UI Buffer.\\nEvidently the RGBA32 UNorm has the more details.  \\n\\n![UI_RenderPipeline](./Readme/ColorDepthComparison.png)\\n\\n'},\n",
       " {'repo': 'ZFTurbo/Kaggle-Planet-Understanding-the-Amazon-from-Space',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': 'Repository contains code to solve Kaggle problem [Planet: Understanding the Amazon from Space](https://www.kaggle.com/c/planet-understanding-the-amazon-from-space). This solution won 3rd place in competition.\\n\\n# Requirements:\\n\\nPython >= 3.4, Keras 1.2.1, Theano 0.9.2, Tensorflow, XGBoost 0.6\\n\\n# How to run:\\n\\nYou need to execute set of scripts one by one:\\n* python a11_find_neighbours.py\\n* python a30_create_keras_models.py\\n* python a30_create_keras_models_land.py\\n* python a30_create_keras_models_weather.py\\n* python a30_create_keras_models_single_class.py\\n* python a31_create_cnn_features_basic.py\\n* python a31_create_cnn_features_land.py\\n* python a31_create_cnn_features_weather.py\\n* python a32_create_cnn_features_single_class.py\\n* python a32_find_neighbours_features.py\\n* python a42_gbm_blender.py\\n* python a42_keras_blender.py\\n* python a50_ensemble_from_cache_v1.py\\n\\n# Notes:\\n1. Recreating all CNN models from scratch on single GPU will require a lot of time (around a month). It can be parallelized using separate GPU on different CNN models. Final models weights size ~50 GB. Msg me if you need these weights.\\n2. Creating neighbours features requires around a day to complete.\\n3. Due to high parallelization, CNN models trained on GPU can slightly differ even in case it was trained on the same code.\\n4. A little bit details about solution available on [Kaggle forum](https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/discussion/38831)\\n\\n# Directory structure:\\n* -- input - input data as it was given on Kaggle\\n* -- Kaggle-Planet-Understanding-the-Amazon-from-Space - all the Python code (this repo)\\n* -- models - all generated models from neural nets will be in this folder.\\n* -- weights - files with weights for pretrained models. Link: [Download](https://mega.nz/#!zMZmCQ7T!ENqFmKZGXKoX6nXRWN7xGRpkUKZpWTwD8rJ_dXA0Sro)\\n* -- modified_data - some intermediate files for neighbour analysis\\n* -- features - all raw features generated by neural nets will be stored in this folder. We already have them calculated. Link: [Download](\\nhttps://mega.nz/#!nYxwRYjY!wyEEKB0jr3D9bzO_sB8MWOASvy7GDtqDZSiudrTUp98\\n)\\n* -- cache - this folder will contain arrays with predictions from XGBoost and Keras blenders\\n* -- subm - final predictions (in format of submit file for Kaggle)\\n\\n# Dataflow\\n\\n![Dataflow](https://raw.githubusercontent.com/ZFTurbo/Kaggle-Planet-Understanding-the-Amazon-from-Space/master/images/Dataflow.png)\\n\\n'},\n",
       " {'repo': 'Spacebrew/spacebrewUnity',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': 'spacebrewUnity\\n==============\\n\\nA Spacebrew client library for Unity3D. \\n\\n## Usage\\n\\nThis is a Unity Package which can be imported into your Unity project. Use the Unity Package Manager to import this git repo into your project ([Detailed instructions](#import-details))\\n\\n__SpacebrewClient__: This is where you configure your Spacebrew client to point to your desired server and setup your publishers and subscribers.\\n\\n![Alt text](/screenshots/Capture.PNG \"Spacebrew Client\")\\n\\n__SpacebrewEvents__: This is what you attach to your game object to have it send and receive Spacebrew events.\\n\\n\\n\\tSpacebrewClient sbClient;\\n\\n\\t// Use this for initialization\\n\\tvoid Start () {\\n\\t\\tGameObject go = GameObject.Find (\"SpacebrewObject\"); // the name of your client object\\n\\t\\tsbClient = go.GetComponent <SpacebrewClient> ();\\n\\n\\t\\t// register an event with the client and a callback function here.\\n\\t\\t// COMMON GOTCHA: THIS MUST MATCH THE NAME VALUE YOU TYPED IN THE EDITOR!!\\n\\t\\tsbClient.addEventListener (this.gameObject, \"mystring\");\\n\\t}\\n\\n\\t// Update is called once per frame\\n\\tvoid Update () {\\n\\t\\tif (Input.GetKeyDown (\"space\")) {\\n\\t\\t\\tprint (\"Sending Spacebrew Message\");\\n\\t\\t\\t// name, type, value\\n\\t\\t\\t// COMMON GOTCHA: THIS MUST MATCH THE NAME VALUE YOU TYPED IN THE EDITOR!!\\n\\t\\t\\tsbClient.sendMessage(\"mybool\", \"boolean\", \"true\");\\n\\t\\t}\\n\\t}\\n\\n\\tpublic void OnSpacebrewEvent(SpacebrewClient.SpacebrewMessage _msg) {\\n\\t\\tprint (\"Received Spacebrew Message\");\\n\\t\\tprint (_msg.name);\\n\\t\\tprint (_msg.value);\\n\\t}\\n\\n\\n### Import Details\\n\\nOnce you have your Unity project open:\\n\\n* go to _Window -> Package Manager_ \\n* click on the __+__ in the upper left corner of the Package Manager window \\n* select _Add package from git URL_\\n* Enter `https://github.com/spacebrew/spacebrewUnity.git` into the  prompt and click _Add_'},\n",
       " {'repo': 'luciotorre/spacecraft',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': 'SpaceCraft\\n==========\\n\\nAn AI space fighting playground.\\n\\nYou can use fabric to do pretty much anything there is to do currently:\\n\\nTo run the tests\\n----------------\\n\\n    $ fab test\\n\\nYou can also generate a pretty html test coverage report using\\n\\n    $ fab coverage\\n\\nTo view the report then check htmlcov/index.html\\n\\nTo start a server\\n-----------------\\n\\n    $ fab run_server\\n\\nExtra arguments can be passed into this command in two different (ugly) ways:\\n\\n    $ fab run_server:--xsize,1234\\n\\nor:\\n\\n    $ fab \"run_server:--xsize 1234\"\\n\\n\\nTo start a client\\n-----------------\\n\\n    $ fab run_client\\n\\nYou can start more than one client at a time\\n\\n\\nTo start a monitor\\n------------------\\n\\n    $ fab run_monitor\\n\\nLike with clients, you can start more than one monitor at a time.\\n\\n\\nTODO\\n----\\n\\n- everything\\n- wraparound\\n- bullets\\n- energy\\n- game logic\\n\\n'},\n",
       " {'repo': 'getspacetime/spacetime',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '[![Build](https://github.com/getspacetime/spacetime/actions/workflows/build.yaml/badge.svg)](https://github.com/getspacetime/spacetime/actions/workflows/build.yaml)\\n<sup>[Discord](https://discord.gg/PeBbYy6WKq)</sup>\\n\\n<div align=\"center\">\\n  <a href=\"https://github.com/getspacetime/spacetime\">\\n    <img src=\"https://user-images.githubusercontent.com/1738479/164597569-c1df8287-5d2b-434a-89fa-170e81cb6d90.png\" alt=\"Logo\" width=\"150\" height=\"150\">\\n  </a>\\n<h3 align=\"center\">spacetime</h3>\\n\\n  <p align=\"center\">\\n    A fully featured cross-platform, cross-transport API Client, wormholes excluded.\\n    <br />\\n    <a href=\"https://github.com/spacetimedotnet/spacetime\"><strong>Explore the docs »</strong></a>\\n    <br />\\n    <br />\\n    <a href=\"https://github.com/spacetimedotnet/spacetime/issues/new?assignees=&labels=&template=bug_report.md&title=\">Report Bug</a>\\n    ·\\n    <a href=\"https://github.com/spacetimedotnet/spacetime/issues/new?assignees=&labels=&template=feature_request.md&title=\">Request Feature</a>\\n  </p>\\n  \\n  <b align=\"center\">Spacetime is not production-ready and is in active development. Pre-release users are welcomed with this warning in mind. </b>\\n</div>\\n\\n<details>\\n  <summary>Table of Contents</summary>\\n  <ol>\\n    <li>\\n      <a href=\"#about-the-project\">About The Project</a>\\n      <ul>\\n        <li><a href=\"#built-with\">Built With</a></li>\\n      </ul>\\n    </li>\\n    <li>\\n      <a href=\"#getting-started\">Getting Started</a>\\n      <ul>\\n        <li><a href=\"#prerequisites\">Prerequisites</a></li>\\n        <li><a href=\"#installation\">Installation</a></li>\\n      </ul>\\n    </li>\\n    <li><a href=\"#usage\">Usage</a></li>\\n    <li>\\n      <a href=\"#contributing\">Contributing</a>\\n      <ul>\\n        <li><a href=\"#architecture\">Architecture</a></li>\\n      </ul>\\n    </li>\\n    <li><a href=\"#license\">License</a></li>\\n    <li><a href=\"#contact\">Contact</a></li>\\n    <li><a href=\"#acknowledgments\">Acknowledgments</a></li>\\n  </ol>\\n</details>\\n\\n## About The Project\\n\\n- [x] REST\\n- [x] gRPC (_we support server reflection! stop reloading protobuf files!_)\\n- [ ] Synchronization (_coming soon, for free!_)\\n- [ ] WebSockets\\n- [ ] Kafka\\n\\n![3027DB7C-25DD-49C3-B7CA-C0F1CA823276](https://user-images.githubusercontent.com/1738479/170391560-29c4e860-e1b4-4907-803d-d88610dd446c.GIF)\\n<img width=\"1595\" alt=\"rest api client\" src=\"https://user-images.githubusercontent.com/1738479/169888145-1d991141-4d78-46b9-8477-3e800c2b2e41.png\">\\n<img width=\"1601\" alt=\"grpc client\" src=\"https://user-images.githubusercontent.com/1738479/169887937-54576e10-c628-4cee-98b1-602cc97cd851.png\">\\n\\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\\n\\n### Built With\\n\\n* [.NET MAUI Blazor](https://docs.microsoft.com/en-us/aspnet/core/blazor/hybrid/tutorials/maui?view=aspnetcore-6.0)\\n* [LiteDB](https://www.litedb.org/)\\n* [TailwindCSS](https://tailwindcss.com/)\\n* [Flurl](https://flurl.dev/)\\n* [Webpack](https://github.com/webpack/webpack)\\n* [npm](https://www.npmjs.com/)\\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\\n\\n## Getting started\\n\\n### Prerequisites\\n* [Visual Studio 2022 Preview](https://visualstudio.microsoft.com/vs/preview/)\\n* [.NET MAUI Workload](https://docs.microsoft.com/en-us/dotnet/maui/get-started/installation) - currently RC1\\n* npm\\n* webpack\\n\\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\\n\\n### Running locally\\n\\nOnce all prerequisites are fulfilled, make sure to run `npm install` in the `wwwroot` folder. When you build the solution, the project will automatically run `npm run build`.\\n\\nNote: hot reloading does not yet work, but it should be fixed soon.\\n\\n### Installation\\n\\n#### Windows\\n\\nUse the [MSIX Installer](https://github.com/spacetimedotnet/spacetime/releases)\\n\\n#### Mac Source\\nClone the repository and run the `dotnet` command in the Spacetime project folder. \\n\\n```\\ndotnet build -t:Run -f net6.0-maccatalyst\\n```\\n\\n#### Mac Binary\\n\\nDownload the [zip file](https://github.com/spacetimedotnet/spacetime/releases) and extract the `.app` \\n\\n\\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\\n\\n## Contributing\\nOpen an issue or tweet me on Twitter with any suggestions or bug reports.\\n\\n### Architecture\\nThe Spacetime MAUI Project uses [Fluxor](https://github.com/mrpmorris/Fluxor) for state management. This means all actions are kicked off using the Dispatcher and handled either in an Effect or a Reducer.\\n\\n**Example:**\\n```\\n<button OnClick=\"Save\">Save</button>\\n@code {\\n    private void Save()\\n    {\\n        Dispatcher.Dispatch(new UpdateRequestAction(Request));\\n    }\\n}\\n```\\n\\nThis will fire an action, and at the very simplest level, can be handled in a reducer method. However, if this action does anything other than mutate state in a reducer, such as an API call, this will be created in an Effect. \\n\\nAll Actions, Reducers, and Effects are in the `Spacetime.Store` namespace, organized by \"feature.\"\\n\\nView the [Fluxor documentation](https://github.com/mrpmorris/Fluxor/blob/master/Tutorials/02-Blazor/02A-StateActionsReducersTutorial/README.md) for further explanation.\\n\\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\\n\\n## Contact\\nChat on [Discord](https://discord.gg/PeBbYy6WKq) or tweet me [@codemullins](https://twitter.com/codemullins)\\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\\n\\n<img width=\"393\" alt=\"MS_Startups_Celebration_Badge_Dark\" src=\"https://user-images.githubusercontent.com/1738479/169885705-9d8eb55c-0c1d-46c1-9a28-b82887ed37d3.png\">\\n'},\n",
       " {'repo': 'vinnynft/space',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': ' Passo / Passo\\n\\nPrimeiro siga os passo de instalação do Python no link a seguir...\\nhttps://www.youtube.com/watch?v=e9NNyxNCF7k&t=26s\\nEsse link se refere ao BOT para Bomb Crypto, o qual foi usado de base para esse BOT\\n\\nApos instalar o Python, baixe o arquivo .ZIP do BOT e extraia os arquivos em uma pasta!\\na dica aqui é uma pasta simples EX: no drive C crie uma pasta chamada \"space\" e jogue os arquivos extraidos lá!\\n\\nFeito isso, abra uma aba de comandos (tecla janela do windows / escreva \"cmd\")\\ncom o CMD aberto escreva os comandos a seguir sem as \"\"\\n\\n   \"cd\\\\space\"  -  nesse caso estamos entrando na pasta onde está o BOT, caso sua pasta seja diferente copie e cole o caminho no lugar do \"space\".\\n\\n   \"pip install -r requirements.txt\"   -  este comando vai instalar as bibliotecas das funções do python que usamos no BOT\\n\\nAgora está tudo pronto...\\n\\nPara iniciar o BOT basta estar com o CMD aberto na pasta do BOT (cd\\\\space) e digitar o seguinte comendo...\\n\\n   \"python space.py\"\\n\\n\\nOBRIGADO, e caso queira contribuir agradecemos...\\n\\n\\tBUSD - BNB - SPE - SPG - BCOIN\\n\\nMEtamask = 0x97993028F185A27aa59505B15a7Dd8D33B4CC1f3\\n\\n\\nDICA --- Tela do jogo bem pequena (22,5cm / 17,5 cm)  (Monitor 1920/1080)'},\n",
       " {'repo': 'yamcs/yamcs',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '# Yamcs Mission Control ![Maven Central](https://img.shields.io/maven-central/v/org.yamcs/yamcs.svg?label=release)\\n\\n* Website: https://yamcs.org\\n* Mailing list: [Google Groups](https://groups.google.com/group/yamcs/)\\n\\nYamcs is a mission control framework developed in Java. It uses an open-ended architecture that allows tailoring its feature set using yaml configuration files. You can also extend the default feature set by writing custom Java classes.\\n\\nTo start developing your own Yamcs application, follow our [Getting Started](https://yamcs.org/getting-started) guide.\\n\\n\\n## Documentation\\n\\n* Server Manual: https://docs.yamcs.org/yamcs-server-manual/\\n* Javadoc: https://docs.yamcs.org/javadoc/yamcs/latest/\\n\\n\\n## License\\n\\nYamcs is licensed under Affero GPLv3.\\n\\nFor commercial licensing please contact [Space Applications Services](https://www.spaceapplications.com) with your use case.\\n\\n\\n## Development Setup\\n\\nTo work on the core components of Yamcs you need JDK11, Maven and npm.\\n\\nBuild Java jars:\\n\\n    mvn clean install -DskipTests\\n\\nBuild web interface:\\n\\n    cd yamcs-web/src/main/webapp\\n    npm install\\n    npm run build\\n    cd -\\n\\nThese commands will produce an optimized production version of the web interface. This process will take a few minutes. For faster incremental builds run in watch mode (`npm run watch`).\\n\\nFor demo and development purposes we work with an all-in-one simulation environment that uses many Yamcs features. In this simulation, Yamcs receives TM from a simple simulator of a landing spacecraft. Yamcs can also send some basic TC. The simulator starts together with Yamcs as a subprocess.\\n\\n    ./run-example.sh simulation\\n\\nThis configuration stores data to `/storage/yamcs-data`. Ensure this folder exists and that you can write to it.\\n\\nWhen Yamcs started successfully, you can visit the built-in web interface by navigating to `http://localhost:8090`.\\n\\n**Note to Windows users:** This repository uses some relative symbolic links. To support this on Windows:\\n* Enable \"Developer Mode\" in Windows (allows to use `mklink` without administrative privileges).\\n* Enable msysgit symlink support: `git config --global core.symlinks true`\\n* If you already cloned the repository prior to these steps, `git status` will tell you how to convert the symlinks. \\n\\n\\n## Contributions\\n\\nWhile Yamcs is managed and developed by Space Applications Services, we also consider pull requests from other contributors. For non-trivial patches we ask you to sign our [CLA](https://yamcs.org/static/Yamcs_Contributor_Agreement_v2.0.pdf).\\n'},\n",
       " {'repo': 'alexa-labs/skill-sample-nodejs-space-explorer',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Skill Sample Nodejs Space Explorer\\n\\n<img src=\"https://m.media-amazon.com/images/G/01/mobile-apps/dex/alexa/alexa-skills-kit/tutorials/fact/header._TTH_.png\" />\\n\\nAlexa Presentation Language (APL) gives developers the ability to create rich, engaging and unique visual experiences for users. This skill is meant to be an example of how to use the language, the capabilities of APL, and how to think about and develop true, multi-modal experiences for Alexa-enabled devices with a screen.\\n\\n## APL 1.1 Release Updates\\n\\nWith the release of APL 1.1, Space Explorer includes updates to demonstrate new features, including:\\n\\n- Video\\n- Animations\\n- Alexa Vector Graphics (AVG)\\n- Alexa Responsive Layouts\\n\\n### Animations\\n\\nThe new animation package allows you to apply the same animation framework across all pages. This drastically reduces the amount of code written, allows parameterization of custom animations, and maintains a consistent feel across the skill. See [soft-stagger.json](packages/soft-stagger.json) for the code behind how this is achieved.\\n\\n### AVG\\n\\nBecause of the dynamic nature of APL responses, vector graphics for the atmosphere data on larger screens are served with a new graphics package. See [atmosphere-graphics.json](packages/atmosphere-graphics.json) for an example of an AVG package.\\n\\n### Alexa Responsive Layouts\\n\\nAlexa Responsive Layouts are pre-made layouts built by Amazon to work across all device categories and viewport sizes. Including these simplifies the code and conditional statements throughout the skill. It also alleviates the need to update the skill every time a new device comes online. Visit the Alexa Design Guide\\'s [Responsive Components](https://developer.amazon.com/docs/alexa-design/background.html) section for more information and usage.\\n\\n## Quicklinks\\n\\n- [Tenets](#design-tenets)\\n- [Voice Navigation](#voice-navigation-supported-utterances)\\n- [Views](#views)\\n- [Data](#data)\\n- [Documentation](#documentation)\\n\\n## Design Tenets\\n\\n### Be Voice-Forward, but not Voice-Only\\n\\n- Anything a user can touch should have a voice counterpart. However, everything said in voice does not need a touch input.\\n- What Alexa says should be relevant to what she shows and vice versa.\\n- Users will likely alternate between looking at a device and looking away throughout the experience. Be sure the voice flow and screen flow are comprehensive on their own and complimentary together. Screens should provide additional context when Alexa is speaking.\\n\\n### Honor User Modality\\n\\nIf a user speaks to Alexa, then Alexa should respond with voice. If a user touches the screen, then Alexa should not respond with voice.\\n\\n### Emphasize Patterns & Consistency\\n\\nAdherence to common patterns will reduce cognitive overhead for users.\\n\\n## Development Tenents\\n\\n### Reusability\\n\\nDon\\'t create new layouts for each new page. Reuse where possible and make components as flexible as possible.\\n\\n### Logical Structure\\n\\nDon\\'t make the code unnecessarily complex, but break things up where it makes sense for reusability.\\n\\n## Voice Navigation (_Supported Utterances_)\\n\\n### Exploring\\n\\nVisit any planet in the Solar System by saying:\\n\\n- _Alexa, **take me to** Mars._\\n- _Alexa, **go to** Venus._\\n- _Alexa, **show me** Neptune._\\n\\nThroughout the skill, you can return to your previous destination by saying:\\n\\n- _Alexa, **go back**._\\n\\n### Learning\\n\\nOnce you\\'ve arrived at a planet, you can learn more about it by saying:\\n\\n- _Alexa, number one._\\n- _Alexa, tell me about it._\\n- _Alexa, how big is it?_\\n- _Alexa, how far away is it?_\\n- _Alexa, what\\'s in its atmosphere?_\\n- _Alexa, how many moons does it have?_\\n\\n### Astronomy Pictures\\n\\nYou can view a random image from NASA\\'s Astronomy Picture of the Day archives by saying:\\n\\n- _Alexa, show me a random image._\\n- _Alexa, show me a space image._\\n\\nIf you\\'re curious about it, say:\\n\\n- _Alexa, tell me about it._\\n\\n### Learn more\\n\\nCurious about other things in our solar system? Try saying:\\n\\n- _Alexa, tell me about the Kuiper Belt._\\n- _Alexa, how much bigger is Uranus than Neptune?_\\n- _Alexa, tell me about Saturn\\'s rings._\\n- _Alexa, what happened to Pluto?_\\n\\n## Views\\n\\nSpace Explorer uses a number of page templates to create engaging experiences for users. The goal is to reuse and be flexible, rather than create new layouts for each viewport class.\\n\\nA single page template exists for each primary view within the skill. Layouts were authored with Large Landscape Hubs as the starting point, then responsive breakpoints were added for larger and smaller devices (Extra Large TV, Medium Landscape Hub, Small Round Hub). When the layout for a particular device varied significantly, an alternative device-specific template was authored, such as the Solar System View on a Small Round Hub.\\n\\n### Solar System View\\n\\n[Solar System View](docs/solar_system_view.md)\\n\\n### Horizontal Image List View\\n\\n[Horizontal Image List View](docs/horizontal_image_list_view.md)\\n\\n### Planet Details\\n\\n[Planet Details View](docs/planet_details_view.md)\\n\\n### Transcript View\\n\\n[Transcript View](docs/transcript_view.md)\\n\\n### Size\\n\\n[Size View](docs/size_view.md)\\n\\n### Distance\\n\\n[Distance View](docs/distance_view.md)\\n\\n### Atmosphere\\n\\n[Atmosphere View](docs/atmosphere_view.md)\\n\\n## Data\\n\\nThe data for this skill is self-contained for training purposes. This was done to ensure you could easily view, edit, and test our sample skill without any issues or added complexity from API keys. We recommend finding or creating an API for data when building skills for distribution in the Skills Store.\\n\\n## Documentation\\n\\n### Technical Documentation\\n\\n[https://developer.amazon.com/docs/alexa-presentation-language/apl-overview.html](https://developer.amazon.com/docs/alexa-presentation-language/apl-overview.html)\\n\\n## License\\n\\nThis library is licensed under the Amazon Software License.\\n'},\n",
       " {'repo': 'kreeben/resin',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# &#9084; Resin.Search\\n\\n[![NuGet version (Resin.Search)](https://img.shields.io/nuget/v/Resin.Search.svg?style=flat-square)](https://www.nuget.org/packages/Resin.Search/) \\n\\nOverview | [How to install](https://github.com/kreeben/resin/blob/master/INSTALL.md) | [User guide](https://github.com/kreeben/resin/blob/master/USER-GUIDE.md) \\n\\n## HTTP search engine/embedded library\\nLaunch a Resin HTTP server or use the Resin search library to search through any vector space. With hardware accelerated vector operations from \\n[MathNet](https://github.com/mathnet/mathnet-numerics) Resin is especially well suited for problem spaces that can be defined as such.\\n\\nVector spaces are configured by implementing [IModel<T>](https://github.com/kreeben/resin/blob/master/src/Sir.VectorSpace/IModel.cs). \\n\\n## Document database\\nResin stores data as document collections. It applies your prefered IModel<T> onto your data while you write and query it. \\nThe write pipeline produces a set of indices (graphs), one for each document field, that you may interact with by using the Resin web GUI, \\nthe Resin read/write JSON HTTP API, or programmatically.\\n\\n## Vector-based indices\\nResin indices are binary search trees and creates clusters of those vectors that are similar to each other, as you populate them with your data. \\nGraph nodes are created in the [Tokenize](https://github.com/kreeben/resin/blob/master/src/Sir.VectorSpace/IModel.cs#L12) method of your model. \\nWhen a node is added to the graph its cosine angle, i.e. its similarity to other nodes, determine its position (path) within the graph.\\n\\n## Customizable vector spaces\\nResin comes pre-loaded with two IModel vector space configurations: one for [text](https://github.com/kreeben/resin/blob/master/src/Sir.Search/Models/BagOfCharsModel.cs) \\nand [another](https://github.com/kreeben/resin/blob/master/src/Sir.Search/Models/LinearClassifierImageModel.cs) for [MNIST](http://yann.lecun.com/exdb/mnist/) images. \\nThe text model has been tested by validating indices generated from [Wikipedia search engine backup files](https://dumps.wikimedia.org/other/cirrussearch/current/) as well as by parsing \\n[Common Crawl](http://commoncrawl.org/) [WAT](https://commoncrawl.org/the-data/get-started/#WAT-Format), [WET](https://commoncrawl.org/the-data/get-started/#WET-Format) \\nand [WARC](https://commoncrawl.org/the-data/get-started/#WARC-Format) files, to determine at which scale Resin may operate in and at what accuracy. \\n\\nThe image model is included mostly as an example of how to implement your own prefered machine-learning algorithm for building custom-made search indices. \\nThe error rate of the image classifier is ~5%. \\n\\n## Performance\\nCurrently, Wikipedia size data sets produce indices capable of sub-second phrase searching. \\n\\n## You may also  \\n- build, validate and optimize indices using the command-line tool [Sir.Cmd](https://github.com/kreeben/resin/blob/master/src/Sir.Cmd/README.md)\\n- read efficiently by specifying which fields to return in the JSON result\\n- implement messaging formats such as XML (or any other, really) if JSON is not suitable for your use case\\n- construct queries that join between fields and even between collections, that you may post as JSON to the read endpoint or create programatically.\\n- construct any type of indexing scheme that produces any type of embeddings with virtually any dimensionality using either sparse or dense vectors.\\n\\n## Applications\\n\\n### Executables\\n\\n- __[Sir.HttpServer](https://github.com/kreeben/resin/blob/master/src/Sir.HttpServer/README.md)__: HTTP search service with HTML GUI and HTTP JSON API for reading and writing.  \\n- __[Sir.Cmd](https://github.com/kreeben/resin/blob/master/src/Sir.Cmd/README.md)__: Command line tool that executes commands that implement `Sir.ICommand`. Write, validate, optimize and more via command-line.\\n\\n### Libraries\\n\\n- __[Sir.Search](https://github.com/kreeben/resin/blob/master/src/Sir.Search/README.md)__: In-process search engine.  \\n- __Sir.Core__: Core types and shared interfaces, such as `IModel`, `ICommand` and `IVector`.\\n- __Sir.CommonCrawl__: Command for downloading and indexing Common Crawl WAT and WET files.  \\n- __Sir.Mnist__: Command for training and testing the accuracy of a index of MNIST images.  \\n- __Sir.Wikipedia__: Command for indexing Wikipedia.  \\n\\n## Roadmap\\n\\n- [x] v0.1a - bag-of-characters vector space language model\\n- [x] v0.2a - HTTP API\\n- [x] v0.3a - query language\\n- [x] v0.4 - linear classifier image model\\n- [ ] v0.5 - semantic language model\\n- [ ] v1.0 - voice model\\n- [ ] v2.0 - image-to-voice\\n- [ ] v2.1 - voice-to-text\\n- [ ] v2.2 - text-to-image\\n- [ ] v2.3 - AI\\n\\n## Backlog\\n\\n### Huge\\n- Distribute data set across many servers (sharding, replication; RPC) or in other ways allow for horisontal scaling\\n\\n### Big\\n- Memory mapping (to increase speed of querying and perhaps also writing; to increase scalability)\\n- Update index (allow removal of documents; allow appending to an already persisted index token\\'s postings list)\\n- Async IO (for scalability)\\n- Indexing of types other than string\\n- Enable combining fields with different types in a document/model\\n- Split application into \"crawler\" and \"search\"'},\n",
       " {'repo': 'GavinPHR/Space-Time-AStar',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Space-Time A* ![](https://img.shields.io/pypi/v/space-time-astar) ![](https://img.shields.io/badge/python-%3E%3D3.5-blue) ![](https://img.shields.io/github/license/GavinPHR/Space-Time-AStar) \\n\\nSpace-Time A* (STA*) Search Algorithm with an Additional Time Dimension to Deal with Dynamic Obstacles.\\n\\n\\n## Installation\\n\\nThe package is named `space-time-astar` and listed on [PyPI](https://pypi.org/project/space-time-astar/). You can use the pip to install:\\n\\n```bash\\npip3 install space-time-astar\\n```\\n\\nFor multiple agents, you might be interested in the `cbs-mapf` package which uses this package as the low-level planner, also on [GitHub](https://github.com/GavinPHR/Multi-Agent-Path-Finding) and [PyPI](https://pypi.org/project/cbs-mapf/).\\n\\n## Usage\\n\\n### Import Planner\\n\\n```python\\nfrom stastar.planner import Planner\\n```\\n\\n### Constructor Parameters\\n```python\\nPlanner(grid_size: int, robot_radius: int, static_obstacles: List[Tuple[int, int]])\\n```\\n- **grid_size**: int - each grid will be square with side length **grid_size**. \\n- **robot_radius**: int - agents are assumed to be circles with radius being **robot_radius**.\\n- **static_obstacles**: List[Tuple[int, int]] - A list of coordinates specifying the static obstacles in the map.\\n\\nIt is important that **grid_size** is not too small and **static_obstacles** does not contain too many coordinates, otherwise the performance will severely deteriorate. What is \\'too many\\' you might ask? It depends on your requirements.\\n\\n### Find Path\\nUse `Planner`\\'s `plan()` method:\\n```\\nplan(start: Tuple[int, int],\\n     goal: Tuple[int, int],\\n     dynamic_obstacles: Dict[int, Set[Tuple[int, int]]],\\n     semi_dynamic_obstacles:Dict[int, Set[Tuple[int, int]]] = dict(),\\n     max_iter:int = 500,\\n     debug:bool = False) -> np.ndarray:\\n```\\n#### Parameters:\\n- **start**: Tuple[int, int] - A start coordinate.\\n- **goal**: Tuple[int, int] - A goal coordinate.\\n- **dynamic_obstacles**: Dict[int, Set[Tuple[int, int]]] - Dynamic obstacles are really other agents reservation in space-time, more details below.\\n- **semi_dynamic_obstacles**: Dict[int, Set[Tuple[int, int]]], *optional* - This parameter exist because we have to take the agents that have reached their destinations into account, they are essentially static obstacles from specific times.\\n- **max_iter**: int, *optional* - Max iterations of the search. Default to `500`. \\n- **debug**: bool, *optional* - Prints some debug message. Default to `False`.\\n\\nThe keys for **dynamic_obstacles** are integers representing time and the values are sets of coordinates. It tells the planner what coordinates we need to avoid at each time step. Currently, it is assumed that dynamic obstacles are other agents (with the same **robot_radius**) and are treated differently to static obstacles.\\n\\n#### Return:\\nA `numpy.ndaarray` with shape `(L, 2)` with `L` being the length of the path. \\n\\n## Theoretical Background\\n\\n[Here](https://www.davidsilver.uk/wp-content/uploads/2020/03/coop-path-AIWisdom.pdf) is a good document about Space-Time A* (STA*) written by David Silver.\\n\\nIn a nutshell, STA* is normal A* plus a time dimension. See the illustration below.\\n\\n<p align=\"center\">\\n  <img width=\"500\" src=\"./fig/sta*_illustration.png\">\\n</p>\\n\\nOn the left block, the first agent plans its path and reserve its path through time (with no regard to the second agent).\\n\\nOn the right block, the second agent plans its path while avoiding the path reserved by the first agent (i.e. the **dynamic_obstacles** argument).\\n\\n### Implementation decisions in `space-time-astar` package\\n\\n- The Manhattan distance, an admissible and consistent heuristic, is used in this implementation. You can change this by changing the `h()` function in `planner.py`. \\n\\n- The agents can be bigger than the grid. \\n\\n- The dynamic obstacles are assumed to be other agents of the same size. This can be changed in the `safe_dynamic()` inner function in `plan()`.\\n\\n\\n## Contributing\\nPull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.\\n\\n\\n## License\\n[MIT](https://opensource.org/licenses/MIT)\\n\\n'},\n",
       " {'repo': 'yamcs/yamcs',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '# Yamcs Mission Control ![Maven Central](https://img.shields.io/maven-central/v/org.yamcs/yamcs.svg?label=release)\\n\\n* Website: https://yamcs.org\\n* Mailing list: [Google Groups](https://groups.google.com/group/yamcs/)\\n\\nYamcs is a mission control framework developed in Java. It uses an open-ended architecture that allows tailoring its feature set using yaml configuration files. You can also extend the default feature set by writing custom Java classes.\\n\\nTo start developing your own Yamcs application, follow our [Getting Started](https://yamcs.org/getting-started) guide.\\n\\n\\n## Documentation\\n\\n* Server Manual: https://docs.yamcs.org/yamcs-server-manual/\\n* Javadoc: https://docs.yamcs.org/javadoc/yamcs/latest/\\n\\n\\n## License\\n\\nYamcs is licensed under Affero GPLv3.\\n\\nFor commercial licensing please contact [Space Applications Services](https://www.spaceapplications.com) with your use case.\\n\\n\\n## Development Setup\\n\\nTo work on the core components of Yamcs you need JDK11, Maven and npm.\\n\\nBuild Java jars:\\n\\n    mvn clean install -DskipTests\\n\\nBuild web interface:\\n\\n    cd yamcs-web/src/main/webapp\\n    npm install\\n    npm run build\\n    cd -\\n\\nThese commands will produce an optimized production version of the web interface. This process will take a few minutes. For faster incremental builds run in watch mode (`npm run watch`).\\n\\nFor demo and development purposes we work with an all-in-one simulation environment that uses many Yamcs features. In this simulation, Yamcs receives TM from a simple simulator of a landing spacecraft. Yamcs can also send some basic TC. The simulator starts together with Yamcs as a subprocess.\\n\\n    ./run-example.sh simulation\\n\\nThis configuration stores data to `/storage/yamcs-data`. Ensure this folder exists and that you can write to it.\\n\\nWhen Yamcs started successfully, you can visit the built-in web interface by navigating to `http://localhost:8090`.\\n\\n**Note to Windows users:** This repository uses some relative symbolic links. To support this on Windows:\\n* Enable \"Developer Mode\" in Windows (allows to use `mklink` without administrative privileges).\\n* Enable msysgit symlink support: `git config --global core.symlinks true`\\n* If you already cloned the repository prior to these steps, `git status` will tell you how to convert the symlinks. \\n\\n\\n## Contributions\\n\\nWhile Yamcs is managed and developed by Space Applications Services, we also consider pull requests from other contributors. For non-trivial patches we ask you to sign our [CLA](https://yamcs.org/static/Yamcs_Contributor_Agreement_v2.0.pdf).\\n'},\n",
       " {'repo': 'ConsumerDataStandardsAustralia/standards',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Consumer Data Right Standards\\n\\nThis repository contains the binding API Standards and Information Security profile created in response to the [Consumer Data Right](https://treasury.gov.au/consumer-data-right \"Treasury\\'s Consumer Data Right webpage\") legislation and the subsequent regulatory rules.  The purpose of the Consumer Data Right regime is to give Australians greater control over their data and is intended to apply sector by sector across the whole Australian economy.\\n\\nThese standards are maintained by the Data Standards Body (DSB), with the Data Standards Chair as the decision maker.  The Data Standards Body is part of the [Treasury](https://www.directory.gov.au/portfolios/treasury/data-standards-body \"Data Standards Body\"). The work of standards development is conducted in consultation with the [Australian Competition and Consumer Commission (ACCC)](https://www.accc.gov.au/focus-areas/consumer-data-right-cdr-0 \"ACCC\\'s CDR webpage\") as co-regulator of the Consumer Data Right, along with the [Office of the Australian Information Commissioner (OAIC)](https://www.oaic.gov.au/consumer-data-right/about-the-consumer-data-right/ \"OAIC CDR webpage\").\\n\\n## Additional Information\\n\\n* [Data Standards Body Web Site](https://consumerdatastandards.gov.au/) - Contains additional information on the CDR and the DSB as well as notifications of the latest developments in the regime.\\n* [Formal Standards Site](https://consumerdatastandardsaustralia.github.io/standards/) - The published contents of the standards in this repository.  This is the formal documentation of the binding standards.\\n* [CDR Register Design Documentation](https://consumerdatastandardsaustralia.github.io/standards) - The documentation for the CDR Register is incorporated into the Consumer Data Standards documentation.\\n\\n## Contributing To The Standards\\n\\nConsultation on the standards as they evolve is performed transparently with any interested contributor invited to participate in accordance with the rules of engagement described below.\\n\\nThere are a number of ways to contribute to these standards:\\n\\n* **[Issues posted on this repository](https://github.com/ConsumerDataStandardsAustralia/standards/issues)** - The issues posted on this repository are used for formal consultation of specific decision proposals that are to be considered by the Data Standards Chair.  These decisions constitute significant changes to the standards and are raised as required.  These issues constitute an audit trail of the decisions taken by the Data Standards Chair.  As this is the case it is requested that contributors do not raise new issues and instead contribute comments to the issues created by the DSB.\\n\\n* **[The standards maintenance repository](https://github.com/ConsumerDataStandardsAustralia/standards-maintenance)** - This repository is used to manage minor changes to established aspects of the standards.  Contributors are encouraged to raise new issues in this repository that will then be prioritised for resolution in a series of multi-week iterations.  Outcomes of these iterations will then be submitted to the Data Standards Chair for approval on this repository as a decision.  The [Consumer Data Right Support Portal](https://cdr-support.zendesk.com/hc/en-us) is the preferred way to search for answers, raise questions and request clarification of the standards.\\n\\n* **[Via Email](mailto:contact@consumerdatastandards.gov.au)** - If you would like to submit a confidential question or item of feedback, or if GitHub feels unfamiliar, the DSB welcome email correspondence related to the standards and the standards development process.\\n\\n\\n## Rules of engagement for this repository\\n\\nWe\\'re committed to undertaking conversations relating to the technical standards in the open. Questions or comments that participants might ask us via email or private message are likely to be questions or comments other participants have as well. Our answers will be of interest to everyone. There are likely to be experiences and lessons everybody working in this ecosystem can learn from. Having these conversations transparently helps us reduce duplication, resolve issues faster and keep everyone up to date with the conversation.\\n\\nWe ask that all contributors to the Consumer Data Standards repositories comply with the [GitHub Community Forum Code of Conduct](https://help.github.com/articles/github-community-forum-code-of-conduct/).\\n\\nIn addition, it would be appreciated if the following rules are adhered to when commenting or contributing:\\n* Please provide a single, considered response to each proposal covering all feedback concerning the proposal.\\n* For transparency, if you work at or are associated with an organisation with an interest in the standards, please indicate this in your response.\\n* Please ensure you are aware of and compliant with any social media guidelines or internal processes for response set by your organisation before providing feedback.\\n* Please refrain from initiating new issues or pull requests in this repository due to the need for formal approval of all aspects of the standards\\n'},\n",
       " {'repo': 'hasit/ARSpaceShooter',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': \"# ARSpaceShooter\\nUnity Space Shooter game in Augmented Reality using Vuforia\\n\\n## Requirements\\n[Unity](http://unity3d.com) Version 4.6.2f1\\n\\n[Vuforia](https://developer.vuforia.com) Unity Extension Version 4.0.103\\n\\n## Setting up the tools\\n1. Download this repository\\n2. Download the Unity Developement Environment\\n3. Download the Vuforia Unity Extension\\n4. Start Unity -> Go to File ->  Open Project -> Select this folder\\n5. Select (double click) '3DMain' from 'Scenes' in the Project Window.\\n6. Press the Run button to run the project inside Unity\\n7. Go to File -> Build Settings -> Build to build an APK for Android\\n\\nWe have also provided an already up-to-date APK in the Android APK folder if you want to just try out the game.\\n\"},\n",
       " {'repo': 'apoclyps/my-dev-space',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': 'Muxer\\n=====\\n\\n[![Build Status](https://travis-ci.com/apoclyps/my-dev-space.svg?token=putHnyd9Fyt2bwsGacCD&branch=production)](https://travis-ci.com/apoclyps/my-dev-space?token=putHnyd9Fyt2bwsGacCD&branch=production)[![GitHub license](https://img.shields.io/github/license/Naereen/StrapDown.js.svg)](https://github.com/Naereen/StrapDown.js/blob/master/LICENSE)[![All Contributors](https://img.shields.io/badge/all_contributors-10-orange.svg?style=flat-square)](#contributors)[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-green.svg)](http://makeapullrequest.com) ![Code of Conduct](https://img.shields.io/badge/%E2%88%9A-Code%20of%20Conduct-blue.svg)\\n\\n> A open source developer community to promote local hackathons, conferences, and meetups, mentoring, calls for speakers, and collaboration.\\n\\nGetting Started\\n---------------\\n\\nThese instructions will get you a copy of the project up and running on your local machine for development and testing purposes. See deployment for notes on how to deploy the project on a live system.\\n\\n### Prerequisites\\n\\nWhat things you need to install the software (latest versions) and how to install them\\n\\n-\\t[Docker](https://docs.docker.com/install/) - Used to build, ship, and run all services\\n-\\t[Python](https://www.python.org/) - Dependency Management for backend services & scripts\\n- [Pipenv](https://pipenv.readthedocs.io/en/latest/) - A tool that aims to bring the best of all packaging worlds\\n-\\t[Node](https://nodejs.org/en/) - Used to create a UI via React, manage frontend dependencies\\n\\n### Setting up a local development environment\\n\\nThe following steps are required for first time setup. These steps will check out the repository, install the project dependencies, and the client dependencies needed for the React frontend and run the service on `http://localhost`. *Note* `http://localhost` will show a gateway error until the client has completed building (typically 1 minute for first time setup).\\n\\n```bash\\ngit clone https://github.com/apoclyps/my-dev-space\\ncd my-dev-space\\nnpm install\\ncd services/client\\nnpm install\\ncd ../../\\ndocker-compose -f docker-compose-dev.yml up -d\\n```\\n\\nOnce the service is up and running, you will need to manually create the required tables in the database and install the optional seed data to complete the local Postgres setup.\\n\\n```bash\\ndocker-compose -f docker-compose-dev.yml run users-service python manage.py recreate_db\\ndocker-compose -f docker-compose-dev.yml run users-service python manage.py seed_db\\ndocker-compose -f docker-compose-dev.yml run events-service python manage.py recreate_db\\n```\\n\\nOn subsequent runs (when the above steps have been completed), you can apply new database migrations to your local service by running:\\n\\n```bash\\ndocker-compose -f docker-compose-dev.yml run users-service python manage.py db upgrade\\ndocker-compose -f docker-compose-dev.yml run events-service python manage.py db upgrade\\n```\\n\\nAlternatively, if you make a change to a model during development, you will need to create and commit a migration file for that service. As a best practice, migration files should be committed independently to code:\\n\\n```bash\\ndocker-compose -f docker-compose-dev.yml run users-service python manage.py db migrate\\ndocker-compose -f docker-compose-dev.yml run events-service python manage.py db migrate\\n```\\n\\nTo load data into the service for development, the recommended solution is to use the load script within the `scripts` folder to populate the local database. Details on how to configure the script can be found in [`scripts/README.md`](scripts/README.md)\\\\.\\n\\n```sh\\ncd scripts/\\n./load_data.sh dev\\n```\\n\\nAnd to tear down the local development stack, simply run:\\n\\n```bash\\ndocker-compose -f docker-compose-dev.yml down\\n```\\n\\nIf you wish to populate your local database with events from external services, you can use the steps outlined in the scripts [README](scripts/README.md).\\n\\nRunning the tests\\n-----------------\\n\\nThe following will run the unit tests for each respective service:\\n\\n###### `client`\\n\\n```bash\\ndocker-compose -f docker-compose-dev.yml run client-test npm test\\n```\\n\\n###### `users-service`\\n\\n```bash\\ndocker-compose -f docker-compose-dev.yml run users-service python manage.py test\\n```\\n\\n###### `events-service`\\n\\n```bash\\ndocker-compose -f docker-compose-dev.yml run events-service python manage.py test\\n```\\n\\n### Running linting\\n\\n###### `client`\\n\\n```bash\\ndocker-compose -f docker-compose-dev.yml run client npm run lint\\n```\\n\\n###### `users-service`\\n\\n```bash\\ndocker-compose -f docker-compose-dev.yml run user-service py.test --black --pep8 --flakes -vv --mccabe --cov=project --cov-report=term-missing --junitxml=test-results/results.xml\\n```\\n\\n###### `events-service`\\n\\n```bash\\ndocker-compose -f docker-compose-dev.yml run events-service py.test --black --pep8 --flakes -vv --mccabe --cov=project --cov-report=term-missing --junitxml=test-results/results.xml\\n```\\n\\n### Running code coverage\\n\\n```bash\\ndocker-compose -f docker-compose-dev.yml run users-service python manage.py cov\\n```\\n\\n### Local Postgres Connections\\n\\n```bash\\ndocker-compose -f docker-compose-dev.yml run events-db sh\\n\\npsql postgres://postgres:postgres@events-db:5432/events_dev\\n```\\n\\n### Running one off scripts\\n\\nAllows scripts/tasks to be run in environments specific docker containers.\\n\\n```\\ndocker-compose -f docker-compose-dev.yml build events-task\\ndocker-compose -f docker-compose-dev.yml run events-task python scripts/test.py\\n```\\n\\n### Debugging Python Applications\\n\\n##### End to End tests\\n\\nIntegration tests used to evaluate all services behave correctly\\n\\n```bash\\nnpm install testcafe -g\\nexport TEST_URL=\\'http://localhost\\'\\ntestcafe chrome e2e\\n```\\n\\n### Deployment\\n\\nDeployments to the staging and production environments require a PR to be opened against the staging/production branches; Upon successfully merging a PR into either branch; Travis CI will build, run, test, and deploy the changes to AWS ECS.\\n\\n### Built With\\n\\n-\\t[React](http://www.dropwizard.io/1.0.2/docs/) - Javascript client framework\\n-\\t[Flask](https://maven.apache.org/) - Python web framework\\n-\\t[Postgres](https://www.postgresql.org/) - Relational Database Management System\\n-\\t[Docker](https://rometools.github.io/rome/) - Build, run, and deploy services\\n-\\t[Swagger](https://swagger.io/) - Generate API documentation\\n-\\t[Nginx](https://www.nginx.com/) - high-performance HTTP server, reverse proxy\\n\\n### Contributors\\n\\nA list of contributors who participated in this project.\\n\\n<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->\\n<!-- prettier-ignore -->\\n| [<img src=\"https://avatars0.githubusercontent.com/u/1443700?v=4\" width=\"100px;\"/><br /><sub><b>Kyle Harrison</b></sub>](http://www.kyleharrison.co.uk)<br />[💻](https://github.com/apoclyps/my-dev-space/commits?author=apoclyps \"Code\") [📖](https://github.com/apoclyps/my-dev-space/commits?author=apoclyps \"Documentation\") | [<img src=\"https://avatars0.githubusercontent.com/u/6596210?v=4\" width=\"100px;\"/><br /><sub><b>Adam Smith</b></sub>](https://github.com/FatalEnigma)<br />[💻](https://github.com/apoclyps/my-dev-space/commits?author=FatalEnigma \"Code\") | [<img src=\"https://avatars2.githubusercontent.com/u/17544636?v=4\" width=\"100px;\"/><br /><sub><b>Ewa G </b></sub>](https://github.com/TheMicroGirl)<br />[💻](https://github.com/apoclyps/my-dev-space/commits?author=TheMicroGirl \"Code\") | [<img src=\"https://avatars2.githubusercontent.com/u/16101792?v=4\" width=\"100px;\"/><br /><sub><b>Michael Grundie</b></sub>](https://www.linkedin.com/in/michaelgrundie)<br />[💻](https://github.com/apoclyps/my-dev-space/commits?author=MichaelGrundie \"Code\") | [<img src=\"https://avatars1.githubusercontent.com/u/9554484?v=4\" width=\"100px;\"/><br /><sub><b>DermotMcAteer</b></sub>](https://github.com/DermotMcAteer)<br />[💻](https://github.com/apoclyps/my-dev-space/commits?author=DermotMcAteer \"Code\") | [<img src=\"https://avatars2.githubusercontent.com/u/28186624?v=4\" width=\"100px;\"/><br /><sub><b>kimmoylan</b></sub>](https://github.com/kimmoylan)<br />[💻](https://github.com/apoclyps/my-dev-space/commits?author=kimmoylan \"Code\") | [<img src=\"https://avatars2.githubusercontent.com/u/2376829?v=4\" width=\"100px;\"/><br /><sub><b>Peter Stevenson</b></sub>](https://github.com/GoldenCrow)<br />[💻](https://github.com/apoclyps/my-dev-space/commits?author=GoldenCrow \"Code\") |\\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\\n| [<img src=\"https://avatars2.githubusercontent.com/u/635903?v=4\" width=\"100px;\"/><br /><sub><b>Alistair Brown</b></sub>](http://alistairjcbrown.com)<br />[💻](https://github.com/apoclyps/my-dev-space/commits?author=alistairjcbrown \"Code\") | [<img src=\"https://avatars0.githubusercontent.com/u/32307798?v=4\" width=\"100px;\"/><br /><sub><b>gingerzoealex</b></sub>](https://github.com/gingerzoealex)<br />[💻](https://github.com/apoclyps/my-dev-space/commits?author=gingerzoealex \"Code\") [🎨](#design-gingerzoealex \"Design\") | [<img src=\"https://avatars3.githubusercontent.com/u/6507238?v=4\" width=\"100px;\"/><br /><sub><b>Annette McCullough</b></sub>](https://github.com/annettemccullough)<br />[💻](https://github.com/apoclyps/my-dev-space/commits?author=annettemccullough \"Code\") |\\n<!-- ALL-CONTRIBUTORS-LIST:END -->\\n\\n### License\\n\\nThis project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details\\n\\n### Contributing\\n\\nPlease read [CONTRIBUTING.md](CONTRIBUTING.md) for details on our code of conduct, and the process for submitting pull requests to us.\\n'},\n",
       " {'repo': 'oinsd/FastAPI-Learning-Example',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# [视频教学地址](https://space.bilibili.com/396891097)\\n## 中文学习教程\\n- 1、本教程每一个案例都可以独立跑，前提是安装好依赖包。\\n- 2、本教程并未按照官方教程顺序，而是按照实际使用顺序编排。\\n\\n# [Video Teaching Address](https://space.bilibili.com/396891097)\\n## FastAPI Learning Example\\n- 1.Each case in this tutorial can run independently, provided that the dependency package is installed.\\n- 2.This tutorial is not in the order of official tutorials, but in the order of actual use.\\n\\n'},\n",
       " {'repo': 'Domenicobrz/SS-refraction-through-depth-peeling-in-threejs',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# SS-refraction-through-depth-peeling-in-threejs\\nScreen space refraction through depth peeling in threejs\\n\\n[Live demo here](https://domenicobrz.github.io/webgl/projects/SSRefractionDepthPeeling/)\\n\\n<img src=\"https://github.com/Domenicobrz/SS-refraction-through-depth-peeling-in-threejs/blob/master/screenshot.jpg?raw=true\" width=\"100%\">\\n'},\n",
       " {'repo': 'anvoynov/GANLatentDiscovery',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Unsupervised Discovery of Interpretable Directions in the GAN Latent Space\\n\\nAuthors official implementation of the [_Unsupervised Discovery of Interpretable Directions in the GAN Latent Space_](https://arxiv.org/abs/2002.03754) (ICML 2020).\\n\\nThis code explores interpretable latent space directions of a pretrained GAN.\\n\\n![An image](./images/rect_icml2020_2.png)\\n_Our approach scheme: latent deformator A aims to produce shifts that are easy to distinguish for the reconstructor R_\\n\\nHere are several examples for Spectal Norm GAN (MNIST & Anime Faces), ProgGAN (CelebA-HQ) and BigGAN (ILSVRC):\\n![An image](./images/tizer.png)\\n\\n## Requirements\\npython 3.6 or later\\\\\\njupyter (for visualization)\\n>torch>=1.4\\\\\\ntorchvision\\\\\\ntqdm\\\\\\ntensorboardX\\n\\nsee `requirement.txt` for exact authors environment.\\n\\n## Training\\n\\nHere is a minimal example of latent rectification run command:\\n```\\npython run_train.py \\\\\\n    --gan_type BigGAN \\\\\\n    --gan_weights models/pretrained/generators/BigGAN/G_ema.pth \\\\\\n    --deformator ortho \\\\\\n    --out rectification_results_dir\\n```\\nthis script will save the latent space directions stored in `LatentDeformator` module weights. \\\\\\nIt also saves images charts with latent directions examples.\\n`gan_type` specifies the generator model. Take into consideration model-specific parameters for StyleGAN2 (`gan_resolution`, `w_shift`) and BigGAN (`target_class`).\\n\\nNote that you can pass as an argument any parameter of `Params` class defined in `trainer.py`\\n\\n## Evaluation\\n\\nRun `evaluation.ipynb` notebook for the discovered directions inspection.\\n\\n## Pre-trained Models\\n\\nRun `python download.py` to download all pretrained generators and latent directions.\\nWe also add `human_annotation.txt` file with annotation of some of directions.\\n\\nThe pretrained models are the unchanged copies from the following sources:\\n`100_celeb_hq_network-snapshot-010403.pth` from https://github.com/ptrblck/prog_gans_pytorch_inference\\n`G_ema.pth` from https://github.com/ajbrock/BigGAN-PyTorch and `stylegan2-ffhq-config-f.pkl` https://github.com/NVlabs/stylegan2\\nconverted with https://github.com/rosinality/stylegan2-pytorch\\n\\n## Results\\n\\nHere are some examples of generated images manipulation by moving along discovered directions:\\n\\n![An image](./images/stylegan2_kid2_eyes.gif)\\n\\n_StyleGAN2 - FFHQ - opened eyes_\\n\\n![An image](./images/bigbigan_mushroom_light.gif)\\n\\n_BigBiGAN - ImageNet - light direction_\\n\\n![An image](./images/bird_rotation.gif)\\n\\n_BigGAN - ImageNet - rotation_\\n\\n## Citation\\n\\n```\\n@inproceedings{voynov2020unsupervised,\\n  title={Unsupervised discovery of interpretable directions in the gan latent space},\\n  author={Voynov, Andrey and Babenko, Artem},\\n  booktitle={International Conference on Machine Learning},\\n  pages={9786--9796},\\n  year={2020},\\n  organization={PMLR}\\n}\\n```\\n\\n## Credits\\nBigGAN code and weights are based on the authors implementation:\\nhttps://github.com/ajbrock/BigGAN-PyTorch\\n\\nProgGAN code and weights are based on:\\nhttps://github.com/ptrblck/prog_gans_pytorch_inference\\n\\nU-net segmentation model code is based on:\\nhttps://github.com/milesial/Pytorch-UNet\\n'},\n",
       " {'repo': 'HiKaylum/SpaceX-PY',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '<div align=\"center\">\\n\\n[![Build Status](https://travis-ci.org/HiKaylum/SpaceX-PY.svg?branch=master)](https://travis-ci.org/TheDigitalTaste/digitalt-cli)\\n[![GitHub issues](https://img.shields.io/github/issues/HiKaylum/SpaceX-PY.svg)](https://github.com/HiKaylum/SpaceX-PY/issues)\\n[![GitHub license](https://img.shields.io/github/license/HiKaylum/SpaceX-PY.svg)](https://github.com/HiKaylum/SpaceX-PY/blob/master/LICENSE)\\n[![GitHub stars](https://img.shields.io/github/stars/HiKaylum/SpaceX-PY.svg)](https://github.com/HiKaylum/SpaceX-PY/stargazers)\\n\\n</div>\\n\\n# SpaceX-PY\\nPython wrapper for the [SpaceX API](https://github.com/r-spacex/SpaceX-API)\\n\\n## Install\\n```BASH\\npip install spacex-py\\n```\\n\\n## Usage\\nThis wrapper matches the [SpaceX API](https://github.com/r-spacex/SpaceX-API) so its pretty easy to use. Lets go through some examples;\\n\\n```PYTHON\\nfrom spacex_py import launches\\n\\n# Returns a tuple\\ngot_launches, header = launches.get_launches()\\n\\n# PyLint being a pain about header? use:\\ngot_launches, _ = launches.get_launches()\\n\\nprint(got_launches)\\n# Prints list: of launches\\n```\\n\\nOkay well, lets get launches using a query;\\n\\n```PYTHON\\nfrom spacex_py import launches\\n\\ngot_launches, _ = launches.get_launches(site_id=\"ksc_lc_39a\")\\n\\nprint(got_launches)\\n# Prints the launhces for the given site\\n```\\n'},\n",
       " {'repo': 'tsarikovskiy/Unity3D-DynamicallyLoadingAnimation',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': \"# Dynamically Loading Animation\\n Unity3D example project\\n\\n## This example will help you with optimization memory usage in Unity3D engine.\\nAnimationLoadManager script load and unload sprites from memeory with animation clips at runtime, that very helpful!\\n\\nThere's two simple public methods:\\n\\n- for loading animation:\\n``` csharp\\nanimationLoadManager.LoadAnimation(animationClipName, boolName);\\n```\\n\\t\\n- for unloading animation from memory when animation is done\\n\\t\\n```csharp\\nanimationLoadManager.UnloadPreviousLoadAnimation();\\n```\\n\\n###if you have any question feel free to contact me https://twitter.com/s_tsarikovskiy\\n\"},\n",
       " {'repo': 'syl20bnr/spacemacs.org',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# spacemacs.org\\n\\n[![Gitter](https://badges.gitter.im/syl20bnr/spacemacs.org.svg)](https://gitter.im/syl20bnr/spacemacs.org?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\\n\\nThis is a **read-only repository**, development happens on\\n[syl20bnr/develop.spacemacs.org][develop].\\n\\nIt is served on [spacemacs.org][].\\n\\n[develop]: https://github.com/syl20bnr/develop.spacemacs.org\\n[spacemacs.org]: http://spacemacs.org\\n'},\n",
       " {'repo': 'yilundu/DQN-DDQN-on-Space-Invaders',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# DQN-DDQN-on-Space-Invaders\\nImplementation of Double Deep Q Networks and Dueling Q Networks using Keras on Space Invaders using OpenAI Gym. Code can be easily generalized to other Atari games.\\n\\n## Prerequistes\\nYou can install all the prerequistes for code use using \\n\\n```text\\npip install -r requirements.txt\\n```\\n\\n## Instructions on Use\\nDetails about the code are covered in the blog [here](https://yilundu.github.io/2016/12/24/Deep-Q-Learning-on-Space-Invaders.html)\\n\\nTo run the code use\\n```python\\n  python main.py\\n```\\nwith arguments where arguments are given by\\n\\n```text\\nusage: main.py [-h] -n NETWORK -m MODE [-l LOAD] [-s SAVE] [-x] [-v]\\n\\nTrain and test different networks on Space Invaders\\n\\noptional arguments:\\n  -h, --help            show this help message and exit\\n  -n NETWORK, --network NETWORK\\n                        Please specify the network you wish to use, either DQN\\n                        or DDQN\\n  -m MODE, --mode MODE  Please specify the mode you wish to run, either train\\n                        or test\\n  -l LOAD, --load LOAD  Please specify the file you wish to load weights\\n                        from(for example saved.h5)\\n  -s SAVE, --save SAVE  Specify folder to render simulation of network in\\n  -x, --statistics      Specify to calculate statistics of network(such as\\n                        average score on game)\\n  -v, --view            Display the network playing a game of space-invaders.\\n                        Is overriden by the -s command\\n  ```\\n  \\n  For example, to test the pre-trained Double Deep Q Network architecture and view the network playing space invaders use\\n  \\n  ```text\\n    python main.py -n DDQN -m test -l saved.h5 -v\\n  ```\\n  \\n or to train the Dueling Q Network architecture and then save the resulting video of the network playing in the test/ directory \\n use \\n \\n   ```text\\n    python main.py -n DQN -m train -s test\\n  ```\\n \\n\\n *Note that as the model is trained, every 10000 images, the program saves the network weights in either saved.h5 of duel_saved.h5 for DDQN and DQN respectively*\\n'},\n",
       " {'repo': 'CHEREF-Mehdi/SkinDetection',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# SkinDetection\\nSkin detection using HSV &amp; YCbCr color space (Python, OpenCV)\\n\\n# About the precedure of detection\\nThe above entire procedure is applied to each and every pixel of the image. \\n\\nThe RGB image value is converted to HSV as well as YCbCr value, the HSV and YCbCr value of each pixel is compared to the standard values of a skin pixel and the decision is made whether the pixel is a skin pixel or not depending on whether\\nthe values lie in a range of predefined threshold values for each parameter.\\n\\nThe ranges for a skin pixel used in this algorithm are as follows:\\n\\n        0<=H<=17 and 15<=S<=170 and 0<=V<=255\\n\\n\\t\\t\\t\\tand\\n\\t\\t\\t\\t\\n        0<=Y<=255 and 135<=Cr<=180 and 85<=Cb<=135\\n\\t\\nPlease cite this method as follow : \\n\\nDjamila Dahmani, Mehdi Cheref, Slimane Larabi,\\nZero-sum game theory model for segmenting skin regions,\\nImage and Vision Computing,\\nVolume 99, 2020, 103925,ISSN 0262-8856,\\nhttps://doi.org/10.1016/j.imavis.2020.103925.\\n\\n# Experimentation \\n\\n#### We have tested the perfomence of this methode using images from two diffrent database :\\n\\n##### HGR (Hand Gesture Recognition) Image Database\\n\\nURL : http://sun.aei.polsl.pl/~mkawulok/gestures/\\n\\n![alt text](https://github.com/CHEREF-Mehdi/SkinDetection/blob/master/Image/ReadMeImages/ROC_HGR.png)\\n\\n##### SFA (A Human Skin Image Database based on FERET and AR Facial Images) Image Database\\n\\nURL : http://www.sel.eesc.usp.br/sfa/\\n\\n![alt text](https://github.com/CHEREF-Mehdi/SkinDetection/blob/master/Image/ReadMeImages/ROC_SFA.png)\\n\\n### In the following images you will see the skin detection results of each color space threshold, and the result of their association\\n\\n![alt text](https://github.com/CHEREF-Mehdi/SkinDetection/blob/master/Image/ReadMeImages/result1.png)\\n\\n![alt text](https://github.com/CHEREF-Mehdi/SkinDetection/blob/master/Image/ReadMeImages/result2.png)\\n\\n![alt text](https://github.com/CHEREF-Mehdi/SkinDetection/blob/master/Image/ReadMeImages/result3.png)\\n\\n### In the following images you will see the skin detection results of this methode using images from two databases SFA and HGR\\n\\n![alt text](https://github.com/CHEREF-Mehdi/SkinDetection/blob/master/Image/ReadMeImages/all_detection.png)\\n\\n\\n\\n\\n'},\n",
       " {'repo': 'Thinkmill/hyperterm-spacegray',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# `hyperterm-spacegray`\\n\\n![Spacegray Theme in Action](https://cloud.githubusercontent.com/assets/7525670/16979989/4ddde96e-4e64-11e6-8383-fd48a942ae53.png)\\n\\nA port of the Spacegray theme to hyperterm, optimized for terminal usage.\\n\\nContributions appreciated!\\n\\n## Installation\\n\\nAdd this to the plugins array in your `.hyperterm.js` and reload hyperterm to install it!\\n\\n![Spacegray Theme Installation Instructions](https://cloud.githubusercontent.com/assets/7525670/16980506/95a69974-4e66-11e6-8eaa-162f01062273.gif)\\n\\n# License\\n\\nLicensed under the MIT license.\\n'},\n",
       " {'repo': 'NikolaySuslov/livecodingspace',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# LiveCoding.space\\n\\n> Collaborative Live Coding Space with support of user-defined languages and **WebVR** ready 3D graphics\\nBased on: **Virtual World Framework (Croquet)** | **A-Frame** | **Ohm language** | **OSC.js** | **Cell.js** | **GunDB** and more...\\n\\nOnline at: **[https://livecoding.space](https://livecoding.space)**\\n\\n#### This repository is contaning the source code of the [Krestianstvo SDK 3](https://krestianstvo.org/sdk3/)   \\nIf you are looking for the next version based on Functional Reactive Programming, you can find it here: **[Krestianstvo SDK 4](https://github.com/NikolaySuslov/krestianstvo)** and its **[Playground](https://github.com/NikolaySuslov/krestianstvo-playground)**\\n\\n![logo](https://krestianstvo.org/docs/assets/webimg.jpg)\\n\\n## Updates\\n\\n- [08.2020] All core application components are now ES6 modules\\n- [10.2019] **Krestianstvo Luminary** working prototype included\\n\\n### [Changelog](CHANGELOG.md)\\n\\n## Architecture\\n\\n**[LiveCoding.space](https://livecoding.space)** architecture moves **[Virtual World Framework](https://github.com/virtual-world-framework/vwf)** architecture towards pure-decentralized application by introducing:\\n\\n- **single page web application**\\n- **client-side router** (generating **instances IDs** by client) ([about Page.js](https://visionmedia.github.io/page.js/))\\n- file storage independent **Reflector**\\n- **[Krestiasntvo Luminary](https://blog.krestianstvo.org/en/krestianstvo-luminary-for-open-croquet-architecutre-and-virtual-world-framework-in-peer-to-peer-web/)** running on Gun DB (client based replcement for **Reflector** server)\\n- **GunDB storage system** for serving ```Proxy VWF components```, ```Worlds prototypes```, ```World save states```, ```User Inventories``` ect. in fully decentralized (peer-to-peer or multi-master) DB ([about GunDB](https://gun.eco/docs/Introduction))\\n- **GunDB SEA** (Security, Encryption, Authorization) framework for user authorization and **P2P identities** ([about SEA](https://gun.eco/docs/Auth))\\n\\nBy default a tiny Reflector server is used for creating a Virtual World instances. But you could try out to connect using the new experimental totaly decentralized solution provided by [**Krestianstvo Luminary**](https://blog.krestianstvo.org/en/krestianstvo-luminary-for-open-croquet-architecutre-and-virtual-world-framework-in-peer-to-peer-web/). That will allow to create Virtual World Instances just within Web Browser by a client,using any avaliable decentralized storage GunDB on internet. **No any Reflector server needed**. In the near future it will be extended with support of securing world instances lists, clients lists, message streams with P2P identities.\\n\\nalongside with the existed features from the initial version ```v0.1```:\\n\\n- **Decentralized network model for A-Frame components** and entities based on VWF replicated computation architecture\\n- **Ohm language driver** for sharing user-defined grammars, parsers, tokenisers inside virtual space\\n- **In browser Code and Properties editor** based on Cell.js\\n- **OSC messaging** through OSC relay on the client\\n- **Avatars** (Simple and GLTF models with animation)\\n- Multi-window or multi-monitor/multi-machine setups with view **offset cameras**\\n- **WebRTC** for video/audio streaming, 3D positional audio support\\n- GearVR, Windows MixedReality motion **controllers** ..\\n\\n## [Documentation](https://krestianstvo.org/docs/sdk3)\\n\\n<img src=\"https://krestianstvo.org/img/003.jpg\" width=\"400\">\\n<br>\\n<img src=\"https://krestianstvo.org/img/004.jpg\" width=\"400\">\\n<br>\\n<img src=\"https://krestianstvo.org/img/avatar.jpg\" width=\"400\">\\n<br>\\n\\n## Contributing\\n\\nAll code is published under the MIT license\\n\\nCopyright (c) 2014-2020 Nikolai Suslov and the Krestianstvo.org project contributors.\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n\\n----\\n\\nA different license may apply to other software included in this package. Please consult their respective license files for the terms of their individual licenses.   \\n[VWF Apache 2.0 License](https://github.com/NikolaySuslov/livecodingspace/blob/master/licenses/LICENSE_VWF.md)  \\n[ADL VW Sandbox Apache 2.0 License](https://github.com/NikolaySuslov/livecodingspace/blob/master/licenses/LICENSE_ADL_Sandbox.md)\\n'},\n",
       " {'repo': 'dgriff777/a3c_continuous',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '## NEWLY ADDED A3G A NEW GPU/CPU ARCHITECTURE OF A3C FOR SUBSTANTIALLY ACCELERATED TRAINING!!\\n*Training with A3G benefits training speed most when using larger models i.e using raw pixels for observations such as training in atari environments that have raw pixels for state representation*\\n\\n# RL A3C Pytorch Continuous\\n\\n![A3C LSTM playing BipedalWalkerHardcore-v2](https://github.com/dgriff777/a3c_continuous/blob/master/demo/BPHC.gif)\\n\\nThis repository includes my implementation with reinforcement learning using Asynchronous Advantage Actor-Critic (A3C) in Pytorch an algorithm from Google Deep Mind\\'s paper \"Asynchronous Methods for Deep Reinforcement Learning.\"\\n\\n# NEWLY ADDED A3G!!\\nNew implementation of A3C that utilizes GPU for speed increase in training. Which we can call **A3G**. A3G as opposed to other versions that try to utilize GPU with A3C algorithm, with A3G each agent has its own network maintained on GPU but shared model is on CPU and agent models are quickly converted to CPU to update shared model which allows updates to be frequent and fast by utilizing Hogwild Training and make updates to shared model asynchronously and without locks. This new method greatly increase training speed and models and can be see in my [rl_a3c_pytorch][55] repo that training that use to take days to train can be trained in as fast as 10minutes for some Atari games!\\n\\n[55]: https://github.com/dgriff777/rl_a3c_pytorch\\n\\n### A3C LSTM\\n\\nThis is continuous domain version of my other a3c repo. Here I show A3C can solve BipedalWalker-v2 but also the much harder BipedalWalkerHardcore-v2 version as well. \"Solved\" meaning to train a model capable of averaging reward over 300 for 100 consecutive episodes\\n\\nAdded trained model for BipedWalkerHardcore-v2\\n\\n## Requirements\\n\\n- Python 2.7+\\n- Openai Gym\\n- Pytorch\\n- setproctitle\\n\\n## Training\\n*When training model it is important to limit number of worker processes to number of cpu cores available as too many processes (e.g. more than one process per cpu core available) will actually be detrimental in training speed and effectiveness*\\n\\nTo train agent in BipedalWalker-v2 environment with 6 different worker processes:\\n*On a MacPro 2014 laptop traing typically takes 15-20mins to get to a winning solution*\\n\\n```\\npython main.py --workers 6 --env BipedalWalker-v2 --save-max True --model MLP --stack-frames 1\\n```\\n\\nTo train agent in BipedalWalkerHardcore-v2 environment with 64 different worker processes:\\n*BipedalWalkerHardcore-v2 is much harder environment compared to normal BipedalWalker*\\n*On a 72 cpu AWS EC2 c5.18xlarge instance training with 64 worker processes takes up to 24hrs to get to model that could solve the environment. Using enhanced A3G design, training model takes only 4-6hrs*\\n\\n```\\npython main.py --workers 64 --env BipedalWalkerHardcore-v2 --save-max True --model CONV --stack-frames 4\\n```\\n\\n#A3C-GPU\\n\\nTo train agent in BipedalWalkerHardcore-v2 environment with 32 different worker processes with new A3C-GPU:\\n\\n```\\npython main.py --env BipedalWalkerHardcore-v2 --workers 32 --gpu-ids 0 1 2 3 --amsgrad True --model CONV --stack-frames 4\\n```\\n\\n\\nHit Ctrl C to end training session properly\\n\\n![A3C LSTM playing BipedalWalkerHardcore-v2](https://github.com/dgriff777/a3c_continuous/blob/master/demo/BPHC3.gif)\\n\\n## Evaluation\\nTo run a 100 episode gym evaluation with trained model\\n```\\npython gym_eval.py --env BipedalWalkerHardcore-v2 --num-episodes 100 --stack-frames 4 --model CONV --new-gym-eval True\\n```\\n\\n## Project Reference\\n\\n- https://github.com/ikostrikov/pytorch-a3c\\n- https://github.com/andrewliao11/pytorch-a3c-mujoco\\n\\n\\n## README STILL UNDER CONSTRUCTION\\n'},\n",
       " {'repo': 'cubei/SpaceCowboy',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': 'SpaceCowboy\\n===========\\n![Banner](/graphics/playstore/funktionsgrafik.png)\\n\\n[![Get it on Google Play](https://developer.android.com/images/brand/en_generic_rgb_wo_45.png)]\\n(https://play.google.com/store/apps/details?id=com.quchen.spacecowboy&hl=de)\\n\\n\\n**Screenshot**\\n![Sceenshot](/graphics/playstore/tablet_screenshot_2.png)\\n\\n\\n**Small 2D Android Game**\\nUsing things like:\\n  - SurfaceView\\n  - Bitmap\\n  - Orientationsensor\\n  - Google Play Services\\n  - bad code style\\n\\n\\nVersion 1.01\\n'},\n",
       " {'repo': 'contentful/contentful-export',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Contentful export tool\\n\\n[![npm](https://img.shields.io/npm/v/contentful-export.svg)](https://www.npmjs.com/package/contentful-export)\\n[![Build Status](https://travis-ci.org/contentful/contentful-export.svg?branch=master)](https://travis-ci.org/contentful/contentful-export)\\n[![Dependency Status](https://img.shields.io/david/contentful/contentful-export.svg)](https://david-dm.org/contentful/contentful-export)\\n[![devDependency Status](https://img.shields.io/david/dev/contentful/contentful-import.svg)](https://david-dm.org/contentful/contentful-export#info=devDependencies)\\n\\n[![semantic-release](https://img.shields.io/badge/%20%20%F0%9F%93%A6%F0%9F%9A%80-semantic--release-e10079.svg)](https://github.com/semantic-release/semantic-release) [![js-standard-style](https://img.shields.io/badge/code%20style-standard-brightgreen.svg)](http://standardjs.com/)\\n\\n[Contentful](https://www.contentful.com) provides a content infrastructure for digital teams to power content in websites, apps, and devices. Unlike a CMS, Contentful was built to integrate with the modern software stack. It offers a central hub for structured content, powerful management and delivery APIs, and a customizable web app that enable developers and content creators to ship digital products faster.\\n\\nThis is a library that helps you backup your Content Model, Content and Assets or move them to a new Contentful space. _It will support Roles & Permissions in a future version._\\n\\nTo import your exported data, please refer to the [contentful-import](https://github.com/contentful/contentful-import) repository.\\n\\n## :exclamation: Usage as CLI\\n> We moved the CLI version of this tool into our [Contentful CLI](https://github.com/contentful/contentful-cli). This allows our users to use and install only one single CLI tool to get the full Contentful experience.\\n>\\n> Please have a look at the [Contentful CLI export command documentation](https://github.com/contentful/contentful-cli/tree/master/docs/space/export) to learn more about how to use this as command line tool.\\n\\n\\n## :cloud: Pre-requisites && Installation\\n\\n### Pre-requisites\\n\\n- Node LTS\\n\\n### :cloud: Installation\\n\\n```bash\\nnpm install contentful-export\\n```\\n\\n## :hand: Usage\\n\\n```javascript\\nconst contentfulExport = require(\\'contentful-export\\')\\n\\nconst options = {\\n  spaceId: \\'<space_id>\\',\\n  managementToken: \\'<content_management_api_key>\\',\\n  ...\\n}\\n\\ncontentfulExport(options)\\n  .then((result) => {\\n    console.log(\\'Your space data:\\', result)\\n  })\\n  .catch((err) => {\\n    console.log(\\'Oh no! Some errors occurred!\\', err)\\n  })\\n```\\n\\n### Querying\\n\\nTo scope your export, you are able to pass query parameters. All search parameters of our API are supported as documented in our [API documentation](https://www.contentful.com/developers/docs/references/content-delivery-api/#/reference/search-parameters).\\n\\n```javascript\\nconst contentfulExport = require(\\'contentful-export\\')\\n\\nconst options = {\\n  spaceId: \\'<space_id>\\',\\n  managementToken: \\'<content_management_api_key>\\',\\n  queryEntries: [\\'content_type=<content_type_id>\\']\\n}\\n\\ncontentfulExport(options)\\n...\\n```\\n\\nThe Export tool also support multiple inline queries.\\n\\n```javascript\\nconst contentfulExport = require(\\'contentful-export\\')\\n\\nconst options = {\\n  spaceId: \\'<space_id>\\',\\n  managementToken: \\'<content_management_api_key>\\',\\n  queryEntries: [\\n    \\'content_type=<content_type_id>\\',\\n    \\'sys.id=<entry_id>\\'\\n  ]\\n}\\n\\ncontentfulExport(options)\\n...\\n```\\n\\n`queryAssets` uses the same syntax as `queryEntries`\\n\\n### Export an environment\\n\\n```javascript\\nconst contentfulExport = require(\\'contentful-export\\')\\n\\nconst options = {\\n  spaceId: \\'<space_id>\\',\\n  managementToken: \\'<content_management_api_key>\\',\\n  environmentId: \\'<environment_id>\\'\\n}\\n\\ncontentfulExport(options)\\n...\\n```\\n\\n## :gear: Configuration options\\n\\n### Basics\\n\\n#### `spaceId` [string] [required]\\nID of the space with source data\\n\\n#### `environmentId` [string] [default: \\'master\\']\\nID of the environment in the source space\\n\\n#### `managementToken` [string] [required]\\nContentful management API token for the space to be exported\\n\\n#### `deliveryToken` [string]\\nContentful Content Delivery API (CDA) token for the space to be exported.\\n\\nProviding `deliveryToken` will export both entries and assets from the\\nContentful Delivery API, instead of the Contentful Management API.\\nThis may be useful if you want to export the latest _published_ versions,\\nas the management API always only exports the entirety of items, with the latest\\nunpublished content. So if you want to make sure only to see the latest\\npublished changes, provide the `deliveryToken`.\\n\\nJust to clarify: When Contentful Management API always returns the latest version (e.g. 50 in this case):\\n\\n```\\n  \"createdAt\": \"2020-01-06T12:00:00.000Z\",\\n  \"updatedAt\": \"2020-04-07T11:00:00.000Z\",\\n  \"publishedVersion\": 23,\\n  \"publishedAt\": \"2020-04-05T14:00:00.000Z\",\\n  \"publishedCounter\": 1,\\n  \"version\": 50,\\n```\\n\\nthe Content Delivery API would return the `publishedVersion` (23). CDA responses don\\'t include\\nversion number.\\n\\nNote: Tags are only available on the Contentful Management API, so they will not be exported if you provide a Contenful Delivery Token. Tags is a new feature that not all users have access to.\\n\\n### Output\\n\\n#### `exportDir` [string] [default: current process working directory]\\nDefines the path for storing the export JSON file\\n\\n#### `saveFile` [boolean] [default: true]\\nSave the export as a JSON file\\n\\n#### `contentFile` [string]\\nThe filename for the exported data\\n\\n### Filtering\\n\\n#### `includeDrafts` [boolean] [default: false]\\nInclude drafts in the exported entries.\\n\\nThe `deliveryToken` option is ignored\\nwhen `includeDrafts` has been set as `true`.\\nIf you want to include drafts, there\\'s no point of getting them through the\\nContent Delivery API.\\n\\n#### `includeArchived` [boolean] [default: false]\\nInclude archived entries in the exported entries\\n\\n#### `skipContentModel` [boolean] [default: false]\\nSkip exporting content models\\n\\n#### `skipEditorInterfaces` [boolean] [default: false]\\nSkip exporting editor interfaces\\n\\n#### `skipContent` [boolean] [default: false]\\nSkip exporting assets and entries.\\n\\n#### `skipRoles` [boolean] [default: false]\\nSkip exporting roles and permissions\\n\\n#### `skipTags` [boolean] [default: false]\\nSkip exporting tags\\n\\n#### `skipWebhooks` [boolean] [default: false]\\nSkip exporting webhooks\\n\\n#### `stripTags` [boolean] [default: false]\\nUntag assets and entries\\n\\n#### `contentOnly` [boolean] [default: false]\\nOnly export entries and assets\\n\\n#### `queryEntries` [array]\\nOnly export entries that match these queries\\n\\n#### `queryAssets` [array]\\nOnly export assets that match these queries\\n\\n#### `downloadAssets` [boolean]\\nDownload actual asset files\\n\\n### Connection\\n\\n#### `host` [string] [default: \\'api.contentful.com\\']\\nThe Management API host\\n\\n#### `proxy` [string]\\nProxy configuration in HTTP auth format: `host:port` or `user:password@host:port`\\n\\n#### `rawProxy` [boolean]\\nPass proxy config to Axios instead of creating a custom httpsAgent\\n\\n#### `maxAllowedLimit` [number] [default: 1000]\\nThe number of items per page per request\\n\\n#### `limit` [number]\\nThe total number of items to return. Can be used with entries or assets. If not provided, then all entries or assets will be returned. The entries or assets will be ordered using: `sys.createdAt,sys.id`.\\n\\n#### `headers` [object]\\nAdditional headers to attach to the requests. \\n\\n### Other\\n\\n#### `errorLogFile` [string]\\nFull path to the error log file\\n\\n#### `useVerboseRenderer` [boolean] [default: false]\\nDisplay progress in new lines instead of displaying a busy spinner and the status in the same line. Useful for CI.\\n\\n## :rescue_worker_helmet: Troubleshooting\\n\\n### Proxy\\n\\nUnable to connect to Contentful through your proxy? Try to set the `rawProxy` option to `true`.\\n\\n```javascript\\ncontentfulExport({\\n  proxy: \\'https://cat:dog@example.com:1234\\',\\n  rawProxy: true,\\n  ...\\n})\\n```\\n\\n### Error: 400 - Bad Request - Response size too big.\\n\\nContentful response sizes are limited (find more info in our [technical limit docs](https://www.contentful.com/developers/docs/technical-limits/)). In order to resolve this issue, limit the amount of entities received within a single request by setting the [`maxAllowedLimit`](#maxallowedlimit-number-default-1000) option:\\n\\n```javascript\\ncontentfulExport({\\n  proxy: \\'https://cat:dog@example.com:1234\\',\\n  rawProxy: true,\\n  maxAllowedLimit: 50\\n  ...\\n})\\n```\\n\\n\\n## :card_file_box: Exported data structure\\n\\nThis is an overview of the exported data:\\n\\n```json\\n{\\n  \"contentTypes\": [],\\n  \"entries\": [],\\n  \"assets\": [],\\n  \"locales\": [],\\n  \"tags\": [],\\n  \"webhooks\": [],\\n  \"roles\": [],\\n  \"editorInterfaces\": []\\n}\\n```\\n\\n*Note:* Tags feature is not available for all users. If you do not have access to this feature, the tags array will always be empty.\\n\\n## :warning: Limitations\\n\\n- This tool currently does **not** support the export of space memberships.\\n- Exported webhooks with credentials will be exported as normal webhooks. Credentials should be added manually afterwards.\\n- If you have custom UI extensions, you need to reinstall them manually in the new space.\\n\\n## :memo: Changelog\\n\\nRead the [releases](https://github.com/contentful/contentful-export/releases) page for more information.\\n\\n## :scroll: License\\n \\nThis project is licensed under MIT license\\n\\n[1]: https://www.contentful.com\\n'},\n",
       " {'repo': 'xingyizhou/UniDet',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Simple multi-dataset detection\\nAn object detector trained on multiple large-scale datasets with a unified label space; Winning solution of ECCV 2020 Robust Vision Challenges.\\n\\n<p align=\"center\"> <img src=\\'docs/unidet_teaser.jpg\\' align=\"center\" height=\"170px\"> </p>\\n\\n> [**Simple multi-dataset detection**](http://arxiv.org/abs/2102.13086),            \\n> Xingyi Zhou, Vladlen Koltun, Philipp Kr&auml;henb&uuml;hl,        \\n> *CVPR 2022 ([arXiv 2102.13086](http://arxiv.org/abs/2102.13086))*         \\n\\nContact: [zhouxy@cs.utexas.edu](mailto:zhouxy@cs.utexas.edu). Any questions or discussions are welcomed! \\n\\n## Features at a glance\\n\\n- We trained a unified object detector on 4 large-scale detection datasets: COCO, Objects365, OpenImages, and Mapillary, with state-of-the-art performance on all of them.\\n\\n- The model predicts class labels in a **learned** unified label space.\\n\\n- The model can be directly used to test on novel datasets outside the training datasets.\\n\\n- In this repo, we also provide state-of-the-art baselines for Objects365 and OpenImages.\\n\\n## Main results\\n\\n- [RVC challenge](http://www.robustvision.net/leaderboard.php?benchmark=object)\\n\\n| COCO test-challenge | OpenImages public test | Mapillary test | Objects365 val |\\n|---------------------|------------------------|----------------|----------------|\\n| 52.9                | 60.6                   | 25.3           | 33.7           |\\n\\nResults are obtained using a Cascade-RCNN with ResNeSt200 trained in an 8x schedule.\\n\\n\\n\\n- Unified model vs. ensemble of dataset-specific models with known test domains.\\n\\n|                       |  COCO     | Objects365   |  OpenImages  |  mean. |\\n|-----------------------|-----------|--------------|--------------|--------|\\n|Unified                | 45.4      | 24.4         | 66.0         | 45.3   |\\n|Dataset-specific models| 42.5      | 24.9         | 65.7         | 44.4   |\\n\\nResults are obtained using a Cascade-RCNN with Res50 trained in an 8x schedule.\\n\\n- Zero-shot cross dataset evaluation\\n\\n|                |  VOC  | VIPER |  CityScapes  | ScanNet | WildDash | CrowdHuman | KITTI | mean |\\n|----------------|-------|-------|--------------|---------|----------|------------|-------|------|\\n|Unified         | 82.9  | 21.3  | 52.6         | 29.8    | 34.7     | 70.7       | 39.9  | 47.3 |\\n|Oracle models   | 80.3  | 31.8  | 54.6         | 44.7    | -        | 80.0       | -     | -    |\\n\\nResults are obtained using a Cascade-RCNN with Res50 trained in an 8x schedule.\\n\\nMore models can be found in our [MODEL ZOO](docs/REPRODUCE.md).\\n\\n## Installation\\n\\nOur project is developed on [detectron2](https://github.com/facebookresearch/detectron2). Please follow the official [detectron2 installation](https://github.com/facebookresearch/detectron2/blob/master/INSTALL.md).\\n\\n## Demo\\n\\nWe use the same inference API as detectorn2. To run inference on an image folder using our pretrained model, run\\n\\n~~~\\npython demo.py --config-file configs/Unified_learned_OCIM_R50_6x+2x.yaml --input images/*.jpg --opts MODEL.WEIGHTS models/Unified_learned_OCIM_R50_6x+2x.pth\\n~~~\\n\\nIf setup correctly, the output should look like:\\n<p align=\"center\"> <img src=\\'docs/example_output2.jpg\\' align=\"center\" height=\"460px\"> </p>\\n\\n*The sample image is from [WildDash](https://wilddash.cc/) dataset.\\n\\nNote that the model predicts all labels in its label hierarchy tree (for example, both `vehicle` and `car` for a car), following the protocol in OpenImages.\\n\\n## Benchmark evaluation and training\\n\\nAfter installation, follow the instructions in [DATASETS.md](docs/DATASETS.md) to setup the (many) datasets. Then check [REPRODUCE.md](docs/REPRODUCE.md) to reproduce the results in the paper.\\n\\n## License\\n\\nOur code is under [Apache 2.0 license](LICENSE).\\n\\n\\n## Citation\\n\\nIf you find this project useful for your research, please use the following BibTeX entry.\\n\\n    @inproceedings{zhou2021simple,\\n      title={Simple multi-dataset detection},\\n      author={Zhou, Xingyi and Koltun, Vladlen and Kr{\\\\\"a}henb{\\\\\"u}hl, Philipp},\\n      booktitle={CVPR},\\n      year={2022}\\n    }'},\n",
       " {'repo': 'WagnerGroup/pyqmc',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '\\n![Python package](https://github.com/WagnerGroup/pyqmc/workflows/Python%20package/badge.svg)\\n[![Documentation Status](https://readthedocs.org/projects/pyqmc/badge/?version=latest)](https://pyqmc.readthedocs.io/en/latest/?badge=latest)\\n[![Gitter chat](https://badges.gitter.im/gitterHQ/gitter.png)](https://gitter.im//pyqmc/community)\\n\\n## PyQMC\\n\\nA python module that implements real-space quantum Monte Carlo techniques. It is primarily meant to interoperate with PySCF. Documentation is available at [readthedocs](https://pyqmc.readthedocs.io/en/latest/).\\n\\n'},\n",
       " {'repo': 'bzgeb/UnityScreenSpaceMetaballs',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '# UnityScreenSpaceMetaballs\\nScreenSpace Metaballs in Unity using Scriptable Renderer Passes and standard meshes. You can read the accompanying blog posts [here](https://bronsonzgeb.com/?p=240) and [here](https://bronsonzgeb.com/?p=291).\\n\\nCredit to the [UniversalRenderingExamples](https://github.com/Unity-Technologies/UniversalRenderingExamples) for the Kawase blur shader and scriptable render pass.\\n\\n![Example](https://github.com/bzgeb/UnityScreenSpaceMetaballs/blob/main/Example.jpg)\\n'},\n",
       " {'repo': 'dask/cachey',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"Caching for Analytic Computations\\n---------------------------------\\n\\nHumans repeat stuff.  Caching helps.\\n\\nNormal caching policies like LRU aren't well suited for analytic computations\\nwhere both the cost of recomputation and the cost of storage routinely vary by\\none million or more.  Consider the following computations\\n\\n```python\\n# Want this\\nnp.std(x)        # tiny result, costly to recompute\\n\\n# Don't want this\\nnp.transpose(x)  # huge result, cheap to recompute\\n```\\n\\nCachey tries to hold on to values that have the following characteristics\\n\\n1. Expensive to recompute (in seconds)\\n2. Cheap to store (in bytes)\\n3. Frequently used\\n4. Recenty used\\n\\nIt accomplishes this by adding the following to each items score on each access\\n\\n    score += compute_time / num_bytes * (1 + eps) ** tick_time\\n\\nFor some small value of epsilon (which determines the memory halflife.) This\\nhas units of inverse bandwidth, has exponential decay of old results and\\nroughly linear amplification of repeated results.\\n\\nExample\\n-------\\n\\n```python\\n>>> from cachey import Cache\\n>>> c = Cache(1e9, 1)  # 1 GB, cut off anything with cost 1 or less\\n\\n>>> c.put('x', 'some value', cost=3)\\n>>> c.put('y', 'other value', cost=2)\\n\\n>>> c.get('x')\\n'some value'\\n```\\n\\nThis also has a `memoize` method\\n\\n```python\\n>>> memo_f = c.memoize(f)\\n```\\n\\nInstall\\n-------\\n\\nCachey is on PyPI and Conda-forge:\\n\\n``` shell\\n$ pip install cachey  # option 1\\n$ conda install cachey -c conda-forge  # option 2\\n```\\n\\nOr install from source\\n\\n``` shell\\n$ python setup.py install  # option 1\\n$ pip install -e .  # option 2 (best for development)\\n```\\n\\nStatus\\n------\\n\\nCachey is new and not robust.\\n\"},\n",
       " {'repo': 'magico13/KCT',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': 'KCT\\n===\\n\\nKerbal Construction Time - An addon for Kerbal Space Program\\n\\n####ABOUT\\n\\nKerbal Construction Time is an addon for Kerbal Space Program, a game developed by Squad, that makes rockets/planes/vessels take time to build before you can launch/fly them. The amount of time is based on the cost of all of the parts that make up the craft. This time is reduced when using parts that have been used before, or when using parts that have been recovered from other vessels (meaning there is an advantage to building reusable craft).\\n\\nCheck out the development forum for additional details and pre-built binaries: http://forum.kerbalspaceprogram.com/threads/92377-0-24-2-Kerbal-Construction-Time-Release-v1-0-2-%289-3-14%29\\n\\n####DEPENDENCIES FOR BUILDING\\n* Assembly-CSharp.dll (from KSP)\\n* UnityEngine.dll (from KSP)\\n\\nI use Visual Studio Express 2013.\\n'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REPOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "scrape_github_data() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscrape_github_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mREPOS\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: scrape_github_data() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "d = a.scrape_github_data(REPOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = BeautifulSoup(d.iloc[1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in s(['style', 'script']):\n",
    "    data.decompose()\n",
    "\n",
    "' '.join(s.stripped_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(s.findAll(text=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.json', \"r\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
